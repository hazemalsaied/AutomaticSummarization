 Citance Number: 1 | Reference Article: W03-0410.xml | Citing Article: D07-1018.xml | Citation Marker Offset: ['9'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['9'] | Citation Text: <S sid="9" ssid="9">Merlo and Stevenson, 2001 and Stevenson and Joanis, 2003 for English semantic verb classes, or Schulte im Walde, 2006 for German semantic verb classes).</S> | Reference Offset: ['8' , '12' , '14' , '13' , '19' , '22' , '23' , '26' , '28' , '27' , '36' ] | Reference Text: <S sid="8" ssid="8">Learning the argument structure properties of verbs?the semantic roles they assign and their mapping to syntactic positions?is both particularly important and difficult.</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="19" ssid="19">Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).</S> <S sid="22" ssid="22">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 10 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['168'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['168'] | Citation Text: <S sid="168" ssid="17">Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.</S> | Reference Offset: ['34' ] | Reference Text: <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 11 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['169'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['169'] | Citation Text: <S sid="169" ssid="18">In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.</S> | Reference Offset: ['23' , '27' , '30' , '34' , '36' , '124' , '125' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="124" ssid="74">3.3 Feature Extraction.</S> <S sid="125" ssid="75">All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 12 | Reference Article: W03-0410.xml | Citing Article: J06-2001.xml | Citation Marker Offset: ['397'] | Citation Marker: Stevenson and Joanis 2003 | Citation Offset: ['397'] | Citation Text: <S sid="397" ssid="325">For example, the accuracy/purity measure (Stevenson and Joanis 2003; Korhonen, Krymolowski, and Marx 2003) evaluates whether a verb is assigned to a correct cluster with respect to the gold standard class of the majority of cluster members.</S> | Reference Offset: ['12' , '13' , '23' , '26' , '28' , '27' , '34' , '36' , '128' , '167' , '199' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="167" ssid="5">regard to the target classes.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 13 | Reference Article: W03-0410.xml | Citing Article: J06-2001.xml | Citation Marker Offset: ['586'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['589'] | Citation Text: <S sid="589" ssid="38">They found that a semisupervised approach where the classifier was trained with five seed verbs from each verb class outperformed both a manual selection of features and the unsupervised 186 approach of Dash, Liu, and Yao (1997), which used an entropy measure to organize data into a multidimensional space.</S> | Reference Offset: ['12' , '13' , '23' , '26' , '28' , '27' , '30' , '36' , '40' , '47' , '54' , '52' , '51' , '57' , '61' , '62' , '124' , '128' , '167' , '181' , '182' , '199' , '209' , '12' , '13' , '23' , '26' , '28' , '27' , '30' , '34' , '40' , '41' , '47' , '44' , '54' , '52' , '51' , '57' , '55' , '60' , '61' , '62' , '124' , '128' , '182' , '199' , '209' , '12' , '14' , '23' , '26' , '28' , '27' , '32' , '30' , '36' , '40' , '41' , '47' , '44' , '54' , '53' , '52' , '51' , '57' , '55' , '65' , '60' , '61' , '62' , '124' , '128' , '167' , '182' , '199' , '209' , '12' , '13' , '23' , '26' , '28' , '27' , '30' , '34' , '36' , '40' , '47' , '44' , '54' , '52' , '51' , '57' , '55' , '61' , '62' , '124' , '128' , '167' , '182' , '199' , '209' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="124" ssid="74">3.3 Feature Extraction.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="167" ssid="5">regard to the target classes.</S> <S sid="181" ssid="19">The 13-way task includes all of our classes.</S> <S sid="182" ssid="20">The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S> <S sid="209" ssid="47">5.3 Unsupervised Feature Selection.</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="41" ssid="8">We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="124" ssid="74">3.3 Feature Extraction.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="182" ssid="20">The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S> <S sid="209" ssid="47">5.3 Unsupervised Feature Selection.</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="32" ssid="32">We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="41" ssid="8">We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="53" ssid="3">3.1 The Verb Classes.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="65" ssid="15">Run versus Sound Emission verbs.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="124" ssid="74">3.3 Feature Extraction.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="167" ssid="5">regard to the target classes.</S> <S sid="182" ssid="20">The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S> <S sid="209" ssid="47">5.3 Unsupervised Feature Selection.</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="124" ssid="74">3.3 Feature Extraction.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="167" ssid="5">regard to the target classes.</S> <S sid="182" ssid="20">The second column of Table 2 includes the accuracy of our supervised learner (the decision tree induction system, C5.0), on the same verb sets as in our clustering experiments.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S> <S sid="209" ssid="47">5.3 Unsupervised Feature Selection.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 14 | Reference Article: W03-0410.xml | Citing Article: N13-1118.xml | Citation Marker Offset: ['52'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['52'] | Citation Text: <S sid="52" ssid="8">Clustering The most widely used method for hierarchical word clustering is AGG (Schulte im Walde and Brew, 2001; Stevenson and Joanis, 2003; Ferrer, 2004; Devereux and Costello, 2005).</S> | Reference Offset: ['17' , '23' , '30' ] | Reference Text: <S sid="17" ssid="17">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 15 | Reference Article: W03-0410.xml | Citing Article: P03-1009.xml | Citation Marker Offset: ['124'] | Citation Marker: (Stevenson and Joanis, 2003) | Citation Offset: ['124'] | Citation Text: <S sid="124" ssid="67">Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003).</S> | Reference Offset: ['23' , '27' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 16 | Reference Article: W03-0410.xml | Citing Article: P03-1009.xml | Citation Marker Offset: ['143'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['143'] | Citation Text: <S sid="143" ssid="86">For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ? 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure.</S> | Reference Offset: ['12' , '13' , '23' , '26' , '28' , '27' , '36' , '40' , '47' , '44' , '54' , '52' , '51' , '57' , '55' , '65' , '60' , '61' , '62' , '128' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="65" ssid="15">Run versus Sound Emission verbs.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 17 | Reference Article: W03-0410.xml | Citing Article: P04-2007.xml | Citation Marker Offset: ['11'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['11'] | Citation Text: <S sid="11" ssid="11">For this reason, various methods for automatically classifying verbs using machine learning techniques have been attempted ((Merlo and Stevenson, 2001), (Stevenson and Joanis, 2003), (Schulte im Walde, 2003)).</S> | Reference Offset: ['10' , '12' , '14' , '19' , '22' , '23' , '26' , '28' , '27' , '30' , '40' , '41' , '47' , '44' , '54' , '53' , '52' , '51' , '57' , '55' , '65' , '60' , '61' , '62' , '128' ] | Reference Text: <S sid="10" ssid="10">Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="19" ssid="19">Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).</S> <S sid="22" ssid="22">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="41" ssid="8">We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="53" ssid="3">3.1 The Verb Classes.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="65" ssid="15">Run versus Sound Emission verbs.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 18 | Reference Article: W03-0410.xml | Citing Article: P04-2007.xml | Citation Marker Offset: ['75'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['75'] | Citation Text: <S sid="75" ssid="7">Our choice of the parameter settings is motivated by the work of (Stevenson and Joanis, 2003).</S> | Reference Offset: ['39' , '46' ] | Reference Text: <S sid="39" ssid="6">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 19 | Reference Article: W03-0410.xml | Citing Article: P04-2007.xml | Citation Marker Offset: ['125'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['125'] | Citation Text: <S sid="125" ssid="57">Nevertheless, our results are comparable to a subset of experiments reported in (Stevenson and Joanis, 2003), where they perform similar clustering experiments on English verbs based on a general description of verbs, obtaining average Adjusted Rand measures of 0.04 and 0.07.</S> | Reference Offset: ['12' , '14' , '13' , '23' , '26' , '28' , '27' , '36' , '47' , '57' , '65' , '60' , '62' , '68' , '128' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="65" ssid="15">Run versus Sound Emission verbs.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="68" ssid="18">Cheat versus Steal and Remove verbs.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 2 | Reference Article: W03-0410.xml | Citing Article: D09-1138.xml | Citation Marker Offset: ['25'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['25'] | Citation Text: <S sid="25" ssid="25">Supervised methods for automatic verb classification have been extensively investigated (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).</S> | Reference Offset: ['12' , '14' , '13' , '22' , '26' , '28' , '27' , '30' , '128' , '158' , '225' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="22" ssid="22">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="158" ssid="29">Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs.</S> <S sid="225" ssid="63">For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 20 | Reference Article: W03-0410.xml | Citing Article: P07-3016.xml | Citation Marker Offset: ['39'] | Citation Marker: (Stevenson and Joanis, 2003) | Citation Offset: ['39'] | Citation Text: <S sid="39" ssid="16">(Stevenson and Joanis, 2003) investigate the applicability of this general feature space to unsupervised verb clustering tasks.</S> | Reference Offset: ['12' , '13' , '23' , '26' , '28' , '27' , '30' , '34' , '36' , '125' , '128' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="125" ssid="75">All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 21 | Reference Article: W03-0410.xml | Citing Article: W06-2910.xml | Citation Marker Offset: ['8'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['8'] | Citation Text: <S sid="8" ssid="8">(Merlo and Stevenson, 2001; Joanis and Stevenson, 2003; Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003; Fer- rer, 2004).</S> | Reference Offset: ['10' , '12' , '13' , '23' , '26' , '28' , '27' , '34' , '36' , '128' , '167' , '199' , '10' , '12' , '14' , '22' , '26' , '28' , '27' , '128' , '199' ] | Reference Text: <S sid="10" ssid="10">Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="167" ssid="5">regard to the target classes.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S> <S sid="10" ssid="10">Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="22" ssid="22">However, a general feature space means that most features will be irrelevant to any given verb discrimination task.</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 22 | Reference Article: W03-0410.xml | Citing Article: W06-2910.xml | Citation Marker Offset: ['13'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['13'] | Citation Text: <S sid="13" ssid="13">In larger-scale classifications such as (Korhonen et al., 2003; Stevenson and Joanis, 2003; Schulte im Walde, 2003), which model verb classes with similarity at the syntax-semantics interface, it is not clear which features are the most salient.</S> | Reference Offset: ['10' , '12' , '14' , '13' , '17' , '23' , '26' , '28' , '27' , '30' , '40' , '41' , '47' , '44' , '54' , '53' , '52' , '51' , '57' , '55' , '65' , '60' , '61' , '62' , '74' , '128' , '225' ] | Reference Text: <S sid="10" ssid="10">Unsupervised or semi-supervised approaches have been successful as well, but have tended to be more restrictive, in relying on human filtering of the results (Riloff and Schmelzenbach, 1998), on the hand- selection of features (Stevenson and Merlo, 1999), or on the use of an extensive grammar (Schulte im Walde and Brew, 2002).</S> <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="17" ssid="17">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="40" ssid="7">Features over Syntactic Slots (120 features) One set of features encodes the frequency of the syntactic slots occurring with a verb (subject, direct and indirect object, and prepositional phrases (PPs) indexed by preposition), which collectively serve as rough approximations to the allowable syntactic frames for a verb.</S> <S sid="41" ssid="8">We also count fixed elements in certain slots (it and there, as in It rains or There appeared a ship), since these are part of the syntactic frame specifications for a verb.</S> <S sid="47" ssid="14">In addition to verb POS (which often indicates tense) and voice (passive/active), we also include counts of modals, auxiliaries, and adverbs, which are partial indicators of these factors.</S> <S sid="44" ssid="11">These allowable alternations in the expressions of arguments vary according to the class of a verb.</S> <S sid="54" ssid="4">Pairs or triples of verb classes from Levin were selected to form the test pairs/triples for each of a number of separate classification tasks.</S> <S sid="53" ssid="3">3.1 The Verb Classes.</S> <S sid="52" ssid="2">Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="57" ssid="7">Benefactive versus Recipient verbs.</S> <S sid="55" ssid="5">These sets exhibit different contrasts between verb classes in terms of their semantic argument assignments, allowing us to evaluate our approach under a range of conditions.</S> <S sid="65" ssid="15">Run versus Sound Emission verbs.</S> <S sid="60" ssid="10">These dative alternation verbs differ in the preposition and the semantic role of its object.</S> <S sid="61" ssid="11">1 For practical reasons, as well as for enabling us to draw more general conclusions from the results, the classes also could neither be too small nor contain mostly infrequent verbs.</S> <S sid="62" ssid="12">Admire versus Amuse verbs.</S> <S sid="74" ssid="24">Wipe versus Steal and Remove verbs.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="225" ssid="63">For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 23 | Reference Article: W03-0410.xml | Citing Article: W06-2910.xml | Citation Marker Offset: ['118'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['119'] | Citation Text: <S sid="119" ssid="22">(Stevenson and Joanis, 2003; Korhonen et al., 2003).4 Accuracy is determined in two steps: ? GermaNet: We randomly extracted 100 verb 4 Note that we can use accuracy for the evaluation because.</S> | Reference Offset: ['23' , '27' , '46' , '168' , '23' , '27' , '34' , '46' , '168' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S> <S sid="168" ssid="6">We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S> <S sid="168" ssid="6">We use , the mean of the silhouette measure from Matlab, which measures how distant a data point is from other clusters.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 24 | Reference Article: W03-0410.xml | Citing Article: E09-1072.xml | Citation Marker Offset: ['80'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['80'] | Citation Text: <S sid="80" ssid="6">Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus.</S> | Reference Offset: ['23' , '28' , '27' , '34' , '116' , '126' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="116" ssid="66">Our experimental verbs were selected as follows.</S> <S sid="126" ssid="76">Since it is a general corpus, we do not expect any strong overall domain bias in verb usage.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 3 | Reference Article: W03-0410.xml | Citing Article: D09-1138.xml | Citation Marker Offset: ['70'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['70'] | Citation Text: <S sid="70" ssid="43">The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008).</S> | Reference Offset: ['12' , '14' , '13' , '23' , '26' , '28' , '27' , '36' , '128' , '158' , '225' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="36" ssid="3">Our feature space was designed to reflect these classes by capturing properties of the semantic arguments of verbs and their mapping to syntactic positions.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="158" ssid="29">Our final measure gives an indication of the overall goodness of the clusters purely in terms of their separation of the data, without number of clusters equal to the number of verbs.</S> <S sid="225" ssid="63">For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 4 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['17'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['17'] | Citation Text: <S sid="17" ssid="17">We adopt as our baseline method a well-known hierarchical method ? agglomerative clustering (AGG) ? which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).</S> | Reference Offset: ['14' , '13' , '17' , '19' , '23' , '28' , '27' , '30' , '46' , '170' , '199' ] | Reference Text: <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="17" ssid="17">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S> <S sid="19" ssid="19">Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S> <S sid="170" ssid="8">A value of 0 suggests that a point is not clearly in a particular cluster.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 5 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['38'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['38'] | Citation Text: <S sid="38" ssid="5">We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin?s original taxonomy (Stevenson and Joanis, 2003).</S> | Reference Offset: ['23' , '28' , '27' , '34' , '181' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="181" ssid="19">The 13-way task includes all of our classes.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 6 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['40'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['40'] | Citation Text: <S sid="40" ssid="7">Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.</S> | Reference Offset: ['12' , '13' , '23' , '26' , '28' , '27' , '34' , '51' , '128' , '199' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="51" ssid="1">We use the same classes and example verbs as in the supervised experiments of Joanis and Stevenson (2003) to enable a comparison between the performance of the unsupervised and supervised methods.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="199" ssid="37">indicated by the class description given in Levin.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 7 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['54'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['54'] | Citation Text: <S sid="54" ssid="2">Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).</S> | Reference Offset: ['12' , '14' , '13' , '23' , '26' , '28' , '27' , '30' , '34' , '135' , '125' , '128' , '143' , '209' ] | Reference Text: <S sid="12" ssid="12">Such classes group together verbs that share both a common semantics (such as transfer of possession or change of state), and a set of syntactic frames for expressing the arguments of the verb (Levin, 1993; FrameNet, 2003).</S> <S sid="14" ssid="14">However, creating a verb classification is highly resource intensive, in terms of both required time and linguistic expertise.</S> <S sid="13" ssid="13">As such, they serve as a means for organizing complex knowledge about verbs in a computational lexicon (Kipper et al., 2000).</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="26" ssid="26">Thus, the problem of dimensionality reduction is a key issue to be addressed in verb class discovery.</S> <S sid="28" ssid="28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="135" ssid="6">We chose hierarchical clustering because it may be possible to find coherent subclusters of verbs even when there are not exactly good clusters, where is the number of classes.</S> <S sid="125" ssid="75">All features were estimated from counts over the British National Corpus (BNC), a 100M word corpus of text samples of recent British English ranging over a wide spectrum of domains.</S> <S sid="128" ssid="78">Indirect objects are identified by a less sophisticated (and even noisier) method, simply assuming that two consecutive NPs after the verb constitute a double object frame.</S> <S sid="143" ssid="14">Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed.</S> <S sid="209" ssid="47">5.3 Unsupervised Feature Selection.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 8 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['82'] | Citation Marker: Stevenson and Joanis, 2003 | Citation Offset: ['83'] | Citation Text: <S sid="83" ssid="31">In addition, a significant amount of information is lost in pairwise clustering.</S> | Reference Offset: ['7' , '8' , '23' , '30' , '34' , '46' , '7' , '8' , '23' , '30' , '34' , '46' ] | Reference Text: <S sid="7" ssid="7">Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies.</S> <S sid="8" ssid="8">Learning the argument structure properties of verbs?the semantic roles they assign and their mapping to syntactic positions?is both particularly important and difficult.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S> <S sid="7" ssid="7">Computational linguists face a lexical acquisition bottleneck, as vast amounts of knowledge about individual words are required for language technologies.</S> <S sid="8" ssid="8">Learning the argument structure properties of verbs?the semantic roles they assign and their mapping to syntactic positions?is both particularly important and difficult.</S> <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
 Citance Number: 9 | Reference Article: W03-0410.xml | Citing Article: D11-1095.xml | Citation Marker Offset: ['167'] | Citation Marker: Stevenson and Joanis (2003) | Citation Offset: ['167'] | Citation Text: <S sid="167" ssid="16">Table 1: Comparison against Stevenson and Joanis (2003)?s result on T1 (using similar features).</S> | Reference Offset: ['23' , '27' , '30' , '34' , '39' , '46' , '129' , '225' ] | Reference Text: <S sid="23" ssid="23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ?the curse of dimensionality??</S> <S sid="27" ssid="27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S> <S sid="30" ssid="30">The unsupervised feature selection method, on the other hand, was not usable for our data.</S> <S sid="34" ssid="1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research (Dorr and Jones, 1996; Kipper et al., 2000; Merlo and Stevenson, 2001; Schulte im Walde and Brew, 2002).</S> <S sid="39" ssid="6">Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</S> <S sid="46" ssid="13">Tense, Voice, and Aspect Features (24 features) Verb meaning, and therefore class membership, interacts in interesting ways with voice, tense, and aspect (Levin, 1993; Merlo and Stevenson, 2001).</S> <S sid="129" ssid="79">From these extracted slots, we calculate the features described in Section 2, yielding a vector of 220 normalized counts for each verb, which forms the input to our machine learning experiments.</S> <S sid="225" ssid="63">For each experimental task, we ran our supervised Table 3: Feature counts for Ling and Seed feature sets.</S>  | Discourse Facet: Method_Citation | Annotator: Fathima Vardha |
