Citance Number: 1 | Reference Article:  P98-2143.xml | Citing Article:  A00-1020.xml | Citation Marker Offset:  ['29'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['28','29'] | Citation Text:  <S sid ="28" ssid = "4">Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.</S><S sid ="29" ssid = "5">(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)).</S> | Reference Offset:  ['128', '138', '148', '98', '52'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="138" ssid = "63">For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 2 | Reference Article:  P98-2143.xml | Citing Article:  C02-1027.xml | Citation Marker Offset:  ['13'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "3">The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).</S> | Reference Offset:  ['2', '8', '10', '11', '52'] | Reference Text:  <S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 3 | Reference Article:  P98-2143.xml | Citing Article:  C02-1027.xml | Citation Marker Offset:  ['65'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['65'] | Citation Text:  <S sid ="65" ssid = "3">This module resolves third-person personal pronouns and is an adaptation of Mitkovâ€™s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).</S> | Reference Offset:  ['8', '11', '25', '76', '98'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 4 | Reference Article:  P98-2143.xml | Citing Article:  C02-1027.xml | Citation Marker Offset:  ['73'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['71','72','73'] | Citation Text:  <S sid ="71" ssid = "9">LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkovâ€™s Anaphora Resolution.</S><S sid ="72" ssid = "10">System.</S><S sid ="73" ssid = "11">3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).</S> | Reference Offset:  ['128', '76', '148', '52', '98'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S> | Discourse Facet:  ['Results_Citation', 'Method_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 5 | Reference Article:  P98-2143.xml | Citing Article:  C02-1027.xml | Citation Marker Offset:  ['79'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['79'] | Citation Text:  <S sid ="79" ssid = "17">Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).</S> | Reference Offset:  ['25', '128', '148', '76', '119'] | Reference Text:  <S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="119" ssid = "44">This measure (Mitkov 1998b) applies only to anaphors &quot;ambiguous&quot; from the point of view of number and gender (i.e. to those &quot;tough&quot; anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 6 | Reference Article:  P98-2143.xml | Citing Article:  C04-1074.xml | Citation Marker Offset:  ['17'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['16','17'] | Citation Text:  <S sid ="17" ssid = "17">Other proposals provide a more realistic picture in that they work as a backend to a parser (Lappin and Leass, 1994) or noun chunker (Mitkov, 1998; Soon et al., 2001; Ng and Cardie, 2002)).</S> | Reference Offset:  ['8', '10', '11', '25', '76'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 7 | Reference Article:  P98-2143.xml | Citing Article:  C04-1074.xml | Citation Marker Offset:  ['61'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['61'] | Citation Text:  <S sid ="60" ssid = "22">Binding constraints have been in the focus of linguistic research for more than thirty years.</S><S sid ="61" ssid = "23">They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).</S> | Reference Offset:  ['128', '76', '52', '98', '148'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 8 | Reference Article:  P98-2143.xml | Citing Article:  C04-1075.xml | Citation Marker Offset:  ['19'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['19'] | Citation Text:  <S sid ="19" ssid = "19">However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998;</S> | Reference Offset:  ['8', '9', '10', '11', '128'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="9" ssid = "9">However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 9 | Reference Article:  P98-2143.xml | Citing Article:  C04-1143.xml | Citation Marker Offset:  ['101'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['101'] | Citation Text:  <S sid ="101" ssid = "44">The search scope for candidate antecedents is set to the current sentence together with the three preceding sentences as suggested in (Mitkov, 1998)</S> | Reference Offset:  ['11', '17', '76', '103', '128'] | Reference Text:  <S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="17" ssid = "3">The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re­ maining candidates (see next section).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="103" ssid = "28">The aggregate score for &quot;the drawer&quot; is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (&quot;the lit paper port LED&quot;) is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera­ tion 0 + section heading 0 + collocation 0 + referen­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 10 | Reference Article:  P98-2143.xml | Citing Article:  D09-1101.xml | Citation Marker Offset:  ['52'] | Citation Marker:  1998 | Citation Offset:  ['52'] | Citation Text:  <S sid ="52" ssid = "6">Like many heuristic-based pronoun resolvers (e.g., Mitkov (1998)), they first apply a set of constraints to filter grammatically incompatible candidate antecedents and then rank the remaining ones using salience factors.</S> | Reference Offset:  ['4', '11', '35', '52', '68'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="35" ssid = "21">Indicating verbs If a verb is a member of the Verb_set = {discuss, present, illustrate, identify, summarise, examine, describe, define, show, check, develop, review, re­ port, outline, consider, investigate, explore, assess, analyse, synthesise, study, survey, deal, cover}, we consider the first NP following it as the preferred an­ tecedent (scores 1 and 0).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="68" ssid = "54">When all preferences (antecedent indicators) are taken into account, however, the right antecedent is still very likely to be tracked down - in the above example, the &quot;non-prepositional noun phrases&quot; heuristics (penalty) would be overturned by the &quot;collocational preference&quot; heuristics.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 11 | Reference Article:  P98-2143.xml | Citing Article:  E99-1031.xml | Citation Marker Offset:  ['7'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['6','7'] | Citation Text:  <S sid ="6" ssid = "6">A huge variety of techniques are described in the literature, many of them achieving high sucÂ­ cess rates on their own evaluation texts (cf.</S><S sid ="7" ssid = "7">Hobbs 1986; Strube 1998; Mitkov 1998).</S> | Reference Offset:  ['128', '76', '98', '148', '52'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 12 | Reference Article:  P98-2143.xml | Citing Article:  E99-1031.xml | Citation Marker Offset:  ['91'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['91'] | Citation Text:  <S sid ="91" ssid = "8">We implemented meta-modules to inÂ­ terface to the genetic algorithm driver and to combine different salience factors into an overÂ­ all score (similar to (Carbonell and Brown, 1988; Mitkov, 1998)).</S> | Reference Offset:  ['8', '128', '76', '103', '11'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="103" ssid = "28">The aggregate score for &quot;the drawer&quot; is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (&quot;the lit paper port LED&quot;) is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera­ tion 0 + section heading 0 + collocation 0 + referen­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 13 | Reference Article:  P98-2143.xml | Citing Article:  H05-1001.xml | Citation Marker Offset:  ['59'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['59'] | Citation Text:  <S sid ="59" ssid = "21">The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998)</S> | Reference Offset:  ['11', '76', '128', '10', '70'] | Reference Text:  <S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="70" ssid = "56">The algorithm for pronoun resolution can be de­ scribed informally as follows: 1.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 14 | Reference Article:  P98-2143.xml | Citing Article:  H05-1001.xml | Citation Marker Offset:  ['61'] | Citation Marker:  1998 | Citation Offset:  ['61','62'] | Citation Text:  <S sid ="61" ssid = "23">3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.</S><S sid ="62" ssid = "24">Mitkovâ€™s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as â€antecedent indicatorsâ€).</S> | Reference Offset:  ['16', '128', '103', '76', '148'] | Reference Text:  <S sid ="16" ssid = "2">It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="103" ssid = "28">The aggregate score for &quot;the drawer&quot; is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (&quot;the lit paper port LED&quot;) is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera­ tion 0 + section heading 0 + collocation 0 + referen­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 15 | Reference Article:  P98-2143.xml | Citing Article:  H05-1001.xml | Citation Marker Offset:  ['63'] | Citation Marker:  1998 | Citation Offset:  ['63'] | Citation Text:  <S sid ="63" ssid = "25">The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).</S> | Reference Offset:  ['4', '11', '14', '16', '17'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="14" ssid = "14">Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).</S><S sid ="16" ssid = "2">It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).</S><S sid ="17" ssid = "3">The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re­ maining candidates (see next section).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 16 | Reference Article:  P98-2143.xml | Citing Article:  H05-1083.xml | Citation Marker Offset:  ['35'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['35'] | Citation Text:  <S sid ="35" ssid = "9"> These feature functions bear similarity to rules used in other coreference systems (Lappin and Leass, 1994; Mitkov, 1998; Stuckardt, 2001), except that the feature weights {λi} are automatically trained over a corpus with coreference information.</S> | Reference Offset:  ['8', '11', '25', '76', '86'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="86" ssid = "11">The lack of syntactic information, for instance, means giving up c-cornmand constraints and subject preference (or on other occasions object preference, see Mitkov I995) which could be used in center tracking.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 16 | Reference Article:  P98-2143.xml | Citing Article:  H05-1083.xml | Citation Marker Offset:  ['123'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['123'] | Citation Text:  <S sid ="123" ssid = "1">F-measure here is more stringent than the accuracy (Ge et al., 1998; Mitkov, 1998; Kehler et al., 2004) computed on antecedent-pronoun pairs.</S> | Reference Offset:  ['11', '76', '128', '10', '98'] | Reference Text:  <S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Hypothesis_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 17 | Reference Article:  P98-2143.xml | Citing Article:  I05-2040.xml | Citation Marker Offset:  ['15'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['15'] | Citation Text:  <S sid ="15" ssid = "15">Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents.</S> | Reference Offset:  ['2', '4', '8', '10', '11'] | Reference Text:  <S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 18 | Reference Article:  P98-2143.xml | Citing Article:  I08-1004.xml | Citation Marker Offset:  ['10'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['10'] | Citation Text:  <S sid ="10" ssid = "10">Indeed, such feature-based methods have been widely applied in parsing (Collins 1999; Charniak 2001), semantic role labeling (Pradhan et al 2005), semantic relation extraction (Zhou et al 2005) and co-reference resolution (Lapin and Leass 1994; Aone and Bennett 1995; Mitkov 1998; Yang et al 2004; Luo and Zitouni 2005; Bergsma and Lin 2006).</S> | Reference Offset:  ['4', '8', '10', '11', '25'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 19 | Reference Article:  P98-2143.xml | Citing Article:  I08-3014.xml | Citation Marker Offset:  ['23'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['22','23'] | Citation Text:  <S sid ="22" ssid = "22">A lot of work has been done in English for the purpose of anaphora resolution and various</S><S sid ="23" ssid = "23"> algorithms have been devised for this purpose (Aone and Bennette, 1996; Brenan , Friedman and Pollard, 1987; Ge, Hale and Charniak, 1998; Grosz, Aravind and Weinstein, 1995; McCarthy and Lehnert, 1995; Lappins and Leass, 1994; Mitkov, 1998; Soon, Ng and Lim, 1999).</S> | Reference Offset:  ['10', '11', '98', '128', '76'] | Reference Text:  <S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 20 | Reference Article:  P98-2143.xml | Citing Article:  I08-3014.xml | Citation Marker Offset:  ['41'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['41'] | Citation Text:  <S sid ="41" ssid = "2">How these factors are helpful in anaphora resolution in English language was worked out by Mitkov (Mitkov, 1998), but their role in Urdu discourse for the resolution of personal pronouns is more cherished.</S> | Reference Offset:  ['1', '4', '8', '10', '11'] | Reference Text:  <S sid ="1" ssid = "1">Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</S><S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 21 | Reference Article:  P98-2143.xml | Citing Article:  J01-4003.xml | Citation Marker Offset:  ['5'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['5'] | Citation Text:  <S sid ="5" ssid = "5">Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora.</S> | Reference Offset:  ['8', '10', '11', '25', '76'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 22 | Reference Article:  P98-2143.xml | Citing Article:  J01-4005.xml | Citation Marker Offset:  ['318'] | Citation Marker:  1998 | Citation Offset:  ['318'] | Citation Text:  <S sid ="318" ssid = "14">Consequently, current anaphora resolution methods rely mainly on constraint and preference heuristics, which employ morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov [1998]).</S> | Reference Offset:  ['8', '11', '25', '76', '98'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 23 | Reference Article:  P98-2143.xml | Citing Article:  J01-4006.xml | Citation Marker Offset:  ['45'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['45'] | Citation Text:  <S sid ="45" ssid = "33">Exa mple s: (we athe r) It is raini ng, (tim e) It is 5 o&apos;clo ck, and (am bien t envi ron men t) It is hot in here . reports provide no exclusion details at all, and even when authors do provide them, the descriptions they use are often incomplete or confusing, as in these examples: â€¢ &quot;7 of the pronouns were non-anaphoric and 16 exophoric&quot; (Mitkov 1998, page 872).</S> | Reference Offset:  ['8', '11', '25', '52', '64'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="64" ssid = "50">Top symptoms like &quot;lexical reiteration&quot; as­ sign score &quot;2&quot; whereas &quot;non-prepositional&quot; noun phrases are given a negative score of &quot;-1&quot;.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 24 | Reference Article:  P98-2143.xml | Citing Article:  J02-1001.xml | Citation Marker Offset:  ['6'] | Citation Marker:  1998 | Citation Offset:  ['6'] | Citation Text:  <S sid ="6" ssid = "6">Since the so-called integrative approach to anaphor resolution was developed in the late 1980s (Carbonell and Brown 1988; Rich and LuperFoy 1988; Asher and Wada 1989), and its practical viability extensively tested (e.g., Lappin and Leass 1994; Mitkov 1997, 1998), it has been common wisdom that factors determining the antecedentsof anaphors divide into filters and preferences.</S> | Reference Offset:  ['4', '8', '9', '11', '16'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="9" ssid = "9">However, to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="16" ssid = "2">It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 25 | Reference Article:  P98-2143.xml | Citing Article:  J05-3004.xml | Citation Marker Offset:  ['8'] | Citation Marker:  1998 | Citation Offset:  ['7','8'] | Citation Text:  <S sid ="7" ssid = "7">Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.</S><S sid ="8" ssid = "8">Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.</S> | Reference Offset:  ['76', '128', '8', '98', '11'] | Reference Text:  <S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 26 | Reference Article:  P98-2143.xml | Citing Article:  N01-1008.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['9','10','11'] | Citation Text:  <S sid ="9" ssid = "9">The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.</S><S sid ="10" ssid = "10">Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.</S><S sid ="11" ssid = "11">(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)).</S> | Reference Offset:  ['128', '76', '98', '52', '148'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation', 'Aim_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 27 | Reference Article:  P98-2143.xml | Citing Article:  N01-1008.xml | Citation Marker Offset:  ['63'] | Citation Marker:  Mitkov 1998 | Citation Offset:  ['63'] | Citation Text:  <S sid ="63" ssid = "63">Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCK TAIL filters its most performant rules through massivetraining data, generated by its AUTOTAGCOFtEF com ponent.</S> | Reference Offset:  ['8', '11', '76', '128', '138'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="138" ssid = "63">For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 28 | Reference Article:  P98-2143.xml | Citing Article:  N04-1004.xml | Citation Marker Offset:  ['45'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['45'] | Citation Text:  <S sid ="45" ssid = "32">Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998).</S> | Reference Offset:  ['8', '10', '11', '14', '15'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="14" ssid = "14">Finally, our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English, but also for other languages (in our case Polish and Arabic).</S><S sid ="15" ssid = "1">With a view to avoiding complex syntactic, seman­ tic and discourse analysis (which is vital for real­ world applications), we developed a robust, knowl­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 29 | Reference Article:  P98-2143.xml | Citing Article:  P00-1022.xml | Citation Marker Offset:  ['27'] | Citation Marker:  1998 | Citation Offset:  ['27'] | Citation Text:  <S sid ="27" ssid = "27">Consequently, current anaphora resolution methods rely mainly on restrictions and preference heuristics, which employ information originating from morpho-syntactic or shallow semantic analysis, (see Mitkov (1998) for example).</S> | Reference Offset:  ['8', '11', '25', '76', '98'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 30 | Reference Article:  P98-2143.xml | Citing Article:  P00-1022.xml | Citation Marker Offset:  ['178'] | Citation Marker:  1998 | Citation Offset:  ['178'] | Citation Text:  <S sid ="178" ssid = "18">Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals.</S> | Reference Offset:  ['76', '93', '98', '103', '119'] | Reference Text:  <S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="93" ssid = "18">3.1 Evaluation A. Our first evaluation exercise (Mitkov &amp; Stys 1997) was based on a random sample text from a technical manual in English (Minolta 1994).</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="103" ssid = "28">The aggregate score for &quot;the drawer&quot; is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (&quot;the lit paper port LED&quot;) is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera­ tion 0 + section heading 0 + collocation 0 + referen­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).</S><S sid ="119" ssid = "44">This measure (Mitkov 1998b) applies only to anaphors &quot;ambiguous&quot; from the point of view of number and gender (i.e. to those &quot;tough&quot; anaphors which, after activating the gender and number filters, still have more than one candidate for antecedent) and is indicative of the performance of the antecedent indicators.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 31 | Reference Article:  P98-2143.xml | Citing Article:  P00-1022.xml | Citation Marker Offset:  ['209'] | Citation Marker:  1998 | Citation Offset:  ['209'] | Citation Text:  <S sid ="209" ssid = "28">Ruslan Mitkov (1998) Robust pronoun resolution th evaluation, several baselines on pronominal anaphora resolution have been implemented, and with limited knowledge.</S> | Reference Offset:  ['2', '8', '10', '11', '52'] | Reference Text:  <S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 32 | Reference Article:  P98-2143.xml | Citing Article:  P01-1006.xml | Citation Marker Offset:  ['65'] | Citation Marker:  Mitkov, 1998b | Citation Offset:  ['65'] | Citation Text:  <S sid ="65" ssid = "31">We selected for comparative evaluation three approaches extensively cited in the literature: Kennedy and Boguraevâ€™s parser- free version of Lappin and Leassâ€™ RAP (Kennedy and Boguraev, 1996), Baldwinâ€™s pronoun resolution method (Baldwin, 1997) and Mitkovâ€™s knowledge-poor pronoun resolution approach (Mitkov, 1998b).</S> | Reference Offset:  ['3', '6', '7', '8', '10'] | Reference Text:  <S sid ="3" ssid = "3">This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</S><S sid ="6" ssid = "6">Evaluation reports a success rate of 89.7% which is better than the suc­ cess rates of the approaches selected for comparison and tested on the same data.</S><S sid ="7" ssid = "7">In addition, preliminary experiments show that the approach can be success­ fully adapted for other languages with minimum modifications.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 33 | Reference Article:  P98-2143.xml | Citing Article:  P01-1006.xml | Citation Marker Offset:  ['82'] | Citation Marker:  Mitkov, 1998b | Citation Offset:  ['82'] | Citation Text:  <S sid ="82" ssid = "48">Mitkovâ€™s approach Mitkovâ€™s approach (Mitkov, 1998b) is a robust anaphora resolution method for technical texts which is based on a set of boosting and impeding indicators applied to each candidate for antecedent.</S> | Reference Offset:  ['1', '2', '8', '10', '11'] | Reference Text:  <S sid ="1" ssid = "1">Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</S><S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 34 | Reference Article:  P98-2143.xml | Citing Article:  P01-1006.xml | Citation Marker Offset:  ['138'] | Citation Marker:  1998b | Citation Offset:  ['138'] | Citation Text:  <S sid ="138" ssid = "104">pronouns, referential distance, average number of candidates for antecedent per pronoun and types of anaphors.7 As expected, the results reported in Table 1 do not match the original results published by Kennedy and Boguraev (1996), Baldwin (1997) and Mitkov (1998b) where the algorithms were tested on different data, employed different pre-processing tools, resorted to different degrees of manual intervention and thus provided no common ground for any reliable comparison.</S> | Reference Offset:  ['3', '8', '10', '11', '16'] | Reference Text:  <S sid ="3" ssid = "3">This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="16" ssid = "2">It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as &quot;antecedent indicators&quot;).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 35 | Reference Article:  P98-2143.xml | Citing Article:  P01-1006.xml | Citation Marker Offset:  ['8'] | Citation Marker:  1998b | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">Our paper discusses a particular configuration of this new evaluation environment incorporating three approaches sharing a common â€knowledge-poor philosophyâ€: Kennedy and Boguraevâ€™s (1996) parser-free algorithm, Baldwinâ€™s (1997) CogNiac and Mitkovâ€™s (1998b) knowledge-poor approach.</S> | Reference Offset:  ['3', '8', '10', '11', '76'] | Reference Text:  <S sid ="3" ssid = "3">This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 36 | Reference Article:  P98-2143.xml | Citing Article:  P03-1023.xml | Citation Marker Offset:  ['13'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">Mitkovâ€™s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators to rank the candidates.</S> | Reference Offset:  ['8', '10', '11', '25', '52'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 37 | Reference Article:  P98-2143.xml | Citing Article:  P04-1017.xml | Citation Marker Offset:  ['191'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['191'] | Citation Text:  <S sid ="191" ssid = "5">The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003).</S> | Reference Offset:  ['4', '8', '11', '52', '76'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 38 | Reference Article:  P98-2143.xml | Citing Article:  P04-1018.xml | Citation Marker Offset:  ['8'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">Early work of anaphora resolution focuses on find ing antecedents of pronouns (Hobbs, 1976; Ge et al., 1998; Mitkov, 1998)</S> | Reference Offset:  ['4', '8', '11', '52', '76'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 39 | Reference Article:  P98-2143.xml | Citing Article:  P05-1021.xml | Citation Marker Offset:  ['17'] | Citation Marker:  1998 | Citation Offset:  ['17'] | Citation Text:  <S sid ="17" ssid = "17">Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.</S> | Reference Offset:  ['4', '8', '10', '11', '76'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 40 | Reference Article:  P98-2143.xml | Citing Article:  P05-1021.xml | Citation Marker Offset:  ['97'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['97'] | Citation Text:  <S sid ="97" ssid = "23">ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).</S> | Reference Offset:  ['76', '98', '103', '128', '130'] | Reference Text:  <S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="103" ssid = "28">The aggregate score for &quot;the drawer&quot; is 7 (definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate refer­ ence 2 = 7), whereas aggregate score for the most recent matching noun phrase (&quot;the lit paper port LED&quot;) is 4 (definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reitera­ tion 0 + section heading 0 + collocation 0 + referen­ tial distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4).</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="130" ssid = "55">In terms of frequency of use (&quot;number of nonzero applications&quot;/&quot;number of anaphors&quot;), the most fre­ quently used indicator proved to be referential dis­ tance used in 98.9% of the cases, followed by term preference (97.8%), givenness (83.3%), lexical reit­ eration (64.4%), definiteness (40%), section heading (37.8%), immediate reference (31.1%) and colloca­ tion (11.1%).</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 41 | Reference Article:  P98-2143.xml | Citing Article:  P06-1006.xml | Citation Marker Offset:  ['12'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['12'] | Citation Text:  <S sid ="12" ssid = "12">These features are calculated by mining the parse trees, and then could be used for resolution by using manually designed rules (Lappin and Leass, 1994; Kennedy and Boguraev, 1996; Mitkov, 1998), or using machine-learning methods (Aone and Bennett, 1995; Yang et al., 2004; Luo and Zitouni, 2005).</S> | Reference Offset:  ['8', '10', '11', '12', '25'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="12" ssid = "12">Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph­ ora resolution.</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 42 | Reference Article:  P98-2143.xml | Citing Article:  P07-1068.xml | Citation Marker Offset:  ['5'] | Citation Marker:  1998 | Citation Offset:  ['5'] | Citation Text:  <S sid ="5" ssid = "5">In knowledge-lean approaches, coreference resolvers employ only morpho-syntactic cues as knowledge sources in the resolution process (e.g., Mitkov (1998), Tetreault (2001)).</S> | Reference Offset:  ['4', '10', '11', '76', '128'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 43 | Reference Article:  P98-2143.xml | Citing Article:  P10-2049.xml | Citation Marker Offset: ['78'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['78'] | Citation Text:  <S sid ="78" ssid = "4">The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.</S> | Reference Offset:  ['128', '138', '11', '76', '8'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="138" ssid = "63">For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S> | Discourse Facet:  ['Results_Citation', 'Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 44 | Reference Article:  P98-2143.xml | Citing Article:  P13-3012.xml | Citation Marker Offset:  ['15'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['15'] | Citation Text:  <S sid ="15" ssid = "2">While not developed within a graph-based framework, factor-based approaches for pronoun resolution (Mitkov, 1998) can be regarded as greedy clustering in a multigraph, where edges representing factors for pronoun resolution have negative or positive weight.</S> | Reference Offset:  ['2', '8', '10', '11', '15'] | Reference Text:  <S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="15" ssid = "1">With a view to avoiding complex syntactic, seman­ tic and discourse analysis (which is vital for real­ world applications), we developed a robust, knowl­ edge-poor approach to pronoun resolution which does not parse and analyse the input in order to identify antecedents of anaphors.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 45 | Reference Article:  P98-2143.xml | Citing Article:  S10-1019.xml | Citation Marker Offset:  ['7'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['5','6','7'] | Citation Text:  <S sid ="4" ssid = "4">Coreference resolution is a field in which major progress has been made in the last decade.</S><S sid ="5" ssid = "5">After a concentration on rule-based systems (cf.</S><S sid ="6" ssid = "6">e.g.</S><S sid ="7" ssid = "7">(Mitkov, 1998; Poesio et al., 2002; Markert and Nissim, 2005)), machine learning methods were embraced (cf.</S> | Reference Offset:  ['128', '76', '98', '148', '52'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 46 | Reference Article:  P98-2143.xml | Citing Article:  W01-0704.xml | Citation Marker Offset:  ['83'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['82','83'] | Citation Text:  <S sid ="82" ssid = "6">They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.</S><S sid ="83" ssid = "7">These proposals have report high success rates for English (89.7%) (Mitkov, 1998) | Reference Offset:  ['128', '76', '148', '52', '10'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S> | Discourse Facet:  ['Results_Citation', 'Method_Citation', 'Hypothesis_Citation', 'Aim_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 47 | Reference Article:  P98-2143.xml | Citing Article:  W01-0717.xml | Citation Marker Offset:  ['145'] | Citation Marker:  Mitkov, 1998b | Citation Offset:  ['145'] | Citation Text:  <S sid ="145" ssid = "1">There are many recent approaches to this prob lem, e. g. syntax-based approaches (Lappin and Leass, 1994), cooccurrence-based approaches(Dagan and Itai, 1990), machine-learning approaches (Connolly et al., 1994; Aone and Ben nett, 1996; Soon et al., 1999), uncertainty reasoning approaches (Mitkov, 1995; Mitkov, 1997), and robust knowledge-poor approaches (Kennedy and Boguarev, 1996; Baldwin, 1997; Mitkov, 1998b; Mitkov, 1999).</S> | Reference Offset:  ['1', '2', '3', '6', '7'] | Reference Text:  <S sid ="1" ssid = "1">Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</S><S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="3" ssid = "3">This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</S><S sid ="6" ssid = "6">Evaluation reports a success rate of 89.7% which is better than the suc­ cess rates of the approaches selected for comparison and tested on the same data.</S><S sid ="7" ssid = "7">In addition, preliminary experiments show that the approach can be success­ fully adapted for other languages with minimum modifications.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 48 | Reference Article:  P98-2143.xml | Citing Article:  W04-0707.xml | Citation Marker Offset:  ['222'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['222'] | Citation Text:  <S sid ="222" ssid = "165">G U I TA R (Poesio and AlexandrovKabadjov, 2004) is a general-purpose anaphoric resolver that includes an implementation of the Vieira / Poesio algorithm for definite descriptions and of Mitkovâ€™s algorithm for pronoun resolution (Mitkov, 1998).</S> | Reference Offset:  ['8', '10', '11', '25', '70'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S><S sid ="70" ssid = "56">The algorithm for pronoun resolution can be de­ scribed informally as follows: 1.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 49 | Reference Article:  P98-2143.xml | Citing Article:  W04-0711.xml | Citation Marker Offset:  ['19'] | Citation Marker:  1998 | Citation Offset:  ['19'] | Citation Text:  <S sid ="19" ssid = "19">(PMID:9701290) Table 1: A protein domain-referring phrase example ments and lexical features, in addressing problems in the biomedical domain (cf.</S><S sid ="20" ssid = "20">Mitkov et al.</S><S sid ="21" ssid = "21">(1998)).</S> | Reference Offset:  ['128', '76', '98', '148', '52'] | Reference Text:  <S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="148" ssid = "9">Similarly to the evaluation for English, we com­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 50 | Reference Article:  P98-2143.xml | Citing Article:  W04-0714.xml | Citation Marker Offset:  ['18'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['18'] | Citation Text:  <S sid ="18" ssid = "18">Our method is an inexpensive, fast and reliable procedure for anaphora resolution, which relies on cheaper and more reliable NLP tools such as part- of-speech (POS) tagger and shallow parsers (Baldwin, 1997; FerrÃ¡ndez et al., 1998; Kennedy and Boguraev, 1996; Mitkov, 1998; Yeh and Chen, 2003).</S> | Reference Offset:  ['8', '10', '11', '12', '25'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="12" ssid = "12">Our work is a continuation of these latest trends in the search for inexpensive, fast and reliable procedures for anaph­ ora resolution.</S><S sid ="25" ssid = "11">The antecedent indicators have been identi­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non­ prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 51 | Reference Article:  P98-2143.xml | Citing Article:  W04-2310.xml | Citation Marker Offset:  ['102'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['102'] | Citation Text:  <S sid ="102" ssid = "7">In most systems ((Mitkov, 1998),(Lappin and Leass, 1994)) the weights that are assigned for different anaphor-antecedent relationships are programmer dependent.</S> | Reference Offset:  ['8', '11', '76', '128', '10'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 52 | Reference Article:  P98-2143.xml | Citing Article:  W04-2310.xml | Citation Marker Offset:  2186-2198 | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">Often domain-specific heuristics are used for anaphora resolution and fine tuned to perform well on a limited corpus, such as in (Mitkov, 1998).</S> | Reference Offset:  ['11', '76', '98', '128', '8'] | Reference Text:  <S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="98" ssid = "23">In order to evaluate the effectiveness of the ap­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as antece­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 53 | Reference Article:  P98-2143.xml | Citing Article:  W06-2302.xml | Citation Marker Offset:  ['48'] | Citation Marker:  1998 | Citation Offset:  ['48'] | Citation Text:  <S sid ="48" ssid = "32">The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic â€œitâ€ occurrences, and assigns animacy.</S> | Reference Offset:  ['4', '8', '10', '11', '76'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 54 | Reference Article:  P98-2143.xml | Citing Article:  W09-2411.xml | Citation Marker Offset:  ['6'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['6'] | Citation Text:  <S sid ="6" ssid = "6">Some of the limitations of the traditional rule based approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora.</S> | Reference Offset:  ['8', '10', '11', '76', '128'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation'] | Annotator:  CIST |


Citance Number: 55 | Reference Article:  P98-2143.xml | Citing Article:  W99-0104.xml | Citation Marker Offset:  ['59'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['59'] | Citation Text:  <S sid ="59" ssid = "22">Other pronominal resolution approaches promote knowledge-poor methods (Mitkov, 1998), eitper by using an ordered set of general heuristics or by combining scores assigned to candidate antecedents.</S> | Reference Offset:  ['4', '8', '11', '52', '76'] | Reference Text:  <S sid ="4" ssid = "4">Input is checked against agreement and for a number of antecedent indicators.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="52" ssid = "38">The heuristics used is that in constructions of the form &quot;...(You) V 1 NP ... con (you) V 2 it (con (you) V3 it)&quot;, where con e {and/or/before/after...}, the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun &quot;it&quot; imme­ diately following V2 and is therefore given preference (scores 2 and 0).</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S> | Discourse Facet:  ['Method_Citation', 'Hypothesis_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 56 | Reference Article:  P98-2143.xml | Citing Article:  W99-0104.xml | Citation Marker Offset:  ['60'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['60'] | Citation Text:  <S sid ="60" ssid = "23">The CogNIAC algorithm {Baldwin, 1997) uses six heuristic rules to resalv.e coreference, whereas the algorithm presented in (Mitkov, 1998) is based on a limited set of preferences (e.g. definitiveness, l cal reiteration or immediate reference).</S> | Reference Offset:  ['8', '10', '11', '17', '24'] | Reference Text:  <S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S><S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="11" ssid = "11">Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or lin­ guistic knowledge (Baldwin 1997; Dagan &amp; ltai 1990; Kennedy &amp; Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).</S><S sid ="17" ssid = "3">The approach works as follows: it takes as an input the output of a text processed by a part-of-speech tagger, identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor and then applies the genre-specific antecedent indicators to the re­ maining candidates (see next section).</S><S sid ="24" ssid = "10">Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the ante­ cedent.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 57 | Reference Article:  P98-2143.xml | Citing Article:  W99-0207.xml | Citation Marker Offset:  ['128'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['128'] | Citation Text:  <S sid ="128" ssid = "4">However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).</S><S sid ="129" ssid = "5">Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf.</S> | Reference Offset:  ['10', '76', '128', '138', '8'] | Reference Text:  <S sid ="10" ssid = "10">While various alternatives have been proposed, making use of e.g. neural networks, a situation se­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin &amp; Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic pro­ cessing of growing language resources.</S><S sid ="76" ssid = "1">tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric &quot;it&quot; occurring in constructions such as &quot;It is important&quot;, &quot;It is necessary&quot; is eliminated by a &quot;referential filter&quot; 5Note that this restriction may not always apply in lan­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. &quot;government&quot;, &quot;team&quot;, &quot;parliament&quot; etc. can be referred to by &quot;they&quot;; equally some plural nouns (e.g. &quot;data&quot;) can be referred to by &quot;it&quot;) and are exempted from the agree­ ment test.</S><S sid ="128" ssid = "53">If we regard as &quot;discriminative power&quot; of each antecedent indicator the ratio &quot;number of successful antecedent identifications when this indicator was applied&quot;/&quot;number of applications of this indicator&quot; (for the non-prepositional noun phrase and definite­ ness being penalising indicators, this figure is calcu­ lated as the ratio &quot;number of unsuccessful antece­ dent identifications&quot;/&quot;number of applications&quot;), the immediate reference emerges as the most discrimi­ native indicator (100%), followed by non­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).</S><S sid ="138" ssid = "63">For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Implication_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 58 | Reference Article:  P98-2143.xml | Citing Article:  W99-0207.xml | Citation Marker Offset:  ['131'] | Citation Marker:  Mitkov, 1998 | Citation Offset:  ['131'] | Citation Text:  <S sid ="131" ssid = "7">Whereas knowledge-based systems like (Carbonell and Brown, 1988) and (Rich and LuperFoy, 1988) combining multiple resolution strategies are expenÂ­ sive in the cost of human effort at development time and limited ability to scale to new domains, more reÂ­ cent knowledge-poor approaches like (Kennedy and Boguraev, 1996) and (Mitkov, 1998) address the problem without sophisticated linguistic knowledge.</S> | Reference Offset:  ['1', '2', '3', '7', '8'] | Reference Text:  <S sid ="1" ssid = "1">Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</S><S sid ="2" ssid = "2">One of the disadvantages of developing a knowledge­ based system, however, is that it is a very labour­ intensive and time-consuming task.</S><S sid ="3" ssid = "3">This paper pres­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</S><S sid ="7" ssid = "7">In addition, preliminary experiments show that the approach can be success­ fully adapted for other languages with minimum modifications.</S><S sid ="8" ssid = "8">For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell &amp; Brown 1988; Carter 1987; Hobbs 1978; Ingria &amp; Stallard 1989; Lappin &amp; McCord 1990; Lappin &amp; Leass 1994; Mitkov 1994; Rich &amp; LuperFoy 1988; Sidner 1979; Webber 1979).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |