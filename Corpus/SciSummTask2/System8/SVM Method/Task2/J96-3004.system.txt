The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.
In this paper we present stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.
Space- or punctuation-delimited 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.
com §Cambridge, UK Email: nc201@eng.cam.ac.uk 1996 Association for Computational Linguistics (a) (b) (c) Figure [§] lxI 1:&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' Chinese sentence in (a) illustrating the lack of word boundaries.
In (b) is plausible segmentation for this sentence; in (c) is an implausible segmentation.
orthographic words are thus only starting point for further analysis and can only be regarded as useful hint at the desired division of the sentence into words.
Whether language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.
Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in Chinese-derived writ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.
raphy: ren2 'person' is fairly uncontroversial case of monographemic word, and rplil zhong1guo2 (middle country) 'China' fairly uncontroversial case of di­ graphernic word.
The relevance of the distinction between, say, phonological words and, say, dictionary words
