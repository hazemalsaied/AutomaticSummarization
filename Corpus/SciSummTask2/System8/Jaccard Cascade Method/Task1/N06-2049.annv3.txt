Citance Number: 1 | Reference Article:  N06-2049.xml | Citing Article:  C10-2139.xml | Citation Marker Offset:  ['290'] | Citation Marker:  Zh ang et al., 200 6 | Citation Offset:  ['290','305'] | Citation Text:  <S sid ="290" ssid = "189">AS C U MS R PK U (Zh ang et al., 200 6) 95.</S><S sid ="305" ssid = "204">2 Table 5: Segmentation performance presented in previous work and of our combination model.</S> | Reference Offset:  ['5', '43', '80', '120'] | Reference Text:  <S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="43" ssid = "21">In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.</S><S sid ="80" ssid = "9">Table 1 shows the performance of the dictionary-based segmentation.</S><S sid ="120" ssid = "49">71 2 0.9 67 0.9 76 Table 3: Effects of combination using the confidence measure.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 2 | Reference Article:  N06-2049.xml | Citing Article:  D13-1031.xml | Citation Marker Offset:  ['274'] | Citation Marker:  2006 | Citation Offset:  ['273','274'] | Citation Text:  <S sid ="273" ssid = "109">CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.</S><S sid ="274" ssid = "110">(2006).</S> | Reference Offset:  ['32', '33', '20', '152'] | Reference Text:  <S sid ="32" ssid = "10">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.</S><S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="20" ssid = "20">Section 3 presents our experimental results.</S><S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 3 | Reference Article:  N06-2049.xml | Citing Article:  I08-4009.xml | Citation Marker Offset:  ['19'] | Citation Marker:  Zhang et al., 2006a | Citation Offset:  ['19'] | Citation Text:  <S sid ="19" ssid = "19">Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).</S> | Reference Offset:  ['5', '82', '14', '3'] | Reference Text:  <S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="82" ssid = "11">In fact, there were no OOV recognition.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="3" ssid = "3">In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 4 | Reference Article:  N06-2049.xml | Citing Article:  I08-4009.xml | Citation Marker Offset:  ['20'] | Citation Marker:  Zhang et al., 2006a | Citation Offset:  ['20'] | Citation Text:  <S sid ="20" ssid = "20">Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).</S> | Reference Offset:  ['29', '57', '58', '108'] | Reference Text:  <S sid ="29" ssid = "7">We will use this advantage in the confidence measure approach.</S><S sid ="57" ssid = "35">In this section we introduce a confidence measure approach to combine the two results.</S><S sid ="58" ssid = "36">We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.</S><S sid ="108" ssid = "37">In section 2.2, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 5 | Reference Article:  N06-2049.xml | Citing Article:  I08-4015.xml | Citation Marker Offset:  ['36'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['36'] | Citation Text:  <S sid ="36" ssid = "22">After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).</S> | Reference Offset:  ['25', '98', '60', '20'] | Reference Text:  <S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="98" ssid = "27">71 6 0.9 64 0.9 72 Table 2: Segmentation results by a pure subword-based IOB tagging.</S><S sid ="60" ssid = "38">Its calculation is defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) where tiob is the word w’s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.</S><S sid ="20" ssid = "20">Section 3 presents our experimental results.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 6 | Reference Article:  N06-2049.xml | Citing Article:  I08-4030.xml | Citation Marker Offset:  ['13'] | Citation Marker:  R. Zhang et al., 2006 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).</S> | Reference Offset:  ['1', '67', '62', '59'] | Reference Text:  <S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="67" ssid = "45">If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.</S><S sid ="62" ssid = "40">Each subword is given a prior IOB tag, tw . C Miob (t|w), a  M  confidence probability derived in the process of IOB tag exp )&apos; )&apos; λk fk (ti−1 , ti , W ) + )&apos; µk gk (ti , W ) /Z,  i=1  k k   (1) ging, is defined as Z = )&apos; T =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) T =t 0 t1 ··· tM P ( T|W ) where we call fk (ti−1 , ti , W ) bigram feature functions because the features trigger the previous observation ti−1 where the numerator is a sum of all the observation sequences with word wi labeled as t. δ(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.</S><S sid ="59" ssid = "37">The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 7 | Reference Article:  N06-2049.xml | Citing Article:  I08-4030.xml | Citation Marker Offset:  ['51'] | Citation Marker:  R. Zhang et al., 2006 | Citation Offset:  ['51'] | Citation Text:  <S sid ="51" ssid = "7">According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.</S> | Reference Offset:  ['25', '134', '14', '44'] | Reference Text:  <S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="134" ssid = "3">Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 8 | Reference Article:  N06-2049.xml | Citing Article:  J11-1005.xml | Citation Marker Offset:  ['277'] | Citation Marker:  2006 | Citation Offset:  ['277'] | Citation Text:  <S sid ="277" ssid = "168">We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.</S> | Reference Offset:  ['4', '129', '3', '73'] | Reference Text:  <S sid ="4" ssid = "4">By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.</S><S sid ="129" ssid = "58">95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.</S><S sid ="3" ssid = "3">In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.</S><S sid ="73" ssid = "2">The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 9 | Reference Article:  N06-2049.xml | Citing Article:  N09-1007.xml | Citation Marker Offset:  ['134'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['134'] | Citation Text:  <S sid ="134" ssid = "17">Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);</S> |  Reference Offset:  ['5', '44', '152', '60'] | Reference Text:  <S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S><S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S><S sid ="60" ssid = "38">Its calculation is defined as: C M(tiob |w) = αC Miob (tiob |w) + (1 − α)δ(tw , tiob )ng (2) where tiob is the word w’s IOB tag assigned by the IOB tagging; tw , a prior IOB tag determined by the results of the dictionary-based segmentation.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 10 | Reference Article:  N06-2049.xml | Citing Article:  P06-2123.xml | Citation Marker Offset:  ['197'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['197'] | Citation Text:  <S sid ="197" ssid = "75">Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).</S> | Reference Offset:  ['7', '11', '8', '62'] | Reference Text:  <S sid ="7" ssid = "7">We found that so far all the existing implementations were using character-based IOB tagging.</S><S sid ="11" ssid = "11">We will give a detailed description of this approach in Section 2.</S><S sid ="8" ssid = "8">In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.</S><S sid ="62" ssid = "40">Each subword is given a prior IOB tag, tw . C Miob (t|w), a  M  confidence probability derived in the process of IOB tag exp )&apos; )&apos; λk fk (ti−1 , ti , W ) + )&apos; µk gk (ti , W ) /Z,  i=1  k k   (1) ging, is defined as Z = )&apos; T =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) T =t 0 t1 ··· tM P ( T|W ) where we call fk (ti−1 , ti , W ) bigram feature functions because the features trigger the previous observation ti−1 where the numerator is a sum of all the observation sequences with word wi labeled as t. δ(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 11 | Reference Article:  N06-2049.xml | Citing Article:  P07-1106.xml | Citation Marker Offset:  ['94'] | Citation Marker:  2006 | Citation Offset:  ['93','94'] | Citation Text:  <S sid ="93" ssid = "9">One existing method that is based on sub-word information, Zhang et al.</S><S sid ="94" ssid = "10">(2006), combines a C R F and a rule-based model.</S> | Reference Offset:  ['57', '99', '150', '152'] | Reference Text:  <S sid ="57" ssid = "35">In this section we introduce a confidence measure approach to combine the two results.</S><S sid ="99" ssid = "28">The upper numbers are of the character- based and the lower ones, the subword-based.</S><S sid ="150" ssid = "19">By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.</S><S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 12 | Reference Article:  N06-2049.xml | Citing Article:  P07-1106.xml | Citation Marker Offset:  ['172'] | Citation Marker:  2006 | Citation Offset:  ['172','173'] | Citation Text:  <S sid ="172" ssid = "72">We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.</S><S sid ="173" ssid = "73">(2006) for comparison.</S> | Reference Offset:  ['129', '4', '32', '2'] | Reference Text:  <S sid ="129" ssid = "58">95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.</S><S sid ="4" ssid = "4">By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.</S><S sid ="32" ssid = "10">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.</S><S sid ="2" ssid = "2">We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 13 | Reference Article:  N06-2049.xml | Citing Article:  P12-1027.xml | Citation Marker Offset:  ['189'] | Citation Marker:  2006 | Citation Offset:  ['188'] | Citation Text:  <S sid ="188" ssid = "41">Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.</S><S sid ="189" ssid = "42">(2006).</S> | Reference Offset:  ['33', '25', '32', '151'] | Reference Text:  <S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="32" ssid = "10">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 14 | Reference Article:  N06-2049.xml | Citing Article:  W06-0118.xml | Citation Marker Offset:  ['14'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['14'] | Citation Text:  <S sid ="14" ssid = "2">Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)</S> | Reference Offset:  ['30', '101', '44', '134'] | Reference Text:  <S sid ="30" ssid = "8">2.1 Subword-based IOB tagging using CRFs.</S><S sid ="101" ssid = "30">The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.</S><S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S><S sid ="134" ssid = "3">Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).</S> | Discourse Facet:  ['Implication_Citation', 'Method_Citation'] | Annotator:  CIST |


Citance Number: 15 | Reference Article:  N06-2049.xml | Citing Article:  W06-0118.xml | Citation Marker Offset:  ['24'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['24'] | Citation Text:  <S sid ="24" ssid = "12">Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.</S> | Reference Offset:  ['1', '151', '134', '32'] | Reference Text:  <S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="134" ssid = "3">Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).</S><S sid ="32" ssid = "10">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 16 | Reference Article:  N06-2049.xml | Citing Article:  W08-0335.xml | Citation Marker Offset:  ['55'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['55','56'] | Citation Text: <S sid ="55" ssid = "21">Part of the work using this tool was described by (Zhang et al., 2006).</S><S sid ="56" ssid = "22">The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.</S> | Reference Offset:  ['14', '72', '136', '149'] | Reference Text:  <S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="72" ssid = "1">We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.</S><S sid ="136" ssid = "5">We proved the new approach enhanced the word segmentation significantly.</S><S sid ="149" ssid = "18">In this work we used it more delicately.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 17 | Reference Article:  N06-2049.xml | Citing Article:  W08-0335.xml | Citation Marker Offset:  ['160'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['160'] | Citation Text:  <S sid ="160" ssid = "19">Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.</S> | Reference Offset:  ['43', '78', '33', '73'] | Reference Text:  <S sid ="43" ssid = "21">In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.</S><S sid ="78" ssid = "7">For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.</S><S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="73" ssid = "2">The data contain four corpora from different sources: Academia Sinica (AS), City University of Hong Kong (CITYU), Peking University (PKU) and Microsoft Research in Beijing (MSR).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 18 | Reference Article:  N06-2049.xml | Citing Article:  W10-4128.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.</S> | Reference Offset:  ['5', '14', '8', '88'] | Reference Text:  <S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="8" ssid = "8">In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.</S><S sid ="88" ssid = "17">For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 19 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).</S> | Reference Offset:  ['19', '151', '133', '134'] | Reference Text:  <S sid ="19" ssid = "19">In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="133" ssid = "2">It was first used in Chinese word segmentation by (Xue and Shen, 2003), where maximum entropy methods were used.</S><S sid ="134" ssid = "3">Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 20 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['25'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['25'] | Citation Text:  <S sid ="25" ssid = "11">Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).</S> | Reference Offset:  ['9', '109', '51', '4'] | Reference Text:  <S sid ="9" ssid = "9">If only Chinese characters are used, the subword-based IOB tagging is downgraded into a character-based one.</S><S sid ="109" ssid = "38">The effect of the confidence measure is shown in Table 3, where we used α = 0.7 and confidence threshold t = 0.8.</S><S sid ="51" ssid = "29">We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff.</S><S sid ="4" ssid = "4">By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 21 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['30'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['30'] | Citation Text:  <S sid ="30" ssid = "16">See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)</S> | Reference Offset:  ['1', '151', '0', '32'] | Reference Text:  <S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="0">Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</S><S sid ="32" ssid = "10">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training 193 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193–196, New York, June 2006.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 22 | Reference Article:  N06-2049.xml | Citing Article:  W10-4138.xml | Citation Marker Offset:  ['22'] | Citation Marker:  2006 | Citation Offset:  ['21','22'] | Citation Text:  <S sid ="21" ssid = "14">Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.</S><S sid ="22" ssid = "15">This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.</S><S sid ="23" ssid = "16">(2006) were proposed.</S> | Reference Offset:  ['131', '74', '68', '11'] | Reference Text:  <S sid ="131" ssid = "60">It proves the proposed word-based IOB tagging was very effective.</S><S sid ="74" ssid = "3">Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.</S><S sid ="68" ssid = "46">A new OOV was thus created.</S><S sid ="11" ssid = "11">We will give a detailed description of this approach in Section 2.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |