Citance Number: 1 | Reference Article:  C00-2123.xml | Citing Article:  C02-1050.xml | Citation Marker Offset:  ['41'] | Citation Marker:  2000 | Citation Offset:  ['41'] | Citation Text: <S sid ="39" ssid = "19">Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.</S><S sid ="40" ssid = "20">(1996), Och et al.</S><S sid ="41" ssid = "21">(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.</S> | Reference Offset:  ['15', '29', '122', '125'] | Reference Text:  <S sid ="15" ssid = "15">This approach is compared to another reordering scheme presented in (Berger et al., 1996).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S><S sid ="122" ssid = "85">Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</S><S sid ="125" ssid = "88">A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 2 | Reference Article:  C00-2123.xml | Citing Article:  C02-1050.xml | Citation Marker Offset:  ['8'] | Citation Marker:  Tillman and Ney, 2000 | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</S> | Reference Offset:  ['9', '25', '52', '2'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="52" ssid = "15">input: source string f1:::fj :::fJ initialization for each cardinality c = 1; 2; ; J do for each pair (C; j), where j 2 C and jCj = c do for each target word e 2 E Qe0 (e; C; j) = p(fj je) max Æ;e00 j02Cnfjg fp(jjj0; J) p(Æ) pÆ(eje0; e00) Qe00 (e0;C n fjg; j0)g words fj in the input string of length J. For the final translation each source position is considered exactly once.</S><S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 3 | Reference Article:  C00-2123.xml | Citing Article:  C02-1050.xml | Citation Marker Offset:  ['43'] | Citation Marker:  2000 | Citation Offset:  ['43'] | Citation Text:  <S sid ="43" ssid = "1">The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.</S> | Reference Offset:  ['2', '170', '192', '3'] | Reference Text:  <S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="192" ssid = "1">In this paper, we have presented a new, eÆcient DP-based search procedure for statistical machine translation.</S><S sid ="3" ssid = "3">A search restriction especially useful for the translation direction from German to English is presented.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 4 | Reference Article:  C00-2123.xml | Citing Article:  C02-1050.xml | Citation Marker Offset:  ['80'] | Citation Marker:  2000 | Citation Offset:  ['80'] | Citation Text:  <S sid ="80" ssid = "38">The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.</S> | Reference Offset:  ['42', '82', '119', '139'] | Reference Text:  <S sid ="42" ssid = "5">The resulting algorithm has a complexity of O(n!).</S><S sid ="82" ssid = "45">The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</S><S sid ="119" ssid = "82">The complexity of the quasimonotone search is O(E3 J (R2+LR)).</S><S sid ="139" ssid = "102">This approach leads to a search procedure with complexity O(E3 J4).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 5 | Reference Article:  C00-2123.xml | Citing Article:  C04-1091.xml | Citation Marker Offset:  ['22'] | Citation Marker:  Tillman and Ney, 2000 | Citation Offset:  ['22'] | Citation Text:  <S sid ="22" ssid = "22">Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).</S> |  Reference Offset:  ['170', '2', '9', '25'] | Reference Text:  <S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 6 | Reference Article:  C00-2123.xml | Citing Article:  E06-1004.xml | Citation Marker Offset:  ['22'] | Citation Marker:  Tillman, 2000 | Citation Offset:  ['22'] | Citation Text:  <S sid ="21" ssid = "21">â€¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).</S><S sid ="22" ssid = "22">1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).</S> | Reference Offset:  ['21', '29', '6', '9'] | Reference Text:  <S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S><S sid ="6" ssid = "6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 7 | Reference Article:  C00-2123.xml | Citing Article:  H01-1062.xml | Citation Marker Offset:  ['113'] | Citation Marker:  20 | Citation Offset:  ['113'] | Citation Text:  <S sid ="113" ssid = "16">To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: â€¢ single-word based approach [20];</S> | Reference Offset:  ['170', '16', '122', '183'] | Reference Text:  <S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="16" ssid = "16">In Section 4, we present the performance measures used and give translation results on the Verbmobil task.</S><S sid ="122" ssid = "85">Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</S><S sid ="183" ssid = "43">The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series, we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0, and this number is reported as the number of search errors.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 8 | Reference Article:  C00-2123.xml | Citing Article:  J03-1005.xml | Citation Marker Offset:  ['117'] | Citation Marker:  2000 | Citation Offset:  ['115','117'] | Citation Text:  <S sid ="115" ssid = "73">This article will present a DP-based beam search decoder for the IBM4 translation model.</S><S sid ="117" ssid = "75">A preliminary version of the work presented here was published in Tillmann and Ney (2000).</S> | Reference Offset:  ['170', '2', '1', '3'] | Reference Text:  <S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="1" ssid = "1">In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</S><S sid ="3" ssid = "3">A search restriction especially useful for the translation direction from German to English is presented.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 9 | Reference Article:  C00-2123.xml | Citing Article:  J04-2003.xml | Citation Marker Offset:  ['35'] | Citation Marker:  Tillmann and Ney 2000 | Citation Offset:  ['35'] | Citation Text:  <S sid ="35" ssid = "35">Many existing systems for statistical machine translation 1 1 (Garc´ıa-Varea and Casacuberta 2001; Germann et al. 2001; Nießen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.</S> | Reference Offset:  ['32', '9', '11', '25'] | Reference Text:  <S sid ="32" ssid = "16">The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun &apos;Zahnarztter min&apos; causes problems, because it must be translated by the two target words dentist&apos;s appointment.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="11" ssid = "11">The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 10 | Reference Article:  C00-2123.xml | Citing Article:  J04-2003.xml | Citation Marker Offset:  ['235'] | Citation Marker:  Tillmann and Ney 2000 | Citation Offset:  ['235'] | Citation Text:  <S sid ="235" ssid = "35">Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).</S> | Reference Offset:  ['9', '21', '25', '29'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 11 | Reference Article:  C00-2123.xml | Citing Article:  J04-4002.xml | Citation Marker Offset:  ['282'] | Citation Marker:  Tillmann and Ney 2000 | Citation Offset:  ['282'] | Citation Text:  <S sid ="282" ssid = "48">We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).</S> | Reference Offset:  ['9', '21', '25', '29'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 12 | Reference Article:  C00-2123.xml | Citing Article:  N03-1010.xml | Citation Marker Offset:  ['16'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['16'] | Citation Text:  <S sid ="16" ssid = "16">Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).</S> | Reference Offset:  ['2', '9', '25', '184'] | Reference Text:  <S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="184" ssid = "44">Depending on the threshold t0, the search algorithm may miss the globally optimal path which typically results in additional translation errors.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 13 | Reference Article:  C00-2123.xml | Citing Article:  P01-1027.xml | Citation Marker Offset:  ['127'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['127'] | Citation Text:  <S sid ="127" ssid = "34">We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.</S> | Reference Offset:  ['21', '25', '170', '27'] | Reference Text:  <S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="27" ssid = "11">What is important and is not expressed by the notation is the so-called coverage constraint: each source position j should be &apos;hit&apos; exactly once by the path of the inverted alignment bI 1 = b1:::bi:::bI . Using the inverted alignments in the maximum approximation, we obtain as search criterion: max I (p(JjI) max eI 1 ( I Yi=1 p(eijei􀀀1 i􀀀2) max bI 1 I Yi=1 [p(bijbi􀀀1; I; J) p(fbi jei)])) = = max I (p(JjI) max eI 1;bI 1 ( I Yi=1 p(eijei􀀀1 i􀀀2) p(bijbi􀀀1; I; J) p(fbi jei)])); where the two products over i have been merged into a single product over i. p(eijei􀀀1 i􀀀2) is the trigram language model probability.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 14 | Reference Article:  C00-2123.xml | Citing Article:  P03-1039.xml | Citation Marker Offset:  ['113'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['113'] | Citation Text:  <S sid ="113" ssid = "22">The decoding algorithm employed for this chunk + weight Ã— j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.</S> | Reference Offset:  ['1', '2', '13', '32'] | Reference Text:  <S sid ="1" ssid = "1">In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</S><S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="13" ssid = "13">In Section 2, we brie y review our approach to statistical machine translation.</S><S sid ="32" ssid = "16">The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun &apos;Zahnarztter min&apos; causes problems, because it must be translated by the two target words dentist&apos;s appointment.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 15 | Reference Article:  C00-2123.xml | Citing Article:  P03-1039.xml | Citation Marker Offset:  ['120'] | Citation Marker:  Tillman and Ney, 2000 | Citation Offset:  ['120'] | Citation Text:  <S sid ="120" ssid = "29">The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).</S> | Reference Offset:  ['25', '6', '9', '21'] | Reference Text:  <S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="6" ssid = "6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 16 | Reference Article:  C00-2123.xml | Citing Article:  W01-0505.xml | Citation Marker Offset:  ['13'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).</S> | Reference Offset:  ['9', '21', '25', '29'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 17 | Reference Article:  C00-2123.xml | Citing Article:  W01-1404.xml | Citation Marker Offset:  ['5'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['5'] | Citation Text:  <S sid ="5" ssid = "5">Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).</S> | Reference Offset:  ['9', '21', '25', '29'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 18 | Reference Article:  C00-2123.xml | Citing Article:  W01-1407.xml | Citation Marker Offset:  ['110'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['110'] | Citation Text:  <S sid ="110" ssid = "26">We used a translation system called â€œsingle- word based approachâ€ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).</S> | Reference Offset:  ['170', '9', '21', '25'] | Reference Text:  <S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 19 | Reference Article:  C00-2123.xml | Citing Article:  W01-1408.xml | Citation Marker Offset:  ['47'] | Citation Marker:  Tillmann, 2001; Tillmann and Ney, 2000 | Citation Offset:  ['47'] | Citation Text:  <S sid ="47" ssid = "23">Search algorithms We evaluate the following two search algorithms: â€¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.</S> | Reference Offset:  ['2', '21', '116', '137'] | Reference Text:  <S sid ="2" ssid = "2">Starting from a DP-based solution to the traveling salesman problem, we present a novel technique to restrict the possible word reordering between source and target language in order to achieve an eÆcient search algorithm.</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="116" ssid = "79">The following recursive equation is evaluated: Qe0 (e; S; C; j) = (2) = p(fj je) max Æ;e00 np(jjj0; J) p(Æ) pÆ(eje0; e00) max (S0;j0) (S0 ;Cnfjg;j0)!(S;C;j) j02Cnfjg Qe00 (e0; S0;C n fjg; j0)o: The search ends in the hypotheses (I; f1; ; Jg; j).</S><S sid ="137" ssid = "100">In this case, we have no finite-state restrictions for the search space.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 20 | Reference Article:  C00-2123.xml | Citing Article:  W02-1020.xml | Citation Marker Offset:  ['62'] | Citation Marker:  Tillmann and Ney, 2000 | Citation Offset:  ['61','62'] | Citation Text:  <S sid ="61" ssid = "19">It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J —for in p(v1, w2</S><S sid ="62" ssid = "20">, wm−1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).</S> | Reference Offset:  ['9', '21', '25', '33'] | Reference Text:  <S sid ="9" ssid = "9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jaj􀀀1; I; J), where the probability of alignment aj for position j depends on the previous alignment position aj􀀀1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="33" ssid = "17">We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |