Citance Number: 1 | Reference Article:   P05-1004.xml | Citing Article:  C10-2101.xml | Citation Marker Offset:  ['74'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['74'] | Citation Text:  <S sid ="74" ssid = "44">Another related task is supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006).</S> | Reference Offset:  ['3', '20', '68', '14'] | Reference Text:  <S sid ="3" ssid = "3">Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.</S><S sid ="20" ssid = "20">Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.</S><S sid ="68" ssid = "1">Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.</S><S sid ="14" ssid = "14">Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.</S> | Discourse Facet:  ['Implication_Citation', 'Method_Citation'] | Annotator:  CIST |


Citance Number: 2 | Reference Article:   P05-1004.xml | Citing Article:  E09-1045.xml | Citation Marker Offset:  ['23'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['23'] | Citation Text:  <S sid ="23" ssid = "23">In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).</S> | Reference Offset:  ['3', '65', '73', '171'] | Reference Text:  <S sid ="3" ssid = "3">Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.</S><S sid ="65" ssid = "18">Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.</S><S sid ="73" ssid = "6">The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).</S><S sid ="171" ssid = "1">We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 3 | Reference Article:   P05-1004.xml | Citing Article:  J07-4005.xml | Citation Marker Offset:  ['229'] | Citation Marker:  2005 | Citation Offset:  ['229'] | Citation Text:  <S sid ="229" ssid = "72">Although we could adapt our method for use with an automatically induced inventory, our method which uses WordNet might also be combined with one that can automatically find new senses from text and then relate these to WordNet synsets, as Ciaramita and Johnson (2003) and Curran (2005) do with unknown nouns.</S> | Reference Offset:  ['20', '36', '147', '2'] | Reference Text:  <S sid ="20" ssid = "20">Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.</S><S sid ="36" ssid = "10">There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.</S><S sid ="147" ssid = "1">Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.</S><S sid ="2" ssid = "2">Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 4 | Reference Article:   P05-1004.xml | Citing Article:  J09-3004.xml | Citation Marker Offset:  ['446'] | Citation Marker:  Curran 2005 | Citation Offset:  ['446'] | Citation Text:  <S sid ="446" ssid = "30">An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.</S> | Reference Offset:  ['1', '18', '92', '96'] | Reference Text:  <S sid ="1" ssid = "1">The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.</S><S sid ="18" ssid = "18">These problems demonstrate the need for automatic or semiautomatic methods for the creation and maintenance of lexical-semantic resources.</S><S sid ="92" ssid = "6">Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.</S><S sid ="96" ssid = "10">Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette’s weighted JACCARD measure performed the best: R E L AT I O N D E S C R I P T I O N adj noun–adjectival modifier relation dobj verb–direct object relation iobj verb–indirect object relation nn noun–noun modifier relation nnprep noun–prepositional head relation subj verb–subject relation Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001) – and is very efficient, analysing over 80 000 words per second.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 5 | Reference Article:   P05-1004.xml | Citing Article:  N06-1017.xml | Citation Marker Offset:  ['26'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['26'] | Citation Text:  <S sid ="26" ssid = "26">There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection.</S> | Reference Offset:  ['1', '108', '147', '42'] | Reference Text:  <S sid ="1" ssid = "1">The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.</S><S sid ="108" ssid = "22">Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).</S><S sid ="147" ssid = "1">Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a super- sense for the unknown nouns.</S><S sid ="42" ssid = "16">Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 6 | Reference Article:   P05-1004.xml | Citing Article:  N06-1017.xml | Citation Marker Offset:  ['189'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['189'] | Citation Text:  <S sid ="189" ssid = "11">Possibilities include associating items with similar existing senses (Widdows, 2003; Curran, 2005; Burchardt et al., 2005) or clustering them into approximate senses.</S> | Reference Offset:  ['42', '106', '108', '26'] | Reference Text:  <S sid ="42" ssid = "16">Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.</S><S sid ="106" ssid = "20">JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen’s configuration for our super- sense tagging experiments.</S><S sid ="108" ssid = "22">Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).</S><S sid ="26" ssid = "26">Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 7 | Reference Article:   P05-1004.xml | Citing Article:  N07-1024.xml | Citation Marker Offset:  ['83'] | Citation Marker:  Curran 2005 | Citation Offset:  ['83'] | Citation Text:  <S sid ="83" ssid = "3">While contextual information is the primary source of information used in WSD research and has been used for acquiring semantic lexicons and classifying unknown words in other languages (e.g., Roark and Charniak 1998; Ci aramita 2003; Curran 2005)</S> | Reference Offset:  ['47', '108', '1', '2'] | Reference Text:  <S sid ="47" ssid = "21">Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.</S><S sid ="108" ssid = "22">Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).</S><S sid ="1" ssid = "1">The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.</S><S sid ="2" ssid = "2">Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 8 | Reference Article:   P05-1004.xml | Citing Article:  P12-2050.xml | Citation Marker Offset:  ['15'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['15'] | Citation Text:  <S sid ="15" ssid = "15">More re cently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; PaaÃŸ and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNetsmapped to English WordNet.3 In principle, we be lieve supersenses ought to apply to nouns and verbsin any language, and need not depend on the avail ability of a semantic lexicon.4 In this work we focuson the noun SSTs, summarized in figure 2 and ap plied to an Arabic sentence in figure 1.</S> | Reference Offset:  ['20', '68', '14', '19'] | Reference Text:  <S sid ="20" ssid = "20">Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.</S><S sid ="68" ssid = "1">Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.</S><S sid ="14" ssid = "14">Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.</S><S sid ="19" ssid = "19">Broad semantic classification is currently used by lexicographers to or- ganise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 9 | Reference Article:   P05-1004.xml | Citing Article:  S07-1032.xml | Citation Marker Offset:  ['16'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['16'] | Citation Text:  <S sid ="16" ssid = "16">Thus, some research has been focused on deriving different sense groupings to overcome the fineâ€“ grained distinctions of WN (Hearst and SchuÂ¨ tze, 1993) (Peters et al., 1998) (Mihalcea and Moldo- van, 2001) (Agirre et al., 2003) and on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997) (Ciaramita and Johnson, 2003) (Villarejo et al., 2005) (Curran, 2005) (Ciaramita and Altun, 2006).</S> | Reference Offset:  ['65', '171', '225', '42'] | Reference Text:  <S sid ="65" ssid = "18">Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.</S><S sid ="171" ssid = "1">We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).</S><S sid ="225" ssid = "1">Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu¨ tze (1993) and Widdows (2003).</S><S sid ="42" ssid = "16">Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 10 | Reference Article:   P05-1004.xml | Citing Article:  S10-1090.xml | Citation Marker Offset:  ['16'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['16'] | Citation Text:  <S sid ="16" ssid = "16">In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).</S> | Reference Offset:  ['3', '65', '73', '171'] | Reference Text:  <S sid ="3" ssid = "3">Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.</S><S sid ="65" ssid = "18">Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.</S><S sid ="73" ssid = "6">The WORD- NET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).</S><S sid ="171" ssid = "1">We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 11 | Reference Article:   P05-1004.xml | Citing Article:  S12-1011.xml | Citation Marker Offset:  ['6'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['6'] | Citation Text:  <S sid ="6" ssid = "6">Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005).</S> | Reference Offset:  ['96', '108', '26', '92'] | Reference Text:  <S sid ="96" ssid = "10">Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette’s weighted JACCARD measure performed the best: R E L AT I O N D E S C R I P T I O N adj noun–adjectival modifier relation dobj verb–direct object relation iobj verb–indirect object relation nn noun–noun modifier relation nnprep noun–prepositional head relation subj verb–subject relation Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001) – and is very efficient, analysing over 80 000 words per second.</S><S sid ="108" ssid = "22">Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).</S><S sid ="26" ssid = "26">Qc 2005 Association for Computational Linguistics L E X -FI L E D E S C R I P T I O N act acts or actions animal animals artifact man-made objects attribute attributes of people and objects body body parts cognition cognitive processes and contents communication communicative processes and contents event natural events feeling feelings and emotions food foods and drinks group groupings of people or objects location spatial position motive goals object natural objects (not man-made) person people phenomenon natural phenomena plant plants possession possession and transfer of possession process natural processes quantity quantities and units of measure relation relations between people/things/ideas shape two and three dimensional shapes state stable states of affairs substance substances time time and temporal relations Table 1: 25 noun lexicographer files in WORDNET</S><S sid ="92" ssid = "6">Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 12 | Reference Article:   P05-1004.xml | Citing Article:  S12-1011.xml | Citation Marker Offset:  ['50'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['50'] | Citation Text:  <S sid ="50" ssid = "2">Supersense tagging (Ciaramita and Johnson, 2003; Curran, 2005) evaluates a modelâ€™s ability to cluster words by their semantics.</S> | Reference Offset:  ['20', '68', '225', '3'] | Reference Text:  <S sid ="20" ssid = "20">Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.</S><S sid ="68" ssid = "1">Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.</S><S sid ="225" ssid = "1">Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu¨ tze (1993) and Widdows (2003).</S><S sid ="3" ssid = "3">Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 13 | Reference Article:   P05-1004.xml | Citing Article:  S12-1023.xml | Citation Marker Offset:  ['234'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['234'] | Citation Text:  <S sid ="234" ssid = "15">A concept analogous to our notion of meta sense (i.e., senses beyond single words) has been used in previous work on class-based WSD (Yarowsky, 1992; Curran, 2005; Izquierdo et al., 2009), and indeed, the CAM might be used for class-based WSD as well.</S> | Reference Offset:  ['65', '108', '6', '12'] | Reference Text:  <S sid ="65" ssid = "18">Ciaramita and Johnson (2003) implement a super- sense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.</S><S sid ="108" ssid = "22">Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).</S><S sid ="6" ssid = "6">Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).</S><S sid ="12" ssid = "12">Also, lexical- semantic resources suffer from: bias towards concepts and senses from particular topics.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 14 | Reference Article:   P05-1004.xml | Citing Article:  W06-1670.xml | Citation Marker Offset:  ['94'] | Citation Marker:  Curran, 2005 | Citation Offset:  ['94'] | Citation Text:  <S sid ="94" ssid = "12">Previous work on prediction at the supersense level (Ciaramita and Johnson, 2003; Curran, 2005) has focused on lexical acquisition (nouns exclusively), thus aiming at word type classification rather than tagging.</S> | Reference Offset:  ['20', '68', '225', '14'] | Reference Text:  <S sid ="20" ssid = "20">Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET’s hierarchical structure to create many annotated training instances from the synset glosses.</S><S sid ="68" ssid = "1">Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.</S><S sid ="225" ssid = "1">Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Schu¨ tze (1993) and Widdows (2003).</S><S sid ="14" ssid = "14">Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |