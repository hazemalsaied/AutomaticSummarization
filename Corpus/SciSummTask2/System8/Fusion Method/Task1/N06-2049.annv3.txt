Citance Number: 1 | Reference Article:  N06-2049.xml | Citing Article:  C10-2139.xml | Citation Marker Offset:  ['290'] | Citation Marker:  Zh ang et al., 200 6 | Citation Offset:  ['290','305'] | Citation Text:  <S sid ="290" ssid = "189">AS C U MS R PK U (Zh ang et al., 200 6) 95.</S><S sid ="305" ssid = "204">2 Table 5: Segmentation performance presented in previous work and of our combination model.</S> | Reference Offset:  ['80', '111', '151', '62'] | Reference Text:  <S sid ="80" ssid = "9">Table 1 shows the performance of the dictionary-based segmentation.</S><S sid ="111" ssid = "40">We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="62" ssid = "40">Each subword is given a prior IOB tag, tw . C Miob (t|w), a  M  confidence probability derived in the process of IOB tag exp )&apos; )&apos; λk fk (ti−1 , ti , W ) + )&apos; µk gk (ti , W ) /Z,  i=1  k k   (1) ging, is defined as Z = )&apos; T =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) T =t 0 t1 ··· tM P ( T|W ) where we call fk (ti−1 , ti , W ) bigram feature functions because the features trigger the previous observation ti−1 where the numerator is a sum of all the observation sequences with word wi labeled as t. δ(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 2 | Reference Article:  N06-2049.xml | Citing Article:  D13-1031.xml | Citation Marker Offset:  ['274'] | Citation Marker:  2006 | Citation Offset:  ['273','274'] | Citation Text:  <S sid ="273" ssid = "109">CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.</S><S sid ="274" ssid = "110">(2006).</S> | Reference Offset:  ['33', '152', '25', '14'] | Reference Text:  <S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S><S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 3 | Reference Article:  N06-2049.xml | Citing Article:  I08-4009.xml | Citation Marker Offset:  ['19'] | Citation Marker:  Zhang et al., 2006a | Citation Offset:  ['19'] | Citation Text:  <S sid ="19" ssid = "19">Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).</S> | Reference Offset:  ['14', '3', '5', '2'] | Reference Text:  <S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="3" ssid = "3">In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates.</S><S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="2" ssid = "2">We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 4 | Reference Article:  N06-2049.xml | Citing Article:  I08-4009.xml | Citation Marker Offset:  ['20'] | Citation Marker:  Zhang et al., 2006a | Citation Offset:  ['20'] | Citation Text:  <S sid ="20" ssid = "20">Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).</S> | Reference Offset:  ['57', '58', '150', '71'] | Reference Text:  <S sid ="57" ssid = "35">In this section we introduce a confidence measure approach to combine the two results.</S><S sid ="58" ssid = "36">We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.</S><S sid ="150" ssid = "19">By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result, we could achieve the optimal performance.</S><S sid ="71" ssid = "49">In Section 3.2 we will present the experimental segmentation results of the confidence measure approach.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 5 | Reference Article:  N06-2049.xml | Citing Article:  I08-4015.xml | Citation Marker Offset:  ['36'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['36'] | Citation Text:  <S sid ="36" ssid = "22">After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006).</S> | Reference Offset:  ['25', '101', '14', '58'] | Reference Text:  <S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="101" ssid = "30">The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="58" ssid = "36">We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 6 | Reference Article:  N06-2049.xml | Citing Article:  I08-4030.xml | Citation Marker Offset:  ['13'] | Citation Marker:  R. Zhang et al., 2006 | Citation Offset:  ['13'] | Citation Text:  <S sid ="13" ssid = "13">If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006).</S> | Reference Offset:  ['58', '59', '67', '62'] | Reference Text:  <S sid ="58" ssid = "36">We define a confidence measure, C M(tiob |w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary-based segmentation.</S><S sid ="59" ssid = "37">The confidence measure comes from two sources: IOB tagging and dictionary- based word segmentation.</S><S sid ="67" ssid = "45">If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.</S><S sid ="62" ssid = "40">Each subword is given a prior IOB tag, tw . C Miob (t|w), a  M  confidence probability derived in the process of IOB tag exp )&apos; )&apos; λk fk (ti−1 , ti , W ) + )&apos; µk gk (ti , W ) /Z,  i=1  k k   (1) ging, is defined as Z = )&apos; T =t0 t1 ···tM p(T |W ) C Miob (t|w ) = L,T =t0 t1 ···tM ,ti =t P(T |W, wi ) T =t 0 t1 ··· tM P ( T|W ) where we call fk (ti−1 , ti , W ) bigram feature functions because the features trigger the previous observation ti−1 where the numerator is a sum of all the observation sequences with word wi labeled as t. δ(tw , tiob )ng denotes the contribution of the dictionary- based segmentation.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 7 | Reference Article:  N06-2049.xml | Citing Article:  I08-4030.xml | Citation Marker Offset:  ['51'] | Citation Marker:  R. Zhang et al., 2006 | Citation Offset:  ['51'] | Citation Text:  <S sid ="51" ssid = "7">According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing.</S> | Reference Offset:  ['14', '134', '44', '25'] | Reference Text:  <S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="134" ssid = "3">Later, this approach was implemented by the CRF-based method (Peng and McCallum, 2004), which was proved to achieve better results than the maximum entropy approach because it can solve the label bias problem (Lafferty et al., 2001).</S><S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S><S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 8 | Reference Article:  N06-2049.xml | Citing Article:  J11-1005.xml | Citation Marker Offset:  ['277'] | Citation Marker:  2006 | Citation Offset:  ['277'] | Citation Text:  <S sid ="277" ssid = "168">We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang, Kikui, and Sumita (2006) for comparison.</S> | Reference Offset:  ['129', '14', '89', '4'] | Reference Text:  <S sid ="129" ssid = "58">95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="89" ssid = "18">The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.</S><S sid ="4" ssid = "4">By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 9 | Reference Article:  N06-2049.xml | Citing Article:  N09-1007.xml | Citation Marker Offset:  ['134'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['134'] | Citation Text:  <S sid ="134" ssid = "17">Z06-a and Z06-b represents the pure sub- word CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006);</S> |  Reference Offset:  ['44', '152', '5', '25'] | Reference Text:  <S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S><S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S><S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S> | Discourse Facet:  Results_Citation | Annotator:  CIST |


Citance Number: 10 | Reference Article:  N06-2049.xml | Citing Article:  P06-2123.xml | Citation Marker Offset:  ['197'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['197'] | Citation Text:  <S sid ="197" ssid = "75">Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).</S> | Reference Offset:  ['7', '104', '2', '14'] | Reference Text:  <S sid ="7" ssid = "7">We found that so far all the existing implementations were using character-based IOB tagging.</S><S sid ="104" ssid = "33">Comparing Table 1 and 2, we found the CRF-modeled IOB tagging yielded better segmentation than the dictionary- based approach.</S><S sid ="2" ssid = "2">We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 11 | Reference Article:  N06-2049.xml | Citing Article:  P07-1106.xml | Citation Marker Offset:  ['94'] | Citation Marker:  2006 | Citation Offset:  ['93','94'] | Citation Text:  <S sid ="93" ssid = "9">One existing method that is based on sub-word information, Zhang et al.</S><S sid ="94" ssid = "10">(2006), combines a C R F and a rule-based model.</S> | Reference Offset:  ['152', '45', '99', '57'] | Reference Text:  <S sid ="152" ssid = "2">Using the CRFs approaches, we prove that it outperformed the character- based method using the CRF approaches.</S><S sid ="45" ssid = "23">The model parameters were trained by maximizing the log-likelihood of the training data using L-BFGS gradient descent optimization method.</S><S sid ="99" ssid = "28">The upper numbers are of the character- based and the lower ones, the subword-based.</S><S sid ="57" ssid = "35">In this section we introduce a confidence measure approach to combine the two results.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 12 | Reference Article:  N06-2049.xml | Citing Article:  P07-1106.xml | Citation Marker Offset:  ['172'] | Citation Marker:  2006 | Citation Offset:  ['172','173'] | Citation Text:  <S sid ="172" ssid = "72">We chose the three models that achieved at least one best score in the closed tests from Emerson (2005), as well as the sub-word-based model of Zhang et al.</S><S sid ="173" ssid = "73">(2006) for comparison.</S> | Reference Offset:  ['129', '14', '4', '89'] | Reference Text:  <S sid ="129" ssid = "58">95 1 Table 4: Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="4" ssid = "4">By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005.</S><S sid ="89" ssid = "18">The segmentation results of the dictionary-based were re-segmented Table 1: Our segmentation results by the dictionary- based approach for the closed test of Bakeoff 2005, very low R-oov rates due to no OOV recognition applied.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 13 | Reference Article:  N06-2049.xml | Citing Article:  P12-1027.xml | Citation Marker Offset:  ['189'] | Citation Marker:  2006 | Citation Offset:  ['188'] | Citation Text:  <S sid ="188" ssid = "41">Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.</S><S sid ="189" ssid = "42">(2006).</S> | Reference Offset:  ['33', '1', '25', '61'] | Reference Text:  <S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="25" ssid = "3">It is composed of three parts: a dictionary-based N-gram word segmentation for segmenting IV words, a subword- based tagging by the CRF for recognizing OOVs, and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging.</S><S sid ="61" ssid = "39">After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 14 | Reference Article:  N06-2049.xml | Citing Article:  W06-0118.xml | Citation Marker Offset:  ['14'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['14'] | Citation Text:  <S sid ="14" ssid = "2">Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006)</S> | Reference Offset:  ['100', '101', '30', '14'] | Reference Text:  <S sid ="100" ssid = "29">using the FMM, and then labeled with “IOB” tags by the CRFs.</S><S sid ="101" ssid = "30">The segmentation results using CRF tagging are shown in Table 2, where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based.</S><S sid ="30" ssid = "8">2.1 Subword-based IOB tagging using CRFs.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 15 | Reference Article:  N06-2049.xml | Citing Article:  W06-0118.xml | Citation Marker Offset:  ['24'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['24'] | Citation Text:  <S sid ="24" ssid = "12">Recently (Zhang et al., 2006) proposed a maximum subword-based IOB tagger for Chinese word segmentation, and our system applies their approach which obtains a very high accuracy on the shared task data from previous SIGHAN competitions.</S> | Reference Offset:  ['151', '1', '5', '43'] | Reference Text:  <S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="43" ssid = "21">In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 16 | Reference Article:  N06-2049.xml | Citing Article:  W08-0335.xml | Citation Marker Offset:  ['55'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['55','56'] | Citation Text: <S sid ="55" ssid = "21">Part of the work using this tool was described by (Zhang et al., 2006).</S><S sid ="56" ssid = "22">The approach was reported to achieve the highest word segmentation accuracy using the data from the second Sighan Bakeoff.</S> | Reference Offset:  ['111', '72', '14', '44'] | Reference Text:  <S sid ="111" ssid = "40">We found the results in Table 3 were better than those in Table 2 and Table 1, which prove that using confidence measure approach achieved the best performance over the dictionary-based segmentation and the IOB tagging approach.</S><S sid ="72" ssid = "1">We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="44" ssid = "22">We downloaded and used the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” According to the CRFs, the probability of an IOB tag sequence, T = t0 t1 · · · tM , given the word sequence, W = w0 w1 · · · wM , is defined by p(T |W ) = and current observation ti simultaneously; gk (ti , W ), the unigram feature functions because they trigger only current observation ti . λk and µk are the model parameters corresponding to feature functions fk and gk respectively.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 17 | Reference Article:  N06-2049.xml | Citing Article:  W08-0335.xml | Citation Marker Offset:  ['160'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['160'] | Citation Text:  <S sid ="160" ssid = "19">Note a lexicon and a LM are the only needed resources for building a dictionary-based CWS, like the â€œdict-hybrid.â€ (Zhang et al., 2006) We used the â€œdict-hybridâ€ to segment the SMT training corpus and test data.</S> | Reference Offset:  ['43', '33', '78', '72'] | Reference Text:  <S sid ="43" ssid = "21">In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.</S><S sid ="33" ssid = "11">Qc 2006 Association for Computational Linguistics input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based segmentation 咘/% 㣅/, ᯹/, ԣ/2 ೼/2 ࣫Ҁ/% Ꮦ/, +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process data.</S><S sid ="78" ssid = "7">For the dictionary-based approach, we extracted a word list from the training data as the vocabulary.</S><S sid ="72" ssid = "1">We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.</S> | Discourse Facet:  ['Method_Citation', 'Results_Citation'] | Annotator:  CIST |


Citance Number: 18 | Reference Article:  N06-2049.xml | Citing Article:  W10-4128.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">Some previous work (Peng et al., 2004; Tseng et al., 2005; Low et al., 2005) illustrated the effectiveness of using characters as tagging units, while literatures (Zhang et al., 2006; Zhao and Kit, 2007a; Zhang and Clark, 2007) focus on employing lexical words or subwords as tagging units.</S> | Reference Offset:  ['8', '88', '5', '61'] | Reference Text:  <S sid ="8" ssid = "8">In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.</S><S sid ="88" ssid = "17">For the subword-based tagging, we added another 2000 most frequent multiple- character words to the lexicons for tagging.</S><S sid ="5" ssid = "5">The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005).</S><S sid ="61" ssid = "39">After the dictionary- based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 19 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004).</S> | Reference Offset:  ['1', '151', '19', '0'] | Reference Text:  <S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="19" ssid = "19">In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.</S><S sid ="0">Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 20 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['25'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['25'] | Citation Text:  <S sid ="25" ssid = "11">Feature Template Description f) in(str, subword-list) is str in subword list g) in(str, confident-word-list) is str in confident-word list Table 2: Subword Features for CRF-based Segmenter dure for constructing a subword list is similar to the one used in (Zhang et al., 2006).</S> | Reference Offset:  ['51', '47', '43', '50'] | Reference Text:  <S sid ="51" ssid = "29">We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff.</S><S sid ="47" ssid = "25">The types of unigram features used in our experiments included the following types: w0 , w−1 , w1 , w−2 , w2 , w0 w−1 , w0 w1 , w−1 w1 , w−2 w−1 , w2 w0 where w stands for word.</S><S sid ="43" ssid = "21">In the third step, we used the CRFs approach to train the IOB tagger (Lafferty et al., 2001) on the training data.</S><S sid ="50" ssid = "28">For the bigram features, we only used the previous and the current observations, t−1 t0 . As to feature selection, we simply used absolute counts for each feature in the training data.</S> | Discourse Facet:  ['Method_Citation', 'Implication_Citation'] | Annotator:  CIST |


Citance Number: 21 | Reference Article:  N06-2049.xml | Citing Article:  W10-4135.xml | Citation Marker Offset:  ['30'] | Citation Marker:  Zhang et al., 2006 | Citation Offset:  ['30'] | Citation Text:  <S sid ="30" ssid = "16">See the details of subword-based Chinese word segmentation in (Zhang et al., 2006)</S> | Reference Offset:  ['1', '151', '0', '19'] | Reference Text:  <S sid ="1" ssid = "1">We proposed two approaches to improve Chinese word segmentation: a subword-based tagging and a confidence measure approach.</S><S sid ="151" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S><S sid ="0">Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</S><S sid ="19" ssid = "19">In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is implemented by the CRFs method.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 22 | Reference Article:  N06-2049.xml | Citing Article:  W10-4138.xml | Citation Marker Offset:  ['22'] | Citation Marker:  2006 | Citation Offset:  ['21','22'] | Citation Text:  <S sid ="21" ssid = "14">Thus, the bigram â€œRAIL ENQUIRIESâ€ gives a misleading probability that â€œRAILâ€ is followed by â€œENQUIRIESâ€ irrespective of what precedes it.</S><S sid ="22" ssid = "15">This problem happens not only with word-token corpora but also with corpora in which all the compounds are tagged as units since overlapping N- grams still appear, therefore corresponding solutions such as those of Zhang et al.</S><S sid ="23" ssid = "16">(2006) were proposed.</S> | Reference Offset:  ['74', '131', '14', '139'] | Reference Text:  <S sid ="74" ssid = "3">Since this work was to evaluate the proposed subword-based IOB tagging, we carried out the closed test only.</S><S sid ="131" ssid = "60">It proves the proposed word-based IOB tagging was very effective.</S><S sid ="14" ssid = "14">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower.</S><S sid ="139" ssid = "8">We think our proposed subword- based tagging played an important role for the good results.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |