This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available. 
Our application of semantic similarity to supersense tagging follows earlier work by Hearst and SchuÂ¨ tze. 
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity. 
One solution would be to take the intersection between vectors across words for each supersense. 
Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor. 
An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words. 
We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows  using latent semantic analysis. 
This has the advantage of producing a much smaller number of vectors to compare against. 
Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson. 
Ciaramita and Johnson  present a tagger which uses synonym set glosses as annotated training examples. 
The question now becomes how to construct vectors of supersenses. 
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD. 
Further, our similarity system does not currently incorporate multi-word terms. 
The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words. 
