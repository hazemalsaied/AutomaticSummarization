The translations of 6 of the 43 words are words in the dictionary (denoted as ÃḃÂÂcomm.ÃḃÂÂ in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as ÃḃÂÂinsuffÃḃÂÂ). lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. New words such as person names, organization names, technical terms, etc. appear frequently. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities. It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. For the training of transliteration probability, we required a ChineseEnglish name list. We use backoff and linear interpolation for probability estimation: P(tc | Tc (C (e))) = Ãáẁ ÃḃÂÂ Pml (tc | Tc (C (e))) + (1 ÃḃÂÂÃáẁ ) ÃḃÂÂ Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify
