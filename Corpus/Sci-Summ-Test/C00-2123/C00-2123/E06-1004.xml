<PAPER>
	<ABSTRACT>
	<SECTION title="Introduction. " number = "1">
			<S sid ="1" ssid = "1">Statistical Machine Translation is a data driven machine translation technique which uses probabilistic models of natural language for automatic The probability distributions P r(f |e) andP r(e) are known as translation model and lan guage model respectively.</S>
			<S sid ="2" ssid = "2">In the classic work on SMT, Brown and his colleagues at IBM introduced the notion of alignment between a sentence f and its translation e and used it in the development of translation models (Brown et al., 1993).</S>
			<S sid ="3" ssid = "3">An alignment between f = f1 . . .</S>
			<S sid ="4" ssid = "4">fm and e = e1 . . .</S>
			<S sid ="5" ssid = "5">el is a many-to-one mapping a : {1, . . .</S>
			<S sid ="6" ssid = "6">, m} → {0, . . .</S>
			<S sid ="7" ssid = "7">, l}.</S>
			<S sid ="8" ssid = "8">Thus, an alignment a between f and e associates the french word fj to the English word translation (Brown et al., 1993), (AlOnaizan et al., 1999).</S>
			<S sid ="9" ssid = "9">The parameters of the models are estimated by iterative maximum-likelihood training on a large parallel corpus of natural language texts using the EM algorithm (Brown et al., 1993).</S>
			<S sid ="10" ssid = "10">The models are then used to decode, i.e. translate texts from the source language to the target eaj a is called the fertility of ei and is denoted by φi . Since P r(f |e) = �a P r(f , a|e), equation 1 can 1 In this paper, we use French and English as the prototypical examples of source and target languages respectively.</S>
			<S sid ="11" ssid = "11">2 e0 is a special word called the null word and is used to account for those words in f that are not connected by a to any of the words of e. be rewritten as follows: e∗ = argmax ) P r(f , a|e)P r(e).</S>
			<S sid ="12" ssid = "12">(2) • Relaxed Decoding Given the model parameters and a sentence f , e determine the most probable translation and a alignment pair for f . Brown and his colleagues developed a series of 5 translation models which have become to be known in the field of machine translation as IBM models.</S>
			<S sid ="13" ssid = "13">For a detailed introduction to IBM translation models, please see (Brown et al., 1993).</S>
			<S sid ="14" ssid = "14">In practice, models 35 are known to give good results and models 12 are used to seed the EM iterations of the higher models.</S>
			<S sid ="15" ssid = "15">IBM model 3 is the prototypical translation model and it models P r(f , a|e) as follows: (e∗, a∗) = argmax P (f , a|e) P (e) (e,a) Viterbi Alignment computation finds applications not only in SMT but also in other areas of Natural Language Processing (Wang, 1998), (Marcu, 2002).</S>
			<S sid ="16" ssid = "16">Expectation Evaluation is the soul of parameter estimation (Brown et al., 1993), (AlOnaizan et al., 1999).</S>
			<S sid ="17" ssid = "17">Conditional Probability computation is important in experimentally studying the concentration of the probability mass P (f , a|e) ≡ n(φ0 | �l \ l i=1 n(φi|ei ) φi! around the Viterbi alignment, i.e. in determining j=1 t fj |eaj × dj: aj I=0 d(j|i, m, l) the goodness of the Viterbi alignment in compar ison to the rest of the alignments.</S>
			<S sid ="18" ssid = "18">Decoding is an integral component of all SMT systems (Wang, Table 1: IBM Model 3 Here, n(φ|e) is the fertility model, t(f |e) is the lexicon model and d(j|i, m, l) is the distortion model.</S>
			<S sid ="19" ssid = "19">The computational tasks involving IBM Models are the following: • Viterbi Alignment Given the model parameters and a sentence pair (f , e), determine the most probable alignment between f and e. a∗ = argmax P (f , a|e) a • Expectation Evaluation This forms the core of model training via the EM algorithm.</S>
			<S sid ="20" ssid = "20">Please see Section 2.3 for a description of the computational task involved in the EM iterations.</S>
			<S sid ="21" ssid = "21">• Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f |e).</S>
			<S sid ="22" ssid = "22">1997), (Tillman, 2000), (Och et al., 2001), (Germann et al., 2003), (Udupa et al., 2004).</S>
			<S sid ="23" ssid = "23">Exact Decoding is the original decoding problem as defined in (Brown et al., 1993) and Relaxed Decoding is the relaxation of the decoding problem typically used in practice.</S>
			<S sid ="24" ssid = "24">While several heuristics have been developed by practitioners of SMT for the computational tasks involving IBM models, not much is known about the computational complexity of these tasks.</S>
			<S sid ="25" ssid = "25">In their seminal paper on SMT, Brown and his colleagues highlighted the problems we face as we go from IBM Models 12 to 35(Brown et al., 1993) 3: “As we progress from Model 1 to Model 5, evaluating the expectations that gives us counts becomes increasingly difficult.</S>
			<S sid ="26" ssid = "26">In Models 3 and 4, we must be content with approximate EM iterations because it is not feasible to carry out sums over all possible alignments for these models.</S>
			<S sid ="27" ssid = "27">In practice, we are never sure that we have found the Viterbi alignment”.</S>
			<S sid ="28" ssid = "28">However, neither their work nor the subsequent P (f |e) = ) a • Exact Decoding P (f , a|e) research in SMT studied the computational complexity of these fundamental problems with the exception of the Decoding problem.</S>
			<S sid ="29" ssid = "29">In (Knight, 1999) it was proved that the Exact Decoding prob Given the model parameters and a sentence f , determine the most probable translation of f . lem is NP-Hard when the language model is a bi- gram model.</S>
			<S sid ="30" ssid = "30">e∗ = argmax e ) P (f , a|e) P (e) a Our results may be summarized as follows: 3 The emphasis is ours..</S>
			<S sid ="31" ssid = "31">1.</S>
			<S sid ="32" ssid = "32">Viterbi Alignment computation is NP-Hard.</S>
			<S sid ="33" ssid = "33">for IBM Models 3, 4, and 5.</S>
	</SECTION>
	<SECTION title="Expectation  Evaluation  in EM Iterations is. " number = "2">
			<S sid ="34" ssid = "1">#P-Complete for IBM Models 3, 4, and 5.</S>
	</SECTION>
	<SECTION title="Conditional    Probability   computation   is. " number = "3">
			<S sid ="35" ssid = "1">#P-Complete for IBM Models 3, 4, and 5.</S>
	</SECTION>
	<SECTION title="Exact Decoding is #P-Hard for IBM Mod-. " number = "4">
			<S sid ="36" ssid = "1">els 3, 4, and 5.</S>
	</SECTION>
	<SECTION title="Relaxed  Decoding  is  NP-Hard   for  IBM. " number = "5">
			<S sid ="37" ssid = "1">Models 3, 4, and 5.</S>
			<S sid ="38" ssid = "2">lies in the class #P, where p(.)</S>
			<S sid ="39" ssid = "3">is a polynomial.</S>
			<S sid ="40" ssid = "4">Given functions f, g : Σ∗ → N, we say that g is polynomial-time Turing reducible to f (i.e. g ≤T f ) if there is a Turing machine with an oracle for f that computes g in time polynomial in the size of the input.</S>
			<S sid ="41" ssid = "5">Similarly, we say that f is #P-Hard, if every function in #P can be polynomial time Turing reduced to f . If f is #P-Hard and is in#P, then we say that f is #P Complete.</S>
			<S sid ="42" ssid = "6">2.1 Viterbi Alignment.</S>
			<S sid ="43" ssid = "7">Computation VITER B I-3 is defined as follows.</S>
			<S sid ="44" ssid = "8">Given the parameters of IBM Model 3 and a sentence pair (f , e), Note that our results for decoding are sharper than that of (Knight, 1999).</S>
			<S sid ="45" ssid = "9">Firstly, we show that Exact Decoding is #P-Hard for IBM Models 35 and not just NP-Hard.</S>
			<S sid ="46" ssid = "10">Secondly, we show that Relaxed Decoding is NP-Hard for Models 35 compute the most probable alignment a∗ and e: a∗ = argmax P (f , a|e).</S>
			<S sid ="47" ssid = "11">a betwen f even when the language model is a uniform distribution.</S>
			<S sid ="48" ssid = "12">The rest of the paper is organized as follows.</S>
			<S sid ="49" ssid = "13">We formally define all the problems discussed in the paper (Section 2).</S>
			<S sid ="50" ssid = "14">Next, we take up each of the problems discussed in this section and derive the stated result for them (Section 3).</S>
			<S sid ="51" ssid = "15">After this, we discuss the implications of our results (Section 4) and suggest future directions (Section 5).</S>
			<S sid ="52" ssid = "16">2 Problem Definition.</S>
			<S sid ="53" ssid = "17">Consider the functions f, g : Σ∗ → {0, 1}.</S>
			<S sid ="54" ssid = "18">We say that g ≤m f (g is polynomial-time many-one reducible to f ), if there exists a polynomial time reduction r(.)</S>
			<S sid ="55" ssid = "19">such that g(x) = f (r(x)) for all 2.2 Conditional Probability.</S>
			<S sid ="56" ssid = "20">Computation PRO BA B ILITY3 is defined as follows.</S>
			<S sid ="57" ssid = "21">Given the parameters of IBM Model 3, and a sentence pair (f , e), compute the probability P (f |e) = �a P (f , a|e).</S>
			<S sid ="58" ssid = "22">2.3 Expectation Evaluation in EM.</S>
			<S sid ="59" ssid = "23">Iterations (f, e)-CO U N T-3, (φ, e)-CO U N T-3, (j, i, m, l)- CO U N T-3, 0CO U N T-3, and 1CO U N T-3 are defined respectively as follows.</S>
			<S sid ="60" ssid = "24">Given the parameters of IBM Model 3, and a sentence pair (f , e), compute the following 4 : c(f |e; f , e) = ) P (a|f , e) ) δ(f, fj )δ(e, eaj ), input instances x ∈ Σ∗.</S>
			<S sid ="61" ssid = "25">This means that given a a j machine to evaluate f (.)</S>
			<S sid ="62" ssid = "26">in polynomial time, thereexists a machine that can evaluate g(.)</S>
			<S sid ="63" ssid = "27">in polyno c(φ|e; f , e) = ) P (a|f , e) ) δ(φ, φi )δ(e, ei ), mial time.</S>
			<S sid ="64" ssid = "28">We say a function f is NP-Hard, if all a i functions in NP are polynomial-time many-one reducible to f . In addition, if f ∈ NP, then we say that f is NP-Complete.</S>
			<S sid ="65" ssid = "29">Also relevant to our work are counting functions that answer queries such as “how many computation paths exist for accepting a particular instance of input?” Let w be a witness for the acceptance of an input instance x and χ(x, w) be a polynomial time witness checking function (i.e. χ(x, w) ∈ P).</S>
			<S sid ="66" ssid = "30">The function f : Σ∗ → N such that c(j|i, m, l; f , e) = ) P (a|f , e)δ(i, aj ), a c(0; f , e) = ) P (a|f , e)(m − 2φ0 ), and a c(1; f , e) = ) P (a|f , e)φ0 . a 2.4 Decoding.</S>
			<S sid ="67" ssid = "31">E-DEC O D IN G -3 and R-DEC O D IN G -3 are defined as follows.</S>
			<S sid ="68" ssid = "32">Given the parameters of IBM Model 3, f (x) = ) w∈Σ∗ |w|≤p(|x|) χ(x, w) 4 As the counts are normalized in the EM iteration, we can replace P (a|f , e) by P (f , a|e) in the Expectation Evaluation tasks.</S>
			<S sid ="69" ssid = "33">and a sentence f , compute its most probable translation according to the following equations respectively.</S>
			<S sid ="70" ssid = "34">Proof: We give a polynomial time many one reduction from SETCOV ER to VITER B I-3.</S>
			<S sid ="71" ssid = "35">Given a collection of sets C = {S1, . . .</S>
			<S sid ="72" ssid = "36">, Sl } and a set X l i=1 Si, we create an instance of VITER B I-3 e∗ = argmax e ) P (f , a|e) P (e) a as follows: For each set Si ∈ C, we create a word ei (1 ≤ i ≤ (e∗, a∗) = argmax P (f , a|e) P (e).</S>
			<S sid ="73" ssid = "37">(e,a) 2.5 SETCOV ER.</S>
			<S sid ="74" ssid = "38">Given a collection of sets C = {S1 , . . .</S>
			<S sid ="75" ssid = "39">, Sl } and l).</S>
			<S sid ="76" ssid = "40">Similarly, for each element vj ∈ X we create a word fj (1 ≤ j ≤ |X| = m).</S>
			<S sid ="77" ssid = "41">We set the model parameters as follows: (1 if vj ∈ Si a set X ⊆ ∪l Si , find the minimum cardinality t(fj |ei) = 0 otherwisesubset Ct of C such that every element in X be longs to at least one member of Ct. SETCOV ER is a well-known NPCompleteproblem.</S>
			<S sid ="78" ssid = "42">If SETCOV ER ≤m f , then f is NP n(φ|e) = 2φ!</S>
			<S sid ="79" ssid = "43">if φ j= 0 1 if φ = 0 p Hard.</S>
			<S sid ="80" ssid = "44">2.6 PER MA N EN T. d(j|i, m, l) = 1.</S>
			<S sid ="81" ssid = "45">Now consider the sentences e = e1 . . .</S>
			<S sid ="82" ssid = "46">el and f = f1 . . .</S>
			<S sid ="83" ssid = "47">fm.</S>
			<S sid ="84" ssid = "48">Given a matrix M = [Mj,i]n×n whose entries are either 0 or 1, compute the following:perm(M) = � dn Mj,π where π is a per mutation of 1, . . .</S>
			<S sid ="85" ssid = "49">, n. P (f , a|e) = n l φ0| ) φi i=1 m \ l n n(φi |ei) φi! i=1 This problem is the same as that of counting the number of perfect matchings in a bipartite graph and is known to be #P-Complete (?).</S>
			<S sid ="86" ssid = "50">If PER MA N EN T ≤T f , then f is #P-Hard.</S>
			<S sid ="87" ssid = "51">× n t fj |eaj j=1 l 1 = 1−δ(φi ,0) n j: aj I=0 d(j|i, m, l) 2.7 CO MPA R EPER MA N EN TS.</S>
			<S sid ="88" ssid = "52">Given two matrices A = [Aj,i]n×n and B = i=1 2 We can construct a cover for X from the output [Bj,i]n×n whose entries are either 0 or 1, determine of VITER B I-3 by defining Ct n = {Si 1 |φi &gt; 0}.</S>
			<S sid ="89" ssid = "53">We −|C!|which of them has a larger permanent.</S>
			<S sid ="90" ssid = "54">PER MA note that P (f , a|e) = di=1 1−δ(φ ,0) = 2 . 2 iN EN T is known to be Turing reducible to CO M PA R EPER MA N EN TS (Jerrum, 2005) and therefore, if CO MPA R EPER MA N EN TS ≤T f , then f is #P Hard.</S>
			<S sid ="91" ssid = "55">3 Main Results.</S>
			<S sid ="92" ssid = "56">In this section, we present the main reductions for the problems with Model 3 as the translation model.</S>
			<S sid ="93" ssid = "57">Our reductions can be easily carried over to Models 4− 5 with minor modifications.</S>
			<S sid ="94" ssid = "58">In order to keep the presentation of the main ideas simple, we let the lexicon, distortion, and fertility models to be any non-negative functions and not just probTherefore, Viterbi alignment results in the mini mum cover for X. 3.2 PRO BA B ILITY3.</S>
			<S sid ="95" ssid = "59">We show that PRO BA B ILITY3 is #P Complete.</S>
			<S sid ="96" ssid = "60">We begin by proving the following: Lemma 2 PER MA N EN T ≤T PRO BA B ILITY3.</S>
			<S sid ="97" ssid = "61">Proof: Given a 0, 1-matrix M = [Mj, i]n×n, we define f = f1 . . .</S>
			<S sid ="98" ssid = "62">fn and e = e1 . . .</S>
			<S sid ="99" ssid = "63">en where each ei and fj is distinct and set the Model 3 parameters as follows: (1 if Mj,i = 1 ability distributions in our reductions.</S>
			<S sid ="100" ssid = "64">3.1 VITER B I-3.</S>
			<S sid ="101" ssid = "65">We show that VITER B I-3 is NP-Hard.</S>
			<S sid ="102" ssid = "66">t(fj |ei ) = n(φ|e) = 0 otherwise (1 if φ = 1 0 otherwise Lemma 1 SETCOV ER ≤m VITER B I-3.</S>
			<S sid ="103" ssid = "67">d(j|i, n, n) = 1.</S>
			<S sid ="104" ssid = "68">Clearly, with the above parameter setting,e1 e2 . . .</S>
			<S sid ="105" ssid = "69">en eˆ.</S>
			<S sid ="106" ssid = "70">We set the translation model para P (f , a|e) = dn Mj, aj if a is a permutation meters as follows: and 0 otherwise.</S>
			<S sid ="107" ssid = "71">Therefore, P (f |e) = ) P (f , a|e) a t(f |e) = 1 if f = fj , e = ei and Mj,i = 1 1 if f = fˆ and e = eˆ 0 otherwise.</S>
			<S sid ="108" ssid = "72">n = ) n Mj, aj = perm(M) The rest of the parameters are set as in Lemma 2.</S>
			<S sid ="109" ssid = "73">Let A be the set of alignments a, such that an+1 = a is a permutation j=1 Thus, by construction, PRO BA B ILITY3 computes perm(M).</S>
			<S sid ="110" ssid = "74">Besides, the construction conserves the number of witnesses.</S>
			<S sid ="111" ssid = "75">Hence, PER MA N EN T ≤T PRO BA B ILITY3.</S>
			<S sid ="112" ssid = "76">n + 1 and an is a permutation of 1, 2, . . .</S>
			<S sid ="113" ssid = "77">, n. Now, n+ 1 c (fˆ|eˆ; f , e\ = ) P (f , a|e) ) δ(fˆ, fj )δ(eˆ, ea ) a j= 1 n+ 1 We now prove that = ) P (f , a|e) ) δ(fˆ, fj )δ(eˆ, eaj ) Lemma 3 PRO BA B ILITY3 is in #P. Proof: Let (f , e) be the input to PRO BA B ILITY3.</S>
			<S sid ="114" ssid = "78">Let m and l be the lengths of f and e respectively.</S>
			<S sid ="115" ssid = "79">With each alignment a∈A = ) P (f , a|e) a∈A n = ) n Mj, a j=1 a = (a1 , a2 , . . .</S>
			<S sid ="116" ssid = "80">, am ) we associate a unique number na = a1a2 . . .</S>
			<S sid ="117" ssid = "81">am in base l + 1.</S>
			<S sid ="118" ssid = "82">Clearly, a∈A j=1 j = perm(M) . 0 ≤ na ≤ (l + 1)m − 1.</S>
			<S sid ="119" ssid = "83">Let w be the binary encoding of na.</S>
			<S sid ="120" ssid = "84">Conversely, with every binary string w we can associate an alignment a if the value of w is in the range 0, . . .</S>
			<S sid ="121" ssid = "85">, (l + 1)m − 1.</S>
			<S sid ="122" ssid = "86">Itrequires O (m log (l + 1)) bits to encode an align ment.</S>
			<S sid ="123" ssid = "87">Thus, given an alignment we can compute its encoding and given the encoding we can compute the corresponding alignment in time polynomial in l and m. Similarly, given an encoding we can compute P (f , a|e) in time polynomial in l and m. Now, if p(.)</S>
			<S sid ="124" ssid = "88">is a polynomial, then function Therefore, PER MA N EN T ≤T CO U N T-3.</S>
			<S sid ="125" ssid = "89">Lemma 5 (f, e)-CO U N T-3 is in #P. Proof: The proof is essentially the same as that of Lemma 3.</S>
			<S sid ="126" ssid = "90">Note that given an encoding w,P (f , a|e) �m δ (fj , f ) δ ea , e can be evalu ated in time polynomial in |(f , e)|.</S>
			<S sid ="127" ssid = "91">Hence, from Lemma 4 and Lemma 5, it follows that Theorem 2 (f, e)-CO U N T-3 is #P-Complete.</S>
			<S sid ="128" ssid = "92">3.4 (j, i, m, l)-CO U N T-3 f (f , e) = ) ∗ w∈{0,1} P (f , a|e) Lemma 6 PER MA N EN T ≤T (j, i, m, l)-CO U N T 3.</S>
			<S sid ="129" ssid = "93">|w|≤p(|(f , e)|) is in #P. Choose p (x) = \x log2 (x + 1)l. Clearly, all alignments can be encoded using atmost p (| (f , e) |) bits.</S>
			<S sid ="130" ssid = "94">Therefore, if (f , e) com putes P (f |e) and hence, PRO BA B ILITY3 is in #P. It follows immediately from Lemma 2 and Lemma 3 that Theorem 1 PRO BA B ILITY3 is #P-Complete.</S>
			<S sid ="131" ssid = "95">3.3 (f, e)-CO U N T-3 Lemma 4 PER MA N EN T ≤T (f, e)-CO U N T-3.</S>
			<S sid ="132" ssid = "96">Proof: The proof is similar to that of Proof: We proceed as in the proof of Lemma 4 with some modifications.</S>
			<S sid ="133" ssid = "97">Let e = e1 . . .</S>
			<S sid ="134" ssid = "98">ei−1eˆei . . .</S>
			<S sid ="135" ssid = "99">en and f = f1 . . .</S>
			<S sid ="136" ssid = "100">fj−1fˆfj . . .</S>
			<S sid ="137" ssid = "101">fn.</S>
			<S sid ="138" ssid = "102">The parameters are set as in Lemma 4.</S>
			<S sid ="139" ssid = "103">Let A be the set of alignments, a, such that a is a permutation of 1, 2, . . .</S>
			<S sid ="140" ssid = "104">, (n + 1) and aj = i. Observe that P (f , a|e) is nonzero only for the alignments in A. It follows immediately that with these parameter settings, c(j|i, n, n; f , e) = perm(M) . Lemma 7 (j, i, m, l)-CO U N T-3 is in #P. Proof: Similar to the proof of Lemma 5.Theorem 3 (j, i, m, l)-CO U N T-3 is #P Lemma 2.</S>
			<S sid ="141" ssid = "105">Let f = f1 f2 . . .</S>
			<S sid ="142" ssid = "106">fn fˆ and e = Complete.</S>
			<S sid ="143" ssid = "107">3.5 (φ, e)-CO U N T-3 Lemma 8 PER MA N EN T ≤T (φ, e)-CO U N T-3.</S>
			<S sid ="144" ssid = "108">3.8 E-DEC O D IN G -3 Lemma 14 CO MPA R EPER MA N EN TS ≤T E Proof: Let e = e1 . . .</S>
			<S sid ="145" ssid = "109">eneˆ k and f = DEC O D IN G -3 Proof: Let M and N be the two 01 matri f1 . . .</S>
			<S sid ="146" ssid = "110">fn &apos; f . . .</S>
			<S sid ="147" ssid = "111">f . Let A be the set of alignment s ces.</S>
			<S sid ="148" ssid = "112">Let f = f1f2 . . .</S>
			<S sid ="149" ssid = "113">fn, e(1) = e(1) (1) 2 . . .</S>
			<S sid ="150" ssid = "114">en for which an is a permutation of 1, 2, . . .</S>
			<S sid ="151" ssid = "115">, n and and e(2) = e(2)e(2) . . .</S>
			<S sid ="152" ssid = "116">e(2) . Further, let e(1) and 1 1 2 n an+k k &apos; e(2) have no words in comm on and each word n+1 =(n + 1) . . .</S>
			<S sid ="153" ssid = "117">(n + 1).</S>
			<S sid ="154" ssid = "118">We set 1 if φ = 1 and e j= eˆ  appears exactly once.</S>
			<S sid ="155" ssid = "119">By setting the bigram lan guage model probabilities of the bigrams that occur in e(1) and e(2) to 1 and all other bigram prob n(φ|e) = 1 if φ = k and e = eˆ 0 otherwise.</S>
			<S sid ="156" ssid = "120">abilities to 0, we can ensure that the only trans lations considered by E-DEC O D IN G 3 are indeed e(1) and e(2) and P e(1) = P e(2) = 1.</S>
			<S sid ="157" ssid = "121">We The rest of the parameters are set as in Lemma 4.</S>
			<S sid ="158" ssid = "122">Note that P (f , a|e) is nonzero only for the align ments in A. It follows immediately that with these then set 1 if f = fj , e = e(1) and Mj,i = 1  i parameter settings, c(k|eˆ; f , e) = perm(M) . t(f |e) = 1 if f = fj , e = e(2) and N j,i = 1 Lemma 9 (φ, e)-CO U N T-3 is in #P. Proof: Similar to the proof of Lemma 5.</S>
			<S sid ="159" ssid = "123">Theorem 4 (φ, e)-CO U N T-3 is #P-Complete.</S>
			<S sid ="160" ssid = "124">n(φ|e) = 0 otherwise (1 φ = 1 0 otherwise 3.6 0CO U N T-3 Lemma 10 PER MA N EN T ≤T 0CO U N T-3.</S>
			<S sid ="161" ssid = "125">Proof: Let e = e1 . . .</S>
			<S sid ="162" ssid = "126">en and f = f1 . . .</S>
			<S sid ="163" ssid = "127">fnfˆ.</S>
			<S sid ="164" ssid = "128">Let A be the set of alignments, a, such that an is a permutation of 1, . . .</S>
			<S sid ="165" ssid = "129">, n and an+1 = 0.</S>
			<S sid ="166" ssid = "130">We set 1 if f = fj , e = ei and Mj, i = 1  d(j|i, n, n) = 1.</S>
			<S sid ="167" ssid = "131">Now, P f |e(1) = perm(M), and P f |e(2) = perm(N ).</S>
			<S sid ="168" ssid = "132">Therefore, given the output of E- DEC O D IN G -3 we can find out which of M and N has a larger permanent.</S>
			<S sid ="169" ssid = "133">Hence E-DEC O D IN G -3 is #P − H ard.</S>
			<S sid ="170" ssid = "134">3.9 R-DEC O D IN G -3 t(f |e) = 1 if f = fˆ and e = NULL 0 other wise.</S>
			<S sid ="171" ssid = "135">Lemma 15 SETCOV ER ≤m R-DEC O D IN G -3 P roof: Given an instanc e of SETC OV ER, we set the param eters as in the proof of Lemm a 1 with The rest of the parameters are set as in Lemma 4.</S>
			<S sid ="172" ssid = "136">It is easy to see that with these settings, c(0;f ,e) = the following modification: ( 1 if φ &gt; 0 perm(M) . Lemma 11 0CO U N T-3 is in #P. n(φ|e) = 2φ!</S>
			<S sid ="173" ssid = "137">0 otherwise.</S>
			<S sid ="174" ssid = "138">Proof: Similar to the proof of Lemma 5.</S>
			<S sid ="175" ssid = "139">Theorem 5 0CO U N T-3 is #P-Complete.</S>
			<S sid ="176" ssid = "140">3.7 1CO U N T-3 Lemma 12 PER MA N EN T ≤T 1CO U N T-3.</S>
			<S sid ="177" ssid = "141">Proof: We set the parameters as in Lemma 10.</S>
			<S sid ="178" ssid = "142">It follows immediately that c(1; f , e) = perm(M) . Lemma 13 1CO U N T-3 is in #P. Proof: Similar to the proof of Lemma 5.</S>
			<S sid ="179" ssid = "143">Theorem 6 1CO U N T-3 is #P-Complete.</S>
			<S sid ="180" ssid = "144">Let e be the optimal translation obtained by solving R-DEC O D IN G -3.</S>
			<S sid ="181" ssid = "145">As the language model is uniform, the exact order of the words in e is not important.</S>
			<S sid ="182" ssid = "146">Now, we observe that: • e contains words only from the set{e1 , e2 , . . .</S>
			<S sid ="183" ssid = "147">, el }.</S>
			<S sid ="184" ssid = "148">This is because, there can not be any zero fertility word as n(0|e) = 0and the only words that can have a non zero fertility are from {e1, e2 , . . .</S>
			<S sid ="185" ssid = "149">, el } due to the way we have set the lexicon parameters.</S>
			<S sid ="186" ssid = "150">• No word occurs more than once in e. Assume on the contrary that the word ei occurs k &gt; 1 times in e. Replace these k occurrences by only one occurrence of ei and connect all the words connected to them to this word.</S>
			<S sid ="187" ssid = "151">This would increase the score of e by a factor of 2k−1 &gt; 1 contradicting the assumption on the optimality of e. As a result, the only candidates for e are subsets of{e1 , e2 , . . .</S>
			<S sid ="188" ssid = "152">, el } in any order.</S>
			<S sid ="189" ssid = "153">It is now straight forward to verify that a minimum set cover can be re covered from e as shown in the proof of Lemma 1.</S>
			<S sid ="190" ssid = "154">3.10 IBM Models 4 and 5.</S>
			<S sid ="191" ssid = "155">The reductions are for Model 3 can be easily extended to Models 4 and 5.</S>
			<S sid ="192" ssid = "156">Thus, we have the following: Theorem 7 Viterbi Alignment computation is NP-Hard for IBM Models 3 − 5.</S>
			<S sid ="193" ssid = "157">Theorem 8 Expectation Evaluation in the EM Steps is #P-Complete for IBM Models 3 − 5.</S>
			<S sid ="194" ssid = "158">Theorem 9 Conditional Probability computation is #P-Complete for IBM Models 3 − 5.</S>
			<S sid ="195" ssid = "159">Theorem 10 Exact Decoding is #P-Hard for IBM Models 3 − 5.</S>
			<S sid ="196" ssid = "160">Theorem 11 Relaxed Decoding is NP-Hard for IBM Models 3 − 5 even when the language model lexicon and alignment models.</S>
			<S sid ="197" ssid = "161">Whereas, in Models 35, the fertility model is independent of the lexicon and alignment models.</S>
			<S sid ="198" ssid = "162">It is precisely this freedom that makes computations on Models 35 harder than the computations on Models 12.</S>
			<S sid ="199" ssid = "163">There are three different ways of dealing with the computational barrier posed by our problems.</S>
			<S sid ="200" ssid = "164">The first of these is to develop a restricted fertility model that permits polynomial time computations.</S>
			<S sid ="201" ssid = "165">It remains to be found what kind of parameterized distributions are suitable for this purpose.</S>
			<S sid ="202" ssid = "166">The second approach is to develop provably good approximation algorithms for these problems as is done with many NP-Hard and #P-Hard problems.</S>
			<S sid ="203" ssid = "167">Provably good approximation algorithms exist for several covering problems including Set Cover and Vertex Cover.</S>
			<S sid ="204" ssid = "168">Viterbi Alignment is itself a special type of covering problem and it remains to be seen whether some of the techniques developed for covering algorithms are useful for finding good approximations to Viterbi Alignment.</S>
			<S sid ="205" ssid = "169">Similarly, there exist several techniques for approximating the permanent of a matrix.</S>
			<S sid ="206" ssid = "170">It needs to be explored if some of these ideas can be adapted for Expectation Evaluation.</S>
			<S sid ="207" ssid = "171">As the third approach to deal with complexity, we can approximate the space of all possi m is a uniform distribution.</S>
			<S sid ="208" ssid = "172">ble (l + 1) alignments by an exponentially large 4 Discussion.</S>
			<S sid ="209" ssid = "173">Our results answer several open questions on the computation of Viterbi Alignment and Expectation Evaluation.</S>
			<S sid ="210" ssid = "174">Unless P = NP and P#P = P, there can be no polynomial time algorithms for either of these problems.</S>
			<S sid ="211" ssid = "175">The evaluation of expectations becomes increasingly difficult as we go from IBM Models 12 to Models 35 exactly because the problem is #P-Complete for the latter models.</S>
			<S sid ="212" ssid = "176">There cannot be any trick for IBM Models 35 that would help us carry out the sums over all possible alignments exactly.</S>
			<S sid ="213" ssid = "177">There cannot exist a closed form expression (whose representation is polynomial in the size of the input) for P (f |e) and the counts in the EM iterations for Models 35.</S>
			<S sid ="214" ssid = "178">It should be noted that the computation of Viterbi Alignment and Expectation Evaluation is easy for Models 12.</S>
			<S sid ="215" ssid = "179">What makes these computations hard for Models 35?</S>
			<S sid ="216" ssid = "180">To answer this question, we observe that Models 12 lack explicit fertility model unlike Models 35.</S>
			<S sid ="217" ssid = "181">In the former models, fertility probabilities are determined by the subspace.</S>
			<S sid ="218" ssid = "182">To be useful such large subspaces should also admit optimal polynomial time algorithms for the problems we have discussed in this paper.</S>
			<S sid ="219" ssid = "183">This is exactly the approach taken by (Udupa, 2005) for solving the decoding and Viterbi alignment problems.</S>
			<S sid ="220" ssid = "184">They show that very efficient polynomial time algorithms can be developed for both Decoding and Viterbi Alignment problems.</S>
			<S sid ="221" ssid = "185">Not only the algorithms are provably superior in a computational complexity sense, (Udupa, 2005) are also able to get substantial improvements in BLEU and NIST scores over the Greedy decoder.</S>
			<S sid ="222" ssid = "186">5 Conclusions.</S>
			<S sid ="223" ssid = "187">IBM models 35 are widely used in SMT.</S>
			<S sid ="224" ssid = "188">The computational tasks discussed in this work form the backbone of all SMT systems that use IBM models.</S>
			<S sid ="225" ssid = "189">We believe that our results on the computational complexity of the tasks in SMT will result in a better understanding of these tasks from a theoretical perspective.</S>
			<S sid ="226" ssid = "190">We also believe that our results may help in the design of effective heuristics for some of these tasks.</S>
			<S sid ="227" ssid = "191">A theoretical analysis of the commonly employed heuristics will also be of interest.An open question in SMT is whether there ex ists closed form expressions (whose representation is polynomial in the size of the input) for P (f |e) and the counts in the EM iterations for models 35 (Brown et al., 1993).</S>
			<S sid ="228" ssid = "192">For models 12, closed form expressions exist for P (f |e) and the counts in the EM iterations for models 35.</S>
			<S sid ="229" ssid = "193">Our results show that there cannot exist a closed form expression (whose representation is polynomial in the size of the input) for P (f |e) and the counts in the EM iterations for Models 35 unless P = NP.</S>
	</SECTION>
</PAPER>
