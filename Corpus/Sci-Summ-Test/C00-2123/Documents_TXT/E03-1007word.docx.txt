
Using POS Information for Statistical Machine Translation into
Morphologically Rich Languages



Nicola Ueffing  and  Hermann Ney
Lehrstuhl fiir Informatik VI- Computer Science Department
RWTH Aachen- University of Technology
{ueffing,ney}@cs.rwth-aachen.de








Abstract


When translating from languages with 
hardly any inl1ectional morphology like 
English into morphologically rich lan­ 
guages,  the English  word forms  often 
do not contain enough information for 
producing the correct fullform in the 
target language.   We investigate  meth­ 
ods for improving the quality of such 
translations by making usc of part-of­ 
speech information and maximum en­ 
tropy modeling. Results Cor translations 
from English into Spanish and Catalan 
are presented on the LC-STAR corpus 
which consists of spontaneously spoken 
dialogues in the domain of appointment 
scheduling and travel planning.


1   Introduction

In this paper, we address the question of how part­ 
of-speech  ( POS)  information  can  help  improv­ 
ing the quality of Statistical Machine Translation 
(SMT). One of the main problems when translat­ 
ing from a language with hm·dly any inflectional 
morphology (which is English in our experiments) 
into one with richer morphology  (here:  Spanish 
and Catalan) is the production of the correct in­ 
flected form in the target lan guage. We introduce 
transformations to the English string that are based 
on the part-of-speech  information  and show how 
this knowledge source  can help SMT. Systematic 
evaluations will show that the quality of the gen-


erated translations is 
improved.
The transfonnations we apply arc the 
following:

Treatment of verbs  In Catalan  and Spanish,  
the pronoun before a verb is often omitted 
and in­ stead, the person is expressed 
via the ending of the verb. The smne 
holds for future tense and for the modes 
expressed through 'would ' and 'should'  
in English. Since this makes it hard to 
generate the correct translation of a given 
English verb, we propose a method re­ 
sulting in English word forms containing 
suf­ ficient infonnation.

Question inversion In    English,     
interrogative phrases have a word order 
that is different from declarative 
sentences: Either an auxil­ iary 'do·is  
inserted or the order of verb and 
pronoun  is inverted.   Since  this is 
dilTerent in Spanish and Catalan, we 
modify the word order in English to 
make it more similar to the 
Spanish/Catalan  one and to help the 
verb treatment mentioned above.

The paper is organized  as follows:  Related  
work is treated in Section  2.  In Section 3, 
we shortly review the statistical approach to 
machine transla­ tion. Then, we introduce the 
transfonnations  that we apply to the less 
inflected language of the two under 
consideration (namely English) in Section 4. 
After describing  the maximum entropy 
approach and the training procedure we use 
for the statisti­ cal lexicon in Section 5, we 
present results on the trilingual LC-STAR 
corpus in Section 6. Then, we conclude and 
present ideas about future work in Section 7.



2    Related Work

Publications dealing with the integration of lin­ 
guistic  information  into  the  process  of  statisti­ 
cal  machine  translation  are  rather  few allhough 
this had already been suggested in (Brown et al.,
1992).  (NieBen and Ney, 200 Ib) introduce  hier­ 
archical lexicon models including baseform and 
POS information for translation from German into 
English. Information contained in the German en­ 
tries  that  are  not  relevant  for  the  generation  of 
the English translation  arc omitted.   Unlike this, 
we investigate methods for enriching English with 
knowledge to help selecting the correct fullform in 
a morphologically richer language.
(NieBen and Ney, 200 l a) propose reordering oper­ 
ations for the language pair Gennan-English that 
help SMT by harmonizing word order between 
source and target. The question inversion we apply 
was inspired by this; nevertheless, we do not per­ 
form  a full morpho-syntactic  analysis, but make 
use only of POS information which can be ob­ 
tained from freely available tools.
(Garcia-Varea et al.. 2001) apply a maximum en­
tropy approach for training the statistical lexicon, 
but do not take any linguistic information into ac­ 
count.
The use ofPOS information for improving statisti­ 
cal alignment quality is described in (Toutanova et 
al.. 2002), but no translation results are presented.

3    Statistical Machine Translation

The goal of machine translation  is the translation 
of an input string -"1, ... , SJ   in the source language 
into a target language string L1, . . . , l r. We choose 
the string that has maximal probability given the 
source  string,  Pr ( t{[ s{).  Applying Bayes' deci­ 
sion rule yields the following criterion:

arg max Pr ( t{ [s{ )
tf
= 	arg m x{Pr( l{) · Pr( s{l l{)} 	(1)
tl

Through this decomposition of the probability, we 
obtain two knowledge sources: the translation and 
the language  model.  Those two can be modelled 
independently of each other.
The  cotTespondence  between  the  words  in  the



source and the target string is described  by 
align­ ments that assign target word positions 
to each source word position. The probability 
of a certain target language word to occur in 
the target string is assumed to depend 
basically only on the source words aligned to 
it.
The search  is denoted  by the arg rna.x 
operation in Eq. 1, i.e. it explores the space 
of all possible target language strings and all 
possible alignments between the source and 
the target language string to find the one with 
maximal probability.
The input string can be preprocessed before 
being passed to the search algorithm.  If 
necessary,  the inverse of these  
transformations  will be  applied to the 
generated output string.  In the work pre­ 
sented here, we restrict ourselves to 
transfotming only one language of the two: 
the source, which has the less inflected 
morphology.
For descriptions  of SMT systems  see for 
exam­ ple (Germann et al., 2001; Och et al., 
1999; Till­ mann and Ney, 2002; Vogel et al.. 
2000; Wang and Waibel, 1997).

4    Transformations in the Less 
Inflected
Language

When  translating  from  English  into  
languages with a highly inflected morphology, 
the production of the cotTect fullfonn often 
causes problems. Our experience on several 
corpora shows that the error rate or a 
translation from English into morpholog­ 
ically richer languages decreases  by 10% 
relative if we aim at producing only the 
correct baseform instead of the fully inflected 
word.   The transfer of the meaning expressed 
in the baseform is easier than deciding on the 
correct inOecLed form.

4.1    Treatment of 
Verbs

Especially the translation of verbs is difficult 
since there  are  many different  inflections  in 
Spanish and Catalan  whereas  there are only  
few in En­ glish.  Moreover, the pronouns and 
modals are of­ ten omiued in Spanish and 
CaLalan and this infor­ mation is expressed 
through the suffix. This makes it very hard for  
word-based systems  to generate the correct  
inflection from the English verb which docs 
not contain sufficient information. Thus, sev­ 
eral English words will have to be aligned to 
the Spanish  or Catalan  verbs.  This process  
is rela-



tively difficult for the algorithm and causes noise 
in the statistical lexicon i C English pronouns are re­ 
garded as translations of Spanish or Catalan verbs. 
In order to enrich the English verb with the needed 
information, we combine pronouns and/or modals 
with following verbs and treat those combinations 
as 'new'  Cullform words in English.  Thus we can 
obtain the information needed to select the con·ect 
verb form in the target language from one single 
English word.  The identif1cation or English  pro­ 
nouns,  modals and verbs was done by POS tag­ 
ging applied to the English part of the corpus.
We decided to transform the source language in­ 
stead of the target language,  because in this case 
we need only the POS tags of the source language 
as additional  knowledge source and nothing else. 
Another  possible  approach  would  have  been  to 
split the suffix in the target language (e.g.  'esta' 
into 'estar P3S'). This would require postprocess­ 
ing tools that are able to generate the correct verb 
ronn !'rom the haserorm and the person and tense 
information.
Table 1 gives examples  of words that have been 
spliced  to fonn  new entries  ol' the English  lexi­ 
con. For example, we splice the phrase 'you think' 
to form the single entry  'you_think'  which con­ 
tains sufficient information for producing the cor­ 
rect  Spanish  verb  form  'crees'  or  the  Catalan
'creus'.  Similarly, the modal auxiliaries can be 
added as well, like in the entry 'you_wilLhave' 
which is much better suited for being translated 
into 'tendnis'  (Spanish) or 'tindras' (Catalan) than 
the verb 'have' alone. Moreover, in a single word 
based lexicon, three single entries would have to 
be added  Cor  the  translation  of  'you  will have' 
into 'tendras': (you,tendras), (will,tendras) and 
(have,tendnis), which spreads the translation prob­ 
ability over far too many entries and makes the 
probability distribution unfocused.
As the last example in Table  l   shows,  'you  can
go'  is spliced only into two words instead of one 
in order to better match the Spanish/Catalan  form.

4.2   Question Treatment

In English interrogative phrases, either an auxil­ 
iary 'do' is inserted or the order of verb and pro­ 
noun is inverted. The au x iliary 'do'  does not carry 
information  that is relevant when translating into



Table 1: Examples of spliced words in the 
English vocabulary
I original            I    POS tags        I spliced 
words   I

yo
u 
go
P
R
P
V
B
P
yo
u_
go
yo
u 
we
nt
PR
P 
V
B
D
yo
u 
we
nt
yo
u 
thi
nk
PR
P 
V
BP
yo
u_
thi
nk
yo
u 
wi
ll 
ha
ve
P
R
P
M
D
V
B
yo
u 
wi
ll 
ha
ve
yo
u 
ca
n 
go
P
R
P
M
D
V
B
yo
u 
ca
n 
go


Spanish or Catalan. Thus, we can remove it 
from the sentence without hanning  the 
translation pro­ cess (as described in (NicBen 
and Ney, 200la) for the language pair 
German-English). However, we do not 
remove a question supporting 'do' in past 
tense, i. e. 'did' is kept in the phrase, because 
this is the only word containing the tense 
information. Afterwards, we can merge the 
pronoun and verb as depicted in Table 2: 'did 
you go' is transformed into 'you_did go'. We 
do not splice 'you_did' and
'go', because the English simple past is 
translated into present perfect in Catalan; and 
it is very likel y to he translated into present 
perl'ect in Spanish, es­ pecially in colloquial 
language  as it is present in this task. The 
form 'you_did go' is well suited to be 
translated into the Spanish 'has ido' or the 
Cata­ lan 'has anat'.
IC there is no question supporting 'do' and 
the or­ der of pronoun and verb is inverted-
see the exam­ ple 'how  are you?'  in Table 3 
- we first swap the two words and then 
perform the splicing step. This is done in order 
to avoid having two lexical entries with the 
same translation:  for example, 'you_are ' and 
the interrogative 'are_you ' hoth have the same 
translation in Spanish or Catalan, 
respectively. Table 3 presents examples of 
transformed EngIish queslions.   Comparing  
them to the Spanish  and Catalan reference,  
we see that it is easier to find a word-to-
word mapping for the modified English 
sentences.

5   Maximum Entropy 
Training

If we merge the pronouns/modals and verbs as 
de­ scribed above, it might happen thalthe 
verh itsell' (or one of its inflections) has 
never been seen in training except fi·om its 
appearance in the new en­ tries in the lexicon  
which result  from the splic-



Table 2: Examples of spliced  words in the English  vocabulary  after question  inversion

ori
gi
na
l
PO
S 
ta
gs
spl
ice
d  
w
or
ds
do 
yo
u 
go
V
BP 
PR
P  
V
B
yo
u_
go
di
d 
yo
u 
go
V
B
D
P
R
P
V
B
yo
u_
di
d 
go
ha
ve 
yo
u 
go
ne
V
B
P
P
R
P
V
B
N
yo
u_
ha
ve 
go
ne
wi
ll 
yo
u 
go
M
D
P
R
P
V
B
yo
u_
wi
lL
go
ca
n 
yo
u 
go
P
R
P
M
D  
V
B
yo
u_
ca
n 
go


Table 3: Examples of transformed English  sentences

Or
igi
na
l
ho
w 
are 
yo
u?
Q
ue
sti
on 
In
ve
rsi
on
ho
w 
yo
u 
ar
c'!
Ve
rb 
Tr
eat
m
en
t
ho
w 
yo
u_
are 
?
Ca
tal
an  
Se
nt
en
ce
co
m 
es
ta
?
Sp
ani
sh 
Se
nt
en
ce
i., 
co
m
o 
est
as 
?
Or
igi
na
l
or 
do 
yo
u 
thi
nk 
we 
wa
nt 
to 
sta
y 
[..
. ] 
?
Q
ue
sti
on 
In
ve
rsi
on
or 
yo
u 
thi
nk 
we 
wa
nt 
to 
st
a
y[
...
] ?
Ve
rb 
Tr
ea
tm
en
t
or 
yo
u_
thi
nk 
we
_w
ant 
to 
sta
y 
[..
. ] 
?
Ca
tal
an  
Se
nt
en
ce
o 
cre
u 
qu
e 
vo
ldr
em 
qu
ed
ar-
no
s 
[..
.] 
?
Sp
ani
sh 
Se
nt
en
ce
(, o 
cre
e 
qu
e 
qu
err
e
m
os 
qu
ed
ar
no
s 
[. . 
. ] 
?
Or
igi
na
l
di
d 
yo
u 
sa
y 
the 
ei
gh
tee
nt
h 
?
Qu
est
ion  
In
ve
rsi
on
yo
u 
di
d 
sa
y 
the 
eig
hte
ent
h  
?
Ve
rb 
Tr
eat
m
en
t
yo
u_
di
d 
sa
y 
the 
ei
gh
te
en
th 
?
Ca
tal
an  
Se
nt
en
ce
has 
dit 
el 
div
uit  
?
Sp
ani
sh 
Se
nt
en
ce
(, 
ha
s 
dic
ho 
el 
di
ec
io
ch
o ?




ing operation. This  makes  it impossible to trans­ 
late  the  verb  itself,  because  it  is  then  unknown 
to the system.   The  same holds  for combinations 
of  pronouns and  verbs  that  are  unseen  in  train­ 
ing, e. g. the training  corpus  contains the bigram
'1 went',   but  not  the  one  'she went'.   In  order 
to overcome this  problem,   we  train  our  lexicon 
model  using maximum entropy.

5.1   The Maximum Entropy Approach

The  maximum entropy  approach (Berger   et  al.,
1996) presents a powerful framework for the com­ 
bination  or several  knowledge sources.  This prin­ 
ciple recommends to choose the distribution which 
preserves as much uncertainty as possible  in terms 
or maximizing the entropy.  The distribution is re­ 
quired to satisfy constraints, which represent facts 
known  from  the data.   These  constraints are ex­
pressed  on the basis of feature functions hm ( s , t),


where  ( s, t) is a pair of source and target 
word. The lexicon probability of a source  word 
given the target word has the following 
functional form


p(si L )  = .%'t )   exp  [LAmhm(s ,    L) l
U
l

with the normalization factor

 

where  /\  = {Am} is the set of model  
parameters with one weight  Am Cor each 
feature  function  hm. The features we usc in 
our model arc

• a lexical  feature  (Cor the entries  of Lhe 
trans­
formed  
vocabulary):

hs',t'(s , t )  = 8(s , s' ) ·8(t, t')



• the verb contained in a transformed  lexicon 
entry (e.g. 'go' Cor 'you_go' or 'you_wi!Lgo):

 

where

1,  ift contains  the verb v
Verb(t, v) = { 0,  otherwise


This enables us to translate  the verb alone  even if 
it occurs  in the training  corpus  only  as a spliced 
entry.
For an introduction to maximum entropy modeling 
and  training  procedures, the reader  is  refened to 
the corresponding literature, for instance (Berger 
et al., 1996) or (Ratnapark:hi, 1997).

5.2    Training

We performed  the following  training  steps:
• transform the  English  (= source  language) 
part or the corpus as described in Seclions 4.1 
and 4.2
• train  the  statistical translation system  using 
this modi fled source language corpus  1
• with the resuiLing alignment, train the lexicon 
model using maximum entropy  with  the fea­ 
tures described in Section  5.1
This  training   can  be  pelformed  using  converg­ 
ing iterative  training  procedures like described  by 
(Darroch   and  Ratcliff.  1972)  or  (Della  Pietra  et 
al., 1997) 2 . The basic training  procedures for the 
translation system  and  the  language  model  need
not be changed.

5.3    Translation process

For translation, we can use an SMT system  where 
the search algorithm does not have to be modified. 
Before  the translation process,  we  transform the 
input  in  the same  way  as the training  corpus  be­ 
fore training  the alignment (see Section  5.2).  We 
simply  have to exclude  those words from splicing 
where the splicing operation yields an unknown 
word.
     1Tbis training was done using the GIZA++ toolkit which 
c;m be downloaded  from http://www-i6.infmmatik.rwth­ 
aachcn.dcroch/softwarc/GTZA++.html



6    
Result
s

6.1    
Corpora

We   performed    experiments   on   the   
trilingual corpus   which   is  successively  
built  within   the LC-STAR   project.     It 
comprises the  languages English,  Spanish 
and Catalan,  whereof  we used English  as 
source  and Spanish  and Catalan  as tar­ get 
languages. At the time of our experiments, 
we had  about  13k  sentences per language 
available; the statistics are given in Table 4.
The corpus consists of transcriptions of 
sponta­ neously  spoken  dialogues.   Thus,   
the  sentences often lack correct  syntactic 
structure. The domain of this  task is 
appointment scheduling and  travel 
arrangements.
The POS  information for  the English part of  
the
corpus was generated using the Brill 
tagger3.
As Table 4 shows,  the splicing  operation 
increases the   cardinality  of   the   English    
vocabulary as well as the number of 
singletons significantly. Nevertheless, they  
are still  below  those  numbers for Spanish  
and Catalan.


6.2    Evaluation 
Metrics

The quality  of the output  of our machine 
transla­ tion system  is measured 
automatically by compar­ ing the generated 
translation to a given reference translation. 
The two following  criteria  are used:

• WER (word error  rate):
The  word error  rate  is based  on  Lhe 
Leven­ shtein distance.  It is computed as 
the min­ imum number of substitution, 
insertion and deletion  operations that 
have to be performed to convert  the 
generated string  into  the ref­ erence  
string.   Since  some sentences in the 
develop  and test set occur several  times 
with different  reference translations 
(which  holds especially    !'or  short   
sentences  like   'okay, good-bye'),  we  
calculate the  minimal   dis­ tance  to  
this  set  of  references as  proposed in 
(NieBen et al., 2000).

• BLEU  (bilingual evaluation  
understudy): (Papineni  et   al.,   2002)   
have   proposed   a


2We   made   use   of   the   toolkit   YASMET   which   can	 	


be	downloaded 	from 	http://v,·ww-i6.informatik.rwth­
aachen.derochlsoftware/YASME'l.html
  

3The 	Brill 	tagger 	can 	be	downloaded 	ti"om http://www.research.microsoft.com/users/brill/



Table 4: Statistics of the training, develop and test set of the English-Spanish-Catalan LC-STAR corpus
(*number of words without punctuation marks)


E
n
g
l
i
s
h
 
	
I
I
	
Spa
nish 	Catalan
I	II

O
ri
gi
na
l 	I      Transformed  I

Tr
ai
ni
ng 	Sentences
W
o
r
d
s
W
o
r
d
s
*
1
3
 
3
5
2

123 
454    
I	114099	II	118 534 	II	118 137

1017
38	I	92383	II	96997 	II	96503
V
oc
ab
ul
ar
y	Size
S
i
n
g
l
e
t
o
n
s
215
4 	j	2776 	jj 	3933 	jj	3572

79o 
C37%) 
1         l  
165 
(42%)  
II 1 
844 
(47%)  
II  I 
658 
(47%)
De
ve
lo
p 	Sentences
W
o
r
d
s
U
n
k
n
o
w
n
 
W
o
r
d
s
2
7
2

226
7 	j	2096 	jj 	2217 	jj	2211

21 	I	22 	II	34 	II	34
Te
st	Sentences
W
o
r
d
s
U
n
k
n
o
w
n
 
W
o
r
d
s
2
6
2

262
6 	I	2460 	II	2451 	II	2470

17 	I	18	II	30 	II	35




method of automatic machine translation 
evaluation,  which  they  call  "BLEU".  It  is 
based on the notion of modified n-gram pre­ 
cision, for which all candidate n-gram counts 
in the translation are collected and clipped 
against their corresponding maximum refer­ 
ence counts.  These clipped candidate counts 
are summed and normalized by the total num­ 
ber of candidate n-grams.   Since BLEU ex­ 
presses quality, we determine 100-BLEU to 
transform it into an error measure.

Although these measures are only approximations, 
they seem to be surflcient at the present level or 
petformance of machine translation systems.

6.3    Experimental Results

We compared  the two statistical  lexica obtained 
from the baseline system and from the maximum 
entropy  training on the transformed  corpus.  For 
the baseline  lexicon,  we observed an average of
5.82  Catalan  translation  candidates  per  English 
word  and  6.16  Spanish   translation  candidates. 
These numbers are significantly reduced in the 
lexicon which was trained on the transformed 
corpus using maximum entropy: there, we have an 
average of 4.20 for Catalan and 4.46 for Spanish. 
Especially for (nominative) English pronouns 
(which have many verbs as translation candidates 
in the baseline lexicon), the number of translation 
candidates  was  substantially  scaled  down  by  a


factor around 4. This shows that our method 
was successful in producing a more focused 
lexicon probability distribution.
We   performed    translation    experiments    
with an   implementation   of   the   IBM-4   
translation model  (Brown  et  al.,  1993).   A  
description  of the system can be found in   
(Tillmann and Ney,
20
02
).
Table 5 presents an assessment or translation 
qual­ ity for  both the language  pairs 
English--Catalan and English-Spanish. We see 
that there is a signif­ icant decrease in error 
rate for the translation into Catalan. This 
change is consistent across bother­ ror rates, 
the WER and 100-BLEU.
For  translations  from  English  into Spanish,  
the
improvement  is less  substantial.     A  reason  
for this might be that the Spanish vocabulary 
contains more entries and the ratio between  
l'uHrorms and baseforms is higher: 1.57 for 
Spanish versus 1.53 for Catalan4 •  This makes 
it more difficult for the system to choose the 
correct inflection when gen­
erating  a Spanish  sentence.   We assume that 
the extension  or our approach  to other  word 
classes than verbs will yield a quality gain for 
translations into Spanish.
Table 6 shows several sentences from the 
English
LC-STAR develop and test corpus that were 
trans-

  4 The lemmatization  of Spanish and CaLalan was 
produced using the analyser  from UPC Barcelona:  
MACO+  andRE­ LAX.



Table 5: Translation error rates[%] for English-Catalan and for English-Spanish

I	Develop 	I	Test	I
WER  I     100-BLEU 	WER  I   I 00-BLEU

Ca
tal
an 	Baseline
3
7
.
6
5
8
.
2
3
3
.
0
4
9
.
2
+
 
T
r
a
n
s
f
o
r
m
a
t
i
o
n
s
3
5
.
0
5
5
.
1
3
0
.
8
4
6
.
6
Sp
an
is
h 	Baseline
3
5
.
4
5
7
.
6
3
2
.
1
4
8
.
9
+
 
T
r
a
n
s
f
o
r
m
a
t
i
o
n
s
3
5
.
0
5
5
.
8
3
1
.
5
4
7
.
6




lated into Catalan.  We see that it is easier for the 
system to generate the correct verb inflection in 
Catalan if the verb is enriched with the pronoun. 
In the baseline system, it happens that words are 
inserted- like 'far'  as translation of 'will'  in the 
second example  which is incorrect.   This can be 
avoided by the splicing of words.
In  the  last  example,   we  see  that  the  baseline 
system  generates  one word each for the English
'I prefer' and does not find the correct translation,
whereas transformations yield an accurate transla­ 
tion of this expression,  because the spliced word 
contains sul'ficienl inl'ormalion.


7    Conclusion and Future Work

We presented a method for improving  quality of 
statistical machine translation from English into 
morphologically richer languages like Spanish and 
Catalan. Using POS tags as additional knowledge 
source, we enrich the English verbs such that they 
contain more information relevant for selecting the 
conect  inflected form in the target language.  The 
lexicon model was then trained using the maxi­ 
mum entropy approach, taking the verbs as addi­ 
tional features.
Results  were given for  translation  from  English
into Spanish  and Catalan  on the LC-STAR cor­ 
pus which consists  of spontaneously  spoken dia­ 
logues in the domain of appointment  scheduling 
and travel arrangement.    Our  experiments  show 
that translation quality can be signif1cantly in­ 
creased through the use of our approach: the word 
error rate on the Catalan development set for ex­ 
ample decreased by 2.5% absolute.
We plan  to investigate other methods of enrich­
ing the English  words  with information.   It  will 
be  interesting  to  see  how  other  word  classes,


e. g. nouns,  can be handled in order  to 
improve quality of translations into languages 
with a highly inflected morphology.

8   
Acknowledgement
s

This work was partly supported by the LC-
STAR project by the European Community 
(IST project ref. no. 2001-32216).


Refere
nces

A.L. Berger, S.A. Della Pietra, and V.J. Della 
Pietra.
1996.   A maximum entropy approach to natural 
language  processing.     Computational  
Linguistics,
22(1):39-72, 
March.

P.F. Brown. S.A.  Della Pietra,  V.J. Della Pietra,  
J.D.
Lafferty, and R.L. Mercer.   1992.  Analysis,  
statis­ tical transfer,  and synthesis in machine  
tnmslation. In  Proc. TMI  1992:  4th  Int. Conl  
on Theoretical
and Methodological  Issues in MT, pages 83-
100, Montreal, P.Q., Canada, June.

P.F. Brown,  S.A. Della  Pietra,  V.J. Della Pietra,  
and R.L. Mercer.   1993.  The mathematics  of 
statistical machine lJ"anslation: Parameter 
estimaLion. Compu­ tational Linguistics, 
19(2):263-311.

J.N. Darroch and D. Ratcliff.  1972.  Generalized 
itera­ tive scaling for log-linear models. 
Annals o_f"Mathe­ matical Statistics, 43:1470-
1480.

S.A.  Della  Pictra,  V.J.  Della  Pictra,  and  J.  
Lafferty.
1997.   Inducing  features  in  random  fields.   
lEt..E
Trans. on Pattern Analysis and Machine 
lnteligence,
19(4):380--393, 
July.

I. Garcia-Varea,  F.J.  Och.  H.  Ney,  and  F.  
Casacu­ berta.   200 I.   Refined lexicon  
models for statisti­ cal machine translation 
using a maximum entropy approach. 	In  
Proc. 39th  Annual  Meeting  of  the Assoc. for 
Computational Linguistics - joint with EACL, 
pages 204-211 , Toulouse, France. July.



Table 6: Examples of English-Catalan translations with and without transformation

So
ur
ce
we
_e
xc
ha
ng
e 
the
m 
an
d, 
tha
t 
wo
ul
d 
be 
go
od
.
Re
fer
en
ce
les 
ca
nvi
em 
i, 
aix
o 
est
ari
a 
be.
Ba
sel
in
e
en
s 
ca
nv
ie
m 
i, 
aix
o 
est
ari
a 
be
.
Ve
rb 
Tr
eat
m
en
t
les 
ca
nv
ie
m 
L 
aix
o 
est
ari
a 
be.
So
ur
ce
ok
ay, 
an
d 
L
wi
ll. 
sp
ea
k 
to 
yo
u 
so
on 
th
en
.
Re
fer
en
ce
d' 
ac
or
d, 
ij
o, 
pa
r/a
re 
am
h 
tu 
avi
at 
do
nc
s.
Ba
sel
in
e
d' 
ac
or
d, 
i 
jo 
far
, 
pa
rla
re 
am
b 
tu 
avi
at 
do
nc
s.
Ve
rb 
Tr
ea
tm
en
t
d' 
ac
or
d, 
i 
jo, 
pa
rla
re 
am
b 
tu 
avi
at 
do
nc
s.
So
ur
ce
Lh
eli
ev
e, 
the 
fli
gh
t is 
ev
er
y 
da
y?
Re
fer
en
ce
cr
ec, 
qu
e 
el 
vo
le
s 
ca
da 
di
a?
Ba
sel
in
e
su
po
so, 
el 
vo
le
s 
ea
da 
di
a?
Ve
rb 
Tr
eat
m
en
t
cre
c, 
qu
e 
el 
vo
l 
es 
ca
da 
di
a?
So
ur
ce
Lp
ref
er 
sin
gl
e.
Re
fer
en
ce
pr
efe
rei
xo 
in
div
id
ua
l.
Ba
sel
in
e
jo 
pre
fer
iri
a 
un
a 
in
di
vi
du
al.
Ve
rb 
Tr
ea
tm
en
t
pr
cfc
rei
xo 
un
a 
in
di
vi
du
al.




U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya­ 
mada.  2001.   Fast decoding  and optimal decoding 
for machine translation.  In Proc. 39th Annual Meet­ 
ing  of  the  Assoc.  for  Computational Linguistics - 
joint  with  FACT., pages 228-235, Toulouse, France, 
July.

S. Nief3en and H. Ney. 2001a. Morpho-syntactic anal­ 
ysis for reordering in statistical machine translation. 
In Proc. MT  Summit  Vlll, pages 247-252, Santiago 
de Compostela, Galicia, Spain, September.

S.  NieBen and  H.  Ney.   2001 b.    Toward hierarchi­ 
cal models for statistical machine translation of in­ 
flected languages.   In 39th Annual  Meeting of the 
Assoc.  for  Computational  J.inguistics  -  joint  with 
EACL  2001:   Proc. Workshop on Data-Driven Ma­ 
chine  Translation, pages 47-54, Toulouse, France, 
July.

S. NieBen, F.J. Och, G. Leusch, and H. Ney. 2000. An 
evaluation tool for machine translation:  Fast evalu­ 
ation for mt research.   In  Proc.  of the  Second  lnt. 
Conf  on Language Resources and Evaluation, pages
39-45, Athens. Greece, May.

F.J. Och, C. Tillmann,  and H. Ney.    1999.  Improved 
alignment   models  for  statistical  machine  transla­ 
tion.    ln  Proc.  .Joint ,\'JGDAT Cmf   on  Empirical 
Methods in  Naturall.anguage Processing  and Very 
Large Cmpora, pages 20-28, University of Mary­ 
land, College Park, MD, June.

K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method  for  automatic  evaluation  of  rna-


chine  translation.    In  Proc. 40th  Annual M 
eeting of the  Assoc.  for Computational  
Linguistics, pages
311-318, Philadelphia, PA, 
July.

A. RaLnaparkhi. 1997.  A simple introduclion to 
max­ imum entropy models for natural 
language process­ ing. Technical Report 97--
08, Institute for Research in Cognitive Science, 
University of Pennsylvania, Philadelphia, PA, 
May.

C. Tillmann and H. Ney. 2002.  Word re-ordering 
and DP beam search for statistical  machine  
translation. to appear in Computational 
Linguistics.

K. Toutanova, H.T. Tlhan, and C.D. Manning.   
2002.
Extensions  to HMM-based  statistical  word  
align­
ment  models.   In  Pmc.  Conf   on  Empirical 
Meth­ odsfor Natural  Language Processing, 
pages 87-94, Philadelphia, PA, July.

S. Vogel , F.J. Och, C. Tillmann, S. Nief3en, H. 
Sawaf, and  H.  Ney.    2000.    Statistical  
methods  for  ma­ chine translation.  In W. 
Wahlstcr, editor, Verbmobil: Foundations of 
Speech-to-Speech Translation, pages
377-393. Springer Verlag: Berlin, Heidelberg, 
New
Y
o
r
k
.

Y.Y. Wang and  A. Waibel.    1 997.   Decoding  
algo­ rithm in statistical  translation.  Tn  Proc. 
35th  Annual Meeting of the Assoc. for 
Computational Linguistics, pages 366-372, 
Madrid, Spain, July.




