Improving Lexical Mapping Model of English-Korean Bitext
Using Structural Features
 
 
Seonho Kimt and Juntae Yoont and Mansuk Songt
tDept. of Computer Science, Yonsei University,
Seoul 120-749, Korea
shkim,mssong@december.yonsei.ac.kr
 
NLP Lab., Daumsoft Inc., Kangnam-gu, Samsung-dong,
Yungjeon Bldg., Seoul 135-728, Korea
jtyoon@daumsoft.com
 
 
Abstract

The problem of finding lexical alignments for given
sentence pairs is computationally expensive. Fur-
thermore, it is much difficult to find lexical align-
ments between Korean and English since they have
considerably different syntactic structures and the
coverage of word-for-word correspondences is low.
This paper presents a method for extracting struc-
tural features which can reduce mapping space by
allowing only probable alignments. We describe how
the features improve the performance of the lexical
alignment model. The structural features provide
the information for the correspondences of parts-of-
speech (POS) sequences which are useful in transla-
tion. Based on maximum entropy (ME) concept, the
structural features are incrementally selected, which
are later embedded in the lexical alignment model.
It turns out that the features help get better lex-
ical alignments of Korean and English by offering
linguistic knowledge.
 
 
1 Introduction
 
 
Aligned bitexts are useful for the derivation of
bilingual lexical resources which are used for ma-
chine translation and cross languages information
retrieval. Thus, a lot of approaches have been sug-
gested to find sets of corresponding word tokens
(Brown et al., 1993; Berger et al., 1996; Melamed,
1997), phrase (Shin et al., 1996), noun phrase (Ku-
piec, 1993), and collocation (Smadja et al., 1996) in
a bitext.
Some works have used lexical association measures
for finding word correspondences (Gale and Church,
1991; Fung and Church, 1994). However, the associ-
ation measures can be misled in cases where a word
in a source language frequently co-occurs with more
than one word in a target language or in cases of
indirect association&apos; (Melamed, 1997).
In other works, iterative parameter re-estimation
&apos;Suppose that Uk and vk are indeed mutual translation
and Uk and Uk-ki often co-occur in text. Then vk and 24+1
will also co-occur more than expected by chance , which is
represented as indirect association.
techniques based on IBM model 1-5 2 have been
employed (Brown et al., 1993). They were usually
incorporated in the EM algorithm (Brown et al.,
1993; Kupiec, 1993; Tillmann and Ney, 2000; Och
et al., 2000). However, we are often faced with some
difficulties as follows, when the IBM model-based
approaches are directly applied to the alignment,
especially on bitext involving a less closely related
language pair.
1. It needs excessive iteration time for parame-
ter estimation and high decoding complexity.
Thus most systems assumed one-to-one corre-
spondence to reduce computational complex-
ity. However, word sequences are not trans-
lated literally word for word. For example, in
cases of collocations, compound nouns, and am-
biguous words with different meaning depen-
dent on the context, they require phrase-level
correspondences.
2. Most systems use little or no linguistic knowl-
edge to structure the underlying models. The
distortion probability and the fertility probabil-
ity for finding word correspondences is a weak
description for word order change between lan-
guages and 1:n mapping modeling. As the re-
sult, lots of ungrammatical sentences and too
many parameters to be estimated are allowed.
In addition, many words are aligned to the
empty word due to the overfitting problem (Och
et al., 2000).
 
 
3. In order to estimate parameters properly, it re-
quires a very large volume of bilingual aligned
text.
 
 
In this paper, we use structural correspondences to
 
 
2
7/1
 
 
P(f, ale) = Hn(Oilei)Ht(fjlea,)d(jlajâ€ž/)
 
 
n is the fertility probability that an English word generates
7/ French words, t is the word alignment probability that the
English word e generates the French word f, and d is the dis-
tortion probability that an English word in a certain position
will generate a French word in a certain position.
 
 
English Korean
can/MD eu1/ENTR1 sa/NNDE ss/AJ
da/ENTE
World/NNP Cup/NNP weoideakeop/NNIN1
to/TO take/VBP off/RP iryugha/VB neart/ENTR1
MD:modal auxiliary, NNP:proper noun, VBP:verb, RP:particle
NNDE:dependent noun, AJ:adjective ENTE: final ending,
NNIN1:proper noun, VB:verb, ENTR1:adnominal ending
 
 
Table 1: Lexical differences between English and Ko-
rean
 
 
overcome the above problem in bilingual text align-
ment.
2 The problems of lexical alignment
in Korean and English bitext
Although one-to-one correspondence assumption has
been shown to give highly accurate results in closely
related language pair (Melamed, 1997), it does not
fit in structurally different language pair such as Ko-
rean and English. According to Shin et al. (1996),
the coverage of one-to-one correspondence in Korean
and English bitext is approximately 34% and many-
to-many mappings exceed 40% in lexical alignments.
Besides, Korean and English show much difference
in terms of unit of mutual translation as shown in
Table 1.
Thus, we use structural or linguistic information
to find accurate lexical alignments of bitext. In gen-
eral, structural information can be represented by
the form of bilingual correspondence of phrase struc-
ture (Watanabe et al., 2000) or dependency struc-
ture (Yamamoto, 2000).
To find these structural correspondences between
a language pair, unification-based grammar (Mat-
sumoto, 1993) or bilingual grammar (Wu, 1997) can
be used. That is, if we try to find structural match of
bitext, syntactic analysis of each text should be done
first. However, syntactic analysis of each text(or bi-
text) is also a hard and computationally complicated
problem.
In this paper, we propose POS sequence feature.
We consider correspondences between POS tag se-
quences of bitext as the structural information which
is important in determining structures and reduc-
ing unnecessary parameters of a statistical alignment
model. We induce the correspondences(structural
features) by a feature selection method based on the
ME framework. Finally, the structural features are
embedded in a lexical alignment model of English
and Korean bitext. Our model offers the following
advantages:
 
 
1. It can overcome the limitation of word-for-word
correspondences.
2. The model can take advantage of the explicit
introduction of some knowledge about the lan-
guage. Therefore, it can reduce a lot of param-
 
 
eters in statistical machine translation by elim-
inating syntactically unlikely alignments. The
lexical alignment model iterates only over a sub-
set of probable alignments.
 
 
3. It is possible to estimate the probability of lex-
ical alignment in a relatively small size of cor-
pora.
4. The structural information is helpful in the con-
struction of a bilingual grammar.
3 Overview of the model
 
 
In general, there exist constaints among POS se-
quences mapping when aligning bitext. For instance,
in many cases a noun in Korean is translated to a
noun or a noun preceded by an article in English.
This POS constraint would be useful information in
alignment of a closely related pair as well as a less
closely related language pair.
Some approaches design an alignment algorithm
that maximizes the number of matching POS in
aligned segments (Papageorgiou et al., 1994). We
also assume that the mapping information of POS
sequences of a language pair is useful in a model of
statistical machine translation.
If there are similarities between corresponding
POS sequences in bitext, the structural feature
would be easily computed or identified. However,
a POS sequence in English often correspond to a to-
tally different POS sequence in Korean as shown in
the following example:
can/MD eui/ENTR1 sa/NNDE1 &apos;iss/AJMA da/ENTE
It is caused by the discrepancy between two lan-
guages.
Korean is an agglutinative language. A sentence
in Korean consists of a series of syntactic units called
eojeol. An eojeol delimited by whitespace is often
composed of a content word and function words.
Tense markers, clausal connectives, particles and so
forth are contained in an eojeol. Thus, one or more
words in English often correspond to an eojeol, i.e.
a couple of morphemes. For instance, a phrase, &apos;to
the school&apos;, in English corresponds to an eojeol &amp;quot;d4-
2___(haggyo-ro, school-to)&apos; in Korean.
Thus, we need a method for mapping POS se-
quences of bitext. In this paper, the correspondences
of POS tag sequences are obtained by automatic fea-
ture selection based on the ME framework. Here, a
feature is defined as a correspondence of POS tag
sequences in bitext.
The outline of finding correspondences of POS tag
sequences is as follows: First, our model starts with
initial features obtained by a supervision step. The
initial features are extracted from a small portion
of bitext, which of weights are trained by the IIS
algorithm (Berger et al., 1996; Pietra et al., 1997).
These are called initial active features.
In the next step, a feature pool is constructed from
training data. At this time, only features giving a
large gain to the model are selected. The final out-
put of feature selection is the set of active features
and the correspondences of POS tag sequences that
are represented with conditional probabilities.
The resulted features are used for the parameters
of bilingual text alignment. In the process, we look
at the words to ensure a correct alignment. That is,
the POS sequences are in fact encoding particular
words.
The underlying process is as follows:
Input: a set L of POS-labeled sentence pairs.
 
 
1. Make a set .F of correspondences of tag se-
quences, (te, tk) from a small portion of L by
hand.
2. Set .F into a set of initial active features, A.
3. Compute the weights of the initial active fea-
tures A using HS algorithm.
4. Create a feature pool P which is a set of possi-
ble combinations of tag sequences from sentence
pairs.
5. Filter P using frequency counts and similarity
with A.
6. Compute the approximate gains of the features
in P.
7. Select features(Ai) with large gain values, and
add A.
8. Compute the lexical alignment of bitext using
p(tk Ite) where (te, tk) E A.
 
 
4 Feature Selection
 
 
As mentioned above, features which represent map-
pings of POS sequences in two languages are auto-
matically learned from bitext. The learning consists
of two steps: (1) supervised step and (2) unsuper-
vised feature selection.
In the supervised step, we manually aligned En-
glish and Korean texts. From the manually aligned
text, we consturct a feature pool which has initial
POS sequence correspondence. In the unsupervised
feature selection, the POS sequence correspondeces
are added to the feature pool.
 
 
4.1 Supervision Step
 
 
In the supervision step, a small portion of bi-
text is tagged using Brill&apos;s tagger (Brill, 1995) and
`MORANY&apos; (Yoon et. al., 1999), each of which
is for English and Korean tagging respectively. We
manually aligned each sentence pair in the bitext
and collected their correspondences of tag sequences.
For simplicity, we adjusted some part of Brill&apos;s tag
set.
First of all, we classified sentential patterns of En-
glish and Korean. Then, we aligned English and
Korean sentence pairs on the basis of the patterns.
Also, we made tag sequence construction rules with
respect to each language by analysis of the cases
where the words of one unit are separated and ad-
jacent. The rules are used when making a feature
pool.
After collecting the correspondences of POS tag
sequences, we used them as a set of initial active
features, A.
 
 
4.2 Construction of a Feature Pool
 
 
In this chapter, we describe how a feature pool is
constructed. Our task is to construct a probabil-
ity model p that produces a corresponding Korean
tag sequence &amp;quot;Tic for a given English tag sequence
J. As features to represent the model, we use co-
occurrence information of POS tag sequences.
Let Ts be all possible correspondences of tag se-
quences for a specific sentence pair, S. We then
define a feature function (or a feature) as follows:
 
 
1 x = te &amp; y = tk Sz
fte,t,(x,y) = pair(te,tk) E TS
0 otherwise
 
 
A feature fte,t,, which indicates co-occurrence be-
tween tags appearing in Ts, expresses information
for predicting that an English POS tag sequence te
maps into a Korean POS tag sequence tk â€¢
In order to make a feature pool, given a sentence
pair, we first construct all possible combinations of
English POS tag sequences and Korean POS tag se-
quences. Among them, only features fte,t, that sat-
isfy the following two conditions are added into the
feature pool P.
 
 
â€¢ count(fte,t,)&gt; 15
â€¢ there exist tk such that (te, tk.) E A and the
similarity value (simply counting of same tag)
between tk and tkx is greater than 0.6
 
 
4.3 Feature selection
 
 
Since the set of P is too vast, a feature selection pro-
cess is needed to select useful features. For this, each
feature is evaluated according to how much they con-
tribute to the likelihood of training data, which is
called gain.
Before explaining feature gain and a process of
feature selection, we will give a brief introduction of
ME. In ME, a feature gives information to a proba-
bility model and it has a weight.
Thus, the model is constrained by features we de-
fined. In general weights of the features are trained
by the improved iterative scaling (IIS) algorithm
that minimizes the Kullback-Leibler divergence be-
tween the model and the empirical distribution of
the training data.
In fact, our model reduces to a simple type of
probability model that can be derived simply from
a ratio counts since the features do not overlap.
Let pA be the optimal model constrained by a set
of initial active features A, then it can be represented
in (1).
 
 
pA(ylx) = LAyix)eE fi (x,Y) (1)
zA(x) = Ep(yix)eE
 
 
The weights (A) of active features are computed by
the IIS before feature selection.
Let ,41, be AU L, and pAL be the optimal model
in the space of probability distribution after adding
feature L. The model p.,611 contains another param-
eter a, in addition to the parameters given by active
features, which is a weight for the feature L. In order
to make it tractable to compute feature selection, we
assume that the addition of a feature L affects only
the single parameter a. The only parameter which
distinguishes models of (2) is a.
 
 
1 (2)
pAf, = zct(x)PA(Yix)eaL(x&apos;Y)
Z(x) =EPA(Yix)ectMx&apos;Y)
 
 
The improvement (gain) of a model after adding a
single feature L can be estimated by measuring dif-
ference of maximum log-likelihood between L(pAL)
and L(pA). We denote the gain for feature L by
A(,41), which is represented in (3).
 
 
A (AL) maxcEGAf, (a) (3)
GAfi (a) = L(PAf) L(PA)
log PAâ€˜f
PA
 
 
The following algorithm is used to compute the
gain of the model with respect to L. We adopted
Berger&apos;s method for computing gains (Berger et
al., 1996). For the details, the reader is referred
to (Berger et al., 1996; Pietra et al., 1997).
 
 
1. Let r = {
2. Set a() = 0
3. Repeat the following until GAL (an) has con-
verged:
Compute anÂ±i from an using
 
 
anÂ±i = an + 1 log (1 G GA:tf: (0))
Compute GAL (an+i) using
GAL (a) = â€”E.73(x)logz(x)+c(fi)
GiAfi(a) = .13(M â€” Ex /3(x)m(x) ,
CA.fi (a) = â€” E. i)(x)p.Afi â€”m(x))2 Ix)
where a = an+i, Afi = Au
M(x) Pc:kfi(filx)
P.Afi(filX) = PAfi (Y1x)fi(x,Y)
4. Set â€” AL(AL) GAL (an)
 
 
This algorithm is iteratively computed using Net-
won&apos;s method. With the gain value, we can recog-
nize importance of a feature, i.e. how much the fea-
ture accords with the model. As a result, we can se-
lect useful features with high gain values and obtain
their conditional probabilities. Put another way, we
can get the correspondence probabilities of POS tag
sequences.
 
 
5 lexical alignment
 
 
In this section, we describe how the correspondence
probabilities between bilingual POS sequences is em-
bedded in lexical alignment. In Korean-English ma-
chine translation, a translation system finds the cor-
responding sentence e given a Korean sentence k.
The fundamental equation of machine translation
can be represented in (4), where the random vari-
ables K and E are a Korean sentence and a En-
glish sentence making up a translation, and the ran-
dom variable A is an alignment between them. The
equation is related with a language model probabil-
ity, P(e), a translation model probability P(kle). In
this paper, we are interested in estimating the trans-
lation model probability P(kle).
 
 
P(elk) = P(e)P(kle)
p(k)
= arg nTx P(e)P(kle) (4)
 
 
In this work, we modified the IBM 1 model using
the structural information for estimating translation
probabilities. The translation probability of a spe-
cific sentence pair, k and e is
 
 
m
P(kle) = e t(kpjlepi)P(Ckp repi ) (5)
j=1 j=1
 
 
In (5), an English sentence e has / possible
phrases, epiep2...e and a Korean sentence k has
m possible phrases, kpikp2...kpm and the phrases of
e have corresponding sequences of POS categories,
Cep1Cep2...C, , and the phrases of k have the se-
quences of PPOS categories Ckp1Ckp2...Ckpm,. E is
 
 
1 if ii(f) PAW
â€”1 otherwise
English Korean
Train Sentences 30,000
Words/Eojeol 282,967 201,771
Vocabulary 47,647 80,360
Morph Types 52,197 57,800
 
 
Table 2: Training corpus size
 
 
English words Korean morphemes Ratio
1 1 31.2
1 2 22.4
1 3 13.6
2 1 6.9
etc etc 25.9
 
 
Table 3: The result of alignment unit
 
 
a small, fixed normalizing number (Brown et al.,
1993).
Note that the probability P(Ckp, rep) is pre-
computed by the feature selection process and only
lexical alignment parameter t(kpj lep,) can be esti-
mated by the EM algorithm using sentence pairs of
bitext, (k(s), e(s)), s = 1,..., S. According to the
equation (4), we can eliminate the combination that
P(Ckp, &apos;Cep) is zero. Thus, the model iterates only
over a subset of probable alignments.
To estimate probability t(kplep), the expected
number of times(fractional counts) that the phrase
ep connects to kp in the translation (kle) is used.
The count denoted by e(kplep; k, e) can be expressed
in (7) using (6) and translation probabilities are rees-
timated by (8).
 
 
P(ale, =
P(k, ale) P(k, ale)
(6)
P(kle) E P(k, ale)
711 1
e(kplep; k, e) =
j=1 i=1
t(kplep)P(ckplcep)
=
t(kplepOP(ckplcepi)+...+t(kplepi)P(ckplcepi)
t(kplep) = sc(kplep; k,
E E e(kp iep; k(s), e(s))
kp s=1
 
 
6 Experiments
 
 
We present results tested on English-Korean bitext
that is extracted from the web site of &apos;Korea Times&apos;
and a magazine for English learning (Table 2).
We manually aligned 700 POS-tagged sentence
pairs to obtain initial parameters for correspon-
dences of tag sequences. As shown in Table 3,
the coverage of word-for-word correspondences in
English-Korean bitext was only 31.2%.
As a result, 1,483 correspondences of tag se-
quences were collected from the manually aligned
bitext (Table 4).
The correspondences were used as initial active
features and the weights of the initial active fea-
tures were computed by ITS algorithm. Table 5
 
 
set description disjoint features
A active features 1,483
P filtered feature pool 8,147
N selected new features 702
 
 
Table 4: Features(correspondences of Tag Sequence)
 
 
shows weights A of the initial active features such
that f+
 
 
,BEP-FJJ,tk E A.
 
 
Through the process of feature pool construction,
8,147 features(tag sequence correspondences) were
selected from the 23,000 sentence pairs. Since we
used the filtering method and tag sequence construc-
tion rules, the size of the feature pool was not large.
In the feature selection step, we chose useful fea-
tures with the gain threshold of 0.008. As a result of
feature selection, we obtained the probability model
that given a specific English POS tag sequence, a
corresponding Korean POS tag sequence happens to
occur.
Table 6 shows some examples of conditional prob-
abilities. The table shows that the determiner of
English is generally translated into NULL or adnom-
inal word in Korean. The conditional probabilities
regarding the correspondences of POS tag sequences
were used as known parameters of the lexical align-
ment model.
 
 
Effect of Smoothing
 
 
As mentioned before, in previous IBM-based mod-
els, many words are aligned to the empty word and
rare words are mis-aligned. Thus, we tested if the
structural features are effective in smoothing. For
this, the accuracy of lexical correspondences was
evaluated both on low frequency words and high fre-
 
 
t e p(tkite)
tk
&apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 NNIN2 0.524131
&apos;4 &apos;4 &apos;4 &apos;4 &apos;4 &apos;4 ,
+++++++
H H H H H H (1)
ANDEÂ±NNIN2 0.15161
ANNUÂ±NNDE2 0.091036
NNIN2+PPCA1 0.063515
NNIN2+NNIN2 0.058322
NNIN2+PPAU 0.05768
ADCO 0.049622
etc
 
 
Table 6: Examples of conditional probability
 
 
fte,tk Ai p(tk Ite) e k
fBEPÂ±JJ,VBMAÂ±ENC03Â±AXÂ±ENTE 10.1369 0.4247 are-Fprepared junbidoi-Feo-Fiss-Fda
fBEPÂ±JJ,V BMA 8.8520 0.1180 is-Fcareful ju&apos;yiha
fBEPÂ±JJ,AJMA 8.6787 0.0996 am-Fhealthy geongangha
fBEPÂ±JJ,AJMAÂ±ENTE 8.2628 0.0655 is-Fnew syaelob-Fda
fBEPÂ±JJ,VBMAÂ±ENTE 7.2379 0.0236 am-Fsure hwagsinha-Fbnida
fBEPÂ±JJ,NNIN2+CO 7.1372 0.0210 am-Frich buja-Fi
fBEPÂ±JJ,NNIN2+COÂ±VBMA 6.9909 0.0183 is-Fselfish igijeog-Fi-Fdoi
fBEPÂ±JJ,NNIN2+PPCA1+VBMAÂ±ENTE 6.8402 0.0157 is-Fpatriotic &apos;aegugja-Fga-Fdoi-Fda
fBEPÂ±JJ,NNIN2+COÂ±ENTE 6.8308 0.0156 is-Freasonable habligeog-Fi-Fda
fBEPÂ±JJ,NNIN2+PPCA2+AXÂ±ENTE 6.4256 0.0105 is-Frephrehensible binanbad-Feul-Fmanha-Fda
fBEPÂ±JJ,NNIN2+PPCA1+VBMA 6.4250 0.0105 am-Fhelpful doum-Fi-Fdoi-Fda
BE:be verb (present tense), DT:determiner, JJ:adjective(ordinal), NN:common noun, RB :adverb
AJMA:adjective, VBMA:verb, AX:auxilary verb ENC03:auxilary ending, ENTE:final ending
ANCO:configurative adnominal, ANDE: demonstrative adnominal , ANNU:numeral adnominal
NNIN2:common noun, NNDE2:unit-dependent noun, CO:copular
PPCA1:nominative postposition, PPCA2:accusative postposition, PPAU:auxilary postposition
 
 
Table 5: Examples of active features
 
 
frequency English Korean Accuracy
Words Eojeols
(Total)
4 100(2149) 100(3317) 70.3%
50,-400 100(712) 100(486) 78.9%
the number of parameters
IBM 1 14,776
Our Model 1,344
 
 
Table 8: Problem Space
Table 7: Evaluation Vocabulary
 
 
quency words.
Table 7 shows the accuracy of some results ob-
tained by lexical alignment. In the training bitext,
English 2149 words and Korean 3317 eojeols with low
frequency 4 and English 712 words and Korean 486
eojeols with high frequency (50-300) were found.
For an evaluation, 100 words and 100 eojeols were se-
lected out of them. The alignments of the words (eo-
jeols) were evaluated. As a result, it turns out that
the structural features are effective in rare words.
 
 
Effect of reducing a parameter space
 
 
Another advantage of the use of structural fea-
tures is to reduce parameter space of the lexical
alignment model. To show the impacts on the size of
the parameter space reduced by our model, we com-
pared the results from our model with those from
the IBM 1-model presented by Brown et al. (1993)
on 100 sentence pairs. The number of parameters
obtained means the counts of possible lexical map-
pings.
As shown in Table 8, the number of prameters
were drastically reduced in our model. Considering
the complexity of another models (IBM 2-5) it is
obvious that they have more parameters.
Effect of improving the results of lexical
alignments
For evaluating the efficacy of the structural fea-
tures, we compared the results with IBM model 1.
 
 
Accuracy
 
 
IBM 1 59.8
Our Model 73.7
 
 
Table 9: Accuracy of n:1 and 1:1 alignments
 
 
As described before, only n(English):1 and 1:1 map-
ping are possible in IBM model since one-to-one cor-
respondence is assumed. Thus, we selected only n:1
and 1:1 alignments out of the alignment results. For
comparison, we investigated the alignments of the
English 100 words with high frequency, which were
explained above.
Table 9 shows the accuracy of lexical alignments.
It is shown that the structural features have an effect
on the alignment even though the amount of data
investigated is small.
However, the overall accuracy of the alignment is
somewhat low, which is mainly due to the small size
of training samples. Table 10 shows some results of
mutual translation. We see a considerable improve-
ment when allowing for structural features (corre-
spondences of POS tag sequences) in lexical align-
ments.
 
 
Error Analysis
 
 
Except the errors by the incorrect parameter esti-
mation, most errors of correspondences of POS tag
sequences are caused by POS tagging errors. In ad-
dition, the correspondences of adverbs turn out to be
 
 
Korean English Probability
jeongbu government 0.7312
jeongbu the government 0.2012
jeongbu republic 0.0328
jeongbu authorities 0.0171
jeongbu officials 0.0119
jeongbu Chinese 0.0041
 
 
Table 10: Examples of alignment
 
 
sometimes erroneous, which is due to the fact that
the position of adverb can be moved quite free.
 
 
7 Conclusion
 
 
Because of the considerable difference between En-
glish and Korean, computation cost of lexical align-
ment is very high. One solution of the problem is to
provide mapping information of syntactic structure
between the two languages.
In this paper, we defined the structural feature as
the correspondence of POS tag sequences and pre-
sented a method for extraction of structural features
for Korean-English bilingual alignment. Firstly, the
initial active features were collected from a small
size of manually aligned bitext, which are trained by
IIS which is a training algorithm for ME. Secondly,
extracted from training data, the features giving a
large gain were added to the set of active features.
Furthermore, the features extracted were tested
for lexical alignment of bitext. The experiment
showed that the features are helpful for reducing
the mapping space in alignment. We expect that
the alignment can be more accurate and efficient by
combining the structural features with translation
lexicon in the future.
 
 
References
 
 
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-73.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21(4):543-565.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Robert L. Mercer. 1993. The math-
ematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
A. P. Dempster, N. M. Laird and D. B. Rubin.
1976. Maximum likelihood from incomplete data
via the EM algorithm. The Royal Statistics Soci-
ety, 39(B) 205-237.
Pascale Fung and Kenneth W. Church. 1994. Kvec:
A New Approach for Aligning Parallel Texts In
Proceedings of COLING 94, 1096-1102.
William A. Gale and Kenneth W. Church. 1991.
Identifying Word Correspondance in Parallel Text
In Proceedings of the DARPA NLP Workshop
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual cor-
pora. Computational Linguistics, 19:75-102.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition MIT Press.
Julian Kupiec. 1993. An algorithm for finding noun
phrase correspondences in bilingual corpora. In
Proceedings of ACL 31, 17-22.
Yuji Matsumoto. 1993. Structural matching of par-
allel texts. In Proceedings of the 31st Annual
Meeting of the ACL, 23-30.
I. Dan Melamed. 1997. A word-to-word model of
translation equivalence. In Proceedings of ACL
35/EACL 8, 16-23.
Franz Josef Och and Hermann Ney. 2000. Improv-
ing Statistical Alignment Models. In Proceedings
of ACL 38, 440-447.
H. Papageorgiou, L. Cranias and S. Piperidis. 1994.
Automatic Alignment in Parallel Corpora. In Pro-
ceedings of ACL 32 (Student Session).
Stephen A. Della Pietra, Vincent J. Della Pietra,
John D. Lafferty. 1997. Inducing features of ran-
dom fields. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 19(4):380-393.
Kengo Sato 1998. Maximum Entropy Model Learn-
ing of the Translation Rules. In Proceedings of
ACL 36/COLING, 1171-1175.
Jung H. Shin, Young S. Han, and Key-Sun
Choi. 1996. Bilingual knowledge acquisition from
Korean-English parallel corpus using alignment
method. In Proceedings of COLING 96.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach.
Computational Linguistics, 22(1):1-38.
Christoph Tillmann and Hermann Ney. 2000 Word
Re-ordering and DP-based Search in Statistical
Machine Translation. In Proceedings of COLING
2000.
Ye-Yi Wang and Alex Waibel. 1998. Modeling with
structures in machine translation. In Proceedings
of ACL 36/COLING
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2000. Finding Structural Correspondences
from Bilingual Parsed Corpus for Corpus-based
Translation. In Proceedings of COLING 2000.
Dekai Wu 1997. Stochastic Inversion Transduction
Grammar and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23-3, pp. 377-
403.
Kaoru Yamamoto and Yuji Matsumoto. 2000. Ac-
quisition of Phrase-level Bilingual Correspondence
using Dependency Structure. In Proceedings of
COLING 2000.
Juntae Yoon, Chunghee Lee, Seonho Kim, and Man-
suk Song. 1999. Morphological Analyzer of Yon-
sei University., Morany: Morphological Analysis
based on Large Lexical Database Extracted from
Corpus. In Proceedings of Hangule and Korean
Information Processing Workshop, pp.92-98.
 
 
 