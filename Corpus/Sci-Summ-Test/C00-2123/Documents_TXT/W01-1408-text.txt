An Efficient A* Search Algorithm for Statistical Machine Translation Franz Josef Och, Nicola Ueffing, Hermann Ney Lehrstuhl fu?r Informatik VI, Computer Science Department RWTH Aachen - University of Technology D-52056 Aachen, Germany {och,ueffing,ney}@informatik.rwth-aachen.de Abstract In this paper, we describe an efficient A* search algorithm for statistical ma.chine translation. In contrary to beam.search or greedy approaches it is possi.ble to guarantee the avoidance of search errors with A*. We develop various so.phisticated admissible and almost ad .missible heuristic functions. Especially our newly developped method to per.form a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sen.tences. We compare the A* search al.gorithm with a beam-search approach on the Hansards task. 1 Introduction The goal of machine translation is the transla.tion of a text given in some source language into a target language. We are given a source string fJ1 = f1...fj ...fJ , which is to be translated into a target string eI1 = e1...ei...eI . Among all possible target strings, we will choose the string with the highest probability: e?I1 = argmaxeI1 { Pr(eJ1 |f I1 ) } = argmax eI1 { Pr(eI1) ? Pr(fJ1 |eI1) } The argmax operation denotes the search prob.lem, i.e. the generation of the output sentence in the target language. Pr(eI1) is the language model of the target language, whereas Pr(fJ1 |eI1) denotes the translation model. Many statistical translation models (Brown et al., 1993; Vogel et al, 1996; Och and Ney, 2000b) try to model word-to-word correspondences be.tween source and target words. These correspon.dences are called an alignment. The model is often further restricted in a way such that each source word is assigned exactly one target word. The alignment mapping is j ? i = aj from source position j to target position i = aj . The alignment aJ1 may contain alignments aj = 0 with the ? empty? word e0 to account for source words that are not aligned to any target word. In (statistical) alignment models Pr(fJ1 , aJ1 |eI1), the alignment aJ1 is introduced as a hidden variable. Typically, the search is performed using the so.called maximum approximation: e?I1 = argmaxeI1 ? ?? ?? Pr(eI1) ? ? aJ1 Pr(fJ1 , aJ1 |eI1) ? ?? ?? = argmax eI1 { Pr(eI1) ? maxaJ1 Pr(fJ1 , aJ1 |eI1) } The search space consists of the set of all possible target language strings eI1 and all possible align.ments aJ1 . 2 IBM Model 4 Various statistical alignment models of the form Pr(fJ1 , aJ1 |eI1) have been introduced in (Brown et al, 1993; Vogel et al, 1996; Och and Ney, 2000a). In this paper we use the so-called Model 4 from (Brown et al, 1993). In Model 4 the statistical alignment model is decomposed into five sub-models: ? the lexicon model p(f |e) for the probability that the source word f is a translation of the target word e, ? the distortion model p=1(j? j? |C(fj), E) for the probability that the translations of two consecutive target words have the position difference j ? j? where C(fj) is the word class of fj and E is the word class of the first of the two consecutive target words, ? the distortion model p>1(j ? j? |C(fj)) for the probability that the words aligned to one target words have the position difference j? j? , ? the fertility model p(? |e) for the probability that a target language word e is aligned to ? source language words, ? the empty word fertility model p(? 0|e0) for the probability that exactly ? 0 words remain unaligned to. The final probability p(fJ1 , aJ1 |eI1) for Model 4 is obtained by multiplying the probabilities of the sub-models for all words. For a detailed descrip.tion for Model 4 the reader is referred to (Brown et al, 1993). We use Model 4 in this paper for two reasons. First, it has been shown that Model 4 produces a very good alignment quality in comparison to various other alignment models (Och and Ney, 2000b). Second, the dependences in the distortion model along the target language words make it quite easy to integrate standard n-gram language models in the search process. This would be more difficult in the HMM alignment model (Vogel et al., 1996). Yet, many of the results presented in the following are also applicable to other align.ment models. 3 Search problem The following tasks have to be performed both us.ing A* and beam search (BS): ? The search space has to be structured into a search graph. This search graph typically includes an initial node, intermediary nodes (partial hypotheses), and goal nodes (com.pleted hypotheses). A node contains the fol.lowing information: ? the predecessor words u, v in the target language, ? the score of the hypothesis, ? a backpointer to the preceding partial hypothesis, ? the model specific information de .scribed at the end of this subsection. ? A scoring function Q(n) + h(n) has to be defined which assigns a score to every node n. For beam search, this is the score Q(n) of a best path to this node. In the A* algorithm, an estimation h(n) of the score of a best path from node n to a goal node is added. (Berger et al, 1996) presented a method to struc .ture the search space. Our search algorithm for Model 4 uses a similar structuring of the search space. We will shortly review the basic concepts of this search space structure: Every partial hy.pothesis consists of a prefix of the target sentence and a corresponding alignment. A partial hypoth.esis is extended by accounting for exactly one ad .ditional word of the source sentence. Every exten.sion yields an extension score which is computed by taking into account the lexicon, distortion, and fertility probabilities involved with this extension. A partial hypothesis is called open if more source words are to be aligned to the current target word in the following extensions. A hypothesis that is not open is said to be closed. Every extension of an open hypothesis will extend the fertility of the previously produced target word and an extension of a closed hypothesis will produce a new word. Therefore, the language model score is added as well if a closed hypothesis is extended. It is prohibitive to consider all possible transla.tions of all words. Instead, we restrict the search to the most promising candidates by calculating ?inverse translations? (Al-Onaizan et al, 1999). The inverse translation probability p(e | f) of a source word f is calculated as p(e | f) = p (f | e) p (e)? e? p (f | e ? ) p (e? ) , where we use a unigram model p (e) to esti.mate the prior probability of a target word be .ing used. Like (Al-Onaizan et al, 1999), we use only the top 12 translations of a given source lan.guage word. In addition, we remove from this list all words whose inverse translation probability is lower than 0.01 times the best inverse translation probability. This observation pruning is the only pruning involved in our A* search algorithm. Ex.periments showed this does not impair translation quality, but the search becomes much more effi.cient. In order to keep the search space as small as possible it is crucial to perform a recombina.tion of search hypotheses. Every two hypothe.ses which can be distinguished by neither the lan.guage model state nor the translation model state can be recombined, only the hypothesis with a better score of the two needs to be considered in the subsequent search process. We use a standard trigram language model, so the relevant language model state of node n consists of the current word w(n) and the previous word v(n) (later on we will describe an improvement to this). The translation model state depends on the specific model depen.dencies of Model 4: ? a coverage set C(n) containing the already translated source language positions, ? the position j(n) of the previously translated source word, ? a flag indicating whether the hypothesis is open or closed, ? the number of source language words which are aligned to the empty word, ? a flag showing whether the hypothesis is a complete hypothesis or not. Efficient language model recombination The recombination procedure which is described above can be improved by taking into account the backing-off structure of the language model. The trigram language model we use has the property that if the count of the bigram N(u, v) = 0, then the probability P (w|u, v) depends only on v. In this case the recombination can be significantly improved by recombining all nodes whose lan.guage model state has the property N(u, v) = 0 only with respect to v. Obviously, this could be generalized to other types of language models as well. Experiments have shown that by using this ef.ficient recombination, the number of needed hy.potheses can be reduced by about a factor of 4. Search algorithms We evaluate the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner. The search algo.rithm is based on a dynamic programming approach and applies various pruning tech.niques in order to restrict the number of con .sidered hypotheses. For more details see (Tillmann, 2001). ? A* search algorithm: In A*, all search hypotheses are managed in a priority queue. The basic A* search (Nils.son, 1971) can be described as follows: 1. initialize priority queue with an empty hypothesis 2. remove the hypothesis with the highest score from the priority queue 3. if this hypothesis is a goal hypothesis: output this hypothesis and terminate 4. produce all extensions of this hypothe.sis and put the extensions to the queue 5. goto 2 The so-called heuristic function estimates the probability of a completion of a partial hypothesis. This function is called admissi.ble if it never underestimates this probabil.ity. Thus, admissible heuristic functions are always optimistic. The A* search algorithm corresponds to the Dijkstra algorithm if the heuristic function is equal to zero. 4 Admissible heuristic function In order to perform an efficient search with the A* search algorithm it is crucial to use a good heuristic function. We only know of the work by (Wang and Waibel, 1997) dealing with heuristic functions for search in statistical machine trans.lation. They developed a simple heuristic func.tion for Model 2 from (Brown et al, 1993) which was non admissible. In the following we de .velop a guaranteed admissible heuristic function for Model 4 taking into account distortion proba.bilities and the coupling of lexicon, fertility, and language model probabilities. The basic idea for developing a heuristic func.tion for the alignment models is the fact that all source sentence positions which have not been covered so far still have to be translated in order to complete the sentence. Therefore, the value of the heuristic function HX(n) for a node n can be deduced if we have an estimation hX(j) of the optimal score of translating position j (here X de .notes different possibilities to choose the heuristic function): HX(n) = ? j 6? C(n) hX(j) , where C(n) is the coverage set. The simplest realization of a heuristic func.tion, denoted as hT (j), takes into account only the translation probability p(f |e): hT (j) = maxe p(fj |e) This heuristic function can be refined by intro.ducing also the fertility probabilities (symbol F) of a target word e: hTF (j) = = max { max e 6=e0, ? p(fj |e) ? ? p(? |e), p(f |e0) } Thereby, a coupling between the translation and fertility probabilities is achieved. We have to take the ? -th root in order to avoid that the fer.tility probability of a target word whose fertility is higher than one is taken into account for every source word aligned to it. For words which are translated by the empty word e0, no fertility prob.ability is used. The language model can be incorporated by considering that for every target word there exists an optimal language model probability: pL(e) = maxu,v p(e|u, v) Here, we assume a trigram language model. Thus, a heuristic function including a coupling between translation, fertility, and language model probabilities (TFL) is given by: hTFL(j) = = max { max e,? p(fj |e) ? ? p(? |e)pL(e), p(f |e0) } This value can be precomputed efficiently before the search process itself starts. The heuristic function for the distortion proba.bilities depends on the used model. For Model 4, we obtain: hD(j) = max j? ,E p(j ? j? |E,C(fj)) Here, E refers to the class of the previously aligned target word. The heuristic functions hD(j) involve maxi.mizations over the source positions j?. The do.main of this variable shrinks during search as more and more words get translated. Therefore, it is possible to improve this heuristic function dur.ing search to perform a maximization only over the free source language positions j?. For Model 4 we compute the following heuristic function with two arguments: hD(j? , j) = max E p(j ? j? |E,C(fj)) Thus, we obtain as an estimation of the distortion probability hD(j) = max j? 6? C(n) hD(j? , j) . This yields the following heuristic functions tak.ing into account translation, fertility, language, and distortion model probabilities: HTFLD(n) = ? j 6? C(n) hTFL(j) ? hD(j) (1) Using these heuristic functions we have the over.head of performing this rest cost estimation for every coverage set in search. The experiments will show that these additional costs are over.compensated by the gain in reducing the search space that has to be expanded during the A* search. To assess the predictive power of the vari.ous components in the heuristic, we compare the value of the heuristic function of the empty hy.pothesis with the score of the optimal transla.tion. A heuristic function is better if the dif.ference between these two values is small. Ta.ble 1 contains a comparison of various heuristic functions. We compare the average costs (nega.tive logarithm of the probabilities) of the optimal translation and the average of the estimated costs of the empty hypothesis. Typically, the estimated costs of TFLD and the real costs differ by factor 3. 
Table 1: Predictive power of admissible and almost admissible heuristic functions. 
sentence HF for initial node empirical goal node 
length T TF TFL TFLD score score 
6 5.1 7.2 12.7 13.0 25.9 35.5 
8 5.7 8.2 16.0 16.3 29.8 43.7 
10 8.1 11.6 19.4 19.7 36.5 55.8 
12 9.5 13.7 20.7 21.1 43.9 63.4 
We will see later in Section 6 that the guar.anteed admissible heuristic functions described 
above result in dramatically more efficient search. 
5 Empirical heuristic functions 
In this section we describe a new method to ob.tain an almost admissible heuristic function by a 
multi pass search. This yields a significantly more 
efficient search than using the admissible heuris .tic functions. Thus, we lose the strict guarantee to 
avoid search errors, but obtain a significant time 
gain. 
The idea of an empirical heuristic function 
is to perform a multi-pass search. In the first 
pass a good admissible heuristic function (here: 
HTFLD) is used. If this search does not need too 
much memory the search process is finished. If 
the search failed, it is restarted using an improved 
heuristic function which had been obtained during 
the initial search process. This heuristic function 
is computed such that it has the property that it 
is admissible with respect to the explored search 
space. That means, the heuristic function is op.timistic with respect to every node in the search 
space explored in the first pass. 
Specifically, during the first pass, we maintain 
a two-dimensional matrix hE(j, j?) with (J +2) ? 
(J + 2) entries which are all initialized with ?. 
The entry hE(j, j? ) is the best score that was com.puted for translating the source language word in 
position j? if the previously covered source sen.tence position is j. The matrix entry is updated 
for every extension of a node n ? n? : 
hE(j(n), j(n? )) := 
= max 
{ 
hE(j(n), j(n? )), p(n, n? ) 
} 
Here, p(n, n? ) is the probability of the extension 
n ? n?. hE(0, j) is the empirical score of start.ing a sentence by covering the j-th source sen.tence position first. Likewise, hE(j, J + 1) is the 
empirical score of finishing a sentence with j as the last source sentence position that was covered. This yields hE(j) = max j? 6? C(n)? j?=J+1 hE(j, j? ) . In this calculation of hE(j), we maximize over the columns of a matrix. The translation of the source sentence can be viewed as a Traveling Salesman Problem where the source sentence po.sitions are the cities that have to be visited. Thus, the maximization over the columns is equivalent to assuring that the position j will be left after the visit. We design an improved heuristic func.tion using the following principle (Aigner, 1993): Each city has to be both reached and left. There.fore, in order to take an upper bound of reaching a city into account, we divide each column of the matrix by its maximum and maximize over the rows of the matrix (Aigner, 1993): hE+(j) = max j? 6? C(n)? j?=j(n) hE(j?, j)/hE(j? ) . We obtain the following empirical heuristic func.tions: HE(n) = ? j 6? C(n)? j=j(n) hE(j) HE+(n) = = ? j 6? C(n)? j=j(n) hE(j) ? ? j? 6? C(n)? j?=J+1 hE+(j? ) If the search fails in the first pass due to the re.striction of the number of hypotheses ? which was 1 million in all experiments ? the search can be started again using HE+(n) as a heuristic. To avoid an overestimation of the actual costs, we multiply the empirical costs by a factor lower than Table 3: Training corpus statistics (* without punctuation marks). French English sentences 49000 49000 words 743903 816964 words* 664058 730880 average sentence length 16.9 14.6 vocabulary size 19831 24892 Table 4: Test corpora statistics. Corpus # Sentences # Words F E T6 50 300 329 T8 50 400 403 T10 50 500 509 T12 50 600 601 T14 50 700 644 1. We found in our experiments that a factor of 0.7 is sufficient. The search was restarted up to 4 times if it failed. Using this method, it is possi.ble to translate sentences that are longer than 10 words with a restriction to 1 million hypotheses. Table 1 shows the value of the empirical heuris.tic function of the empty node compared to the score of the optimal goal node. The estimated costs and the real costs now differ only by a fac.tor of 1.5 instead of a factor of 3 for the TFLD heuristic function before. 6 Results We present results on the HANSARDS task which consists of proceedings of the Canadian parlia.ment that are kept both in French and in English. Table 3 shows the details of our training corpus. We used different the test corpora with sentences of length 6-14 words (Table 4). In all experiments, we use the following two error criteria: ? WER (word error rate): The WER is computed as the minimum number of substitution, insertion and dele.tion operations that have to be performed to convert the generated string into the target string. ? PER (position independent word error rate): The word order of a French/English sentence pair can be quite different. As a result, the word order of the automatically generated target sentence can be different from that of the given target sentence, but nevertheless acceptable so that the WER measure alone could be misleading. In order to overcome this problem, we introduce the position inde.pendent word error rate (PER) as additional measure. This measure compares the words in the two sentences without taking the word order into account. In the following experiments we restricted the maximum number of active search hypotheses in A* search to 1 million. Every hypothesis has an effective memory requirement of about 100 Byte. Therefore, we obtain a dynamic memory require.ment of about 100 MByte. In order to speed up the search, we restricted the reordering of words in IBM-style (Berger et al., 1996; Tillmann, 2001). According to this re.striction, up to 3 source sentence positions may be skipped and translated later, i. e. during the search process there may be up to 3 uncovered positions left of the rightmost covered position in the source sentence. The word error rate does not increase compared to a non-restricted reordering, but the search becomes much more efficient. Table 5 shows how many sentences with differ.ent sentence lengths can be translated using beam search and A* with various heuristic functions. Obviously, the BS approach is able to translate any sentence length, therefore the search success rate is 100%. Without any heuristic function A* is only able to translate all 8-word sentences (with the restriction of a maximum number of 1 million hypotheses). Using more sophisticated heuristic functions we are also able to translate all 10-word sentences with A*. Table 6 compares the search errors of A* and BS. During the BS search, translation pruning is carried out. The different hypotheses are dis.tinguished according to the set of covered posi.tions of the source sentence. For every set, the best score of all hypotheses is computed. Only those hypotheses are kept whose score is greater than this best score multiplied with a threshold. We chose the threshold to be 2.5, 5.0, 7.5 and 10.0 (see Table 6). Table 2: Effect of observation pruning on the translation quality (average over all test sets). # inverse 10 12 14 16 18 20 translations WER 73.81 73.33 75.50 76.23 76.19 76.59 PER 68.02 66.93 70.07 71.16 71.24 71.16 Table 5: Search Success Rate (1 million hypothe.ses) [%]. sentence length 6 8 10 12 BS 100 100 100 100 A*: no 100 100 86 12 T 100 100 88 20 TF 100 100 88 22 TFL 100 100 92 36 TFLD 100 100 92 36 E 100 100 100 74 E+ 100 100 100 84 Table 6: Search errors [%]. sentence length 6 8 10 12 14 BS 2.5 26 28 38 50 38 5.0 2 0 2 6 4 7.5 0 0 0 4 2 10.0 0 0 0 4 2 A* 0 0 0 0 0 For A* we never observe any search errors. In the case of the admissible heuristic functions, this is guaranteed by the approach. As can be seen from Table 6, the BS algorithm with a large beam rarely produces search errors. Table 7 compares the translation efficiency of the various search algorithms. We see that beam search even with a very large beam producing only very few search errors is much more efficient than the used A* search algorithm. Table 8 contains an assessment of translation quality comparison of A* and BS using the T6, T8, T10, T12-test corpus. For A*, we use the E+ rest cost estimation as this gives optimal results. From the 200 sentences of these test corpora we can translate 192 sentences using the 1 million hy.potheses constraint. For the remaining sentences we performed a search with 4 million hypotheses Table 7: Average search time [s] per sentence. sentence length 6 8 10 12 BS: 2.5 0.06 0.18 0.60 1.16 5.0 0.24 0.84 2.90 6.48 7.5 0.50 2.14 7.06 16.26 10.0 0.78 3.30 11.86 26.42 A*: E+ 1.58 13.04 100 394 (cf. below) which lead to a success for all the 12.word sentences. The number of hypotheses in A* search We restricted the maximal number of hypotheses to 1 million. This was sufficient for translating 10-word sentences, as the search algorithm suc.cess rate in Table 5 shows. For longer sentences it is necessary to allow for a larger number of hy.potheses. For the sentences of lengths 12 and 14, we performed an A* search (E+) with 2, 4 and 8 million possible hypotheses. The search algo.rithm success rate for those searches is contained in Table 9. We see a significant effect on the num.ber of successful searches. 7 Conclusion We have developed sophisticated admissible and almost admissible heuristic functions for statis.tical machine translation. We have focussed on Model 4, but most of the computations could be easily extended to other statistical alignment models (like HMM or Model 5). We especially have observed the following effects: ? The heuristic function has a strong effect on the efficiency of the A* search. Without any heuristic function only 75 % of the test cor.pus sentences can be translated (using the 1 million hypotheses constraint). Using the Table 8: Translation quality. BS (2.5) BS (5.0) BS (7.5) BS (10.0) A* (E+) WER 69.65 68.78 68.68 68.68 68.68 PER 62.65 61.62 61.51 61.51 61.45 Table 9: A* (E+) Success Rate for 12- and 14-word sentences [%]. # hypotheses 1 million 2 million 4 million 8 million 12 42 80 100 100 14 2 20 70 100 best admissible heuristic function TFLD we can translate 82 %. ? Using the empirical heuristic function we can translate 96 % of the sentences with A* search. This heuristic function does not guarantee to avoid search errors, but this case never occurred in our experiments. From these results we conclude that it is often possible to faster compute acceptable results us.ing a beam search approach. Therefore, this is the method of choice in practice. From a theo.retical viewpoint it is interesting that using A* it is possible to translate guaranteed without search errors. In addition, without having a chance to perform search without search errors it is almost impossible to assess if errors in translation should be assigned to the model/training or to the search heuristics. Therefore, the A* algorithm is espe.cially useful during the development of a statisti.cal machine translation system. Acknowledgment This paper is based on work supported partly by the VERBMOBIL project (contract number 01 IV 701 T4) by the German Federal Min.istry of Education, Science, Research and Tech.nology. In addition, this work was supported by the National Science Foundation under Grant No. IIS-9820687 through the 1999 Workshop on Language Engineering, Center for Language and Speech Processing, Johns Hopkins University. References M. Aigner. 1993. Diskrete Mathematik. Verlag Vieweg, 
Braunschweig/Wiesbaden, Germany. 
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Laf.ferty, I. D. Melamed, F. J. Och, D. Purdy, N. A. 
Smith, and D. Yarowsky. 1999. Statistical ma.chine translation, final report, JHU workshop. 
http://www.clsp.jhu.edu/ws99/projects/ 
mt/final report/mt-final-report.ps. 
A. L. Berger, S. A. Della Pietra P. F. Brown, V. J. Della 
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer. 
1996. Language translation apparatus and method of us.ing context -based translation models. In United States 
Patent, number 5510981. April. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. 
Mercer. 1993. The mathematics of statistical machine 
translation: Parameter estimation. Computational Lin.guistics, 19(2):263 ? 311. 
N. Nilsson. 1971. Problem-Solving Methods in Artificial 
Intelligence. McGraw-Hill, McGraw-Hill, New York. 
F. J. Och and H. Ney. 2000a. A comparison of alignment 
models for statistical machine translation. In COLING ?00: The 18th Int. Conf. on Computational Linguistics, pages 1086? 1090, Saarbru? cken, Germany, August. F. J. Och and H. Ney. 2000b. Improved statistical alignment 
models. In Proc. of the 38th Annual Meeting of the As.sociation for Computational Linguistics, pages 440? 447, 
Hongkong, China, October. 
C. Tillmann and H. Ney. 2000. Word re-ordering and DP.based search in statistical machine translation. In COL-
ING ? 00: The 18th Int. Conf. on Computational Linguis.tics, pages 850?856, Saarbru? cken, Germany, August. 
C. Tillmann. 2001. Word Re-Ordering and Dynamic Pro.gramming based Search Algorithms for Statistical Ma.chine Translation. Ph.D. thesis, RWTH Aachen, Ger.many, May. 
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM -based word 
alignment in statistical translation. In COLING ?96: The 
16th Int. Conf. on Computational Linguistics, pages 836? 
841, Copenhagen, August. 
Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in 
statistical translation. In Proc. 35th Annual Conf. of the 
Association for Computational Linguistics, pages 366? 
372, Madrid, Spain, July. 
