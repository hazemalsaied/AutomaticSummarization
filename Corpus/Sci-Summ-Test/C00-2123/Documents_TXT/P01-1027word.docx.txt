Refined Lexicon Models for Statistical Machine Translation using a
Maximum Entropy Approach



Ismael Garc´ıa Varea

Dpto. de Informa´tica Univ. 
de Castilla-La Mancha 
Campus Universitario s/n
02071 Albacete, Spain
ivarea@info-ab.uclm.es



Franz J. Och and 
Hermann Ney 
Lehrstuhl fu¨ r Inf. VI 
RWTH Aachen 
Ahornstr., 55
D-52056 Aachen, Germany
och|ney @cs.rwth-aachen.de



Francisco Casacuberta

Dpto. de Sist. Inf. y Comp. 
Inst. Tecn. de Inf. (UPV) 
Avda. de Los Naranjos, s/n
46071 Valencia, Spain
fcn@iti.upv.es





Abstract

Typically, the lexicon models used in 
statistical machine translation systems 
do not include any kind of linguistic 
or contextual information, which often 
leads to problems in performing a cor- 
rect word sense disambiguation.  One 
way to deal with this problem within 
the statistical framework is to use max- 
imum entropy methods.  In this paper, 
we present how to use this type of in- 
formation within a statistical machine 
translation system.  We show that it is 
possible to significantly decrease train- 
ing and test corpus perplexity of the 
translation models. In addition, we per- 
form a rescoring of     -Best lists us- 
ing our maximum entropy model and 
thereby yield an improvement in trans- 
lation quality. Experimental results are 
presented on the so-called “Verbmobil 
Task”.


1   Introduction

Typically, the lexicon models used in statistical 
machine translation systems are only single-word 
based, that is one word in the source language cor- 
responds to only one word in the target language.
  Those lexicon models lack from context infor- 
mation that can be extracted from the same paral- 
lel corpus. This additional information could be:

Simple context information: information of 
the words surrounding the word pair;

Syntactic  information:	part-of-speech  in- 
formation,  syntactic  constituent,  sentence


Semantic information:  disambiguation in- 
formation (e.g.     from WordNet), cur- 
rent/previous speech or dialog act.

To include this additional information within the 
statistical framework we use the maximum en- 
tropy approach. This approach has been applied 
in natural language processing to a variety of 
tasks. (Berger et al., 1996) applies this approach 
to the so-called IBM Candide system to build con- 
text dependent models, compute automatic sen- 
tence splitting and to improve word reordering in 
translation. Similar techniques are used in (Pap- 
ineni et al., 1996; Papineni et al., 1998) for so- 
called direct translation models instead of those 
proposed in (Brown et al., 1993). (Foster, 2000) 
describes two methods for incorporating informa- 
tion about the relative position of bilingual word 
pairs into a maximum entropy translation model. 
Other authors have applied this approach to lan- 
guage modeling (Rosenfeld, 1996; Martin et al.,
1999; Peters and Klakow, 1999). A short review 
of the maximum entropy approach is outlined in 
Section 3.

2   Statistical Machine Translation

The goal of the translation process in statisti- 
cal machine translation can be formulated as fol- 
lows:  A source language string
is to be translated into a target language string
            .   In the experiments reported in 
this paper, the source language is German and the 
target language is English. Every target string is 
considered as a possible translation for the input. 
If we assign a probability                   to each pair 
of strings              , then according to Bayes’ de- 
cision rule, we have to choose the target string


model               and the string translation model
.
  Many existing systems for statistical machine 
translation (Berger et al., 1994; Wang and Waibel,
1997; Tillmann et al., 1997; Nießen et al., 1998) 
make use of a special way of structuring the string 
translation model like proposed by (Brown et al.,


Source Language Text



Transformation

J
1

Global Search:










Pr( f 1     1


1993): The correspondence between the words in



I	J      I


the source and the target string is described by


maximize  Pr( e 1 )

I


Pr( f 1 | e 1 )


alignments that assign one target word position
to each source word position. The lexicon prob- 
ability	of a certain target word	to occur
in the target string is assumed to depend basically


over   e 1


Pr(


e I )



Language Model


only on the source word    aligned to it.
  These alignment models are similar to the con- 
cept of Hidden Markov models (HMM) in speech 
recognition.   The alignment mapping is
from source position	to target position
      .  The alignment       may contain align- 
ments               with the ‘empty’ word      to ac- 
count for source words that are not aligned to 
any target word. In (statistical) alignment models
            , the alignment	is introduced as 
a hidden variable.
  Typically, the search is performed using the so- 
called maximum approximation:








The search space consists of the set of all possible 
target language strings      and all possible align- 
ments     .
  The overall architecture of the statistical trans- 
lation approach is depicted in Figure 1.

3   Maximum entropy modeling

The translation probability                         can be 
rewritten as follows:


Transformation



Target Language Text


Figure 1: Architecture of the translation approach 
based on Bayes’ decision rule.


Typically, the probability                                  is 
approximated by a lexicon model                 by 
dropping the dependencies on        ,        , and    .
Obviously, this simplification is not true for a lot
of natural language phenomena. The straightfor- 
ward approach to include more dependencies in 
the lexicon model would be to add additional de- 
pendencies(e.g.                          ). This approach 
would yield a significant data sparseness problem.
  Here, the role of maximum entropy (ME) is to 
build a stochastic model that efficiently takes a 
larger context into account. In the following, we 
will use             to denote the probability that the 
ME model assigns to    in the context    in order 
to distinguish this model from the basic lexicon 
model           .
  In the maximum entropy approach we describe 
all properties that we feel are useful by so-called 
feature functions	.	For example, if we 
want to model the existence or absence of a spe- 
cific word	in the context of an English word 
which has the translation		 we can express this 
dependency using the following feature function:


if	and
otherwise	(1)


parametric form of a model             taking into 
account  only  the  feature  functions
is given by:






  Here           is a normalization factor.  The re- 
sulting model has an exponential form with free 
parameters                                .   The parameter 
values which maximize the likelihood for a given 
training corpus can be computed with the so- 
called GIS algorithm (general iterative scaling) 
or its improved version IIS (Pietra et al., 1997; 
Berger et al., 1996).
  It is important to notice that we will have to ob- 
tain one ME model for each target word observed 
in the training data.

4	Contextual information and training 
events

In order to train the ME model              associated 
to a target word  , we need to construct a corre- 
sponding training sample from the whole bilin- 
gual corpus depending on the contextual informa- 
tion that we want to use. To construct this sample, 
we need to know the word-to-word alignment be- 
tween each sentence pair within the corpus. That 
is obtained using the Viterbi alignment provided 
by a translation model as described in (Brown et 
al., 1993). Specifically, we use the Viterbi align- 
ment that was produced by Model 5. We use the 
program GIZA++ (Och and Ney, 2000b; Och and 
Ney, 2000a), which is an extension of the training 
program available in EGYPT (Al-Onaizan et al.,
1999).
  Berger et al. (1996)  use  the  words  that  sur- 
round a specific word pair          as contextual in- 
formation. The authors propose as context the 3 
words to the left and the 3 words to the right of 
the target word. In this work we use the following 
contextual information:

Target context: As in (Berger et al., 1996) we 
consider a window of 3 words to the left and 
to the right of the target word considered.

Source context:  In addition, we consider a 
window of 3 words to the left of the source


word    which is connected to   according to 
the Viterbi alignment.

Word classes: Instead of using a dependency 
on the word identity we include also a de- 
pendency on word classes. By doing this, we 
improve the generalization of the models and 
include some semantic and syntactic infor- 
mation with. The word classes are computed 
automatically using another statistical train- 
ing procedure (Och, 1999) which often pro- 
duces word classes including words with the 
same semantic meaning in the same class.

  A training event, for a specific target word  , is 
composed by three items:

The source word    aligned to  .

The context in which the aligned pair 
appears.

The number of occurrences of the event in 
the training corpus.

Table 1 shows some examples of training events 
for the target word “which”.

5   Features

Once we have a set of training events for each tar- 
get word we need to describe our feature func- 
tions. We do this by first specifying a large pool 
of possible features and then by selecting a subset 
of “good” features from this pool.

5.1   Features definition
All  the  features  we  consider  form  a  triple
(       label-1 label-2) where:

pos: is the position that label-2 has in a spe- 
cific context.

label-1: is the source word    of the aligned 
word pair            or the word class of the 
source word    (        ).

label-2: is one word of the aligned word pair 
or the word class to which these words
belong (                 ).

Using this notation and given a context   :


Table 1: Some training events for the English word “which”. The symbol “  ” is the placeholder of the 
English word “which” in the English context. In the German part the placeholder (“  ”) corresponds 
to the word aligned to “which”, in the first example the German word “die”, the word “das” in the 
second and the word “was” in the third. The considered English and German contexts are separated by 
the double bar “  ”.The last number in the rightmost position is the number of occurrences of the event 
in the whole corpus.

Al
ig. 
w
or
d (  
)
C
o
n
t
e
x
t
 
(
  
)
# 
of 
oc
cu
r.

d
i
e
 
d
a
s
 
w
a
s


2


1


1


Table 2: Meaning of different feature categories where	represents a specific target word and	repre- 
sents a specific source word.

Ca
te
go
ry
if and 
only if 
...
1
2
2
3
3
6
7

a
n
d
 
a
n
d
 
a
n
d
 
a
n
d
 
a
n
d

a
n
d









for the word pair	, we use the following 
categories of features:

1. (	)

2. (	) and

3. (	) and

4. (	) and

5. (	) and

6. (	) and

7. (	) and

8. (	) and

9. (	) and

  Category 1 features depend only on the source 
word	and the target word	. A ME model that


uses only those, predicts each source translation 
with the probability		determined by the 
empirical data.  This is exactly the standard lex- 
icon probability	employed in the transla- 
tion model described in (Brown et al., 1993) and
in Section 2.

  Category 2 describes features which depend in 
addition on the word    one position to the left or 
to the right of    .  The same explanation is valid 
for category 3 but in this case    could appears in 
any position of the context   .  Categories 4 and
5 are the analogous categories to 2 and 3 using 
word classes instead of words. In the categories
6, 7, 8 and 9 the source context is used instead of 
the target context.  Table 2 gives an overview of 
the different feature categories.

  Examples of specific features and their respec- 
tive category are shown in Table 3.


Table 3: The 10 most important features and their 
respective category and    values for the English 
word “which”.


Table 4:  Number of features used according to 
different cut-off threshold. In the second column 
of the table are shown the number of features used 
when only the English context is considered. The 
third column correspond to English, German and 
Word-Classes contexts.



















5.2   Feature selection

The number of possible features that can be used 
according to the German and English vocabular- 
ies and word classes is huge.   In order to re- 
duce the number of features we perform a thresh- 
old based feature selection, that is every feature 
which occurs less than     times is not used. The 
aim of the feature selection is two-fold.  Firstly, 
we obtain smaller models by using less features, 
and secondly, we hope to avoid overfitting on the 
training data.
  In order to obtain the threshold    we compare 
the test corpus perplexity for various thresholds. 
The different threshold used in the experiments 
range from 0 to 512.  The threshold is used as a 
cut-off for the number of occurrences that a spe- 
cific feature must appear. So a cut-off of 0 means 
that all features observed in the training data are 
used.  A cut-off of 32 means those features that 
appear 32 times or more are considered to train 
the maximum entropy models.
We select the English words that appear at least
150 times in the training sample which are in total
348 of the 4673 words contained in the English 
vocabulary.  Table 4 shows the different number 
of features considered for the 348 English words 
selected using different thresholds.
  In choosing a reasonable threshold we have to 
balance the number of features and observed per- 
plexity.







6   Experimental results

6.1   Training and test corpus

The “Verbmobil Task” is a speech translation task 
in the domain of appointment scheduling, travel 
planning, and hotel reservation.  The task is dif- 
ficult because it consists of spontaneous speech 
and the syntactic structures of the sentences are 
less restricted and highly variable. For the rescor- 
ing experiments we use the corpus described in 
Table 5.


Table 5:  Corpus characteristics for translation 
task.


Ge
rm
an
E
ng
lis
h
T
r
a
i
n
	
S
e
n
t
e
n
c
e
s
 
W
o
r
d
s
 
V
o
c
a
b
u
l
a
r
y
5
8
 
3
3
2

51
9 
52
3
54
9 
92
1

7
 
9
4
0
4
 
6
7
3
Te
st	Sentences
W
o
r
d
s
P
P
 
(
t
r
i
g
r
.
 
L
M
)
1
4
7

1
 
9
6
8
2
 
1
7
3

(
4
0
.
3
)
2
8
.
8


  To train the maximum entropy models we used 
the “Ristad ME Toolkit” described in (Ristad,
1997).  We performed 100 iteration of the Im- 
proved Iterative Scaling algorithm (Pietra et al.,
1997)  using  the  corpus  described  in  Table  6,



Table 6:  Corpus characteristics for perplexity 
quality experiments.


Ge
rm
an
E
ng
lis
h
T
r
a
i
n
	
S
e
n
t
e
n
c
e
s
 
W
o
r
d
s
 
V
o
c
a
b
u
l
a
r
y
5
0
 
0
0
0

45
4 
61
9
48
2 
34
4

7
 
4
5
6
4
 
4
2
0
Te
st	Sentences
W
o
r
d
s
V
o
c
a
b
u
l
a
r
y
8
0
7
3

6
4 
8
7
5
6
5 
5
4
7

2
 
5
7
9
1
 
6
6
6


which is a subset of the corpus shown in Table 5.

6.2   Training and test perplexities

In order to compute the training and test perplex- 
ities, we split the whole aligned training corpus 
in two parts as shown in Table 6.  The training 
and test perplexities are shown in Table 7.   As 
expected, the perplexity reduction in the test cor- 
pus is lower than in the training corpus, but in 
both cases better perplexities are obtained using 
the ME models. The best value is obtained when 
a threshold of 4 is used.
  We expected to observe strong overfitting ef- 
fects when a too small cut-off for features gets 
used.  Yet, for most words the best test corpus 
perplexity is observed when we use all features 
including those that occur only once.


Table  7:    Training  and  Test  perplexities  us- 
ing different contextual information and different 
thresholds	. The reference perplexities obtained 
with the basic translation model 5 are TrainPP =
10.38 and TestPP = 13.22.


6.3   Translation results

In order to make use of the ME models in a statis- 
tical translation system we implemented a rescor- 
ing algorithm.  This algorithm take as input the 
standard lexicon model (not using maximum en- 
tropy) and the 348 models obtained with the ME 
training. For an hypothesis sentence     and a cor- 
responding alignment      the algorithm modifies 
the score                         according to the refined 
maximum entropy lexicon model.
  We carried out some preliminary experiments 
with the    -best lists of hypotheses provided by 
the translation system in order to make a rescor- 
ing of each i-th hypothesis and reorder the list ac- 
cording to the new score computed with the re- 
fined lexicon model.  Unfortunately, our     -best 
extraction algorithm is sub-optimal, i.e.  not the 
true best     translations are extracted. In addition, 
so far we had to use a limit of only     translations 
per sentence. Therefore, the results of the transla- 
tion experiments are only preliminary.
  For the evaluation of the translation quality 
we use the automatically computable Word Er- 
ror Rate (WER). The WER corresponds to the 
edit distance between the produced translation 
and one predefined reference translation. A short- 
coming of the WER is the fact that it requires a 
perfect word order.  This is particularly a prob- 
lem for the Verbmobil task, where the word or- 
der of the German-English sentence pair can be 
quite different.   As a result, the word order of 
the automatically generated target sentence can 
be different from that of the target sentence, but 
nevertheless acceptable so that the WER measure 
alone can be misleading.  In order to overcome 
this problem, we introduce as additional measure 
the position-independent word error rate (PER). 
This measure compares the words in the two sen- 
tences without taking the word order into account. 
Depending on whether the translated sentence is 
longer or shorter than the target translation, the 
remaining words result in either insertion or dele- 
tion errors in addition to substitution errors. The 
PER is guaranteed to be less than or equal to the 
WER.
  We use the top-10 list of hypothesis provided 
by the translation system described in (Tillmann 
and Ney, 2000) for rescoring the hypothesis us- 
ing the ME models and sort them according to the


new maximum entropy score. The translation re- 
sults in terms of error rates are shown in Table 8. 
We use Model 4 in order to perform the transla- 
tion experiments because Model 4 typically gives 
better translation results than Model 5.
  We see that the translation quality improves 
slightly with respect to the WER and PER. The 
translation quality improvements so far are quite 
small compared to the perplexity measure im- 
provements. We attribute this to the fact that the 
algorithm for computing the    -best lists is sub- 
optimal.


Table 8:  Preliminary translation results for the 
Verbmobil Test-147 for different contextual infor- 
mation and different thresholds using the top-10 
translations.  The baseline translation results for 
model 4 are WER=54.80 and PER=43.07.


En
glis
h
Englis
h+Ger
man

W
E
R
P
E
R
W
E
R
P
E
R
0
2
4
8
1
6
3
2
6
4
1
2
8
2
5
6
5
1
2
54
.5
7
54
.1
6
54
.5
3
54
.7
6
54
.7
6
54
.8
0
54
.2
1
54
.5
7
54
.9
9
55
.0
8
42
.9
8
42
.4
3
42
.7
1
43
.2
1
43
.5
3
43
.1
2
42
.8
9
42
.9
8
43
.1
2
43
.3
0
54
.0
2
54
.0
7
54
.1
1
54
.3
9
54
.0
2
54
.5
3
54
.5
3
54
.6
7
54
.5
7
54
.8
5
4
2
.
4
8
4
2
.
7
1
4
2
.
7
5
4
3
.
0
7
4
2
.
7
5
4
2
.
9
4
4
2
.
8
9
4
3
.
1
2
4
2
.
8
9
4
3
.
2
1


  Table 9 shows some examples where the trans- 
lation obtained with the rescoring procedure is 
better than the best hypothesis provided by the 
translation system.

7   Conclusions

We have developed refined lexicon models for 
statistical machine translation by using maximum 
entropy models.  We have been able to obtain a 
significant better test corpus perplexity and also a 
slight improvement in translation quality. We be- 
lieve that by performing a rescoring on translation 
word graphs we will obtain a more significant im- 
provement in translation quality.
  For the future we plan to investigate more re- 
fined feature selection methods in order to make 
the maximum entropy models smaller and better


generalizing. In addition, we want to 
investigate more syntactic, semantic features 
and to include features that go beyond sentence 
boundaries.


Referenc
es

Yaser	Al-Onaizan,	Jan	Curin,	Michael
	Jahr, Kevin  Knight,   John  Lafferty,   Dan  
Melamed, David  Purdy,  Franz  J.  Och,   Noah  
A.  Smith, and  David  Yarowsky.	 1999.	 
Statistical  ma- chine  translation,  final  report,  
JHU  workshop. 
http://www.clsp.jhu.edu/ws99/pro- 
jects/mt/final report/mt-final- 
report.ps.

A. L. Berger, P. F. Brown, S. A. Della Pietra, et 
al.
1994. The candide system for machine translation.
In Proc. , ARPA Workshop on Human Language
Technology, pages 157–162.

Adam L. Berger, Stephen A. Della Pietra, and 
Vin- cent J. Della Pietra.  1996.  A maximum 
entropy approach to natural language processing.  
Compu- tational Linguistics, 22(1):39–72, 
March.

Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.
Della Pietra, and Robert L. Mercer.   1993.   The
mathematics of statistical machine translation: 
Pa- rameter  estimation.	Computational 
Linguistics,
19(2):263–311.

George Foster. 2000. Incorporating position 
informa- tion into a maximum entropy/minimum 
divergence translation model.   In Proc. of 
CoNNL-2000 and LLL-2000, pages 37–52, 
Lisbon, Portugal.

Sven Martin,  Christoph Hamacher, Jo¨ rg 
Liermann, Frank Wessel, and Hermann Ney.  
1999.  Assess- ment of smoothing methods and 
complex stochas- tic language modeling. In IEEE 
International Con- ference on Acoustics, Speech 
and Signal Process- ing, volume I, pages 1939–
1942, Budapest, Hun- gary, September.

Sonja Nießen, Stephan Vogel, Hermann Ney, and 
Christoph Tillmann.   1998.   A DP-based search 
algorithm for statistical machine translation.   In 
COLING-ACL ’98: 36th Annual Meeting of the 
As- sociation for Computational Linguistics and 
17th Int.  Conf.  on  Computational  Linguistics,  
pages
960–967, Montreal, Canada, August.

Franz J. Och and Hermann Ney.   2000a.   Giza++: 
Training	of	statistical	translation	models. 
http://www-i6.Informatik.RWTH- 
Aachen.DE/˜och/software/GIZA++.htm
l.

Franz J. Och and Hermann Ney. 2000b. Improved 
sta- tistical alignment models. In Proc. of the 
38th An- nual Meeting of the Association for 
Computational Linguistics, pages 440–447, 
Hongkong, China, Oc- tober.


Table 9: Four examples showing the translation obtained with the Model 4 and the ME model for a 
given German source sentence.

S
R
C:	Danach wollten wir eigentlich noch Abendessen gehen.
M
4
:
	
W
e
 
a
c
t
u
a
l
l
y
 
c
o
n
c
l
u
d
i
n
g
 
d
i
n
n
e
r
 
t
o
g
e
t
h
e
r
.
 
M
E
:
	
A
f
t
e
r
w
a
r
d
s
 
w
e
 
w
a
n
t
e
d
 
t
o
 
g
o
 
t
o
 
d
i
n
n
e
r
.
S
R
C:	Bei mir oder bei Ihnen?
M
4
:
	
F
o
r
 
m
e
 
o
r
 
f
o
r
 
y
o
u
?
 
M
E
:
	
A
t
 
y
o
u
r
 
o
r
 
m
y
 
p
l
a
c
e
?
S
R
C:	Das wa¨re genau das richtige.
M
4
:
	
T
h
a
t
 
i
s
 
e
x
a
c
t
l
y
 
i
t
 
s
p
i
r
i
t
.
 
M
E
:
	
T
h
a
t
 
i
s
 
t
h
e
 
r
i
g
h
t
 
t
h
i
n
g
.
S
R
C:	Ja, das sieht bei mir eigentlich im Januar ziemlich gut aus.
M
4
:
	
Y
e
s
,
 
t
h
a
t
 
d
o
e
s
 
n
o
t
 
s
u
i
t
 
m
e
 
i
n
 
J
a
n
u
a
r
y
 
l
o
o
k
s
 
p
r
e
t
t
y
 
g
o
o
d
.
 
M
E
:
	
Y
e
s
,
 
t
h
a
t
 
l
o
o
k
s
 
p
r
e
t
t
y
 
g
o
o
d
 
f
o
r
 
m
e
 
a
c
t
u
a
l
l
y
 
i
n
 
J
a
n
u
a
r
y
.




Franz J. Och.  1999.  An efficient method for deter- 
mining bilingual word classes. In EACL ’99: Ninth 
Conf. of the Europ. Chapter of the Association for 
Computational Linguistics, pages 71–76, Bergen, 
Norway, June.

K.A. Papineni, S. Roukos, and R.T. Ward.	1996.
Feature-based language understanding.  In ESCA,
Eurospeech, pages 1435–1438, Rhodes, Greece.

K.A. Papineni, S. Roukos, and R.T. Ward.	1998.
Maximum likelihood and discriminative training of
direct translation models.   In Proc. Int. Conf. on
Acoustics, Speech, and Signal Processing, pages
189–192.

Jochen Peters and Dietrich Klakow.  1999.  Compact 
maximum entropy language models.  In Proceed- 
ings of the IEEE Workshop on Automatic Speech 
Recognition and Understanding, Keystone, CO, 
December.

Stephen Della Pietra, Vincent Della Pietra, and John 
Lafferty. 1997. Inducing features in random fields. 
IEEE Trans. on Pattern Analysis and Machine In- 
teligence, 19(4):380–393, July.

Eric S. Ristad.  1997.  Maximum entropy modelling 
toolkit. Technical report, Princeton Univesity.

R. Rosenfeld. 1996. A maximum entropy approach to 
adaptive statistical language modeling.  Computer, 
Speech and Language, 10:187–228.

Christoph Tillmann and Hermann Ney.  2000.  Word 
re-ordering and dp-based search in statistical ma- 
chine  translation.    In  8th  International Confer- 
ence on Computational Linguistics (CoLing 2000), 
pages 850–856, Saarbru¨ cken, Germany, July.


C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in 
statistical translation.  In Proc. 35th Annual Conf. 
of the Association for Computational Linguistics,
pages 289–296, Madrid, Spain, July.

Ye-Yi Wang and Alex Waibel. 1997. Decoding algo- 
rithm in statistical translation. In Proc. 35th Annual 
Conf. of the Association for Computational Linguis- 
tics, pages 366–372, Madrid, Spain, July.








