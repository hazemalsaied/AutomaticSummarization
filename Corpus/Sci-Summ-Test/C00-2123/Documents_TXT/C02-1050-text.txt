Bidirectional Decoding for Statistical Machine Translation Taro WATANABE ? ? and Eiichiro SUMITA ? {taro.watanabe, eiichiro.sumita}@atr.co.jp ? ATR Spoken Language Translation Research Laboratories ? Department of Information Science 2-2-2 Hikaridai Seika-cho, Kyoto University Soraku-gun, Kyoto 619-0288 JAPAN Sakyo-ku, Kyoto 606-8501, JAPAN AbstractThis paper describes the right-to-left decoding method, which translates an input string by gen .erating in right-to-left direction. In addition, pre.sented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions. The experimental results on Japanese and English translation showed that the right-to-left was better for Englith -to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation. It was also ob.served that the bidirectional method was better for English-to-Japanese translation.1 Introduction The statistical approach to machine translation re.gards the machine translation problem as the maxi.mum likelihood solution of a translation target text given a translation source text. According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the transla.tion is the maximum a posteriori solution of a dis.tribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al, 1993). Although there exists efficient algorithms to es.timate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a se.quence of words. There exists stack decoding al.gorithm (Berger et al, 1996), A* search algorithm (Och et al, 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; Garcia-Varea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with prun.ing technologies assuming almost linearly aligned translation source and target texts. The algorithms proposed above cannot deal with drastically differ.ent word correspondence, such as Japanese and En.glish translation, where Japanese is SOV while SVO in English. Germann et al (2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application. This paper presents two decoding methods, one is the right-to-left decoding based on the left-to.right beam search algorithm, which generates out.puts from the end of a sentence. The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one. The experimental results of Japanese and English translation indicated that the right-to-left decoding was better for English-to-Japanese trans .lation, while the left-to-right decoding was better for Japanese-to-English decoding. The above re.sults could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the begin.ning of a sentence, hence prefers left-to-right de.coding. On the other hand, Japanese takes postfix structure, setting attention around the end of a sen.tence, therefore favors right-to-left decoding. The bidirectional decoding, which can take both of the benefits of decoding method, was superior to mono.directional decoding methods. The next section briefly describes the SMT fo.cusing on the IBM Model 4. Then, the Section 3 presents decoding algorithms in three direction, left.to-right, right-to-left and bi-direction. The Section 4 presents the results of Japanese and English trans .lation followed by discussions. 2 Statistical Machine Translation Statistical machine translation regards machine translation as a process of translating a source lan-NULL0 could1 you2 recommend3 another4 hotel5 hoka no hoteru o shokaishi teitadake masu ka a = (4, 4, 5, 0, 3, 1, 1, 0) Figure 1: An example of alignment for Japanese and English sentences guage text (f) into a target language text (e) with the following formula: e = arg max e P(e|f) The Bayes Rule is applied to the above to derive: e = arg max e P(f|e)P(e) The translation process is treated as a noisy chan.nel model, like those used in speech recognition in which there exists e transcribed as f, and a trans .lation is to infer the best e from f in terms of P(f|e)P(e). The former term, P(f|e), is a translation model representing some correspondence between bilingual text. The latter, P(e), is the language model denoting the likelihood of the channel source text. In addition, a word correspondence model, called alignment a, is introduced to the translation model to represent a positional correspondence of the channel target and source words: e = arg max e ? a P(f, a|e)P(e) An example of an alignment is shown in Figure 1, where the English sentence ? could you recommend another hotel? is mapped onto the Japanese ?hoka no hoteru o shokaishi teitadake masu ka ? , and both ?hoka? and ?no? are aligned to ? another? , etc. The NULL symbol at index 0 is also a lexical entry in which no morpheme is aligned from the channel target morpheme, such as ? masu? and ?ka? in this Japanese example. 2.1 IBM Model 4 The IBM Model 4, main focus in this paper, is com.posed of the following models (see Figure 2): ? Lexical Model ? t( f |e) : Word-for-word trans.lation model, representing the probability of a source word f being translated into a target word e. ? Fertility Model ? n(?|e) : Representing the probability of a source word e generating ? words. ? Distortion Model ? d : The probability of dis.tortion. In Model 4, the model is decomposed into two sets of parameters: ? d1( j ? c ? i|A(ei),B( f j)) : Distortion prob.ability for head words. The head word is the first of the target words generated from a source word a cept, that is the channel source word with fertility more than and equal to one. The head word po.sition j is determined by the word classes of the previous source word, A(ei), and target word, B( f j), relative to the centroid of the previous source word, c ? i . ? d>1( j ? j?|B( f j)) : Distortion probabil.ity for non-head words. The position of a non-head word j is determined by the word class and relative to the previous tar.get word generated from the cept ( j? ). ? NULL Translation Model ? p1 : A fixed prob.ability of inserting a NULL word after deter.mining each target word f . For details, refer to Brown et al (1993). 2.2 Search Problem The search problem of statistical machine trans .lation is to induce the maximum likely channel source sequence, e, given f and the model, P(f|e) = ? a P(f, a|e) and P(e). For the space of a is ex .tremely large, |a|l+1, where the l is the output length, an approximation of P(f|e) ' P(f, a|e) is used when exploring the possible candidates of translation. This problem is known to be NP-Complete (Knight, 1999), for the re-ordering property in the model further complicates the search. One of the solution is the left-to-right generation of output by consuming input words in any-order. Under this constraint, many researchers had contributed algo.rithms and associated pruning strategies, such as Berger et al (1996), Och et al (2001), Wang and Waibel (1997), Tillmann and Ney (2000) Garcia-Varea and Casacuberta (2001) and Germann et al (2001), though they all based on almost linearly Translation Model Lexical Model ? t( f j|ei) Fertility Model ? n(? i |ei) Distortion Model Head ? d1( j ? c ? i|A(e? i )B( f j)) Non-Head ? d1>( j ? j?|B( f j)) NULL Translation Model (m??0 ?0 ) pm? 2? 00 p ?0 1 Figure 2: Translation Model (IBM Model 4) aligned language pairs, and not suitable for lan.guage pairs with totally different alignment corre.spondence, such as Japanese and English. 3 Decoding Algorithms The decoding methods presented in this paper ex .plore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al (2001), and operation applied to each hypothesis is similar to those explained in Berger et al (1996), Och et al (2001) and Germann et al. (2001). The algorithm is depicted in Algorithm 1 where C = { jk : k = 1...|C|} represents a set of input string position 1. The algorithm assumes two kinds of partial hypotheses2, translated partially from an input string, one is an open hypothesis that can be extended by raising the fertility. The other is a close hypothesis that is to be extended by in.serting a string e ? to the hypothesis. The e? is a se.quence of output word, consisting of a word with the fertility more than one (translation of f j) and other words with zero fertility. The translation of f j can be computed either by inverse translation table (Och et al, 2001; Al-Onaizan et al, 1999). The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al, 2001). The extension operator applied to an open hypothe.sis (e,C) is: ? align j to ei ? this creates a new hypothesis by raising the fertility of ei by consuming the input word f j. The generated hypothesis can be treated as either closed or open, that means to stop raising the fertility or raise the fertility further more. The operators applied to a close hypothesis are: 1For simplicity, the dependence of alignment, a is omitted. 2There exist a complete hypothesis, that is a candidate of translation. Algorithm 1 Beam Decoding Search input source string: f1 f2... fm for all cardinality c = 0, 1, ...m ? 1 do for all (e,C) where |C| = c do for all j = 1, ...m and j < C do if (e,C) is open then align j to ei and keep it open align j to ei and close it else align j to NULL insert e?, align from j and open it insert e?, align from j and close it end if end for end for end for ? align j to NULL ? raise the fertility for the NULL word. ? insert e?, align from j ? this operator insert a string e ? and align one input word f j to one of the word in e ? . After this operation, the new hypothesis can be regarded as either open or closed. Pruning is inevitable in the process of decoding, and applied is the beam search pruning, in which the maximum number of hypotheses to be considered is limited. In addition, fertility pruning is also in.troduced which suppress the word with large num.ber of fertility. The skipping based criteria, such as introduced by Och et al (2001), is not appropri.ate for the language pairs with drastically different alignment, such as Japanese and English, hence was not considered in this paper. Depending on the out.put generation direction, the algorithm can generate either in left-to-right or right-to-left, by alternating some constraints of insertion of output words. e1 ... el e?1 ... e?l? f1 f2 ... f j ... fm e e? Figure 3: string insertion operator for left-to-right decoding method. A string e? was appended after the partial output string, e, and the last word in e ? was aligned from f j. e?1 ... e?l? e1 ... el f1 f2 ... f j ... fm e? e Figure 4: string insertion operation for right-to-left decoding method. A string e? was prepended before the partial output string, e, and the first word in e ? was aligned from f j. 3.1 Left-to-Right Decoding The left-to-right decoding enforces the restriction where the insertion of e ? is allowed after the par.tially generated e, and alignment from the input word f j is restricted to the end of the word of e?. Hence, the operator applied to an open hypothesis raise the fertility for the word at the end of e (refer to Figure 3). The language which place emphasis around the beginning of a sentence, such as English, will be suitable in this direction, for the Language Model score P(e) can estimate what should come first. Hence, the decoder can discriminate a hypothesis better or not. 3.2 Right-to-Left Decoding The right-to-left decoding does the reverse of the left-to-right decoding, in which the insertion of e? is allowed only before the e and the f j is aligned to the beginning of the word of e ? (see Figure 4). Therefore, the open hypothesis is extended by rais .ing the fertility of the beginning of the word of e. In prepending a string to a partial hypothesis, an align.ment vector should be reassigned so that the values can point out correct index. Again, the right-to-left direction is suitable for the language which enforces stronger constraints at the end of sentence, such as Japanese, similar to the reason mentioned above. e f 1 ... ei ... eblbef eb (a) merging two open hy.potheses e f 1 ... e f l f e ? eb1 ... eblbef eb (b) merging two close hypotheses with in.serted e ? Figure 5: Merging left-to-right and right-to-left hypotheses (ef and eb) in bidirectional decoding method. Figure 5(a) merge two open hypotheses, while Figure 5(b) merge them with inserted zero fer.tility words. 3.3 Bidirectional Decoding The bidirectional decoding decode the input words in both direction, one with left-to-right decoding method up to the cardinality of dm/2e and right-to.left direction up to the cardinality of bm/2c, where m is the input length. Then, the two hypotheses are merged when both are open and can share the same output word e, which resulted in raising the fertility of e. If both of them are closed hypotheses, then an additional sequence of zero fertility words (or NULL sequence) are inserted (refer to Figure 5). 3.4 Computational Complexity The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m), as re.ported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3. The bidirectional method involves merging of two hypotheses, hence additional O( ( m m/2 ) ) is required. 3.5 Effects of Decoding Direction The decoding algorithm generating in left-to-right direction fills the output sequence from the begin.ning of a sentence by consuming the input words in any order and by selecting the corresponding trans .lation. Therefore, the languages with prefix structure, such as English, German or French, can take the benefits of this direction, because the language model/translation model can differentiate ? good? hypotheses to ? bad ? hypotheses around the begin.ning of the output sentences. Therefore, the nar.rowing the search space by the beam search crite.3The term |E|3 is the case for trigram language model. ria (pruning) would not affect the overall quality. On the other hand, if right-to-left decoding method were applied to such a language above, the dif.ference of good hypotheses and bad hypotheses is small, hence the drop of hypotheses would affect the quality of translation. The similar statement can hold for postfix lan.guages, such as Japanese, where emphasis is placed around the end of a sentence. For such languages, right-to-left decoding will be suitable but left-to.right decoding will degrade the quality of transla.tion. The bidirectional decoding is expected to take the benefits of both of the directions, and will show the best results in any kind of languages. 4 Experimental Results The corpus for this experiment consists of 172,481 bilingual sentences of English and Japanese ex .tracted from a large-scale travel conversation corpus (Takezawa et al, 2002). The statistics of the corpus are shown in Table 1. The database was split into three parts: a training set of 152,183 sentence pairs, a validation set of 10,148, and a test set of 10,150. The translation models, both for the Japanese-to-English (J-E) and English-to-Japanese (E -J) trans.lation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity. In modeling IBM Model 4, POSs were used as word classes. From the viterbi alignments of the training cor.pus, A list of possible insertion of zero fertility words were extracted with frequency more than 10, around 1,300 sequences of words for both of the J-E and E-J translations. The test set consists of 150 Japanese sentences varying by the sentence length of 6, 8 and 10. The translation was carried out by three decoding methods:left-to-right, right-to.left and bidirectional one. The translation results were evaluated by word.error-rate (WER) and position independent word.error-rate (PER) (Watanabe et al, 2002; Och et al, 2001). The WER is the measure by penalizing in.sertion/deletion/replacement by 1. The PER is the one similar to WER but ignores the positions, al.lowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection. It has been also evaluated by subjective evaluation (SE) with the criteria ranging from A(perfect) to D(non-Table 1: Statistics on a travel conversation corpus Japanese English # of sentences 172,481 # of words 1,186,620 1,005,080 vocabulary size 22,801 15,768 avg. sentence length 6.88 5.83 3-gram perplexity 26.16 36.92 Table 3: Comparison of the three decoders by the ratio each decoder produced search errors. J-E E-J LtoR 11.3 12.0 RtoL 59.3 34.0 Bi 15.3 15.3 sense) 4 (Sumita et al, 1999). Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE. Table 3 shows the ratio of producing search errors, computed by comparing the translation model and lnguage model scores for the outputs from three decoding methods. Sample Japanese-to-English translations performed by the decoders is presented in Figure 6. 5 Discussions From Table 2, the left-to-right decoding method per.formed better than the right-to-left one in Japanese.to-English translation as expected in Section 3.5. Furthermore, the bidirectional decoding method was slightly better than the left-to-right one, for it could combine the benefits of both directions. Similar analysis could hold for English-to-Japanese translation, and the right-to-left decoding method was slightly superior to the left-to-right one in terms of WER/PER scores, though the SE score dropped from 8.7% to 6.7% in C-ranked sentences. Overall quality measured by the SE rate for ac.cepted senteces, ranging from A to C, dropped from 68.0% into 66.0%. In addition, the bidirectional method in English-to-Japanese translation was not evaluated as high as those in Japanese-to-English translation: the results were closer to the left-to.right method. This might be due to the nature of lan.4The meanings of the symbol are follows: A ? perfect: no problem in either information or grammar; B ? fair: easy to understand but some important information is missing or it is grammatically flawed; C ? acceptable: broken but under.standable with effort; D ? nonsense: important information has been translated incorrectly. Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E -J) translations by left-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods. Trans. Alg. WER PER SE A B C D J-E LtoR 70.0 64.8 26.7% 23.3% 20.0% 30.0% RtoL 74.6 66.9 21.3% 24.7% 18.0% 36.0% Bi 69.9 63.7 27.3% 22.7% 20.7% 29.3% E-J LtoR 66.2 57.6 49.3% 10.0% 8.7% 32.0% RtoL 64.0 56.1 49.3% 10.0% 6.7% 34.0% Bi 66.0 58.0 48.7% 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ? s my wallet was stolen RtoL: here ? s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ? d like a seat on the terrace RtoL: i ? d prefer excuse me Bi: i ? d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ? s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ? re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history. It is ex.pected that the use of the suitable language model context direction corresponding to a generation di.rection would assign appropriate probability, hence would be able to differentiate better hypotheses. Table 3 indicats that the right-to-left decoding method produced more errors than other methods regardless of translaiton directions. This is ex .plained by the use of the left history language model, not the right context one, as stated above. Nevertheless, the search error decreased from 59.3 into 34.0 by alternating the translation direction for the right-to-left decoding method, which still sup.ports the use of the correct rendering direction for translation target language. 6 Conclusion The decoding methods for statistical machine trans .lation presented here varies the output directions, left-to-right, right-to-left and bi-direction, and were experimented with drastically different language pairs, English and Japanese. The results indicated that the left-to-right decoding method was suit .able for Japanese-to-English translation while the right-to-left decoding method fit with English-to-Japanese translation. In addition, the bidirectional decoding method was superior to mono-directional decoding method for Japanese-to-English transla.tion. This suggests that the translation output gen .eration should match with the underlying linguistic structure for the output language. Acknowledgement The research reported here was supported in part by a contract with the Telecommunications Advance.ment Organization of Japan entitled, ? A study of speech dialogue translation technology based on a large corpus? . References Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Frantz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation final report, jhu workshop 1999, 12. A. Berger, P. Brown, S. Pietra, V. Pietra, J. Gillett, 
A. Kehler, and R. Mercer. 1996. Language trans .lation apparatus and method of using context.based translation models. Technical report, 
United States Patent, Patent Number 5510981, 
April. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine transla.tion: Parameter estimation. Computational Lin.guistics, 19(2):263 ? 311. Ismael Garcia-Varea and Francisco Casacuberta. 2001. Search algorithms for statistical machine translation based on dynamic programming and pruning techniques. In MT Summit VIII, Santiago de Compostela, Galicia, Spain, september. Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast de .coding and optimal decoding for machine trans .lation. In Proc. of ACL-01, Toulouse, France. Kevin Knight. 1999. Decoding complexity in word-replacement translation models. Computa.tional Linguistics, 25(4):607 ? 615. Franz Josef Och, Nicola Ueffing, and Hermann Ney. 2001. An efficient a* search algorithm for statis.tical machine translation. In Proc. of the ACL.2001 Workshop on Data-Driven Machine Trans.lation, pages 55? 62, Toulouse, France, July. Eiichiro Sumita, Setsuo Yamada, Kazuhide Ya.mamoto, Michael Paul, Hideki Kashioka, Kai Ishikawa, and Satoshi Shirai. 1999. Solutions to problems inherent in spoken-language transla.tion: The ATR-MATRIX approach. In Machine Translation Summit VII, pages 229?235. Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Ya.mamoto. 2002. Toward a broad-coverage bilin.gual corpus for speech translation of travel con.versations in the real world. In Proc. of LREC 2002, pages 147? 152, Las Palmas, Canary Is.lands, Spain, May. Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and dp-based search in statistical ma.chine translation. In Proc. of the COLING 2000, July-August. Ye-Yi Wang and Alex Waibel. 1997. Decoding al.gorithm in statistical machine translation. In Pro.ceedings of the 35th Annual Meeting of the Asso .ciation for Computational Linguistics. Taro Watanabe, Kenji Imamura, and Eiichiro Sumita. 2002. Statistical machine translation based on hierarchical phrase alignment. In Proc. of TMI 2002, Keihanna, Japan, March. 