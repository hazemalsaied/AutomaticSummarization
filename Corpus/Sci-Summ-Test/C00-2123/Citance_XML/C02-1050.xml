<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the right-to-left decoding method, which translates an input string by generating in right-to-left direction.</S>
		<S sid ="2" ssid = "2">In addition, presented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions.</S>
		<S sid ="3" ssid = "3">The experimental results on Japanese and English translation showed that the right-to-left was better for Englith-to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation.</S>
		<S sid ="4" ssid = "4">It was also observed that the bidirectional method was better for English-to-Japanese translation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">The statistical approach to machine translation regards the machine translation problem as the maximum likelihood solution of a translation target text given a translation source text.</S>
			<S sid ="6" ssid = "6">According to the Bayes Rule, the problem is transformed into the noisy channel model paradigm, where the translation is the maximum a posteriori solution of a distribution for a channel target text given a channel source text and a prior distribution for the channel source text (Brown et al., 1993).</S>
			<S sid ="7" ssid = "7">Although there exists efficient algorithms to estimate the parameters for the statistical machine translation (SMT), one of the problems of SMT is the search algorithms for the translation given a sequence of words.</S>
			<S sid ="8" ssid = "8">There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</S>
			<S sid ="9" ssid = "9">The algorithms proposed above cannot deal with drastically different word correspondence, such as Japanese and English translation, where Japanese is SOV while SVO in English.</S>
			<S sid ="10" ssid = "10">Germann et al.</S>
			<S sid ="11" ssid = "11">(2001) suggested greedy method and integer programming decoding, though the first method suffer from the similar problem as described above and the second is impractical for the real-world application.</S>
			<S sid ="12" ssid = "12">This paper presents two decoding methods, one is the right-to-left decoding based on the left-to- right beam search algorithm, which generates outputs from the end of a sentence.</S>
			<S sid ="13" ssid = "13">The second one is the bidirectional decoding method which decodes in both of the left-to-right and right-to-left directions and merges the two hypothesized partial sentences into one.</S>
			<S sid ="14" ssid = "14">The experimental results of Japanese and English translation indicated that the right-to-left decoding was better for English-to-Japanese translation, while the left-to-right decoding was better for Japanese-to-English decoding.</S>
			<S sid ="15" ssid = "15">The above results could be justified by the structural difference of Japanese and English, where English takes the prefix structure that places emphasis at the beginning of a sentence, hence prefers left-to-right decoding.</S>
			<S sid ="16" ssid = "16">On the other hand, Japanese takes postfix structure, setting attention around the end of a sentence, therefore favors right-to-left decoding.</S>
			<S sid ="17" ssid = "17">The bidirectional decoding, which can take both of the benefits of decoding method, was superior to mono- directional decoding methods.</S>
			<S sid ="18" ssid = "18">The next section briefly describes the SMT focusing on the IBM Model 4.</S>
			<S sid ="19" ssid = "19">Then, the Section 3 presents decoding algorithms in three direction, left- to-right, right-to-left and bi-direction.</S>
			<S sid ="20" ssid = "20">The Section 4 presents the results of Japanese and English translation followed by discussions.</S>
	</SECTION>
	<SECTION title="Statistical Machine Translation. " number = "2">
			<S sid ="21" ssid = "1">Statistical machine translation regards machine translation as a process of translating a source lan NULL0 could1 you2 recommend3 another4 hotel5 hoka no hoteru o shokaishi teitadake masu ka a = (4, 4, 5, 0, 3, 1, 1, 0) Figure 1: An example of alignment for Japanese and English sentences guage text (f) into a target language text (e) with the following formula: e = arg max P(e f) e The Bayes Rule is applied to the above to derive: e = arg max P(f e)P(e) e The translation process is treated as a noisy channel model, like those used in speech recognition in which there exists e transcribed as f, and a translation is to infer the best e from f in terms of P(f|e)P(e).</S>
			<S sid ="22" ssid = "2">The former term, P(f|e), is a translation model representing some correspondence between bilingual text.</S>
			<S sid ="23" ssid = "3">The latter, P(e), is the language model denoting the likelihood of the channel source text.</S>
			<S sid ="24" ssid = "4">In addition, a word correspondence model, called alignment a, is introduced to the translation model to represent a positional correspondence of the channel target and source words: e = arg max ) P(f, a e)P(e) e a An example of an alignment is shown in Figure 1, where the English sentence “could you recommend another hotel” is mapped onto the Japanese “hoka no hoteru o shokaishi teitadake masu ka”, and both “hoka” and “no” are aligned to “another”, etc. The NULL symbol at index 0 is also a lexical entry in which no morpheme is aligned from the channel target morpheme, such as “masu” and “ka” in this Japanese example.</S>
			<S sid ="25" ssid = "5">2.1 IBM Model 4.</S>
			<S sid ="26" ssid = "6">The IBM Model 4, main focus in this paper, is composed of the following models (see Figure 2): • Lexical Model — t( f |e) : Word-for-word translation model, representing the probability of a source word f being translated into a target word e. • Fertility Model — n(φ|e) : Representing the probability of a source word e generating φ words.</S>
			<S sid ="27" ssid = "7">• Distortion Model — d : The probability of distortion.</S>
			<S sid ="28" ssid = "8">In Model 4, the model is decomposed into two sets of parameters: – d1( j − cρi|A(ei ), B( f j )) : Distortion probability for head words.</S>
			<S sid ="29" ssid = "9">The head word is the first of the target words generated from a source word a cept, that is the channel source word with fertility more than and equal to one.</S>
			<S sid ="30" ssid = "10">The head word position j is determined by the word classes of the previous source word, A(ei ), and target word, B( f j ), relative to the centroid of the previous source word, cρi . – d&gt;1 ( j − j |B( f j )) : Distortion probability for non-head words.</S>
			<S sid ="31" ssid = "11">The position of a non-head word j is determined by the word class and relative to the previous target word generated from the cept ( j ).</S>
			<S sid ="32" ssid = "12">• NULL Translation Model — p1 : A fixed probability of inserting a NULL word after determining each target word f . For details, refer to Brown et al.</S>
			<S sid ="33" ssid = "13">(1993).</S>
			<S sid ="34" ssid = "14">2.2 Search Problem.</S>
			<S sid ="35" ssid = "15">The search problem of statistical machine translation is to induce the maximum likely channel source sequence, e, given f and the model, P(f|e) =La P(f, a|e) and P(e).</S>
			<S sid ="36" ssid = "16">For the space of a is ex tremely large, |a|l+1 , where the l is the output length, an approximation of P(f|e) � P(f, a|e) is used when exploring the possible candidates of translation.</S>
			<S sid ="37" ssid = "17">This problem is known to be NP-Complete (Knight, 1999), for the reordering property in the model further complicates the search.</S>
			<S sid ="38" ssid = "18">One of the solution is the left-to-right generation of output by consuming input words in any-order.</S>
			<S sid ="39" ssid = "19">Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.</S>
			<S sid ="40" ssid = "20">(1996), Och et al.</S>
			<S sid ="41" ssid = "21">(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.</S>
			<S sid ="42" ssid = "22">(2001), though they all based on almost linearly Lexical Model t( f j |ei ) Translation Model Fertility Model n(φi |ei ) Distortion Model Head d1 ( j − cρi |A(eρi )B( f j )) NULL Translation Model m−φ0O pm−2φ0 φ0 Non-Head d1&gt; ( j − j |B( f j )) φ0 0 p1 Figure 2: Translation Model (IBM Model 4) aligned language pairs, and not suitable for language pairs with totally different alignment correspondence, such as Japanese and English.</S>
	</SECTION>
	<SECTION title="Decoding Algorithms. " number = "3">
			<S sid ="43" ssid = "1">The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.</S>
			<S sid ="44" ssid = "2">(2001), and operation applied to each hypothesis is similar to those explained in Berger et al.</S>
			<S sid ="45" ssid = "3">(1996), Och et al.</S>
			<S sid ="46" ssid = "4">(2001) and Germann et al.</S>
			<S sid ="47" ssid = "5">(2001).</S>
			<S sid ="48" ssid = "6">The algorithm is depicted in Algorithm 1 where C = { jk : k = 1...|C|} represents a set of input string position 1.</S>
			<S sid ="49" ssid = "7">The algorithm assumes two kinds of partial hypotheses2, translated partially from an input string, one is an open hypothesis that can be extended by raising the fertility.</S>
			<S sid ="50" ssid = "8">The other is a close hypothesis that is to be extended by inserting a string e to the hypothesis.</S>
			<S sid ="51" ssid = "9">The e is a sequence of output word, consisting of a word with the fertility more than one (translation of f j) and other words with zero fertility.</S>
			<S sid ="52" ssid = "10">The translation of f j can be computed either by inverse translation table (Och et al., 2001; AlOnaizan et al., 1999).</S>
			<S sid ="53" ssid = "11">The list of zero fertility words can be obtained from the viterbi alignment of training corpus (Germann et al., 2001).</S>
			<S sid ="54" ssid = "12">The extension operator applied to an open hypothesis (e, C) is: • align j to ei — this creates a new hypothesis by raising the fertility of ei by consuming the input word f j. The generated hypothesis can be treated as either closed or open, that means to stop raising the fertility or raise the fertility further more.</S>
			<S sid ="55" ssid = "13">The operators applied to a close hypothesis are: 1 For simplicity, the dependence of alignment, a is omitted..</S>
			<S sid ="56" ssid = "14">2 There exist a complete hypothesis, that is a candidate of translation.</S>
			<S sid ="57" ssid = "15">Algorithm 1 Beam Decoding Search input source string: f1 f2 ... fm for all cardinality c = 0, 1, ...m − 1 do for all (e, C) where |C| = c do for all j = 1, ...m and j � C do if (e, C) is open then align j to ei and keep it open align j to ei and close it else align j to NULL insert e , align from j and open it insert e , align from j and close it end if end for end for end for • align j to NULL — raise the fertility for the NULL word.</S>
			<S sid ="58" ssid = "16">• insert e , align from j — this operator insert a string e and align one input word f j to one of the word in e . After this operation, the new hypothesis can be regarded as either open or closed.</S>
			<S sid ="59" ssid = "17">Pruning is inevitable in the process of decoding, and applied is the beam search pruning, in which the maximum number of hypotheses to be considered is limited.</S>
			<S sid ="60" ssid = "18">In addition, fertility pruning is also introduced which suppress the word with large number of fertility.</S>
			<S sid ="61" ssid = "19">The skipping based criteria, such as introduced by Och et al.</S>
			<S sid ="62" ssid = "20">(2001), is not appropriate for the language pairs with drastically different alignment, such as Japanese and English, hence was not considered in this paper.</S>
			<S sid ="63" ssid = "21">Depending on the output generation direction, the algorithm can generate either in left-to-right or right-to-left, by alternating some constraints of insertion of output words.</S>
			<S sid ="64" ssid = "22">e1 ... el e 1 ... e lt e e f1 f2 ... f j ... fm Figure 3: string insertion operator for left-to-right decoding method.</S>
			<S sid ="65" ssid = "23">A string e was appended after the partial output string, e, and the last word in e was aligned from f j. e 1 ... e lt e1 ... el e e f1 f2 ... f j ... fm Figure 4: string insertion operation for right-to-left decoding method.</S>
			<S sid ="66" ssid = "24">A string e was prepended before the partial output string, e, and the first word in e was aligned from f j. 3.1 Left-to-Right Decoding.</S>
			<S sid ="67" ssid = "25">The left-to-right decoding enforces the restriction where the insertion of e is allowed after the partially generated e, and alignment from the input word f j is restricted to the end of the word of e . Hence, the operator applied to an open hypothesis raise the fertility for the word at the end of e (refer to Figure 3).</S>
			<S sid ="68" ssid = "26">The language which place emphasis around the beginning of a sentence, such as English, will be suitable in this direction, for the Language Model score P(e) can estimate what should come first.</S>
			<S sid ="69" ssid = "27">Hence, the decoder can discriminate a hypothesis better or not.</S>
			<S sid ="70" ssid = "28">3.2 Right-to-Left Decoding.</S>
			<S sid ="71" ssid = "29">The right-to-left decoding does the reverse of the left-to-right decoding, in which the insertion of e is allowed only before the e and the f j is aligned to the beginning of the word of e (see Figure 4).</S>
			<S sid ="72" ssid = "30">Therefore, the open hypothesis is extended by raising the fertility of the beginning of the word of e. In prepending a string to a partial hypothesis, an alignment vector should be reassigned so that the values can point out correct index.</S>
			<S sid ="73" ssid = "31">Again, the right-to-left direction is suitable for the language which enforces stronger constraints at the end of sentence, such as Japanese, similar to the reason mentioned above.</S>
			<S sid ="74" ssid = "32">e f 1 ... ei ... eb lb ef eb (a) merging two open hypotheses e f 1 ... e f l f e eb1 ... eb lb ef eb (b) merging two close hypotheses with inserted et Figure 5: Merging left-to-right and right-to-left hypotheses (ef and eb) in bidirectional decoding method.</S>
			<S sid ="75" ssid = "33">Figure 5(a) merge two open hypotheses, while Figure 5(b) merge them with inserted zero fertility words.</S>
			<S sid ="76" ssid = "34">3.3 Bidirectional Decoding.</S>
			<S sid ="77" ssid = "35">The bidirectional decoding decode the input words in both direction, one with left-to-right decoding method up to the cardinality of rm/21 and right-to- left direction up to the cardinality of Lm/2J, where m is the input length.</S>
			<S sid ="78" ssid = "36">Then, the two hypotheses are merged when both are open and can share the same output word e, which resulted in raising the fertility of e. If both of them are closed hypotheses, then an additional sequence of zero fertility words (or NULL sequence) are inserted (refer to Figure 5).</S>
			<S sid ="79" ssid = "37">3.4 Computational Complexity.</S>
			<S sid ="80" ssid = "38">The computational complexity for the left-to-right and right-to-left is the same, O(|E|3m22m ), as reported by Tillmann and Ney (2000), in which |E| is the size of the vocabulary for output sentences 3.</S>
			<S sid ="81" ssid = "39">The bidirectional method involves merging of two hypotheses, hence additional O( m O) is required.</S>
			<S sid ="82" ssid = "40">3.5 Effects of Decoding Direction.</S>
			<S sid ="83" ssid = "41">The decoding algorithm generating in left-to-right direction fills the output sequence from the beginning of a sentence by consuming the input words in any order and by selecting the corresponding translation.</S>
			<S sid ="84" ssid = "42">Therefore, the languages with prefix structure, such as English, German or French, can take the benefits of this direction, because the language model/translation model can differentiate “good” hypotheses to “bad” hypotheses around the beginning of the output sentences.</S>
			<S sid ="85" ssid = "43">Therefore, the narrowing the search space by the beam search crite 3 The term |E|3 is the case for trigram language model..</S>
			<S sid ="86" ssid = "44">ria (pruning) would not affect the overall quality.</S>
			<S sid ="87" ssid = "45">On the other hand, if right-to-left decoding method were applied to such a language above, the difference of good hypotheses and bad hypotheses is small, hence the drop of hypotheses would affect the quality of translation.</S>
			<S sid ="88" ssid = "46">The similar statement can hold for postfix languages, such as Japanese, where emphasis is placed around the end of a sentence.</S>
			<S sid ="89" ssid = "47">For such languages, right-to-left decoding will be suitable but left-to- right decoding will degrade the quality of translation.</S>
			<S sid ="90" ssid = "48">The bidirectional decoding is expected to take the benefits of both of the directions, and will show the best results in any kind of languages.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "4">
			<S sid ="91" ssid = "1">The corpus for this experiment consists of 172,481 bilingual sentences of English and Japanese extracted from a large-scale travel conversation corpus (Takezawa et al., 2002).</S>
			<S sid ="92" ssid = "2">The statistics of the corpus are shown in Table 1.</S>
			<S sid ="93" ssid = "3">The database was split into three parts: a training set of 152,183 sentence pairs, a validation set of 10,148, and a test set of 10,150.</S>
			<S sid ="94" ssid = "4">The translation models, both for the Japanese-to- English (J-E) and English-to-Japanese (E-J) translation, were trained toward IBM Model 4 on the training set and cross-validated on validation set to terminate the iteration by observing perplexity.</S>
			<S sid ="95" ssid = "5">In modeling IBM Model 4, POSs were used as word classes.</S>
			<S sid ="96" ssid = "6">From the viterbi alignments of the training corpus, A list of possible insertion of zero fertility words were extracted with frequency more than 10, around 1,300 sequences of words for both of the J- E and E-J translations.</S>
			<S sid ="97" ssid = "7">The test set consists of 150 Japanese sentences varying by the sentence length of 6, 8 and 10.</S>
			<S sid ="98" ssid = "8">The translation was carried out by three decoding methods:left-to-right, right-to- left and bidirectional one.</S>
			<S sid ="99" ssid = "9">The translation results were evaluated by word- error-rate (WER) and position independent word- error-rate (PER) (Watanabe et al., 2002; Och et al.,2001).</S>
			<S sid ="100" ssid = "10">The WER is the measure by penalizing in Table 1: Statistics on a travel conversation corpus J a p a n e s e E n g l i s h # of se nt en ce s 17 2,48 1 # of w or ds 1, 18 6, 62 0 1, 0 0 5, 0 8 0 vo ca bu lar y siz e 2 2 , 8 0 1 1 5 , 7 6 8 av g. se nt en ce le ng th 6 . 8 8 5 . 8 33 gr a m pe rpl ex ity 2 6 . 1 6 3 6 . 9 2 Table 3: Comparison of the three decoders by the ratio each decoder produced search errors.J EE J Lt o R R to L B i 11 .3 59 .3 15 .3 12 .0 34 .0 15 .3 sense) 4 (Sumita et al., 1999).</S>
			<S sid ="101" ssid = "11">Table 2 summarizes the results of decoding by left-to-right, right-to-left and bidirectional method evaluated with WER, PER and SE.</S>
			<S sid ="102" ssid = "12">Table 3 shows the ratio of producing search errors, computed by comparing the translation model and lnguage model scores for the outputs from three decoding methods.</S>
			<S sid ="103" ssid = "13">Sample Japanese-to-English translations performed by the decoders is presented in Figure 6.</S>
	</SECTION>
	<SECTION title="Discussions. " number = "5">
			<S sid ="104" ssid = "1">From Table 2, the left-to-right decoding method performed better than the right-to-left one in Japanese- to-English translation as expected in Section 3.5.</S>
			<S sid ="105" ssid = "2">Furthermore, the bidirectional decoding method was slightly better than the left-to-right one, for it could combine the benefits of both directions.</S>
			<S sid ="106" ssid = "3">Similar analysis could hold for English-to- Japanese translation, and the right-to-left decoding method was slightly superior to the left-to-right one in terms of WER/PER scores, though the SE score dropped from 8.7% to 6.7% in Cranked sentences.</S>
			<S sid ="107" ssid = "4">Overall quality measured by the SE rate for accepted senteces, ranging from A to C, dropped from 68.0% into 66.0%.</S>
			<S sid ="108" ssid = "5">In addition, the bidirectional method in English-to-Japanese translation was not evaluated as high as those in Japanese-to-English translation: the results were closer to the left-to- right method.</S>
			<S sid ="109" ssid = "6">This might be due to the nature of lan sertion/deletion/replacement by 1.</S>
			<S sid ="110" ssid = "7">The PER is the one similar to WER but ignores the positions, allowing the reordered outputs, hence can estimate the accuracy for the tranlslation word selection.</S>
			<S sid ="111" ssid = "8">It has been also evaluated by subjective evaluation (SE) with the criteria ranging from A(perfect) to D(non 4 The meanings of the symbol are follows: A — perfect:.</S>
			<S sid ="112" ssid = "9">no problem in either information or grammar; B — fair: easy to understand but some important information is missing or it is grammatically flawed; C — acceptable: broken but understandable with effort; D — nonsense: important information has been translated incorrectly.</S>
			<S sid ="113" ssid = "10">Table 2: Summary of results for Japanese-to-English (J-E) and English-to-Japanese (E-J) translations by left-to-right (LtoR), right-to-left (RtoL) and bidirectional (Bi) decoding methods.</S>
			<S sid ="114" ssid = "11">Tr an s. Alg.</S>
			<S sid ="115" ssid = "12">W E R PE R S E A B C DJ E LtoR R t o L B i 7 0.</S>
			<S sid ="116" ssid = "13">0 7 4.</S>
			<S sid ="117" ssid = "14">6 6 9.</S>
			<S sid ="118" ssid = "15">9 64 .8 66 .9 63 .7 26 .7 % 23.3% 20.0% 30.0% 21 .3 % 24.7% 18.0% 36.0% 27 .3 % 22.7% 20.7% 29.3%E J LtoR R t o L B i 6 6.</S>
			<S sid ="119" ssid = "16">2 6 4.</S>
			<S sid ="120" ssid = "17">0 6 6.</S>
			<S sid ="121" ssid = "18">0 57 .6 56 .1 58 .0 49 .3 % 10.0% 8.7% 32.0% 49 .3 % 10.0% 6.7% 34.0% 48 .7 % 8.0% 10.0% 33.3% input: suri ni saifu o sura re mashi ta (i had my pocket picked) LtoR: here ’s my wallet was stolen RtoL: here ’s my wallet was stolen Bi: i had my wallet stolen input: sumimasen ga terasu no seki ga ii no desu ga (excuse me but can we have a table on the terrace) LtoR: excuse me i ’d like a seat on the terrace RtoL: i ’d prefer excuse me Bi: i ’d like a seat on the terrace input: nan ji ni owaru no desu (what time will it be over) LtoR: what time should i be at the end RtoL: it ’s what time will it be over Bi : at what time is it end input: nimotsu o ue ni age te morae masu ka (will you put my luggage on the rack) LtoR: could you put my baggage here RtoL: do you have overhead luggage Bi: could you put my baggage input: ee ani to imouto ga hitori zutsu i masu (yes i have a brother and a sister) LtoR: yes brother and sister there a daughter RtoL: you ’re yes brother and sister daughter Bi: yes my daughter is there a brother and sister Figure 6: Examples of Japanese-to-English translation guage model employed for this experiment, for the language model probabilities were assigned based on the left history, not the right history.</S>
			<S sid ="122" ssid = "19">It is expected that the use of the suitable language model context direction corresponding to a generation direction would assign appropriate probability, hence would be able to differentiate better hypotheses.</S>
			<S sid ="123" ssid = "20">Table 3 indicats that the right-to-left decoding method produced more errors than other methods regardless of translaiton directions.</S>
			<S sid ="124" ssid = "21">This is explained by the use of the left history language model, not the right context one, as stated above.</S>
			<S sid ="125" ssid = "22">Nevertheless, the search error decreased from 59.3 into 34.0 by alternating the translation direction for the right-to-left decoding method, which still supports the use of the correct rendering direction for translation target language.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="126" ssid = "1">The decoding methods for statistical machine translation presented here varies the output directions, left-to-right, right-to-left and bi-direction, and were experimented with drastically different language pairs, English and Japanese.</S>
			<S sid ="127" ssid = "2">The results indicated that the left-to-right decoding method was suitable for Japanese-to-English translation while the right-to-left decoding method fit with English-to- Japanese translation.</S>
			<S sid ="128" ssid = "3">In addition, the bidirectional decoding method was superior to mono-directional decoding method for Japanese-to-English translation.</S>
			<S sid ="129" ssid = "4">This suggests that the translation output generation should match with the underlying linguistic structure for the output language.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="130" ssid = "5">The research reported here was supported in part by a contract with the Telecommunications Advancement Organization of Japan entitled, “A study of speech dialogue translation technology based on a large corpus”.</S>
	</SECTION>
</PAPER>
