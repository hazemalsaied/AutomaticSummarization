<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The decoding problem in Statistical Machine Translation (SMT) is a computationally hard combinatorial optimization problem.</S>
		<S sid ="2" ssid = "2">In this paper, we propose a new algorithmic framework for solving the decoding problem and demonstrate its utility.</S>
		<S sid ="3" ssid = "3">In the new algorithmic framework, the decoding problem can be solved both exactly and approximately.</S>
		<S sid ="4" ssid = "4">The key idea behind the framework is the modeling of the decoding problem as one that involves alternating maximization of two relatively simpler subproblems.</S>
		<S sid ="5" ssid = "5">We show how the subproblems can be solved efficiently and how their solutions can be combined to arrive at a solution for the decoding problem.</S>
		<S sid ="6" ssid = "6">A family of provably fast decoding algorithms can be derived from the basic techniques underlying the framework and we present a few illustrations.</S>
		<S sid ="7" ssid = "7">Our first algorithm is a provably linear time search algorithm.</S>
		<S sid ="8" ssid = "8">We use this algorithm as a subroutine in the other algorithms.</S>
		<S sid ="9" ssid = "9">We believe that decoding algorithms derived from our framework can be of practical significance.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="10" ssid = "10">Decoding is one of the three fundamental problems in classical SMT (translation model and language model being the other two) as proposed by IBM in the early 1990’s (Brown et al., 1993).</S>
			<S sid ="11" ssid = "11">In the decoding problem we are given the language and translation models and a source language sentence and are asked to find themost probable translation for the sentence.</S>
			<S sid ="12" ssid = "12">De coding is a discrete optimization problem whose search space is prohibitively large.</S>
			<S sid ="13" ssid = "13">The challenge is, therefore, in devising a scheme to efficiently search the solution space for the solution.</S>
			<S sid ="14" ssid = "14">Decoding is known to belong to a class of computational problems popularly known as NP- hard problems (Knight, 1999).</S>
			<S sid ="15" ssid = "15">NP-hard problems are known to be computationally hard and have eluded polynomial time algorithms (Garey and Johnson, 1979).</S>
			<S sid ="16" ssid = "16">The first algorithms for the decoding problem were based on what is known among the speech recognition community as stack-based search (Jelinek, 1969).</S>
			<S sid ="17" ssid = "17">The original IBM solution to the decoding problem employed a restricted stack-based search (Berger et al., 1996).</S>
			<S sid ="18" ssid = "18">This idea was further explored by Wang and Waibel (Wang and Waibel, 1997) who developed a faster stack-based search algorithm.</S>
			<S sid ="19" ssid = "19">In perhaps the first work on the computational complexity of Decoding, Kevin Knight showed that the problem is closely related to the more famous Traveling Salesman problem (TSP).</S>
			<S sid ="20" ssid = "20">Independently, Christoph Tillman adapted the Held-Karp dynamic programming algorithm for TSP (Held and Karp, 1962) to Decoding (Tillman, 2001).</S>
			<S sid ="21" ssid = "21">The original Held- Karp algorithm for TSP is an exponential time dynamic programming algorithm and Tillman’s adaptation to Decoding has a prohibitive com plexity of O (l3m2 2m ) ≈ O (m5 2m ) (where mand l are the lengths of the source and tar get sentences respectively).</S>
			<S sid ="22" ssid = "22">Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ≈ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).</S>
			<S sid ="23" ssid = "23">An optimal decoder based on the well-known A∗ heuristic was implemented and benchmarked in (Och et al., 2001).</S>
			<S sid ="24" ssid = "24">Since optimal solution can not be computed for practical problem instances in a reasonable amount of time, much of recent work has focused on good quality suboptimal solutions.</S>
			<S sid ="25" ssid = "25">An O (m6) greedy search algorithm was developed (Germann et al., 2003) whose complexity was re duced further to O (m2) (Germann, 2003).</S>
			<S sid ="26" ssid = "26">In this paper, we propose an algorithmic framework for solving the decoding problem and show that several efficient decoding algorithms can be derived from the techniques developed in the framework.</S>
			<S sid ="27" ssid = "27">We model the search problem as an alternating search problem.</S>
			<S sid ="28" ssid = "28">The search, therefore, alternates between two subproblems, both of which are much easier to solve in practice.</S>
			<S sid ="29" ssid = "29">By breaking the decoding problem into two simpler search problems, we are able to provide handles for solving the problem efficiently.</S>
			<S sid ="30" ssid = "30">The solutions of the subproblems can be combined easily to arrive at a solution for the original problem.</S>
			<S sid ="31" ssid = "31">The first subproblem fixes an alignment and seeks the best translation with that alignment.</S>
			<S sid ="32" ssid = "32">Starting with an initial alignment between the source sentence and its translation, the second subproblem asks for an improved alignment.</S>
			<S sid ="33" ssid = "33">We show that both of these problems are easy to solve and provide efficient solutions for them.</S>
			<S sid ="34" ssid = "34">In an iterative search for a local optimal solution, we alternate between the two algorithms and refine our solution.</S>
			<S sid ="35" ssid = "35">The algorithmic framework provides handles for solving the decoding problem at several levels of complexity.</S>
			<S sid ="36" ssid = "36">At one extreme, the framework yields an algorithm for solving the decoding problem optimally.</S>
			<S sid ="37" ssid = "37">At the other extreme, it yields a provably linear time algorithm for finding suboptimal solutions to the problem.</S>
			<S sid ="38" ssid = "38">We show that the algorithmic handles provided by our framework can be employed to develop a very fast decoding algorithm which finds good quality translations.</S>
			<S sid ="39" ssid = "39">Our fast suboptimal search algorithms can translate sentences that are 50 words long in about 5 seconds on a simple computing facility.</S>
			<S sid ="40" ssid = "40">The rest of the paper is devoted to the development and discussion of our framework.</S>
			<S sid ="41" ssid = "41">We start with a mathematical formulation of the decoding problem (Section 2).</S>
			<S sid ="42" ssid = "42">We then develop the alternating search paradigm and use it to develop several decoding algorithms (Section 3).</S>
			<S sid ="43" ssid = "43">Next, we demonstrate the practical utility of our algorithms with the help of results from our initial experiments (Section 5).</S>
	</SECTION>
	<SECTION title="Decoding. " number = "2">
			<S sid ="44" ssid = "1">The decoding problem in SMT is one of finding Rewriting the translation model P r(f |e) as �a P r(f , a|e), where a denotes an alignmentbetween the source sentence and the target sen tence, the problem can be restated as: eˆ = argmaxe ) P r(f , a|e)P r(e).</S>
			<S sid ="45" ssid = "2">(2) a Even when the translation model P r(f |e) is as simple as IBM Model 1 and the language model P r(e) is a bigram language model, the decoding problem is NP-hard (Knight, 1999).</S>
			<S sid ="46" ssid = "3">Unless P = NP, there is no hope of an efficient algorithm for the decoding problem.</S>
			<S sid ="47" ssid = "4">Since the Fundamental Equation of SMT does not yield an easy handle to design a solution (exact or even an approximate one) for the problem, most researchers have instead worked on solving the following relatively simpler problem (Germann et al., 2003): (eˆ, aˆ) = argmax(e,a) P r(f , a|e)P r(e).</S>
			<S sid ="48" ssid = "5">(3) We call the search problem specified by Equation 3 as RELAXED DECODING.</S>
			<S sid ="49" ssid = "6">Note that RELAXED DECODING relaxes STRICT DECODING to a joint optimization problem.</S>
			<S sid ="50" ssid = "7">The search in RELAXED DECODING is for a pair (eˆ, aˆ).</S>
			<S sid ="51" ssid = "8">While RELAXED DECODING is simpler than STRICT DECODING, it is also, unfortunately, NP hard for even IBM Model 1 and Bigram language model.</S>
			<S sid ="52" ssid = "9">Therefore, all practical solutions to RELAXED DECODING have focused on finding suboptimal solutions.</S>
			<S sid ="53" ssid = "10">The challenge is in devising fast search strategies to find good suboptimal solutions.</S>
			<S sid ="54" ssid = "11">Table 1 lists the combinatorial optimization problems in the domain of decoding.</S>
			<S sid ="55" ssid = "12">In the remainder of the paper, m and l denote the length of the source language sentence and its translation respectively.</S>
	</SECTION>
	<SECTION title="Framework for Decoding. " number = "3">
			<S sid ="56" ssid = "1">We begin with a couple of useful observations about the decoding problem.</S>
			<S sid ="57" ssid = "2">Although decep the most probable translation eˆ in the target tively simple, these observations are very cru language of a given source language sentence f in accordance with the Fundamental Equation of SMT (Brown et al., 1993): eˆ = argmaxe P r(f |e)P r(e).</S>
			<S sid ="58" ssid = "3">(1) In the remainder of this paper we will refer to the search problem specified by Equation 1 as STRICT DECODING.</S>
			<S sid ="59" ssid = "4">cial for developing our framework.</S>
			<S sid ="60" ssid = "5">They are the source for algorithmic handles for breakingthe decoding problem into two relatively eas ier search problems.</S>
			<S sid ="61" ssid = "6">The first of these observations concerns with solving the problem when we know in advance the mapping between the source and target sentences.</S>
			<S sid ="62" ssid = "7">This leads to the development of an extremely simple algorithm for decoding when the alignment is known (or Pr ob le m Se ar ch S T R I C T D E C O D I N G R E L A X E D D E C O D I N G FI XE D AL IG N M EN T DE CO DI NG VI TE RB I AL IG N M EN T eˆ = arg ma xe P r(f |e) P r(e ) (eˆ, aˆ) = arg ma x(e ,a) P r(f , a|e )P r(e ) eˆ = arg ma xe P r(f , a˜|e )P r(e ) aˆ = arg ma xa P r(f , a|e˜ ) Table 1: Combinatorial Search Problems in Decoding can be guessed).</S>
			<S sid ="63" ssid = "8">Our second observation is on finding a better alignment between the source and target sentences starting with an initial (possibly suboptimal) alignment.</S>
			<S sid ="64" ssid = "9">The insight provided by the two observations are employed in building a powerful algorithmic framework.</S>
			<S sid ="65" ssid = "10">3.1 Handles for attacking the Decoding.</S>
			<S sid ="66" ssid = "11">Problem Our goal is to arrive at algorithmic handles for attacking RELAXED DECODING.</S>
			<S sid ="67" ssid = "12">In this section, we make couple of useful observations and develop algorithmic handles from the insight provided by them.</S>
			<S sid ="68" ssid = "13">The first of the two observations is: Observation 1 For a given target length l and a given alignment a˜ that maps source words to target positions, it is easy to compute the optimal target sentence eˆ.</S>
			<S sid ="69" ssid = "14">eˆ = argmaxe P r(f , a˜|e)P r(e).</S>
			<S sid ="70" ssid = "15">(4) Let us call the search problem specified by Equation 4 as FIXED ALIGNMENT DECODING.</S>
			<S sid ="71" ssid = "16">What Observation 1 is saying is that once the target sentence length and the source to target mapping is fixed, the optimal target sen tence (with the specified target length and alignment) can be computed efficiently.</S>
			<S sid ="72" ssid = "17">As we will show later, the optimal solution for FIXED ALIGNMENT DECODING can be computed in O (m) time for IBM models 15 using dynamic programming.</S>
			<S sid ="73" ssid = "18">As we can always guess an alignment (as is the case with many decoding algorithms in the literature), the above observation provides an algorithmic handle for finding suboptimal solutions for RELAXED DECODING.</S>
			<S sid ="74" ssid = "19">Our second observation is on computing the optimal alignment between the source sentence and the target sentence.</S>
			<S sid ="75" ssid = "20">Observation 2 For a given target sentence e˜, It is easy to determine the optimal (Viterbi) alignment between the source sentence and its translation.</S>
			<S sid ="76" ssid = "21">In fact, for IBM models 1 and 2, the Viterbi alignment can be computed using a straight forward algorithm in O (ml) time.</S>
			<S sid ="77" ssid = "22">For higher models, an approximate Viterbi alignment can be computed iteratively by an iterative procedure called local search.</S>
			<S sid ="78" ssid = "23">In each iteration of local search, we look in the neighborhood of the current best alignment for a better alignment (Brown et al., 1993).</S>
			<S sid ="79" ssid = "24">The first iteration can start with any arbitrary alignment (say the Viterbi alignment of Model 2).</S>
			<S sid ="80" ssid = "25">It is possible to implement one iteration of local search in O (ml) time.</S>
			<S sid ="81" ssid = "26">Typically, the number of iterations is bounded in practice by O (m), and therefore, local search takes O (m2l) time.</S>
			<S sid ="82" ssid = "27">Our framework is not strictly dependent on the computation of an optimal alignment.</S>
			<S sid ="83" ssid = "28">Any alignment which is better than the current alignment is good enough for it to work.</S>
			<S sid ="84" ssid = "29">It is straight forward to find one such alignment using restricted swaps and moves in O (m) time.</S>
			<S sid ="85" ssid = "30">In the remainder of this paper, we use the term Viterbi to denote any linear time algorithm for computing an improved alignment between the source sentence and its translation.</S>
			<S sid ="86" ssid = "31">3.2 Illustrative Algorithms.</S>
			<S sid ="87" ssid = "32">In this section, we show how the handles provided by the above two observations can be employed to solve RELAXED DECODING.</S>
			<S sid ="88" ssid = "33">The two handles are in some sense complementary to each other.</S>
			<S sid ="89" ssid = "34">When the alignment is known, we can efficiently determine the optimal translation with that alignment.</S>
			<S sid ="90" ssid = "35">On the other hand, when the translation is known, we can efficiently determine a better alignment.</S>
			<S sid ="91" ssid = "36">Therefore, we can use one to improve the other.</S>
			<S sid ="92" ssid = "37">We begin with the following simple linear time decoding algorithm which is based on the first observation.</S>
			<S sid ="93" ssid = "38">it is easy to compute the optimal alignment aˆ that maps the source words to the target words.</S>
			<S sid ="94" ssid = "39">aˆ = argmaxa P r(f , a|e˜).</S>
			<S sid ="95" ssid = "40">(5) Algorithm NaiveDecode (b) If P r (f , e, a) &gt; P r (f , eˆ, aˆ) then Input: Source language sentence f of length m &gt; 0.</S>
			<S sid ="96" ssid = "41">i. ii.</S>
			<S sid ="97" ssid = "42">eˆ = e aˆ = a. Optional Inputs: Target sentence length l, alignment a˜ between the source words and target positions.</S>
			<S sid ="98" ssid = "43">Output: Target language sentence eˆ of length l. 1.</S>
			<S sid ="99" ssid = "44">If l is not specified, let l = m..</S>
			<S sid ="100" ssid = "45">2.</S>
			<S sid ="101" ssid = "46">If an alignment is not specified, guess some.</S>
			<S sid ="102" ssid = "47">alignment a˜.</S>
			<S sid ="103" ssid = "48">3.</S>
			<S sid ="104" ssid = "49">Compute the optimal translation eˆ by solv-.</S>
			<S sid ="105" ssid = "50">ing FIXED ALIGNMENT DECODING, i.e., eˆ = argmaxe P r(f , a˜|e)P r(e).</S>
			<S sid ="106" ssid = "51">4.</S>
			<S sid ="107" ssid = "52">return eˆ.</S>
			<S sid ="108" ssid = "53">When the length of the translation is not specified, NaiveDecode assumes that the translation is of the same length as the source sentence.</S>
			<S sid ="109" ssid = "54">If an alignment that maps the source words to target positions is not specified, the algorithm guesses an alignment a˜ (a˜ can be the trivial alignment that maps the source word fj 4.</S>
			<S sid ="110" ssid = "55">return (eˆ, aˆ).</S>
			<S sid ="111" ssid = "56">NaiveOptimalDecode considers various target lengths and all possible alignments between the source words and the target positions.</S>
			<S sid ="112" ssid = "57">For each target length l and alignment a it employs NaiveDecode to find the best solution.</S>
			<S sid ="113" ssid = "58">There are (l + 1)m candidate alignments for a target length l and O (m) candidate target lengths.</S>
			<S sid ="114" ssid = "59">Therefore, NaiveOptimalDecode explores Θ (m(l + 1)m) alignments.</S>
			<S sid ="115" ssid = "60">For each of these candidate alignments, it makes a call to NaiveDecode.</S>
			<S sid ="116" ssid = "61">The time complexity of NaiveOptimalDecode is, therefore, O (m2(l + 1)m ).</S>
			<S sid ="117" ssid = "62">Although an exponential time algorithm, it can compute the optimal solution for RELAXED DECODING.</S>
			<S sid ="118" ssid = "63">With NaiveDecode and NaiveOptimalDecode we have demonstrated the power of the algorithmic handle provided by Observation 1.</S>
			<S sid ="119" ssid = "64">It is important to note that these two algorithms are at the two extremities of the spectrum.</S>
			<S sid ="120" ssid = "65">to target position j, that is, a˜j = j, or can NaiveDecode is a linear time decoding algorithmbe guessed more intelligently).</S>
			<S sid ="121" ssid = "66">It then com putes the optimal translation for the source sentence f , with the length of the target sen tence and the alignment between the source andthat computes a suboptimal solution for RE LAXED DECODING while NaiveOptimalDecode is an exponential time algorithm that computes the optimal solution.</S>
			<S sid ="122" ssid = "67">What we want are algo the target sentences kept fixed to l and a˜re rithms that are close to NaiveDecode in com spectively, by maximizing P r(f , a˜|e)P r(e).</S>
			<S sid ="123" ssid = "68">As FIXED ALIGNMENT DECODING can be solved in O (m) time, NaiveDecode takes only O(m) time.</S>
			<S sid ="124" ssid = "69">The value of NaiveDecode lies not in itself per se, but in its instrumental role in designing more superior algorithms.</S>
			<S sid ="125" ssid = "70">The power of NaiveDecode can be demonstrated with the following optimal algorithm for RELAXED DECODING.</S>
			<S sid ="126" ssid = "71">Algorithm NaiveOptimalDecode Input: Source language sentence f of length m &gt; 0.</S>
			<S sid ="127" ssid = "72">Output: Target language sentence eˆ of length l, m ≤ l ≤ 2m.</S>
			<S sid ="128" ssid = "73">1.</S>
			<S sid ="129" ssid = "74">Let eˆ = null and aˆ = null..</S>
			<S sid ="130" ssid = "75">2.</S>
			<S sid ="131" ssid = "76">For each l = m , . . .</S>
			<S sid ="132" ssid = "77">, 2m do.</S>
			<S sid ="133" ssid = "78">3.</S>
			<S sid ="134" ssid = "79">For each alignment a between the source.</S>
			<S sid ="135" ssid = "80">words and the target positions do (a) Let e = N aiveDecode(f , l, a).</S>
			<S sid ="136" ssid = "81">plexity and to NaiveOptimalDecode in quality.</S>
			<S sid ="137" ssid = "82">It is possible to reduce the complexity of NaiveOptimalDecode significantly by carefully reducing the number of alignments that are examined.</S>
			<S sid ="138" ssid = "83">Instead of examining all Θ(m(l + 1)m ) alignments, if we examine only a small number, say g (m), alignments in NaiveOptimalDe- code, we can find a solution in O (mg (m)) time.</S>
			<S sid ="139" ssid = "84">In the next section, we show how to restrict the search to only a small number of promising alignments.</S>
			<S sid ="140" ssid = "85">3.3 Alternating Maximization.</S>
			<S sid ="141" ssid = "86">We now show how to use the two algorithmic handles to come up with a fast search paradigm.</S>
			<S sid ="142" ssid = "87">We alternate between searching the best translation given an alignment and searching the best alignment given a translation.</S>
			<S sid ="143" ssid = "88">Since the two subproblems are complementary, they can be used to improve the solution computed by one another by alternating between the two problems.</S>
			<S sid ="144" ssid = "89">Algorithm AlternatingSearch Input: Source language sentence f of length m &gt; 0.</S>
			<S sid ="145" ssid = "90">Output: Target language sentence e(o) of length l (m/2 ≤ l ≤ 2m).</S>
			<S sid ="146" ssid = "91">1.</S>
			<S sid ="147" ssid = "92">Let e(o) = null and a(o) = null..</S>
			<S sid ="148" ssid = "93">2.</S>
			<S sid ="149" ssid = "94">For each l = m/2, . . .</S>
			<S sid ="150" ssid = "95">, 2m do.</S>
			<S sid ="151" ssid = "96">(a) Let e = null and a = null.</S>
			<S sid ="152" ssid = "97">(b) While there is improvement in solution do i. Let e = N aiveDecode (f , l, a).</S>
			<S sid ="153" ssid = "98">ii.</S>
			<S sid ="154" ssid = "99">Let ˆa = V iterbi (f , e).</S>
			<S sid ="155" ssid = "100">(c) If P r (f , e, a) &gt; P r (f , e(o), a(o) ) then i. e(o) = e ii.</S>
			<S sid ="156" ssid = "101">a(o) = a. 3.</S>
			<S sid ="157" ssid = "102">return e(o).</S>
			<S sid ="158" ssid = "103">AlternatingSearch searches for a good translation by varying the length of the target sentence.</S>
			<S sid ="159" ssid = "104">For a sentence length l, the algorithm finds a translation of length l and then iteratively improves the translation.</S>
			<S sid ="160" ssid = "105">In each iteration it solves two subproblems: FIXED ALIGNMENT DECODING and VITERBI ALIGNMENT.</S>
			<S sid ="161" ssid = "106">The input to each iteration are the source sentence f , the target sentence length l, and an alignment a between the source and target sentences.</S>
			<S sid ="162" ssid = "107">So, AlternatingSearch finds a better translation e for f by solving FIXED ALIGNMENT DECODING.</S>
			<S sid ="163" ssid = "108">For this purpose it employs NaiveDecode.</S>
			<S sid ="164" ssid = "109">Having computed e, the algorithm computes a better alignment (aˆ) between e and f by solving VITERBI ALIGNMENT using Viterbi algorithm.</S>
			<S sid ="165" ssid = "110">The new alignment thus found is used by the algorithm in the subsequent iteration.</S>
			<S sid ="166" ssid = "111">At the end of each iteration the algorithm checks whether it has made progress.</S>
			<S sid ="167" ssid = "112">The algorithm returns the best translation of the source f across a range of target sentence lengths.</S>
			<S sid ="168" ssid = "113">The analysis of AlternatingSearch is complicated by the fact that the number of iterations (see step 2.b) depends on the input.</S>
			<S sid ="169" ssid = "114">It is reasonable to assume that the length of the source sentence (m) is an upper bound on the number of iterations.</S>
			<S sid ="170" ssid = "115">In practice, however, the number of iterations is typically O (1).</S>
			<S sid ="171" ssid = "116">There are 3m/2 candidate sentence lengths for the translation (l varies from m/2 to 2m) and both NaiveDe- code and Viterbi are O (m).</S>
			<S sid ="172" ssid = "117">Therefore, the time complexity of AlternatingSearch is O (m2).</S>
	</SECTION>
	<SECTION title="A Linear Time Algorithm for" number = "4">
			<S sid ="173" ssid = "1">FIXED ALIGNMENT DECODING A key component of all our algorithms is a linear time algorithm for the problem FIXED ALIGNMENT DECODING.</S>
			<S sid ="174" ssid = "2">Recall that in FIXED ALIGNMENT DECODING, we are given the target length l and a mapping a˜ from source words to target positions.</S>
			<S sid ="175" ssid = "3">The goal is then to find the optimal translation with a˜ as the alignment.</S>
			<S sid ="176" ssid = "4">In this section, we give a dynamic programming based solution to this problem.</S>
			<S sid ="177" ssid = "5">Our solution is based on a new formulation of IBM translation models.</S>
			<S sid ="178" ssid = "6">We begin our discussion with a few technical definitions.</S>
			<S sid ="179" ssid = "7">Alignment a˜ maps each of the source words fj , j = 1, . . .</S>
			<S sid ="180" ssid = "8">, m to a target position in the range [0, . . .</S>
			<S sid ="181" ssid = "9">, l].</S>
			<S sid ="182" ssid = "10">Define a mapping ψ from [0, . . .</S>
			<S sid ="183" ssid = "11">, l] to subsets of {1, . . .</S>
			<S sid ="184" ssid = "12">, m} as follows: ψ(i) = {j : j ∈ {1, . . .</S>
			<S sid ="185" ssid = "13">, m} ∧ a˜j = i} ∨ i = 0, . . .</S>
			<S sid ="186" ssid = "14">, l. ψ(i) is the set of source positions which are mapped to the target location i by the alignment a˜ and the fertility of the target position i is φi = |ψ(i)|.</S>
			<S sid ="187" ssid = "15">We can rewrite each of the IBM models P r (f , ˜a|e) as follows: l P r (f , ˜a|e) = ξ n TiDi Ni.</S>
			<S sid ="188" ssid = "16">i=1 Table 2 shows the breaking of P r (f , ˜a|e) into the constituents Ti, Di and Ni.</S>
			<S sid ="189" ssid = "17">As a consequence, we can write P r (f , ˜a|e) P r (e) as: l P r (f , ˜a|e) P r (e) = ξλ n TiDiNiLi i=1 where Li = trigram(ei|ei−2 , ei−1 ) and λ is the trigram probability of the boundary word.</S>
			<S sid ="190" ssid = "18">The above reformulation of the optimization function of the decoding problem allows us to employ Dynamic Programming for solving FIXED ALIGNMENT DECODING efficiently.</S>
			<S sid ="191" ssid = "19">Note that each word ei has only a constant number of candidates in the vocabulary.</S>
			<S sid ="192" ssid = "20">Therefore, the set of words e1 , . . .</S>
			<S sid ="193" ssid = "21">, el that maximizes the LHS of the above optimization function can be found in O (m) time using the standard Dynamic Programming algorithm (Cormen et al., 2001).</S>
	</SECTION>
	<SECTION title="Experiments and Results. " number = "5">
			<S sid ="194" ssid = "1">In this section we describe our experimental setup and present the initial results.</S>
			<S sid ="195" ssid = "2">Our goal (l+1)m 0 1 ilk∈ψ(i) k∈ψ(i) Table 2: P r (f, a˜|e) for IBM Models was not only to evaluate the performance of our algorithms on real data, but also to evaluate how easy it is to code the algorithm and whether a straightforward implementation of the algorithm with no parameter tuning can give satisfactory results.</S>
			<S sid ="196" ssid = "3">We implemented the algorithms in C++ and conducted the experiments on an IBM RS-6000 dual processor machine with 1 GB of RAM.</S>
			<S sid ="197" ssid = "4">We built a French-English translation model (IBM Model 3) by training over a corpus of 100 K sentence pairs from the Hansard corpus.</S>
			<S sid ="198" ssid = "5">The translation direction was from French to English.</S>
			<S sid ="199" ssid = "6">We built an English language model by training over a corpus consisting of about 800 million words.</S>
			<S sid ="200" ssid = "7">We divided the test sentences into several classes based on their length.</S>
			<S sid ="201" ssid = "8">Each length class consisted of 300 test French sentences.</S>
			<S sid ="202" ssid = "9">We implemented four algorithms -1.1 (NaiveDe- code), 1.2 (Alternating Search with l restricted to m), 2.1 (NaiveDecode with l varying from m/2 to 2m) and 2.2 (Alternating Search).</S>
			<S sid ="203" ssid = "10">In order to compare the performance of the algorithms proposed in this paper with a previous decoding algorithm, we also implemented scale.</S>
			<S sid ="204" ssid = "11">Plot 2 shows the NIST score of the translations for each length class while Plot 3 shows the average log score of the translations (-ve log of P r (f , a|e) P r (e) ) again for each length class.It can be seen from Plot 1 that all of our al gorithms are indeed very fast in practice.</S>
			<S sid ="205" ssid = "12">They are, in fact, an order faster than the Held-Karp algorithm.</S>
			<S sid ="206" ssid = "13">Our algorithms are able to trans late even long sentences (50+ words) in a few seconds.</S>
			<S sid ="207" ssid = "14">Plot 3 shows that the log scores of the translations computed by our algorithms are very close to those computed by the Held-Karp algorithm.</S>
			<S sid ="208" ssid = "15">Plot 2 compares the NIST scores ob tained with each of the algorithm.</S>
			<S sid ="209" ssid = "16">Among the four algorithms based on our framework, Algorithm 2.2 gives the best NIST scores as expected.</S>
			<S sid ="210" ssid = "17">Although, the log scores of our algorithms are comparable to those of the Held- Karp algorithm, our NIST scores are lower.</S>
			<S sid ="211" ssid = "18">It should be noted that the mathematical quantity that our algorithm tries to optimize is the log score.</S>
			<S sid ="212" ssid = "19">Plot 3 shows that our algorithms are quite good at finding solutions with good scores.</S>
			<S sid ="213" ssid = "20">Decoding Time the dynamic programming based algorithm by (Tillman, 2001).</S>
			<S sid ="214" ssid = "21">For each of the algorithms, we computed the following: 10000 1000 &quot;algorithm 1.1&quot; &quot;algorithm 1.2&quot; &quot;algorithm 2.1&quot; &quot;algorithm 2.2&quot; &quot;algorithm H-K&quot; 1.</S>
			<S sid ="215" ssid = "22">Average time taken for translation for.</S>
			<S sid ="216" ssid = "23">each length class.</S>
			<S sid ="217" ssid = "24">2.</S>
			<S sid ="218" ssid = "25">NIST score of the translations for each.</S>
			<S sid ="219" ssid = "26">length class.</S>
			<S sid ="220" ssid = "27">3.</S>
			<S sid ="221" ssid = "28">Average value of the optimization.</S>
			<S sid ="222" ssid = "29">function for the translations for each length class.</S>
			<S sid ="223" ssid = "30">The results of the experiments are summarized in Plots 1, 2 and 3.</S>
			<S sid ="224" ssid = "31">In all the plots, the length class is denoted by the x-axis.</S>
			<S sid ="225" ssid = "32">1120 indicates the class with sentences of length between 11 words to 20 words.</S>
			<S sid ="226" ssid = "33">51 indicates the group of sentences with sentence length 51 or more.</S>
			<S sid ="227" ssid = "34">Plot 1 shows the average time taken by the algorithms for translating the sentences in each length class.</S>
			<S sid ="228" ssid = "35">Time is shown in seconds on a log 100 10 1 0.1 0.01010 1120 2130 3140 4150 51 Sentence Length Figure 1: Average decoding time</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="229" ssid = "1">The algorithmic framework developed in this paper is powerful as it yields several decoding algorithms.</S>
			<S sid ="230" ssid = "2">At one end of the spectrum is a provably linear time algorithm for computing a suboptimal solution and at the other end is an exponential time algorithm for computing NIST Scores 7 6.5 &quot;algorithm 1.1&quot; &quot;algorithm 1.2&quot; &quot;algorithm 2.1&quot; &quot;algorithm 2.2&quot; P. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.</S>
			<S sid ="231" ssid = "3">1993.</S>
			<S sid ="232" ssid = "4">The mathematics of 6 5.5 5 4.5 4 3.5 3&quot;algorithm H K&quot; ma chi ne tra nsl ati on: Pa ra me ter est im ati on.</S>
			<S sid ="233" ssid = "5">Co mp uta tio nal Li ng uis tic s, 19 (2) :2 63 – 31 1.</S>
			<S sid ="234" ssid = "6">T. H . C or m en , C . E. L ei se rs o n, R . L. R iv es t, a n d C . St ei n. 2 0 0 1.</S>
			<S sid ="235" ssid = "7">T he M I T P re ss , C a m - br id ge . M. R. Gare y and D. S. John son.</S>
			<S sid ="236" ssid = "8">197 9.</S>
			<S sid ="237" ssid = "9">W. H. Fre em an an d Co m pa ny, Ne w Yo rk.</S>
			<S sid ="238" ssid = "10">U. Ger man n, M. Jahr , D. Mar cu, and K. Ya010 1120 2130 3140 4150 51 Sentence Length Figure 2: NIST scores Logscores mada.</S>
			<S sid ="239" ssid = "11">2003.</S>
			<S sid ="240" ssid = "12">Fast decoding and optimal decoding for machine translation.</S>
			<S sid ="241" ssid = "13">Artificial Intelligence.</S>
			<S sid ="242" ssid = "14">Ulrich Germann.</S>
			<S sid ="243" ssid = "15">2003.</S>
			<S sid ="244" ssid = "16">Greedy decoding for 400 350 300 250 200 150 100 50 0 &quot;algorithm 1.1&quot; &quot;algorithm 1.2&quot; &quot;algorithm 2.1&quot; &quot;algorithm 2.2&quot; &quot;algorithm H-K&quot; st at is ti ca l m ac hi ne tr a ns la ti o n in al m os t li n- ea r ti m e. In Proceedings of HLTNAACL 20 03.</S>
			<S sid ="245" ssid = "17">Ed mo nt on, Ca na da . M. H el d a n d R. K ar p. 1 9 6 2.</S>
			<S sid ="246" ssid = "18">A d y n a m ic pr o- gr a m m in g a p pr o ac h to se q ue nc in g pr o bl e m s. J. SI A M , 1 0( 1) :1 9 6 – 2 1 0.</S>
			<S sid ="247" ssid = "19">F. Je li ne k. 1 9 6 9.</S>
			<S sid ="248" ssid = "20">A fa st se q ue nt ia l de co di n g al - go ri th m us in g a st ac k. I B M J o ur n al R es ea ch a n d D ev el o p m e nt , 1 3: 6 7 5 – 6 8 5.</S>
			<S sid ="249" ssid = "21">010 1120 2130 3140 4150 51- Sentence Length Figure 3: Log score the optimal solution.</S>
			<S sid ="250" ssid = "22">We have also shown that alternating maximization can be employed to come up with O (m2) decoding algorithm.</S>
			<S sid ="251" ssid = "23">Two questions in this connection are: 1.</S>
			<S sid ="252" ssid = "24">Is it possible to reduce the complexity.</S>
			<S sid ="253" ssid = "25">of AlternatingSearch to O (m)?</S>
			<S sid ="254" ssid = "26">2.</S>
			<S sid ="255" ssid = "27">Instead of exploring each alignment.</S>
			<S sid ="256" ssid = "28">separately, is it possible to explore a bunch of alignments in one shot?</S>
			<S sid ="257" ssid = "29">Answers to these questions will result in faster and more efficient decoding algorithms.</S>
	</SECTION>
	<SECTION title="Acknowledgements. " number = "7">
			<S sid ="258" ssid = "1">We are grateful to Raghu Krishnapuram for his insightful comments on an earlier draft of this paper and Pasumarti Kamesam for his help during the course of this work.</S>
	</SECTION>
</PAPER>
