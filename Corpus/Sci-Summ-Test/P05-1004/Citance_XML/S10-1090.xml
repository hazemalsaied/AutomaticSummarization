<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper summarizes our participation in task #17 of SemEval–2 (All–words WSD on a specific domain) using a supervised class-based Word Sense Disambiguation system.</S>
		<S sid ="2" ssid = "2">Basically, we use Support Vector Machines (SVM) as learning algorithm and a set of simple features to build three different models.</S>
		<S sid ="3" ssid = "3">Each model considers a different training corpus: SemCor (SC), examples from monosemous words extracted automatically from background data (BG), and both SC and BG (SCBG).</S>
		<S sid ="4" ssid = "4">Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire class-based annotated examples from the domain text.</S>
		<S sid ="5" ssid = "5">We use the class-based examples gathered from the domain corpus to adapt our traditional system trained on SemCor.</S>
		<S sid ="6" ssid = "6">The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">As empirically demonstrated by the last SensEval and SemEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed.</S>
			<S sid ="8" ssid = "8">In fact, supervised word-based WSD systems are very dependent of the corpora used for training and testing the system (Escudero et al., 2000).</S>
			<S sid ="9" ssid = "9">One possible reason could be the use of inappropriate level of abstraction.</S>
			<S sid ="10" ssid = "10">where each class corresponds to a particular synset of the word.</S>
			<S sid ="11" ssid = "11">But, WordNet (WN) has been widely criticized for being a sense repository that often provides too fine–grained sense distinctions for higher level applications like Machine Translation or Question &amp; Answering.</S>
			<S sid ="12" ssid = "12">In fact, WSD at this level of granularity has resisted all attempts of inferring robust broad-coverage models.</S>
			<S sid ="13" ssid = "13">It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples.</S>
			<S sid ="14" ssid = "14">Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Schu¨ tze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007).</S>
			<S sid ="15" ssid = "15">That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</S>
			<S sid ="16" ssid = "16">In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005), (Kohomban and Lee, 2005) and (Ciaramita and Altun, 2006).</S>
			<S sid ="17" ssid = "17">That is, grouping senses of different words into the same explicit and comprehensive semantic class.</S>
			<S sid ="18" ssid = "18">Most of the later approaches used the original Lexico- graphical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions.</S>
			<S sid ="19" ssid = "19">We suspect that selecting the appropriate level of abstraction could be on between both levels.</S>
			<S sid ="20" ssid = "20">Thus, we use the semantic classes modeled by the Basic Level Concepts1 (BLC) (Izquierdo et al., 2007).</S>
			<S sid ="21" ssid = "21">Our previous research using BLC empirically demonstrated that this automatically derived Most supervised systems simply model each polysemous word as a classification problem 1 http://adimen.si.ehu.es/web/BLC 402 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 402–406, Uppsala, Sweden, 1516 July 2010.</S>
			<S sid ="22" ssid = "22">Qc 2010 Association for Computational Linguistics set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation (WSD) (Izquierdo et al., 2009).</S>
			<S sid ="23" ssid = "23">Now, we also show that class-based WSD allows to successfully incorporate monosemous examples from the domain text.</S>
			<S sid ="24" ssid = "24">In fact, 3, and A has 2, so D is the first maximum).</S>
			<S sid ="25" ssid = "25">A 2 the robustness of our class-based WSD approach is shown by our system that just uses the Sem Cor examples (SC).</S>
			<S sid ="26" ssid = "26">It performs without any kind B C D 2 3 BLC of domain adaptation as the Most Frequent Sense (MFS) baseline.</S>
			<S sid ="27" ssid = "27">This paper describes our participation in SemEval2010 Task 17 (Agirre et al., 2010).</S>
			<S sid ="28" ssid = "28">In section 2 semantic classes used and selection algorithm used to obtain them automatically from WordNet are described.</S>
			<S sid ="29" ssid = "29">In section 3 the technique employed to extract monosemous examples from background data is described.</S>
			<S sid ="30" ssid = "30">Section 4 explains the general approach of our system, and the experiments designed, and finally, in section 5, the results and some analysis are shown.</S>
	</SECTION>
	<SECTION title="Semantic Classes. " number = "2">
			<S sid ="31" ssid = "1">The set of semantic classes used in this work are the Basic Level Concepts2 (BLC) (Izquierdo et al., 2007).</S>
			<S sid ="32" ssid = "2">These concepts are small sets of meanings representing the whole nominal and verbal part of WN.</S>
			<S sid ="33" ssid = "3">BLC can be obtained by a very simple method that uses basic structural WordNet properties.</S>
			<S sid ="34" ssid = "4">In fact, the algorithm only considers the relative number of relations of each synset along the hypernymy chain.</S>
			<S sid ="35" ssid = "5">The process follows a bottom- up approach using the chain of hypernymy relations.</S>
			<S sid ="36" ssid = "6">For each synset in WN, the process selects as its BLC the first local maximum according to the relative number of relations.</S>
			<S sid ="37" ssid = "7">The local maximum is the synset in the hypernymy chain having more relations than its immediate hyponym and immediate hypernym.</S>
			<S sid ="38" ssid = "8">For synsets having multiple hypernyms, the path having the local maximum with higher number of relations is selected.</S>
			<S sid ="39" ssid = "9">Usually, this process finishes having a number of preliminary BLC.</S>
			<S sid ="40" ssid = "10">Figure 1 shows an example of selection of a BLC.</S>
			<S sid ="41" ssid = "11">The figure represents the hypernymy hierarchy of WordNet, with circles representing synsets, and links between them representing hypernym relations.</S>
			<S sid ="42" ssid = "12">The algorithm selects the D synset as BLC for J, due to D is the first maximum in the hypernymy chain, according to the number of relations (F has 2 hyponyms, D has 2 http://adimen.si.ehu.es/web/BLC E F G H 2 I J Synset Figure 1: Example of BLC selection Obviously, while ascending through this chain, more synsets are subsumed by each concept.</S>
			<S sid ="43" ssid = "13">The process finishes checking if the number of concepts subsumed by the preliminary list of BLC is higher than a certain threshold.</S>
			<S sid ="44" ssid = "14">For those BLC not representing enough concepts according to the threshold, the process selects the next local maximum following the hypernymy hierarchy.</S>
			<S sid ="45" ssid = "15">Thus, depending on the type of relations considered to be counted and the threshold established, different sets of BLC can be easily obtained for each WN version.</S>
			<S sid ="46" ssid = "16">We have selected the set which considers WN version 3.0, the total number of relations per synset, and a minimum threshold of 20 concepts to filter out not representative BLC (BLC–20).</S>
			<S sid ="47" ssid = "17">This set has shown to reach good performance on previous SensEval and SemEval exercices (Izquierdo et al., 2009).</S>
			<S sid ="48" ssid = "18">There are 649 different BLC for nouns on WordNet 3.0, and 616 for verbs.</S>
			<S sid ="49" ssid = "19">Table 2 shows the three most frequent BLC per POS, with the number of synsets subsumed by each concept, and its WordNet gloss.</S>
	</SECTION>
	<SECTION title="Using Monosemous Examples from the. " number = "3">
			<S sid ="50" ssid = "1">Domain We did not applied any kind of specific domain adaptation technique to our class-based supervised system.</S>
			<S sid ="51" ssid = "2">In order to adapt our supervised system to the environmental domain we only increased the training data with new examples of the domain.</S>
			<S sid ="52" ssid = "3">To acquire these examples, we used the environmental domain background documents provided by the organizers.</S>
			<S sid ="53" ssid = "4">Specifically, we used the 122 back P o S Nu m. B L C Glo ss No un s 4.</S>
			<S sid ="54" ssid = "5">79 2 1.</S>
			<S sid ="55" ssid = "6">93 5 1.</S>
			<S sid ="56" ssid = "7">84 6 p er s o n. n. 0 1 a cti vi ty .n .0 1 ac t. n. 0 2 a hu ma n bei ng any sp ecif ic be ha vio r so me thin g tha t pe opl e do or cau se to ha pp en Ve rb s 1.</S>
			<S sid ="57" ssid = "8">54 1 1.</S>
			<S sid ="58" ssid = "9">08 5 5 1 9 ch an ge .v. 01 ch an ge .v. 02 m ov e. v. 0 2 cau se to ch an ge; ma ke diff ere nt; cau se a tra nsf or ma tio n u n d er g o a ch a n g e; b ec o m e dif fe re nt in es se nc e; lo si ng on e’ s or it s or ig in al n a- tu re ca us e to m ov e or s hi ft in to a n e w p os iti o n or pl ac e, b ot h in a co nc re te an d in a n ab str ac t se ns e Table 1: Most frequent BLC–20 semantic classes on WordNet 3.0 ground documents3.</S>
			<S sid ="59" ssid = "10">TreeTagger has been used to preprocess the documents, performing PoS tagging and lemmatization.</S>
			<S sid ="60" ssid = "11">Since the background documents are not semantically annotated, and our supervised system needs labeled data, we have selected only the monosemous words occurring in the documents.</S>
			<S sid ="61" ssid = "12">In this way, we have obtained automatically a large set of examples annotated with BLC.</S>
			<S sid ="62" ssid = "13">Table 3 presents the total number of training examples extracted from SemCor (SC) and from the background documents (BG).</S>
			<S sid ="63" ssid = "14">As expected, by this method a large number of monosemous examples can be obtained for nouns and verbs.</S>
			<S sid ="64" ssid = "15">Also as expected, verbs are much less productive than nouns.</S>
			<S sid ="65" ssid = "16">However, all these background examples correspond to a reduced set of 7,646 monosemous words.</S>
			<S sid ="66" ssid = "17">N o u n s Ve rb s N + V S C B G 8 7.</S>
			<S sid ="67" ssid = "18">9 7 8 19 3.5 36 48.</S>
			<S sid ="68" ssid = "19">26 7 10.</S>
			<S sid ="69" ssid = "20">82 1 13 6.2 45 20 4.3 57 Tot al 28 1.5 14 59.</S>
			<S sid ="70" ssid = "21">08 8 34 0.6 02 Table 2: Number of training examples Table 3 lists the ten most frequent monosemous nouns and verbs occurring in the background documents.</S>
			<S sid ="71" ssid = "22">Note that all these examples are monosemous according to BLC–20 semantic classes.</S>
			<S sid ="72" ssid = "23">N o u n s V e r b s L e m m a # ex . Le m m a # ex.</S>
			<S sid ="73" ssid = "24">1 2 3 4 5 6 7 8 9 10 bi od iv er sit y h a b i t a t s p e c i e c l i m a t e e u r o p e a n e c o s y s t e m r i v e r g r a s s l a n d d a t u m d i r e c t i v e 7.4 76 7.2 06 7.0 67 3.5 39 2.8 18 2.6 69 2.4 20 2.3 03 2.2 76 2.1 97 m o ni to r a c hi e v e ta rg et s el e ct e n a bl e s e e m pi n e e v al u at e e x pl or e b el ie v e 78 8 78 4 48 4 34 5 33 4 28 7 28 1 24 6 20 0 17 2 Table 3: Most frequent monosemic words in BG</S>
	</SECTION>
	<SECTION title="System Overview. " number = "4">
			<S sid ="74" ssid = "1">Our system applies a supervised machine learning approach.</S>
			<S sid ="75" ssid = "2">We apply a feature extractor to represent the training examples of the examples acquired from SemCor and the background documents.</S>
			<S sid ="76" ssid = "3">Then, a machine learning engine uses the annotated examples to train a set of classifiers.</S>
			<S sid ="77" ssid = "4">Support Vector Machines (SVM) have been proven to be robust and very competitive in many NLP tasks, and in WSD in particular (Ma`rquez et al., 2006).</S>
			<S sid ="78" ssid = "5">We used the SVM-Light implementation4 (Joachims, 1998).</S>
			<S sid ="79" ssid = "6">We create a classifier for each semantic class.</S>
			<S sid ="80" ssid = "7">This approach has several advantages compared to word–based approach.</S>
			<S sid ="81" ssid = "8">The training data per classifier is increased (we can use examples of different target words for a single classifier, whenever all examples belong to the same semantic class), the polysemy is reduced (some different word senses can be collapsed into the same semantic class), and, finally, semantic classes provide higher levels of abstraction.</S>
			<S sid ="82" ssid = "9">For each polysemous word occurring in the test corpus, we obtain its potential BLC–20 classes.</S>
			<S sid ="83" ssid = "10">Then, we only apply the classifiers corresponding to the BLC20 classes of the polysemous word.</S>
			<S sid ="84" ssid = "11">Finally, our system simply selects the BLC–20 class with the greater prediction.</S>
			<S sid ="85" ssid = "12">In order to obtain the correct WordNet 3.0 synset required by the task, we apply a simple heuristic that has shown to be robust and accurate (Kohomban and Lee, 2005).</S>
			<S sid ="86" ssid = "13">Our classifiers obtain first the semantic class, and then, the synset of the first WordNet sense that fits with the semantic class is assigned to the word.</S>
			<S sid ="87" ssid = "14">We selected a simple feature set widely used in many WSD systems.</S>
			<S sid ="88" ssid = "15">In particular, we use a window of five tokens around the target word to extract word forms, lemmas; bigrams and trigrams of word forms and lemmas; trigrams of PoS tags, 3 We used the documents contained on the trial data and.</S>
			<S sid ="89" ssid = "16">the background.</S>
			<S sid ="90" ssid = "17">4 http://svmli ght.joachim s.org and also the most frequent BLC–20 semantic class of the target word in the training corpus.</S>
			<S sid ="91" ssid = "18">Our system is fully described in (Izquierdo et al., 2009).</S>
			<S sid ="92" ssid = "19">The novelty introduced here is the use of semantic classes to obtain monosemous examples from the domain corpus.</S>
			<S sid ="93" ssid = "20">Following the same framework (BLC–20 semantic architecture and basic set of features) we designed three runs, each one using a different training corpus.</S>
			<S sid ="94" ssid = "21">• SC: only training examples extracted from SemCor • BG: only monosemous examples extracted from the background data • SCBG: training examples extracted from SemCor and monosemous background data The first run shows the behavior of a supervised system trained on a general corpus, and tested in a specific domain.</S>
			<S sid ="95" ssid = "22">The second one analyzes the contribution of the monosemous examples extracted from the background data.</S>
			<S sid ="96" ssid = "23">Finally, the third run studies the robustness of the approach when combining the training examples from SemCor and from the background.</S>
	</SECTION>
	<SECTION title="Results and Discussion. " number = "5">
			<S sid ="97" ssid = "1">A total of 29 runs has been submitted for the English All–words WSD on a Specific Domain.</S>
			<S sid ="98" ssid = "2">Table 5 shows the ranking results of our three runs with respect to the other participants.</S>
			<S sid ="99" ssid = "3">The figures for the first sense (1sense) and random sense (Random) baselines are included.</S>
			<S sid ="100" ssid = "4">In general, the results obtained are not very high.</S>
			<S sid ="101" ssid = "5">The best system only achieves a precision of 0.570, and the first sense baseline reaches a precision of 0.505.</S>
			<S sid ="102" ssid = "6">This shows that the task is hard to solve, and the domain adaptation of WSD systems is not an easy task.</S>
			<S sid ="103" ssid = "7">Interestingly, our worst result is obtained by the system using only the monosemous background examples (BG).</S>
			<S sid ="104" ssid = "8">This system ranks 23th with a Precision and Recall of 0.380 (0.385 for nouns and 0.366 for verbs).</S>
			<S sid ="105" ssid = "9">The system using only SemCor (SC) ranks 6th with Precision and Recall of 0.505 (0.527 for nouns and 0.443 for verbs).</S>
			<S sid ="106" ssid = "10">This is also the performance of the first sense baseline.</S>
			<S sid ="107" ssid = "11">As expected, the best result of our three runs is obtained when combining the examples from SemCor and the background (SCBG).</S>
			<S sid ="108" ssid = "12">This supervised system obtains the 5th position with a Precision and Recall of 0.513 (0.534 for nouns, 0.454 for verbs) which is slightly above the baseline.</S>
			<S sid ="109" ssid = "13">R a n k Pr eci sio n Re cal l 1 2 3 4 (S CB G) 5 1se nse (S C) 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 (B G) 23 24 25 26 27 28 29 Ra nd om 0 . 5 7 0 0 . 5 5 4 0 . 5 3 4 0 . 5 2 2 0 . 5 1 3 0 . 5 0 5 0 . 5 0 5 0 . 5 1 2 0 . 5 0 6 0 . 5 0 4 0 . 4 8 1 0 . 4 9 2 0 . 4 6 1 0 . 4 4 7 0 . 4 3 6 0 . 4 4 0 0 . 4 9 6 0 . 4 9 8 0 . 4 3 3 0 . 4 2 6 0 . 4 2 4 0 . 4 3 7 0 . 3 8 4 0 . 3 8 0 0 . 3 8 1 0 . 3 5 1 0 . 3 7 0 0 . 3 2 8 0 . 3 2 1 0 . 3 1 2 0 . 2 3 0 0.</S>
			<S sid ="110" ssid = "14">5 5 5 0.</S>
			<S sid ="111" ssid = "15">5 4 0 0.</S>
			<S sid ="112" ssid = "16">5 2 8 0.</S>
			<S sid ="113" ssid = "17">5 1 6 0.</S>
			<S sid ="114" ssid = "18">5 1 3 0.</S>
			<S sid ="115" ssid = "19">5 0 5 0.</S>
			<S sid ="116" ssid = "20">5 0 5 0.</S>
			<S sid ="117" ssid = "21">4 9 5 0.</S>
			<S sid ="118" ssid = "22">4 9 3 0.</S>
			<S sid ="119" ssid = "23">4 9 1 0.</S>
			<S sid ="120" ssid = "24">4 8 1 0.</S>
			<S sid ="121" ssid = "25">4 7 9 0.</S>
			<S sid ="122" ssid = "26">4 6 0 0.</S>
			<S sid ="123" ssid = "27">4 4 1 0.</S>
			<S sid ="124" ssid = "28">4 3 5 0.</S>
			<S sid ="125" ssid = "29">4 3 4 0.</S>
			<S sid ="126" ssid = "30">4 3 3 0.</S>
			<S sid ="127" ssid = "31">4 3 2 0.</S>
			<S sid ="128" ssid = "32">4 3 1 0.</S>
			<S sid ="129" ssid = "33">4 2 5 0.</S>
			<S sid ="130" ssid = "34">4 2 2 0.</S>
			<S sid ="131" ssid = "35">3 9 2 0.</S>
			<S sid ="132" ssid = "36">3 8 4 0.</S>
			<S sid ="133" ssid = "37">3 8 0 0.</S>
			<S sid ="134" ssid = "38">3 5 6 0.</S>
			<S sid ="135" ssid = "39">3 5 0 0.</S>
			<S sid ="136" ssid = "40">3 4 5 0.</S>
			<S sid ="137" ssid = "41">3 2 2 0.</S>
			<S sid ="138" ssid = "42">3 1 5 0.</S>
			<S sid ="139" ssid = "43">3 0 3 0.</S>
			<S sid ="140" ssid = "44">2 3 0 Table 4: Results of task#17 Possibly, the reason of low performance of the BG system is the high correlation between the features of the target word and its semantic class.</S>
			<S sid ="141" ssid = "45">In this case, these features correspond to the monosemous word while when testing corresponds to the target word.</S>
			<S sid ="142" ssid = "46">However, it also seems that class- based systems are robust enough to incorporate large sets of monosemous examples from the domain text.</S>
			<S sid ="143" ssid = "47">In fact, to our knowledge, this is the first time that a supervised WSD algorithm have been successfully adapted to an specific domain.</S>
			<S sid ="144" ssid = "48">Furthermore, our system trained only on SemCor also achieves a good performance, reaching the first sense baseline, showing that class-based WSD approaches seem to be robust to domain variations.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="145" ssid = "49">This paper has been supported by the European Union under the project KYOTO (FP7 ICT 211423), the Valencian Region Government under PROMETEO project for excellence groups and the Spanish Government under the projects KNOW2 (TIN200914715-C0404) and TEXT- MESS-2 (TIN200913391-C0404).</S>
	</SECTION>
</PAPER>
