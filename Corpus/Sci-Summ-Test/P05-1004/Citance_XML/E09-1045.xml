<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">As empirically demonstrated by the last SensEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed.</S>
		<S sid ="2" ssid = "2">One possible reason could be the use of inappropriate set of meanings.</S>
		<S sid ="3" ssid = "3">In fact, WordNet has been used as a defacto standard repository of meanings.</S>
		<S sid ="4" ssid = "4">However, to our knowledge, the meanings represented by WordNet have been only used for WSD at a very fine-grained sense level or at a very coarse-grained class level.</S>
		<S sid ="5" ssid = "5">We suspect that selecting the appropriate level of abstraction could be on between both levels.</S>
		<S sid ="6" ssid = "6">We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet.</S>
		<S sid ="7" ssid = "7">We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation, allowing accuracy figures over 80%.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Word Sense Disambiguation (WSD) is an intermediate Natural Language Processing (NLP) task which consists in assigning the correct semantic interpretation to ambiguous words in context.</S>
			<S sid ="9" ssid = "9">One of the most successful approaches in the last years is the supervised learning from examples, in which statistical or Machine Learning classification models are induced from semantically annotated corpora (Ma`rquez et al., 2006).</S>
			<S sid ="10" ssid = "10">Generally, supervised systems have obtained better results than the unsupervised ones, as shown by experimental work and international evaluation exercises such ∗This paper has been supported by the European Union under the projects QALL-ME (FP6 IST033860) and KYOTO (FP7 ICT211423), and the Spanish Government under the project Text-Mess (TIN200615265-C0601) and KNOW (TIN200615049-C0301) as Senseval1.</S>
			<S sid ="11" ssid = "11">These annotated corpora are usually manually tagged by lexicographers with word senses taken from a particular lexical semantic resource –most commonly WordNet2 (WN) (Fell- baum, 1998).</S>
			<S sid ="12" ssid = "12">WN has been widely criticized for being a sense repository that often provides too fine–grained sense distinctions for higher level applications like Machine Translation or Question &amp; Answering.</S>
			<S sid ="13" ssid = "13">In fact, WSD at this level of granularity has resisted all attempts of inferring robust broad- coverage models.</S>
			<S sid ="14" ssid = "14">It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples.</S>
			<S sid ="15" ssid = "15">Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.</S>
			<S sid ="16" ssid = "16">Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004).</S>
			<S sid ="17" ssid = "17">Interestingly, this result is difficult to outperform by state-of-the-art sense-based WSD systems.</S>
			<S sid ="18" ssid = "18">Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Schu¨ tze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLaCalle, 2003), (Navigli, 2006) and (Snow et al., 2007).</S>
			<S sid ="19" ssid = "19">That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.</S>
			<S sid ="20" ssid = "20">Wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine–grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.</S>
			<S sid ="21" ssid = "21">In this way, Wikipedia provides a new very large source of annotated data, constantly expanded (Mihalcea, 2007).</S>
			<S sid ="22" ssid = "22">1 http://www.senseval.org 2 http://wordnet.princeton.edu 3 http://www.wikipedia.org Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389–397, Athens, Greece, 30 March – 3 April 2009.</S>
			<S sid ="23" ssid = "23">Qc 2009 Association for Computational Linguistics In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).</S>
			<S sid ="24" ssid = "24">That is, grouping senses of different words into the same explicit and comprehensive semantic class.</S>
			<S sid ="25" ssid = "25">Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions.</S>
			<S sid ="26" ssid = "26">However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavaglia`, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts (Vossen et al., 1998), Top Concept Ontology labels (Alvez et al., 2008) or Basic Level Concepts (Izquierdo et al., 2007).</S>
			<S sid ="27" ssid = "27">Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD.</S>
			<S sid ="28" ssid = "28">Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.</S>
			<S sid ="29" ssid = "29">Furthermore, to our knowledge, to date no comparative evaluation has been performed on SensEval data exploring different levels of abstraction.</S>
			<S sid ="30" ssid = "30">In fact, (Villarejo et al., 2005) studied the performance of class–based WSD comparing only SuperSenses and SUMO by 10–fold cross–validation on SemCor, but they did not provide results for SensEval2 nor SensEval3.</S>
			<S sid ="31" ssid = "31">This paper empirically explores on the supervised WSD task the performance of different levels of abstraction provided by WordNet Domains (Magnini and Cavaglia`, 2000), SUMO labels (Niles and Pease, 2001) and Basic Level Concepts (Izquierdo et al., 2007).</S>
			<S sid ="32" ssid = "32">We refer to this approach as class–based WSD since the classifiers are created at a class level instead of at a sense level.</S>
			<S sid ="33" ssid = "33">Class-based WSD clusters senses of different words into the same explicit and comprehensive grouping.</S>
			<S sid ="34" ssid = "34">Only those cases belonging to the same semantic class are grouped to train the classifier.</S>
			<S sid ="35" ssid = "35">For example, the coarser word grouping obtained in (Snow et al., 2007) only has one remaining sense for “church”.</S>
			<S sid ="36" ssid = "36">Using a set of Base Level Concepts (Izquierdo et al., 2007), the three senses of “church” are still represented by faith.n#3, building.n#1 and religious ceremony.n#1.</S>
			<S sid ="37" ssid = "37">The contribution of this work is threefold.</S>
			<S sid ="38" ssid = "38">We empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers.</S>
			<S sid ="39" ssid = "39">After this introduction, section 2 presents the sense-groupings used in this study.</S>
			<S sid ="40" ssid = "40">In section 3 the approach followed to build the class–based system is explained.</S>
			<S sid ="41" ssid = "41">Experiments and results are shown in section 4.</S>
			<S sid ="42" ssid = "42">Finally some conclusions are drawn in section 5.</S>
	</SECTION>
	<SECTION title="Semantic Classes. " number = "2">
			<S sid ="43" ssid = "1">WordNet (Fellbaum, 1998) synsets are organized in forty five Lexicographer Files, more recetly called SuperSenses, based on open syntactic categories (nouns, verbs, adjectives and adverbs) and logical groupings, such as person, phenomenon, feeling, location, etc. There are 26 basic categories for nouns, 15 for verbs, 3 for adjectives and 1 for adverbs.</S>
			<S sid ="44" ssid = "2">WordNet Domains4 (Magnini and Cavaglia`, 2000) is a hierarchy of 165 Domain Labels which have been used to label all WN synsets.</S>
			<S sid ="45" ssid = "3">Information brought by Domain Labels is complementary to what is already in WN.</S>
			<S sid ="46" ssid = "4">First of all a Domain Labels may include synsets of different syntactic categories: for instance MEDICINE groups together senses from nouns, such as doctor and hospital, and from Verbs such as to operate.</S>
			<S sid ="47" ssid = "5">Second, a Domain Label may also contain senses from different WordNet subhierarchies.</S>
			<S sid ="48" ssid = "6">For example, SPORT contains senses such as athlete, deriving from life form, game equipment, from physical object, sport from act, and playing field, from location.</S>
			<S sid ="49" ssid = "7">SUMO5 (Niles and Pease, 2001) was created as part of the IEEE Standard Upper Ontology Working Group.</S>
			<S sid ="50" ssid = "8">The goal of this Working Group is to develop a standard upper ontology to promote data interoperability, information search and retrieval, automated inference, and natural language processing.</S>
			<S sid ="51" ssid = "9">SUMO consists of a set of concepts, relations, and axioms that formalize an upper on- tology.</S>
			<S sid ="52" ssid = "10">For these experiments, we used the complete WN1.6 mapping with 1,019 SUMO labels.</S>
			<S sid ="53" ssid = "11">4 http://wndomains.itc.it/ 5 http://www.ontologyportal.org/ Basic Level Concepts6 (BLC) (Izquierdo et al., 2007) are small sets of meanings representing the whole nominal and verbal part of WN.</S>
			<S sid ="54" ssid = "12">BLC can be obtained by a very simple method that uses basic structural WN properties.</S>
			<S sid ="55" ssid = "13">In fact, the algorithm only considers the relative number of relations of each synset along the hypernymy chain.</S>
			<S sid ="56" ssid = "14">The process follows a bottom-up approach using the chain of hypernymy relations.</S>
			<S sid ="57" ssid = "15">For each synset in WN, the process selects as its BLC the first local maximum according to the relative number of relations.</S>
			<S sid ="58" ssid = "16">The local maximum is the synset in the hypernymy chain having more relations than its immediate hyponym and immediate hypernym.</S>
			<S sid ="59" ssid = "17">For synsets having multiple hypernyms, the path having the local maximum with higher number of relations is selected.</S>
			<S sid ="60" ssid = "18">Usually, this process finishes having a number of preliminary BLC.</S>
			<S sid ="61" ssid = "19">Obviously, while ascending through this chain, more synsets are subsumed by each concept.</S>
			<S sid ="62" ssid = "20">The process finishes checking if the number of concepts subsumed by the preliminary list of BLC is higher than a certain threshold.</S>
			<S sid ="63" ssid = "21">For those BLC not representing enough concepts according to the threshold, the process selects the next local maximum following the hypernymy hierarchy.</S>
			<S sid ="64" ssid = "22">Thus, depending on the type of relations considered to be counted and the threshold established, different sets of BLC can be easily obtained for each WN version.</S>
			<S sid ="65" ssid = "23">In this paper, we empirically explore the performance of the different levels of abstraction provided by Basic Level Concepts (BLC) (Izquierdo et al., 2007).</S>
			<S sid ="66" ssid = "24">Table 1 presents the total number of BLC and its average depth for WN1.6, varying the threshold and the type of relations considered (all relations or only hyponymy).</S>
			<S sid ="67" ssid = "25">Thr es.</S>
			<S sid ="68" ssid = "26">Rel.</S>
			<S sid ="69" ssid = "27">PoS #B LC Av.</S>
			<S sid ="70" ssid = "28">dep th.</S>
			<S sid ="71" ssid = "29">0 all Nou n Ver b 3,0 94 1,2 56 7 . 0 9 3 . 3 2 hyp o Nou n Ver b 2,4 90 1,0 41 7 . 0 9 3 . 3 1 2 0 all Nou n Ver b 5 5 8 6 7 3 5 . 8 1 1 . 2 5 hyp o Nou n Ver b 5 5 8 6 7 2 5 . 8 0 1 . 2 1 5 0 all Nou n Ver b 2 5 3 6 3 3 5 . 2 1 1 . 1 3 hyp o Nou n Ver b 2 4 8 6 3 3 5 . 2 1 1 . 1 0 Table 1: BLC for WN1.6 using all or hyponym relations 6 http://adimen.si.ehu.es/web/BLC C l a s s i f i e r Ex a m pl es # o f e x a m p l e s ch ur ch .n # 2 (s e ns e a p pr o a c h) ch urc h.n #2 5 8 buil ding , edifi ce (cla ss app roa ch) ch ur ch .n #2 bu ildi ng .n #1 ho tel .n #1 ho spi tal .n #1 ba rn.</S>
			<S sid ="72" ssid = "30">n# 1 . . .</S>
			<S sid ="73" ssid = "31">5 8 4 8 3 9 2 0 1 7 . . .</S>
			<S sid ="74" ssid = "32">TO TA L= 371 exa mp les Table 2: Examples and number of them in Semcor, for sense approach and for class approach</S>
	</SECTION>
	<SECTION title="Class-based. " number = "3">
			<S sid ="75" ssid = "1">WSD We followed a supervised machine learning approach to develop a set of class-based WSD tag- gers.</S>
			<S sid ="76" ssid = "2">Our systems use an implementation of a Support Vector Machine algorithm to train the classifiers (one per class) on semantic annotated corpora for acquiring positive and negative examples of each class and on the definition of a set of features for representing these examples.</S>
			<S sid ="77" ssid = "3">The system decides and selects among the possible semantic classes defined for a word.</S>
			<S sid ="78" ssid = "4">In the sense approach, one classifier is generated for each word sense, and the classifiers choose between the possible senses for the word.</S>
			<S sid ="79" ssid = "5">The examples to train a single classifier for a concrete word are all the examples of this word sense.</S>
			<S sid ="80" ssid = "6">In the semantic–class approach, one classifier is generated for each semantic class.</S>
			<S sid ="81" ssid = "7">So, when we want to label a word, our program obtains the set of possible semantic classes for this word, and then launch each of the semantic classifiers related with these semantic categories.</S>
			<S sid ="82" ssid = "8">The most likely category is selected for the word.</S>
			<S sid ="83" ssid = "9">In this approach, contrary to the word sense approach, to train a classifier we can use all examples of all words belonging to the class represented by the classifier.</S>
			<S sid ="84" ssid = "10">In table 2 an example for a sense of “church” is shown.</S>
			<S sid ="85" ssid = "11">We think that this approach has several advantages.</S>
			<S sid ="86" ssid = "12">First, semantic classes reduce the average polysemy degree of words (some word senses are grouped together within the same class).</S>
			<S sid ="87" ssid = "13">Moreover, the well known problem of acquisition bottleneck in supervised machine learning algorithms is attenuated, because the number of examples for each classifier is increased.</S>
			<S sid ="88" ssid = "14">3.1 The learning algorithm:.</S>
			<S sid ="89" ssid = "15">SVM Support Vector Machines (SVM) have been proven to be robust and very competitive in many NLP tasks, and in WSD in particular (Ma`rquez et al., 2006).</S>
			<S sid ="90" ssid = "16">For these experiments, we used SVM- Light (Joachims, 1998).</S>
			<S sid ="91" ssid = "17">SVM are used to learn an hyperplane that separates the positive from the negative examples with the maximum margin.</S>
			<S sid ="92" ssid = "18">It means that the hyperplane is located in an intermediate position between positive and negative examples, trying to keep the maximum distance to the closest positive example, and to the closest negative example.</S>
			<S sid ="93" ssid = "19">In some cases, it is not possible to get a hyperplane that divides the space linearly, or it is better to allow some errors to obtain a more efficient hyperplane.</S>
			<S sid ="94" ssid = "20">This is known as “soft- margin SVM”, and requires the estimation of a parameter (C), that represent the trade-off allowed between training errors and the margin.</S>
			<S sid ="95" ssid = "21">We have set this value to 0.01, which has been proved as a good value for SVM in WSD tasks.</S>
			<S sid ="96" ssid = "22">When classifying an example, we obtain the value of the output function for each SVM classifier corresponding to each semantic class for the word example.</S>
			<S sid ="97" ssid = "23">Our system simply selects the class with the greater value.</S>
			<S sid ="98" ssid = "24">3.2 Corpora.</S>
			<S sid ="99" ssid = "25">Three semantic annotated corpora have been used for training and testing.</S>
			<S sid ="100" ssid = "26">SemCor has been used for training while the corpora from the English all-words tasks of SensEval2 and SensEval3 has been used for testing.</S>
			<S sid ="101" ssid = "27">We also considered SemEval2007 coarse–grained task corpus for testing, but this dataset was discarded because this corpus is also annotated with clusters of word senses.</S>
			<S sid ="102" ssid = "28">SemCor (Miller et al., 1993) is a subset of the Brown Corpus plus the novel The Red Badge of Courage, and it has been developed by the same group that created WordNet.</S>
			<S sid ="103" ssid = "29">It contains 253 texts and around 700,000 running words, and more than 200,000 are also lemmatized and sense-tagged according to Princeton WordNet 1.6.</S>
			<S sid ="104" ssid = "30">SensEval27 English all-words corpus (here- inafter SE2) (Palmer et al., 2001) consists on 5,000 words of text from three WSJ articles representing different domains from the Penn TreeBank II.</S>
			<S sid ="105" ssid = "31">The sense inventory used for tagging is WordNet 1.7.</S>
			<S sid ="106" ssid = "32">Finally, SensEval38 English all-words cor-.</S>
			<S sid ="107" ssid = "33">pus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from two WSJ articles and one excerpt from the Brown Corpus.</S>
			<S sid ="108" ssid = "34">Sense repository of WordNet 1.7.1 was used to tag 2,041 words with their proper senses.</S>
			<S sid ="109" ssid = "35">7 http://www.sle.sharp.co.uk/senseval2 8 http://www.senseval.org/senseval3 3.3 Feature types.</S>
			<S sid ="110" ssid = "36">We have defined a set of features to represent the examples according to previous works in WSD and the nature of class-based WSD.</S>
			<S sid ="111" ssid = "37">Features widely used in the literature as in (Yarowsky, 1994) have been selected.</S>
			<S sid ="112" ssid = "38">These features are pieces of information that occur in the context of the target word, and can be organized as: Local features: bigrams and trigrams that contain the target word, including part-of-speech (PoS), lemmas or word-forms.</S>
			<S sid ="113" ssid = "39">Topical features: word–forms or lemmas appearing in windows around the target word.</S>
			<S sid ="114" ssid = "40">In particular, our systems use the following basic features: Word–forms and lemmas in a window of 10 words around the target word PoS: the concatenation of the preceding/following three/five PoS Bigrams and trigrams formed by lemmas and word-forms and obtained in a window of 5 words.</S>
			<S sid ="115" ssid = "41">We use of all tokens regardless their PoS to build bi/trigrams.</S>
			<S sid ="116" ssid = "42">The target word is replaced by X in these features to increase the generalization of them for the semantic classifiers Moreover, we also defined a set of Semantic Features to explode different semantic resources in order to enrich the set of basic features: Most frequent semantic class calculated over SemCor, the most frequent semantic class for the target word.</S>
			<S sid ="117" ssid = "43">Monosemous semantic classes semantic classes of the monosemous words arround the target word in a window of size 5.</S>
			<S sid ="118" ssid = "44">Several types of semantic classes have been considered to create these features.</S>
			<S sid ="119" ssid = "45">In particular, two different sets of BLC (BLC20 and BLC509), SuperSenses, WordNet Domains (WND) and SUMO.</S>
			<S sid ="120" ssid = "46">In order to increase the generalization capabilities of the classifiers we filter out irrelevant features.</S>
			<S sid ="121" ssid = "47">We measure the relevance of a feature10.</S>
			<S sid ="122" ssid = "48">f for a class c in terms of the frequency of f. For each class c, and for each feature f of that class, we calculate the frequency of the feature within the class (the number of times that it occurs in examples 9 We have selected these set since they represent different levels of abstraction.</S>
			<S sid ="123" ssid = "49">Remember that 20 and 50 refer to the threshold of minimum number of synsets that a possible BLC must subsume to be considered as a proper BLC.</S>
			<S sid ="124" ssid = "50">These BLC sets were built using all kind of relations.</S>
			<S sid ="125" ssid = "51">10 That is, the value of the feature, for example a feature type can be word-form, and a feature of that type can be “houses” of the class), and also obtain the total frequency of the feature, for all the classes.</S>
			<S sid ="126" ssid = "52">We divide both values (classFreq / totalFreq) and if the result is not greater than a certain threshold t, the feature is removed from the feature list of the class c11.</S>
			<S sid ="127" ssid = "53">In this way, we ensure that the features selected for a class are more frequently related with that class than with others.</S>
			<S sid ="128" ssid = "54">We set this threshold t to 0.25, obtained empirically with very preliminary versions of the classifiers on SensEval3 test.</S>
	</SECTION>
	<SECTION title="Experiments and Results. " number = "4">
			<S sid ="129" ssid = "1">To analyze the influence of each feature type in the class-based WSD, we designed a large set of experiments.</S>
			<S sid ="130" ssid = "2">An experiment is defined by two sets of semantic classes.</S>
			<S sid ="131" ssid = "3">First, the semantic class type for selecting the examples used to build the classifiers (determining the abstraction level of the system).</S>
			<S sid ="132" ssid = "4">In this case, we tested: sense12, BLC20, BLC50, WordNet Domains (WND), SUMO and Super- Sense (SS).</S>
			<S sid ="133" ssid = "5">Second, the semantic class type used for building the semantic features.</S>
			<S sid ="134" ssid = "6">In this case, we tested: BLC20, BLC50, SuperSense, WND and SUMO.</S>
			<S sid ="135" ssid = "7">Combining them, we generated the set of experiments described later.</S>
			<S sid ="136" ssid = "8">Table 3: Average polysemy on SE2 and SE3 Table 3 presents the average polysemy on SE2 and SE3 of the different semantic classes.</S>
			<S sid ="137" ssid = "9">4.1 Baselines.</S>
			<S sid ="138" ssid = "10">The most frequent classes (MFC) of each word calculated over SemCor are considered to be the baselines of our systems.</S>
			<S sid ="139" ssid = "11">Ties between classes on a specific word are solved obtaining the global frequency in SemCor of each of these tied classes, and selecting the more frequent class over the whole training corpus.</S>
			<S sid ="140" ssid = "12">When there are no occurrences of a word of the test corpus in SemCor (we are not able to calculate the most frequent class of the word), we obtain again the global frequency for each of its possible semantic classes (obtained 11 Depending on the experiment, around 30% of the original features are removed by this filter.</S>
			<S sid ="141" ssid = "13">12 We included this evaluation for comparison purposes.</S>
			<S sid ="142" ssid = "14">from WN) over SemCor, and we select the most frequent.</S>
			<S sid ="143" ssid = "15">4.2 Results.</S>
			<S sid ="144" ssid = "16">Tables 4 and 5 present the F1 measures (harmonic mean of recall and precision) for nouns and verbs respectively when training our systems on SemCor and testing on SE2 and SE3.</S>
			<S sid ="145" ssid = "17">Those results showing a statistically significant13 positive difference when compared with the baseline are in marked bold.</S>
			<S sid ="146" ssid = "18">Column labeled as “Class” refers to the target set of semantic classes for the classifiers, that is, the desired semantic level for each example.</S>
			<S sid ="147" ssid = "19">Column labeled as “Sem.</S>
			<S sid ="148" ssid = "20">Feat.” indicates the class of the semantic features used to train the classifiers.</S>
			<S sid ="149" ssid = "21">For example, class BLC20 combined with Semantic Feature BLC20 means that this set of classes were used both to label the test examples and to define the semantic features.</S>
			<S sid ="150" ssid = "22">In order to compare their contribution we also performed a “basicFeat” test without including semantic features.</S>
			<S sid ="151" ssid = "23">As expected according to most literature in WSD, the performances of the MFC baselines are very high.</S>
			<S sid ="152" ssid = "24">In particular, those corresponding to nouns (ranging from 70% to 80%).</S>
			<S sid ="153" ssid = "25">While nominal baselines seem to perform similarly in both SE2 and SE3, verbal baselines appear to be consistently much lower for SE2 than for SE3.</S>
			<S sid ="154" ssid = "26">In SE2, verbal baselines range from 44% to 68% while in SE3 verbal baselines range from 52% to 79%.</S>
			<S sid ="155" ssid = "27">An exception is the results for verbs considering WND: the results are very high due to the low polysemy for verbs according to WND.</S>
			<S sid ="156" ssid = "28">As expected, when increasing the level of abstraction (from senses to SuperSenses) the results also increase.</S>
			<S sid ="157" ssid = "29">Finally, it also seems that SE2 task is more difficult than SE3 since the MFC baselines are lower.</S>
			<S sid ="158" ssid = "30">As expected, the results of the systems increase while augmenting the level of abstraction (from senses to SuperSenses), and almost in every case, the baseline results are reached or outperformed.</S>
			<S sid ="159" ssid = "31">This is very relevant since the baseline results are very high.</S>
			<S sid ="160" ssid = "32">Regarding nouns, a very different behaviour is observed for SE2 and SE3.</S>
			<S sid ="161" ssid = "33">While for SE3 none of the system presents a significant improvement over the baselines, for SE2 a significant improvement is obtained by using several types of seman since the current system have been designed for class-based evaluation only.</S>
			<S sid ="162" ssid = "34">13 Using the.</S>
			<S sid ="163" ssid = "35">McNemar’s test.</S>
			<S sid ="164" ssid = "36">tic features.</S>
			<S sid ="165" ssid = "37">In particular, when using WordNet Domains but also BLC20.</S>
			<S sid ="166" ssid = "38">In general, BLC20 semantic features seem to be better than BLC50 and SuperSenses.</S>
			<S sid ="167" ssid = "39">Regarding verbs, the system obtains significant improvements over the baselines using different types of semantic features both in SE2 and SE3.</S>
			<S sid ="168" ssid = "40">In particular, when using again WordNet Domains as semantic features.</S>
			<S sid ="169" ssid = "41">In general, the results obtained by BLC20 are not so much different to the results of BLC50 (in a few cases, this difference is greater than 2 points).</S>
			<S sid ="170" ssid = "42">For instance, for nouns, if we consider the number of classes within BLC20 (558 classes), BLC50 (253 classes) and SuperSense (24 classes), BLC classifiers obtain high performance rates while maintaining much higher expressive power than SuperSenses.</S>
			<S sid ="171" ssid = "43">In fact, using Super- Senses (40 classes for nouns and verbs) we can obtain a very accurate semantic tagger with performances close to 80%.</S>
			<S sid ="172" ssid = "44">Even better, we can use BLC20 for tagging nouns (558 semantic classes and F1 over 75%) and SuperSenses for verbs (14 semantic classes and F1 around 75%).</S>
			<S sid ="173" ssid = "45">Obviously, the classifiers using WordNet Domains as target grouping obtain very high performances due to its reduced average polysemy.</S>
			<S sid ="174" ssid = "46">However, when used as semantic features it seems to improve the results in most of the cases.</S>
			<S sid ="175" ssid = "47">In addition, we obtain very competitive classifiers at a sense level.</S>
			<S sid ="176" ssid = "48">4.3 Learning curves.</S>
			<S sid ="177" ssid = "49">We also performed a set of experiments for measuring the behaviour of the class-based WSD system when gradually increasing the number of training examples.</S>
			<S sid ="178" ssid = "50">These experiments have been carried for nouns and verbs, but only noun results are shown since in both cases, the trend is very similar but more clear for nouns.</S>
			<S sid ="179" ssid = "51">The training corpus has been divided in portions of 5% of the total number of files.</S>
			<S sid ="180" ssid = "52">That is, complete files are added to the training corpus of each incremental test.</S>
			<S sid ="181" ssid = "53">The files were randomly selected to generate portions of 5%, 10%, 15%, etc. of the SemCor corpus14.</S>
			<S sid ="182" ssid = "54">Then, we train the system on each of the training portions and we test the system on SE2 and SE3.</S>
			<S sid ="183" ssid = "55">Finally, we also compare the 14 Each portion contains also the same files than the previous portion.</S>
			<S sid ="184" ssid = "56">For example, all files in the 25% portion are also contained in the 30% portion.</S>
			<S sid ="185" ssid = "57">Cl as s Sem . Fea t. SensE val2 SensE val3 Pol y A l l Pol y A l l Sen se b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 59.</S>
			<S sid ="186" ssid = "58">66 61.</S>
			<S sid ="187" ssid = "59">13 61.</S>
			<S sid ="188" ssid = "60">93 61.</S>
			<S sid ="189" ssid = "61">79 61.</S>
			<S sid ="190" ssid = "62">00 61.</S>
			<S sid ="191" ssid = "63">13 61.</S>
			<S sid ="192" ssid = "64">66 70.</S>
			<S sid ="193" ssid = "65">02 71.</S>
			<S sid ="194" ssid = "66">20 71.</S>
			<S sid ="195" ssid = "67">79 71.</S>
			<S sid ="196" ssid = "68">69 71.</S>
			<S sid ="197" ssid = "69">10 71.</S>
			<S sid ="198" ssid = "70">20 71.</S>
			<S sid ="199" ssid = "71">59 64.</S>
			<S sid ="200" ssid = "72">45 65.</S>
			<S sid ="201" ssid = "73">45 65.</S>
			<S sid ="202" ssid = "74">45 65.</S>
			<S sid ="203" ssid = "75">30 64.</S>
			<S sid ="204" ssid = "76">86 65.</S>
			<S sid ="205" ssid = "77">45 65.</S>
			<S sid ="206" ssid = "78">45 72.</S>
			<S sid ="207" ssid = "79">30 73.</S>
			<S sid ="208" ssid = "80">15 73.</S>
			<S sid ="209" ssid = "81">15 73.</S>
			<S sid ="210" ssid = "82">04 72.</S>
			<S sid ="211" ssid = "83">70 73.</S>
			<S sid ="212" ssid = "84">15 73.</S>
			<S sid ="213" ssid = "85">15 BL C20 b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 65.</S>
			<S sid ="214" ssid = "86">92 65.</S>
			<S sid ="215" ssid = "87">65 68.</S>
			<S sid ="216" ssid = "88">70 68.</S>
			<S sid ="217" ssid = "89">83 65.</S>
			<S sid ="218" ssid = "90">12 68.</S>
			<S sid ="219" ssid = "91">97 68.</S>
			<S sid ="220" ssid = "92">57 75.</S>
			<S sid ="221" ssid = "93">71 75.</S>
			<S sid ="222" ssid = "94">52 77.</S>
			<S sid ="223" ssid = "95">69 77.</S>
			<S sid ="224" ssid = "96">79 75.</S>
			<S sid ="225" ssid = "97">14 77.</S>
			<S sid ="226" ssid = "98">88 77.</S>
			<S sid ="227" ssid = "99">60 67.</S>
			<S sid ="228" ssid = "100">98 64.</S>
			<S sid ="229" ssid = "101">64 68.</S>
			<S sid ="230" ssid = "102">29 67.</S>
			<S sid ="231" ssid = "103">22 64.</S>
			<S sid ="232" ssid = "104">64 65.</S>
			<S sid ="233" ssid = "105">25 64.</S>
			<S sid ="234" ssid = "106">49 76.</S>
			<S sid ="235" ssid = "107">29 73.</S>
			<S sid ="236" ssid = "108">82 76.</S>
			<S sid ="237" ssid = "109">52 75.</S>
			<S sid ="238" ssid = "110">73 73.</S>
			<S sid ="239" ssid = "111">82 74.</S>
			<S sid ="240" ssid = "112">24 73.</S>
			<S sid ="241" ssid = "113">71 BL C50 b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 67.</S>
			<S sid ="242" ssid = "114">20 64.</S>
			<S sid ="243" ssid = "115">28 69.</S>
			<S sid ="244" ssid = "116">72 67.</S>
			<S sid ="245" ssid = "117">20 65.</S>
			<S sid ="246" ssid = "118">60 70.</S>
			<S sid ="247" ssid = "119">39 71.</S>
			<S sid ="248" ssid = "120">31 76.</S>
			<S sid ="249" ssid = "121">65 74.</S>
			<S sid ="250" ssid = "122">57 78.</S>
			<S sid ="251" ssid = "123">45 76.</S>
			<S sid ="252" ssid = "124">65 75.</S>
			<S sid ="253" ssid = "125">52 78.</S>
			<S sid ="254" ssid = "126">92 79.</S>
			<S sid ="255" ssid = "127">58 68.</S>
			<S sid ="256" ssid = "128">01 66.</S>
			<S sid ="257" ssid = "129">77 68.</S>
			<S sid ="258" ssid = "130">16 68.</S>
			<S sid ="259" ssid = "131">01 65.</S>
			<S sid ="260" ssid = "132">07 65.</S>
			<S sid ="261" ssid = "133">38 66.</S>
			<S sid ="262" ssid = "134">31 76.</S>
			<S sid ="263" ssid = "135">74 75.</S>
			<S sid ="264" ssid = "136">84 76.</S>
			<S sid ="265" ssid = "137">85 76.</S>
			<S sid ="266" ssid = "138">74 74.</S>
			<S sid ="267" ssid = "139">61 74.</S>
			<S sid ="268" ssid = "140">83 75.</S>
			<S sid ="269" ssid = "141">51 W ND b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 78.</S>
			<S sid ="270" ssid = "142">97 70.</S>
			<S sid ="271" ssid = "143">96 72.</S>
			<S sid ="272" ssid = "144">53 73.</S>
			<S sid ="273" ssid = "145">25 74.</S>
			<S sid ="274" ssid = "146">39 78.</S>
			<S sid ="275" ssid = "147">83 75.</S>
			<S sid ="276" ssid = "148">11 86.</S>
			<S sid ="277" ssid = "149">11 80.</S>
			<S sid ="278" ssid = "150">81 81.</S>
			<S sid ="279" ssid = "151">85 82.</S>
			<S sid ="280" ssid = "152">33 83.</S>
			<S sid ="281" ssid = "153">08 86.</S>
			<S sid ="282" ssid = "154">01 83.</S>
			<S sid ="283" ssid = "155">55 76.</S>
			<S sid ="284" ssid = "156">74 67.</S>
			<S sid ="285" ssid = "157">85 72.</S>
			<S sid ="286" ssid = "158">37 71.</S>
			<S sid ="287" ssid = "159">41 68.</S>
			<S sid ="288" ssid = "160">82 76.</S>
			<S sid ="289" ssid = "161">58 73.</S>
			<S sid ="290" ssid = "162">02 83.</S>
			<S sid ="291" ssid = "163">8 77.</S>
			<S sid ="292" ssid = "164">64 80.</S>
			<S sid ="293" ssid = "165">79 80.</S>
			<S sid ="294" ssid = "166">11 78.</S>
			<S sid ="295" ssid = "167">31 83.</S>
			<S sid ="296" ssid = "168">71 81.</S>
			<S sid ="297" ssid = "169">24 SU MO b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 66.</S>
			<S sid ="298" ssid = "170">40 68.</S>
			<S sid ="299" ssid = "171">53 65.</S>
			<S sid ="300" ssid = "172">60 65.</S>
			<S sid ="301" ssid = "173">60 68.</S>
			<S sid ="302" ssid = "174">39 68.</S>
			<S sid ="303" ssid = "175">92 68.</S>
			<S sid ="304" ssid = "176">92 76.</S>
			<S sid ="305" ssid = "177">09 77.</S>
			<S sid ="306" ssid = "178">60 75.</S>
			<S sid ="307" ssid = "179">52 75.</S>
			<S sid ="308" ssid = "180">52 77.</S>
			<S sid ="309" ssid = "181">50 77.</S>
			<S sid ="310" ssid = "182">88 77.</S>
			<S sid ="311" ssid = "183">88 71.</S>
			<S sid ="312" ssid = "184">96 68.</S>
			<S sid ="313" ssid = "185">10 68.</S>
			<S sid ="314" ssid = "186">10 68.</S>
			<S sid ="315" ssid = "187">72 68.</S>
			<S sid ="316" ssid = "188">41 69.</S>
			<S sid ="317" ssid = "189">03 70.</S>
			<S sid ="318" ssid = "190">88 79.</S>
			<S sid ="319" ssid = "191">55 76.</S>
			<S sid ="320" ssid = "192">74 76.</S>
			<S sid ="321" ssid = "193">74 77.</S>
			<S sid ="322" ssid = "194">19 76.</S>
			<S sid ="323" ssid = "195">97 77.</S>
			<S sid ="324" ssid = "196">42 78.</S>
			<S sid ="325" ssid = "197">76 S S b a s e li n e b a s i c F e a t B L C 2 0 B L C 5 0 S S W N D S U M O 70.</S>
			<S sid ="326" ssid = "198">48 69.</S>
			<S sid ="327" ssid = "199">77 71.</S>
			<S sid ="328" ssid = "200">47 70.</S>
			<S sid ="329" ssid = "201">20 70.</S>
			<S sid ="330" ssid = "202">34 73.</S>
			<S sid ="331" ssid = "203">59 70.</S>
			<S sid ="332" ssid = "204">62 80.</S>
			<S sid ="333" ssid = "205">41 79.</S>
			<S sid ="334" ssid = "206">94 81.</S>
			<S sid ="335" ssid = "207">07 80.</S>
			<S sid ="336" ssid = "208">22 80.</S>
			<S sid ="337" ssid = "209">32 82.</S>
			<S sid ="338" ssid = "210">47 80.</S>
			<S sid ="339" ssid = "211">51 72.</S>
			<S sid ="340" ssid = "212">59 69.</S>
			<S sid ="341" ssid = "213">60 72.</S>
			<S sid ="342" ssid = "214">43 72.</S>
			<S sid ="343" ssid = "215">92 65.</S>
			<S sid ="344" ssid = "216">12 70.</S>
			<S sid ="345" ssid = "217">10 71.</S>
			<S sid ="346" ssid = "218">93 81.</S>
			<S sid ="347" ssid = "219">50 79.</S>
			<S sid ="348" ssid = "220">48 81.</S>
			<S sid ="349" ssid = "221">39 81.</S>
			<S sid ="350" ssid = "222">73 76.</S>
			<S sid ="351" ssid = "223">46 79.</S>
			<S sid ="352" ssid = "224">82 81.</S>
			<S sid ="353" ssid = "225">05 Table 4: Results for nouns resulting system with the baseline computed over the same training portion.</S>
			<S sid ="354" ssid = "226">Figures 1 and 2 present the learning curves over SE2 and SE3, respectively, of a class-based WSD system based on BLC20 using the basic features and the semantic features built with WordNet Domains.</S>
			<S sid ="355" ssid = "227">Surprisingly, in SE2 the system only improves the F1 measure around 2% while increasing the training corpus from 25% to 100% of SemCor.</S>
			<S sid ="356" ssid = "228">In SE3, the system again only improves the F1 measure around 3% while increasing the training corpus from 30% to 100% of SemCor.</S>
			<S sid ="357" ssid = "229">That is, most of the knowledge required for the class-based WSD system seems to be already present on a small part of SemCor.</S>
			<S sid ="358" ssid = "230">Figures 3 and 4 present the learning curves over SE2 and SE3, respectively, of a class-based WSD system based on SuperSenses using the basic features and the semantic features built with WordNet Domains.</S>
			<S sid ="359" ssid = "231">Again, in SE2 the system only improves the F1 80 System SV2 MFC SV2 78 76 74 72 F1 70 68 66 64 62 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 % corpus Figure 1: Learning curve of BLC20 on SE2 78 System SV3 MFC SV3 76 74 72 F1 70 68 66 64 Table 5: Results for verbs measure around 2% while increasing the training corpus from 25% to 100% of SemCor.</S>
			<S sid ="360" ssid = "232">In SE3, the system again only improves the F1 measure around 2% while increasing the training corpus from 30% to 100% of SemCor.</S>
			<S sid ="361" ssid = "233">That is, with only 25% of the whole corpus, the class-based WSD system reaches a F1 close to the performance using all corpus.</S>
			<S sid ="362" ssid = "234">This evaluation seems to indicate that the class-based approach to WSD reduces dramatically the required amount of training examples.</S>
			<S sid ="363" ssid = "235">In both cases, when using BLC20 or Super- Senses as semantic classes for tagging, the be- haviour of the system is similar to MFC baseline.</S>
			<S sid ="364" ssid = "236">This is very interesting since the MFC obtains high results due to the way it is defined, since the MFC over the total corpus is assigned if there are no occurrences of the word in the training corpus.</S>
			<S sid ="365" ssid = "237">Without this definition, there would be a large number of words in the test set with no occurrences when using small training portions.</S>
			<S sid ="366" ssid = "238">In these cases, the recall of the baselines (and in turn F1) would be 62 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 % corp us Figure 2: Learning curve of BLC20 on SE3 much lower.</S>
	</SECTION>
	<SECTION title="Conclusions and. " number = "5">
			<S sid ="367" ssid = "1">discussion We explored on the WSD task the performance of different levels of abstraction and sense groupings.</S>
			<S sid ="368" ssid = "2">We empirically demonstrated that Base Level Concepts are able to group word senses into an adequate medium level of abstraction to perform supervised class–based disambiguation.</S>
			<S sid ="369" ssid = "3">We also demonstrated that the semantic classes provide a rich information about polysemous words and can be successfully used as semantic features.</S>
			<S sid ="370" ssid = "4">Finally we confirm the fact that the class– based approach reduces dramatically the required amount of training examples, opening the way to solve the well known acquisition bottleneck problem for supervised machine learning algorithms.</S>
			<S sid ="371" ssid = "5">In general, the results obtained by BLC20 are not very different to the results of BLC50.</S>
			<S sid ="372" ssid = "6">Thus, we can select a medium level of abstraction, without having a significant decrease of the perfor 84 System SV2 features, we can generate robust and competitive 82 80 78 F1 76 74 72 70 68 MFC SV2 c l a s s b a s e d c l a s s i f i e r s . T o our kno wle dge, the best resul ts for clas s– bas ed WS D are thos e repo rted by (Cia rami ta and Altu n, 200 6).</S>
			<S sid ="373" ssid = "7">Thi s syst em perf orm s a sequ ence tagg ing usin g a perc eptr on– train ed HM M, usin g Sup erSe nses , train ing on Sem Cor and testi ng on Sen sEv al3.</S>
			<S sid ="374" ssid = "8">The syst em achi eves an F1– scor e of 70.5 4, obta inin g a signi fica nt impr ove men t fro m a base line syst em whi ch scor es only 64.</S>
			<S sid ="375" ssid = "9">09.</S>
			<S sid ="376" ssid = "10">In this case , the first sens e bas eline is the Sup erSe nse 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 of the most frequent synset for a word, according % corpus Figure 3: Learning curve of SuperSense on SE2 82 System SV3 to the WN sense ranking.</S>
			<S sid ="377" ssid = "11">Although this result is achieved for the all words SensEval3 task, including adjectives, we can compare both results since in SE2 and SE3 adjectives obtain very high per 80 78 F1 76 74 72 70 MFC SV3 form anc e figu res.</S>
			<S sid ="378" ssid = "12">Usin g Sup erSe nses , adje ctive s only have thre e clas ses (W N Lexi cogr aphi c File s 00, 01 and 44) and mor e than 80% of the m belo ng to clas s 00.</S>
			<S sid ="379" ssid = "13">This yiel ds to reall y very high perf or- man ces for adje ctive s whic h usu ally are over 90 %.</S>
			<S sid ="380" ssid = "14">A s we have see n, sup ervis ed WS D syst ems are ver y dep end ent of the corp ora use d to trai n and test the syst em.</S>
			<S sid ="381" ssid = "15">We plan to exte nd our syst em by sele ctin g new corp ora to train or test.</S>
			<S sid ="382" ssid = "16">For inst anc e, by usin g the sens e ann otat ed glos ses fro m Word 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Net.</S>
			<S sid ="383" ssid = "17">% corpus Figure 4: Learning curve of SuperSense on SE3 mance.</S>
			<S sid ="384" ssid = "18">Considering the number of classes, BLC classifiers obtain high performance rates while maintaining much higher expressive power than SuperSenses.</S>
			<S sid ="385" ssid = "19">However, using SuperSenses (46 classes) we can obtain a very accurate semantic tagger with performances around 80%.</S>
			<S sid ="386" ssid = "20">Even better, we can use BLC20 for tagging nouns (558 semantic classes and F1 over 75%) and SuperSenses for verbs (14 semantic classes and F1 around 75%).</S>
			<S sid ="387" ssid = "21">As BLC are defined by a simple and fully automatic method, they can provide a user–defined level of abstraction that can be more suitable for certain NLP tasks.</S>
			<S sid ="388" ssid = "22">Moreover, the traditional set of features used for sense-based classifiers do not seem to be the most adequate or representative for the class-based approach.</S>
			<S sid ="389" ssid = "23">We have enriched the usual set of features, by adding semantic information from the monosemous words of the context and the MFC of the target word.</S>
			<S sid ="390" ssid = "24">With this new enriched set of</S>
	</SECTION>
</PAPER>
