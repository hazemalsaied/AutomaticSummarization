<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This article presents a novel bootstrapping approach for improving the quality of feature vector weighting in distributional word similarity.</S>
		<S sid ="2" ssid = "2">The method was motivated by attempts to utilize distributional similarity for identifying the concrete semantic relationship of lexical entailment.</S>
		<S sid ="3" ssid = "3">Our analysis revealed that a major reason for the rather loose semantic similarity obtained by distributional similarity methods is insufﬁcient quality of the word feature vectors, caused by deﬁcient feature weighting.</S>
		<S sid ="4" ssid = "4">This observation led to the deﬁnition of a bootstrapping scheme which yields improved feature weights, and hence higher quality feature vectors.</S>
		<S sid ="5" ssid = "5">The underlying idea of our approach is that features which are common to similar words are also most characteristic for their meanings, and thus should be promoted.</S>
		<S sid ="6" ssid = "6">This idea is realized via a bootstrapping step applied to an initial standard approximation of the similarity space.</S>
		<S sid ="7" ssid = "7">The superior performance of the bootstrapping method was assessed in two different experiments, one based on direct human gold-standard annotation and the other based on an automatically created disambiguation dataset.</S>
		<S sid ="8" ssid = "8">These results are further supported by applying a novel quantitative measurement of the quality of feature weighting functions.</S>
		<S sid ="9" ssid = "9">Improved feature weighting also allows massive feature reduction, which indicates that the most characteristic features for a word are indeed concentrated at the top ranks of its vector.</S>
		<S sid ="10" ssid = "10">Finally, experiments with three prominent similarity measures and two feature weighting functions showed that the bootstrapping scheme is robust and is independent of the original functions over which it is applied.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="11" ssid = "11">1.1 Motivation.</S>
			<S sid ="12" ssid = "12">Distributional word similarity has long been an active research area (Hindle 1990; Ruge 1992; Grefenstette 1994; Lee 1997; Lin 1998; Dagan, Lee, and Pereira 1999; Weeds and ∗ Department of Information Science, Bar-Ilan University, RamatGan, Israel.</S>
			<S sid ="13" ssid = "13">Email: zhitomim@mail.biu.ac.il.</S>
			<S sid ="14" ssid = "14">∗∗ Department of Computer Science, Bar-Ilan University, RamatGan, Israel.</S>
			<S sid ="15" ssid = "15">Email: dagan@cs.biu.ac.il.</S>
			<S sid ="16" ssid = "16">Submission received: 6 December 2006; revised submission received: 9 July 2008; accepted for publication: 21November 2008.</S>
			<S sid ="17" ssid = "17">© 2009 Association for Computational Linguistics Weir 2005).</S>
			<S sid ="18" ssid = "18">This paradigm is inspired by Harris’s distributional hypothesis (Harris 1968), which states that semantically similar words tend to appear in similar contexts.</S>
			<S sid ="19" ssid = "19">In a computational realization, each word is characterized by a weighted feature vector, where features typically correspond to other words that co-occur with the characterized word in the context.</S>
			<S sid ="20" ssid = "20">Distributional similarity measures quantify the degree of similarity between a pair of such feature vectors.</S>
			<S sid ="21" ssid = "21">It is then assumed that two words that occur within similar contexts, as measured by similarity of their context vectors, are indeed semantically similar.</S>
			<S sid ="22" ssid = "22">The distributional word similarity measures were often applied for two types of inferences.</S>
			<S sid ="23" ssid = "23">The ﬁrst type is making similarity-based generalizations for smoothing word co-occurrence probabilities, in applications such as language modeling and disambiguation.</S>
			<S sid ="24" ssid = "24">For example, assume that we need to estimate the likelihood of the verb– object co-occurrence pair visit–country, although it did not appear in our sample corpus.</S>
			<S sid ="25" ssid = "25">Co-occurrences of the verb visit with words that are distributionally similar to country, such as state, city, and region, however, do appear in the corpus.</S>
			<S sid ="26" ssid = "26">Consequently, we may infer that visit–country is also a plausible expression, using some mathematical scheme of similarity-based generalization (Essen and Steinbiss 1992; Dagan, Marcus, and Markovitch 1995; Karov and Edelman 1996; Ng and Lee 1996; Ng 1997; Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005).</S>
			<S sid ="27" ssid = "27">The rationale behind this inference is that if two words are distributionally similar then the occurrence of one word in some contexts indicates that the other word is also likely to occur in such contexts.</S>
			<S sid ="28" ssid = "28">A second type of semantic inference, which primarily motivated our own research, is meaning-preserving lexical substitution.</S>
			<S sid ="29" ssid = "29">Many NLP applications, such as question answering, information retrieval, information extraction, and (multi-document) sum- marization, need to recognize that one word can be substituted by another one in a given context while preserving, or entailing the original meaning.</S>
			<S sid ="30" ssid = "30">Naturally, recognizing such substitutable lexical entailments is a prominent component within the textual entailment recognition paradigm, which models semantic inference as an application- independent task (Dagan, Glickman, and Magnini 2006).</S>
			<S sid ="31" ssid = "31">Accordingly, several textual entailment systems did utilize the output of distributional similarity measures to model entailing lexical substitutions (Jijkoun and de Rijke 2005; Adams 2006; Ferrandez et al. 2006; Nicholson, Stokes, and Baldwin 2006; Vanderwende, Menezes, and Snow 2006).</S>
			<S sid ="32" ssid = "32">In some of these papers the distributional information typically complements manual lexical resources in textual entailment systems, most notably WordNet (Fellbaum 1998).</S>
			<S sid ="33" ssid = "33">Lexical substitution typically requires that the meaning of one word entails the meaning of the other.</S>
			<S sid ="34" ssid = "34">For instance, in question answering, the word company in a question can be substituted in an answer text by ﬁrm, automaker, or subsidiary, whose meanings entail the meaning of company.</S>
			<S sid ="35" ssid = "35">However, as it turns out, traditional distributional similarity measures do not capture well such lexical substitution relationships, but rather capture a somewhat broader (and looser) notion of semantic similarity.</S>
			<S sid ="36" ssid = "36">For example, quite distant cohyponyms such as party and company also come out as distributionally similar to country, due to a partial overlap of their semantic properties.</S>
			<S sid ="37" ssid = "37">Clearly, the meanings of these words do not entail each other.</S>
			<S sid ="38" ssid = "38">Motivated by these observations, our long-term goal is to investigate whether the distributional similarity scheme may be improved to yield tighter semantic similarities, and eventually better approximation of lexical entailments.</S>
			<S sid ="39" ssid = "39">This article presents one component of this research plan, which focuses on improving the underlying semantic 436 quality of distributional word feature vectors.</S>
			<S sid ="40" ssid = "40">The article describes the methodology, deﬁnitions, and analysis of our investigation and the resulting bootstrapping scheme for feature weighting which yielded improved empirical performance.</S>
			<S sid ="41" ssid = "41">1.2 Main Contributions and Outline.</S>
			<S sid ="42" ssid = "42">As a starting point for our investigation, an operational deﬁnition was needed for evaluating the correctness of candidate pairs of similar words.</S>
			<S sid ="43" ssid = "43">Following the lexical substitution motivation, in Section 3 we formulate the substitutable lexical entailment relation (or lexical entailment, for brevity), reﬁning earlier deﬁnitions in Geffet and Dagan (2004, 2005).</S>
			<S sid ="44" ssid = "44">Generally speaking, this relation holds for a pair of words if a possible meaning of one word entails a meaning of the other, and the entailing word can substitute the entailed one in some typical contexts.</S>
			<S sid ="45" ssid = "45">Lexical entailment overlaps partly with traditional lexical semantic relationships, while capturing more generally the lexical substitution needs of applications.</S>
			<S sid ="46" ssid = "46">Empirically, high inter-annotator agreement was obtained when judging the output of distributional similarity measures for lexical entailment.</S>
			<S sid ="47" ssid = "47">Next, we analyzed the typical behavior of existing word similarity measures relative to the lexical entailment criterion.</S>
			<S sid ="48" ssid = "48">Choosing the commonly used measure of Lin (1998) as a representative case, the analysis shows that quite noisy feature vectors are a major cause for generating rather “loose” semantic similarities.</S>
			<S sid ="49" ssid = "49">On the other hand, one may expect that features which seem to be most characteristic for a word’s meaning should receive the highest feature weights.</S>
			<S sid ="50" ssid = "50">This does not seem to be the case, however, for common feature weighting functions, such as Point-wise Mutual Information (Church and Patrick 1990; Hindle 1990).</S>
			<S sid ="51" ssid = "51">Following these observations, we developed a bootstrapping formula that improves the original feature weights (Section 4), leading to better feature vectors and better similarity predictions.</S>
			<S sid ="52" ssid = "52">The general idea is to promote the weights of features that are common for semantically similar words, since these features are likely to be most characteristic for the word’s meaning.</S>
			<S sid ="53" ssid = "53">This idea is implemented by a bootstrapping scheme, where the initial (and cruder) similarity measure provides an initial approximation for semantic word similarity.</S>
			<S sid ="54" ssid = "54">The bootstrapping method yields a high concentration of semantically characteristic features among the top-ranked features of the vector, which also allows aggressive feature reduction.</S>
			<S sid ="55" ssid = "55">The bootstrapping scheme was evaluated in two experimental settings, which correspond to the two types of applications for distributional similarity.</S>
			<S sid ="56" ssid = "56">First, it achieved signiﬁcant improvements in predicting lexical entailment as assessed by human judgments, when applied over several base similarity measures (Section 5).</S>
			<S sid ="57" ssid = "57">Additional analysis relative to the lexical entailment dataset revealed cleaner and more characteristic feature vectors for the bootstrapping method.</S>
			<S sid ="58" ssid = "58">To obtain a quantitative analysis of this behavior, we deﬁned a measure called average common-feature rank ratio.</S>
			<S sid ="59" ssid = "59">This measure captures the idea that a prominent feature for a word is expected to be prominent also for semantically similar words, while being less prominent for unrelated words.</S>
			<S sid ="60" ssid = "60">To the best of our knowledge this is the ﬁrst proposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure.</S>
			<S sid ="61" ssid = "61">As a second evaluation, we applied the bootstrapping scheme for similarity-based prediction of co-occurrence likelihood within a typical pseudo-word sense disambiguation experiment, obtaining substantial error reductions (Section 7).</S>
			<S sid ="62" ssid = "62">Section 8 concludes 437 this article, suggesting the relevance of our analysis and bootstrapping scheme for the general use of distributional feature vectors.1</S>
	</SECTION>
	<SECTION title="Background: Distributional Similarity Models. " number = "2">
			<S sid ="63" ssid = "1">This section reviews the components of the distributional similarity approach and speciﬁes the measures and functions that were utilized by our work.</S>
			<S sid ="64" ssid = "2">The Distributional Hypothesis assumes that semantically similar words appear in similar contexts, suggesting that semantic similarity can be detected by comparing contexts of words.</S>
			<S sid ="65" ssid = "3">This is the underlying principle of the vector-based distributional similarity model, which comprises two phases.</S>
			<S sid ="66" ssid = "4">First, context features for each word are constructed and assigned weights; then, the weighted feature vectors of pairs of words are compared by a vector similarity measure.</S>
			<S sid ="67" ssid = "5">The following two subsections review typical methods for each phase.</S>
			<S sid ="68" ssid = "6">2.1 Features and Weighting Functions.</S>
			<S sid ="69" ssid = "7">In the typical computational setting, word contexts are represented by feature vectors.</S>
			<S sid ="70" ssid = "8">A feature represents another word (or term) w· with which w co-occurs, and possibly speciﬁes also the syntactic relationship between the two words, as in Grefenstette (1994), Lin (1998), and Weeds and Weir (2005).</S>
			<S sid ="71" ssid = "9">Thus, a word (or term) w is represented by a feature vector, where each entry in the vector corresponds to a feature f . Pado and Lapata (2007) demonstrate that using syntactic dependency-based features helps to distinguish among classes of lexical relations, which seems to be more difﬁcult when using “bag of words” features that are based on co-occurrence in a text window.</S>
			<S sid ="72" ssid = "10">A syntactic-based feature f for a word w is deﬁned as a triple: ( fw, syn rel, f role∗ where fw is a context word (or term) that co-occurs with w under the syntactic dependency relation syn rel.</S>
			<S sid ="73" ssid = "11">The feature role ( f role) corresponds to the role of the feature word fw in the syntactic dependency, being either the head (denoted h) or the modiﬁer (de noted m) of the relation.</S>
			<S sid ="74" ssid = "12">For example, given the word company, the feature (earnings, gen, h∗ corresponds to the genitive relationship company’s earnings, and (investor, pcomp of, m∗ corresponds to the prepositional complement relationship the company of the investor.2 Throughout this article we use syntactic dependency relationships generated by the Minipar dependency parser (Lin 1993).</S>
			<S sid ="75" ssid = "13">Table 1 lists common Minipar dependency relations involving nouns.</S>
			<S sid ="76" ssid = "14">Minipar also identiﬁes multi-word expressions, which is 1A preliminary version of the bootstrapping method was presented in Geffet and Dagan (2004).</S>
			<S sid ="77" ssid = "15">That paper presented initial results for the bootstrapping scheme, when applied only over Lin’s measure and tested by the manually judged dataset of lexical entailment.</S>
			<S sid ="78" ssid = "16">The current research extends our initial results in many respects.</S>
			<S sid ="79" ssid = "17">It reﬁnes the deﬁnition of lexical entailment; utilizes a revised test set of larger scope and higher quality, annotated by three assessors; extends the experiments to two additional similarity measures; provides comparative qualitative and quantitative analysis of the bootstrapped vectors, while employing our proposed average common-feature rank ratio; and presents an additional evaluation based on a pseudo-WSD task.</S>
			<S sid ="80" ssid = "18">2 Following a common practice, we consider the relationship between a head noun (company in the.</S>
			<S sid ="81" ssid = "19">example) and the nominal complement of a modifying prepositional phrase (investor) as a single direct dependency relationship.</S>
			<S sid ="82" ssid = "20">The preposition itself is encoded in the dependency relation name, with a distinct relation for each preposition.</S>
			<S sid ="83" ssid = "21">438 Table 1 Common grammatical relations of Minipar involving nouns.</S>
			<S sid ="84" ssid = "22">Relation Description appo apposition comp1 ﬁrst complement det determiner gen genitive marker mod the relationship between a word and its adjunct modiﬁer pnmod post nominal modiﬁer pcomp nominal complement of prepositions post post determiner vrel passive verb modiﬁer of nouns obj object of verbs obj2 second object of ditransitive verbs subj subject of verbs s surface subject advantageous for detecting distributional similarity for such terms.</S>
			<S sid ="85" ssid = "23">For example, Curran (2004) reports that multi-word expressions make up between 14–25% of the synonyms in a gold-standard thesaurus.</S>
			<S sid ="86" ssid = "24">Thus, in our representation the corpus is ﬁrst transformed to a set S of dependency relationship instances of the form (w,f ∗, where each pair corresponds to a single co occurrence of w and f in the corpus.</S>
			<S sid ="87" ssid = "25">f is termed as a feature of w. Then, a word w is represented by a feature vector, where each entry in the vector corresponds to one feature f . The value of the entry is determined by a feature weighting function weight(w, f ), which quantiﬁes the degree of statistical association between w and f in the set S. For example, some feature weighting functions are based on the logarithm of the word–feature co-occurrence frequency (Ruge 1992), or on the conditional probability of the feature given the word (Pereira, Tishby, and Lee 1993; Dagan, Lee, and Pereira 1999; Lee 1999).</S>
			<S sid ="88" ssid = "26">Probably the most widely used feature weighting function is (point-wise) Mutual Information (MI) (Church and Patrick 1990; Hindle 1990; Luk 1995; Lin 1998; Gauch, Wang, and Rachakonda 1999; Dagan 2000; Baroni and Vegnaduzzo 2004; Chklovski and Pantel 2004; Pantel and Ravichandran 2004; Pantel, Ravichandran, and Hovy 2004; Weeds, Weir, and McCarthy 2004), deﬁned by: weightMI (w, f ) = log P(w,f ) (1) We calculate the MI weights by the following statistics in the space of co-occurrence instances S: weight (w, f ) = log count(w, f ) · nrels (2) MI 2 count(w) · count( f ) where count(w, f ) is the frequency of the co-occurrence pair (w,f ∗ in S, count(w) and count( f ) are the independent frequencies of w and f in S, and nrels is the size of S. High MI weights are assumed to correspond to strong word–feature associations.</S>
			<S sid ="89" ssid = "27">439 Curran and Moens (2002) argue that, generally, informative features are statistically correlated with their corresponding headword.</S>
			<S sid ="90" ssid = "28">Thus, they suggest that any statistical test used for collocations is a good starting point for improving feature- weight functions.</S>
			<S sid ="91" ssid = "29">In their experiments the t-test-based metric yielded the best empirical performance.</S>
			<S sid ="92" ssid = "30">However, a known weakness of MI and most of the other statistical weighting functions used for collocation extraction, including t-test and χ2 , is their tendency to inﬂate the weights for rare features (Dunning 1993).</S>
			<S sid ="93" ssid = "31">In addition, a major property of lexical collocations is their “non-substitutability”, as termed in Manning and Schutze (1999).</S>
			<S sid ="94" ssid = "32">That is, typically neither a headword nor a modiﬁer in the collocation can be substituted by their synonyms or other related terms.</S>
			<S sid ="95" ssid = "33">This implies that using modiﬁers within strong collocations as features for a head word would provide a rather small amount of common features for semantically similar words.</S>
			<S sid ="96" ssid = "34">Hence, these functions seem less suitable for learning broader substitutability relationships, such as lexical entailment.</S>
			<S sid ="97" ssid = "35">Similarity measures that utilize MI weights showed good performance, however.</S>
			<S sid ="98" ssid = "36">In particular, a common practice is to ﬁlter out features by minimal frequency and weight thresholds.</S>
			<S sid ="99" ssid = "37">Then, a word’s vector is constructed from the remaining (not ﬁltered) features that are strongly associated with the word.</S>
			<S sid ="100" ssid = "38">These features are denoted here as active features.</S>
			<S sid ="101" ssid = "39">In the current work we use MI for data analysis, and for the evaluations of vector quality and word similarity performance.</S>
			<S sid ="102" ssid = "40">2.2 Vector Similarity Measures.</S>
			<S sid ="103" ssid = "41">Once feature vectors have been constructed the similarity between two words is de- ﬁned by some vector similarity measure.</S>
			<S sid ="104" ssid = "42">Similarity measures which have been used in the cited papers include weighted Jaccard (Grefenstette 1994), Cosine (Ruge 1992), and various information theoretic measures, as introduced and reviewed in Lee (1997, 1999).</S>
			<S sid ="105" ssid = "43">In the current work we experiment with the following three popular similarity measures.</S>
			<S sid ="106" ssid = "44">1.</S>
			<S sid ="107" ssid = "45">The basic Jaccard measure compares the number of common features with.</S>
			<S sid ="108" ssid = "46">the overall number of features for a pair of words.</S>
			<S sid ="109" ssid = "47">One of the weighted generalizations of this scheme to non-binary values replaces intersection with minimum weight, union with maximum weight, and set cardinality with summation.</S>
			<S sid ="110" ssid = "48">This measure is commonly referred to as weighted Jaccard (WJ) (Grefenstette 1994; Dagan, Marcus, and Markovitch 1995; Dagan 2000; Gasperin and Vieira 2004), deﬁned as follows: L.f ∈F(w)∩F(v) min(weight(w,f ),weight(v,f )) WJ L.f ∈F(w)∪F(v) max(weight(w,f ),weight(v,f )) (3) where F(w) and F(v) are the sets of active features of the two words w and v. The appealing property of this measure is that it considers the association weights rather than just the number of common features.</S>
			<S sid ="111" ssid = "49">2.</S>
			<S sid ="112" ssid = "50">The standard Cosine measure (COS), which is popularly employed for.</S>
			<S sid ="113" ssid = "51">information retrieval (Salton and McGill 1983) and also utilized for 440 learning distributionally similar words (Ruge 1992; Caraballo 1999; Gauch, Wang, and Rachakonda 1999; Pantel and Ravichandran 2004), is deﬁned as follows: simCOS (w, v) = √L.</S>
			<S sid ="114" ssid = "52">L.f (weight(w,f )·weight(v,f )) L.</S>
			<S sid ="115" ssid = "53">(4) f (weight(w,f ))2 ·√ f (weight(v,f ))2 This measure computes the cosine of the angle between the two feature vectors, which normalizes the vector lengths and thus avoids inﬂated discrimination between vectors of signiﬁcantly different lengths.</S>
			<S sid ="116" ssid = "54">motivated by Information Theory principles.</S>
			<S sid ="117" ssid = "55">This measure behaves quite similarly to the weighted Jaccard measure (Weeds, Weir, and McCarthy 2004), and is deﬁned as follows: L.f ∈F(w)∩F(v) (weightMI (w,f )+weightMI (v,f )) sim (w, v) = f ∈F(w) weightMI (w,f )+ L.f ∈F(v) weightMI (v,f ) (5) where F(w) and F(v) are the active features of the two words.</S>
			<S sid ="118" ssid = "56">The weight function used originally by Lin is MI (Equation 1).</S>
			<S sid ="119" ssid = "57">It is interesting to note that a relatively recent work by Weeds and Weir (2005) investigates a more generic similarity framework.</S>
			<S sid ="120" ssid = "58">Within their framework, the similarity of two nouns is viewed as the ability to predict the distribution of one of them based on that of the other.</S>
			<S sid ="121" ssid = "59">Their proposed formula combines the precision and recall of a potential “retrieval” of similar words based on the features of the target word.</S>
			<S sid ="122" ssid = "60">The precision of w’s prediction of v’s feature distribution indicates how many of the features of the word w co-occurred with the word v. The recall of w’s prediction of v’s features indicates how many of the features of v co-occurred with w. Words with both high precision and high recall can be obtained by computing their harmonic mean, mh (or F-score), and a weighted arithmetic mean.</S>
			<S sid ="123" ssid = "61">However, after empirical tuning of weights for the arithmetic mean, Weeds and Weir ’s formula practically reduces to Lin’s measure, as was anticipated by their own analysis (in Section 4 of their paper).</S>
			<S sid ="124" ssid = "62">Consequently, we choose the Lin measure (Equation 5) (henceforth denoted as LIN) as representative for the state of the art and utilize it for data analysis and as a starting point for improvement.</S>
			<S sid ="125" ssid = "63">To further explore and evaluate our new weighting scheme, independently of a single similarity measure, we conduct evaluations also with the other two similarity measures of weighted Jaccard and Cosine.</S>
	</SECTION>
	<SECTION title="Substitutable Lexical Entailment. " number = "3">
			<S sid ="126" ssid = "1">As mentioned in the Introduction, the long term research goal which inspired our work is modeling meaning–entailing lexical substitution.</S>
			<S sid ="127" ssid = "2">Motivated by this goal, we proposed in earlier work (Geffet and Dagan 2004, 2005) a new type of lexical relationship which aims to capture such lexical substitution needs.</S>
			<S sid ="128" ssid = "3">Here we adopt that approach and formulate a reﬁned deﬁnition for this relationship, termed substitutable lexical entailment.</S>
			<S sid ="129" ssid = "4">In the context of the current article, utilizing a concrete target notion of word similarity enabled us to apply direct human judgment for the “correctness” (relative to the deﬁned notion) of candidate word pairs suggested by distributional similarity.</S>
			<S sid ="130" ssid = "5">Utilizing these judgments we could analyze the behavior of alternative 441 distributional vector representations and, in particular, conduct error analysis for word pair candidates that were judged negatively.</S>
			<S sid ="131" ssid = "6">The discussion in the Introduction suggested that multiple text understanding applications need to identify term pairs whose meanings are both entailing and sub- stitutable.</S>
			<S sid ="132" ssid = "7">Such pairs seem to be most appropriate for lexical substitution in a meaning preserving scenario.</S>
			<S sid ="133" ssid = "8">To model this goal we present an operational deﬁnition for a lexical semantic relationship that integrates the two aspects of entailment and substitutability,3 which is termed substitutable lexical entailment (or lexical entailment, for brevity).</S>
			<S sid ="134" ssid = "9">This relationship holds for a given directional pair of terms (w, v), saying that w entails v, if the following two conditions are fulﬁlled: 1.</S>
			<S sid ="135" ssid = "10">Word meaning entailment: the meaning of a possible sense of w implies a. possible sense of v; 2.</S>
			<S sid ="136" ssid = "11">Substitutability: w can substitute for v in some naturally occurring sentence,.</S>
			<S sid ="137" ssid = "12">such that the meaning of the modiﬁed sentence would entail the meaning of the original one.</S>
			<S sid ="138" ssid = "13">To operationally assess the ﬁrst condition (by annotators) we propose considering the meaning of terms by existential statements of the form “there exists an instance of the meaning of the term w in some context” (notice that, unlike propositions, it is not intuitive for annotators to assign truth values to terms).</S>
			<S sid ="139" ssid = "14">For example, the word company would correspond to the existential statement “there exists an instance of the concept company in some context.” Thus, if in some context “there is a company” (in the sense of “commercial organization”) then necessarily “there is a ﬁrm” in that context (in the corresponding sense).</S>
			<S sid ="140" ssid = "15">Therefore, we conclude that the meaning of company implies the meaning of ﬁrm.</S>
			<S sid ="141" ssid = "16">On the other hand, “there is an organization” does not necessarily imply the existence of company, since organization might stand for some non-proﬁt association, as well.</S>
			<S sid ="142" ssid = "17">Therefore, we conclude that organization does not entail company.</S>
			<S sid ="143" ssid = "18">To assess the second condition, the annotators need to identify some natural context in which the lexical substitution would satisfy entailment between the modiﬁed sentence and the original one.</S>
			<S sid ="144" ssid = "19">Practically, in our experiments presented in Section 5 the human assessors could consult external lexical resources and the entire Web to obtain all the senses of the words and possible sentences for substitution.</S>
			<S sid ="145" ssid = "20">We note that the task of identifying the common sense of two given words is quite easy since they mutually disambiguate each other, and once the common sense is known it naturally helps ﬁnding a corresponding common context.</S>
			<S sid ="146" ssid = "21">We note that this condition is important, in particular, in order to eliminate cases of anaphora and co-reference in contexts, where two words quite different in their meaning can sometimes appear in the same contexts only due to the text pragmatics in a particular situation.</S>
			<S sid ="147" ssid = "22">For example, in some situations worker and demonstrator could be used interchangeably in text, but clearly it is a discourse co- reference rather than common meaning that makes the substitution possible.</S>
			<S sid ="148" ssid = "23">Instead, we are interested in identifying word pairs in which one word’s meaning provides a reference to the entailed word’s meaning.</S>
			<S sid ="149" ssid = "24">This purpose is exactly captured by the existential propositions of the ﬁrst criterion above.</S>
			<S sid ="150" ssid = "25">3 The WordNet deﬁnition of the lexical entailment relation is speciﬁed only for verbs and, therefore, is not.</S>
			<S sid ="151" ssid = "26">felicitous for general purposes: A verb X entails Y if X cannot be done unless Y is, or has been, done (e.g., snore and sleep).</S>
			<S sid ="152" ssid = "27">442 As reported further in Section 5.1, we observed that assessing these two conditions for candidate word similarity pairs was quite intuitive for annotators, and yielded good cross-annotator agreement.</S>
			<S sid ="153" ssid = "28">Overall, substitutable lexical entailment captures directly the typical lexical substitution scenario in text understanding applications, as well as in generic textual entailment modeling.</S>
			<S sid ="154" ssid = "29">In fact, this relation partially overlaps with several traditional lexical semantic relations that are known as relevant for lexical substitution, such as synonymy, hyponymy, and some cases of meronymy.</S>
			<S sid ="155" ssid = "30">For example, we say that the meaning of company is lexically entailed by the meaning of ﬁrm (synonym) or automaker (hyponym), while the word government entails minister (meronym) as The government voted for the new law entails A minister in the government voted for the new law.</S>
			<S sid ="156" ssid = "31">On the other hand, lexical entailment is not just a superset of other known relations, but it is rather designed to select those sub-cases of other lexical relations that are needed for applied entailment inference.</S>
			<S sid ="157" ssid = "32">For example, lexical entailment does not cover all cases of meronyms (e.g., division does not entail company), but only some sub-cases of part- whole relationship mentioned herein.</S>
			<S sid ="158" ssid = "33">In addition, some other relations are also covered by lexical entailment, like ocean and water and murder and death, which do not seem to directly correspond to meronymy or hyponymy relations.</S>
			<S sid ="159" ssid = "34">Notice also that whereas lexical entailment is a directional relation that speciﬁes which word of the pair entails the other, the relation may hold in both directions for a pair of words, as is the case for synonyms.</S>
			<S sid ="160" ssid = "35">More detailed motivations for the substitutable lexical entailment relation and analysis of its relationship to traditional lexical semantic relations appear in Geffet (2006) and Geffet and Dagan (2004, 2005).</S>
	</SECTION>
	<SECTION title="Bootstrapping Feature Weights. " number = "4">
			<S sid ="161" ssid = "1">To gain a better understanding of distributional similarity behavior we ﬁrst analyzed the output of the LIN measure, as a representative case for the state of the art, and regarding lexical entailment as a reference evaluation criterion.</S>
			<S sid ="162" ssid = "2">We judge as correct, with respect to lexical entailment, those candidate pairs of the distributional similarity method for which entailment holds at least in one direction.</S>
			<S sid ="163" ssid = "3">For example, the word area is entailed by country, since the existence of country entails the existence of area, and the sentence There is no rain in subtropical countries during the summer period entails the sentence There is no rain in subtropical areas during the summer period.</S>
			<S sid ="164" ssid = "4">As another example, democracy is a type of country in the political sense, thus the existence entailment holds and also the sentence Israel is a democracy in the Middle East entails Israel is a country in the Middle East.</S>
			<S sid ="165" ssid = "5">On the other hand, our analysis revealed that many candidate word similarity pairs suggested by distributional similarity measures do not correspond to “tight” semantic relationships.</S>
			<S sid ="166" ssid = "6">In particular, many word pairs suggested by the LIN measure do not satisfy the lexical entailment relation, as demonstrated in Table 2.</S>
			<S sid ="167" ssid = "7">A deeper look at the corresponding word feature vectors reveals typical reasons for these lexical entailment prediction errors.</S>
			<S sid ="168" ssid = "8">Most relevant for the scope of the current article, in many cases highly ranked features in a word vector (when sorting the features by their weight) do not seem very characteristic for the word meaning.</S>
			<S sid ="169" ssid = "9">This is demonstrated in Table 3, which shows the top 10 features in the vector for country.</S>
			<S sid ="170" ssid = "10">As can be seen, some of the top features are either too speciﬁc (landlocked, airspace), and are thus less reliable, or too general (destination, ambition), thus not indicative and may co-occur with many different types of words.</S>
			<S sid ="171" ssid = "11">On the other hand, intuitively more characteristic features of country, like population and governor, occur further down the 443 Table 2 The top 20 most similar words for country (and their ranks) in the similarity list of LIN, followed by the next four words in the similarity list that were judged as entailing at least in one direction.</S>
			<S sid ="172" ssid = "12">nati on 1 *ci ty 7 ec on om y 13 *co mp an y 19 regi on 2 ter rit or y 8 *ne igh bo r 14 *in du str y 20 stat e 3 are a 9 *se cto r 15 kin gd om 30 *wo rld 4 *to wn 10 *m em ber 16 pla ce 35 *isl and 5 rep ubl ic 11 *p art y 17 col on y 41 *pr ovi nce 6 *n ort h 12 go ver nm ent 18 de mo cra cy 82 Twelve out of 20 top similarities (60%) were judged as mutually non-entailing and are marked with an asterisk.</S>
			<S sid ="173" ssid = "13">The similarity data was produced as described in Section 5.</S>
			<S sid ="174" ssid = "14">Table 3 The top 10 ranked features for country produced by MI, the weighting function employed in the LIN method.</S>
			<S sid ="175" ssid = "15">Fea ture we igh tMI Co mm erci al ban k, gen, h 8 . 0 8 Des tina tion , pco mp of, m 7 . 9 7 Air spa ce, pco mp of, h 7 . 8 3 Lan dloc ked, mod , m 7 . 7 9 Trad e bala nce, gen, h 7 . 7 8 Sov erei gnty , pco mp of, h 7 . 7 8 Am biti on, nn, h 7 . 7 7 Bou rse, gen, h 7 . 7 2 Poli ticia n, gen, h 7 . 5 4 Bor der, pco mp of, h 7 . 5 3 sorted feature list, at positions 461 and 832.</S>
			<S sid ="176" ssid = "16">Overall, features that seem to characterize the word meaning well are scattered across the ranked feature list, while many non- indicative features receive high weights.</S>
			<S sid ="177" ssid = "17">This behavior often yields high similarity scores for word pairs whose semantic similarity is rather loose while missing some much tighter similarities.</S>
			<S sid ="178" ssid = "18">Furthermore, we observed that characteristic features for a word w, which should receive higher weights, are expected to be common for w and other words that are semantically similar to it.</S>
			<S sid ="179" ssid = "19">This observation suggests a computational scheme which would promote the weights of features that are common for semantically similar words.</S>
			<S sid ="180" ssid = "20">Of course, there is an inherent circularity in such a scheme: to determine which features should receive high weights we need to know which words are semantically similar, while computing distributional semantic similarity already requires predetermined feature weights.</S>
			<S sid ="181" ssid = "21">This kind of circularity can be approached by a bootstrapping scheme.</S>
			<S sid ="182" ssid = "22">We ﬁrst compute initial distributional similarity values, based on an initial feature weighting function.</S>
			<S sid ="183" ssid = "23">Then, to learn more accurate feature weights for a word w, we promote features that characterize other words that are initially known to be similar to w. By the same rationale, features that do not characterize many words that are sufﬁciently similar to w are demoted.</S>
			<S sid ="184" ssid = "24">Even if such features happen to have a strong direct statistical association with w they would not be considered reliable, because they are not supported by additional words that have a similar meaning to that of w. 444 4.1 Bootstrapped Feature Weight Deﬁnition.</S>
			<S sid ="185" ssid = "25">The bootstrapped feature weight is deﬁned as follows.</S>
			<S sid ="186" ssid = "26">First, some standard word similarity measure sim is computed to obtain an initial approximation of the similarity space.</S>
			<S sid ="187" ssid = "27">Then, we deﬁne the word set of a feature f , denoted by WS( f ), as the set of words for which f is an active feature.</S>
			<S sid ="188" ssid = "28">Recall from Section 2.2 that an active feature is a feature that is strongly associated with the word, that is, its (initial) weight is higher than an empirically predeﬁned threshold, θweight . The semantic neighborhood of w, denoted by N(w), is deﬁned as the set of all words v which are considered sufﬁciently similar to w, satisfying sim(w, v) &gt; θsim , where θsim is a second empirically determined threshold.</S>
			<S sid ="189" ssid = "29">The bootstrapped feature weight, denoted weightB , is then deﬁned by: weightB (w, f ) = L.v WS( f ) N(w) sim(w, v) (6) ∈ ∩ That is, we identify all words v that are in the semantic neighborhood of w and are also characterized by f , and then sum the values of their similarities to w. Intuitively, summing these similarity values captures simultaneously a desired balance between feature speciﬁcity and generality, addressing the observations in the beginning of this section.</S>
			<S sid ="190" ssid = "30">Some features might characterize just a single word that is very similar to w, but then the sum of similarities will include a single element, yielding a relatively low weight.</S>
			<S sid ="191" ssid = "31">This is why the sum of similarities is used rather than an average value, which might become too high by chance when computed over just a single element (or very few elements).</S>
			<S sid ="192" ssid = "32">Relatively generic features, which occur with many words and are thus less indicative, may characterize more words within N(w) but then on average the similarity values of these words with w is likely to be lower, contributing smaller values to the sum.</S>
			<S sid ="193" ssid = "33">To receive a high overall weight a reliable feature has to characterize multiple words that are highly similar to w. We note that the bootstrapped weight is a sum of word similarity values rather than a direct function of word–feature association values, which is the more common approach.</S>
			<S sid ="194" ssid = "34">It thus does not depend on the exact statistical co-occurrence level between w and f . Instead, it depends on a more global assessment of the association between f and the semantic vicinity of w. We notice that the bootstrapped weight is determined separately relative to each individual word.</S>
			<S sid ="195" ssid = "35">This differs from measures that are global word-independent functions of the feature, such as the feature entropy used in Grefenstette (1994) and the feature term strength relative to a predeﬁned class as employed in Pekar, Krkoska, and Staab (2004) for supervised word classiﬁcation.</S>
			<S sid ="196" ssid = "36">4.2 Feature Reduction and Similarity Re-Computation.</S>
			<S sid ="197" ssid = "37">Once the bootstrapped weights have been computed, their accuracy is sufﬁcient to allow for aggressive feature reduction.</S>
			<S sid ="198" ssid = "38">As shown in the following section, in our experiments it sufﬁced to use only the top 100 features for each word in order to obtain optimal word similarity results, because the most informative features now receive the highest weights.</S>
			<S sid ="199" ssid = "39">Finally, similarity between words is re-computed over the reduced vectors using the sim function with weightB replacing the original feature weights.</S>
			<S sid ="200" ssid = "40">The resulting similarity measure is further referred to as simB . 445</S>
	</SECTION>
	<SECTION title="Evaluation by Lexical Entailment. " number = "5">
			<S sid ="201" ssid = "1">To test the effectiveness of the bootstrapped weighting scheme, we ﬁrst evaluated whether it contributes to better prediction of lexical entailment.</S>
			<S sid ="202" ssid = "2">This evaluation was based on gold-standard annotations determined by human judgments of the substitutable lexical entailment relation, as deﬁned in Section 3.</S>
			<S sid ="203" ssid = "3">The new similarity scheme, simB , based on the bootstrapped weights, was ﬁrst computed using the standard LIN method as the initial similarity measure.</S>
			<S sid ="204" ssid = "4">The resulting similarity lists of simLIN (the original LIN method) and simB (Bootstrapped LIN) schemes were evaluated for a sam ple of nouns (Section 5.2).</S>
			<S sid ="205" ssid = "5">Then, the evaluation was extended (Section 5.3) to apply the bootstrapping scheme over the two additional similarity measures that were presented in Section 2.2, simWJ (weighted Jaccard) and simCOS (Cosine).</S>
			<S sid ="206" ssid = "6">Along with these lexical entailment evaluations we also analyzed directly the quality of the bootstrapped feature vectors, according to the average common-feature rank ratio measure, which was deﬁned in Section 6.</S>
			<S sid ="207" ssid = "7">5.1 Experimental Setting.</S>
			<S sid ="208" ssid = "8">Our experiments were conducted using statistics from an 18 million token subset of theReuters RCV1 corpus (known as Reuters Corpus, Volume 1, English Language, 1996 0820 to 199708-19), parsed by Lin’s Minipar dependency parser (Lin 1993).</S>
			<S sid ="209" ssid = "9">The test set of candidate word similarity pairs was constructed for a sample of 30 randomly selected nouns whose corpus frequency exceeds 500.</S>
			<S sid ="210" ssid = "10">In our primary experiment we computed the top 40 most similar words for each noun by the simLIN and by B LIN measur es, yieldin g 1,200 pairs for each metho d, and 2,400 pairs altoget her.</S>
			<S sid ="211" ssid = "11">About 800 of these pairs were common for the two methods, therefore leaving approximately 1,600 distinct candidate word similarity pairs.</S>
			<S sid ="212" ssid = "12">Because the lexical entailment relation is directional, each candidate pair was duplicated to create two directional pairs, yielding a test set of 3,200 pairs.</S>
			<S sid ="213" ssid = "13">Thus, for each pair of words, w and v, the two ordered pairs (w, v) and (v, w) were created to be judged separately for entailment in the speciﬁed direction (whether the ﬁrst word entails the other).</S>
			<S sid ="214" ssid = "14">Consequently, a non-directional candidate similarity pair w, v is considered as a correct entailment if it was assessed as an entailing pair at least in one direction.</S>
			<S sid ="215" ssid = "15">The assessors were only provided with a list of word pairs without any contextual information and could consult any available dictionary, WordNet, and the Web.</S>
			<S sid ="216" ssid = "16">The judgment criterion follows the criterion presented in Section 3.</S>
			<S sid ="217" ssid = "17">In particular, the judges were asked to apply the two operational conditions, existence and sub- stitutability in context, to each given pair.</S>
			<S sid ="218" ssid = "18">Prior to performing the ﬁnal test of the annotation experiment, the judges were presented with an annotated set of entailing and non-entailing pairs along with the existential statements and sample sentences for substitution, demonstrating how the two conditions could be applied in different cases of entailment.</S>
			<S sid ="219" ssid = "19">In addition, they had to judge a training set of several dozen pairs and then discuss their judgment decisions with each other to gain a better understanding of the two criteria.</S>
			<S sid ="220" ssid = "20">The following example illustrates this process.</S>
			<S sid ="221" ssid = "21">Given a non-directional pair {company, organization} two directional pairs are created: (company, organization) and (organization, company).</S>
			<S sid ="222" ssid = "22">The former pair is judged as a correct entailment: the existence of a company entails the existence of an organization, and the meaning of the sentence: John works for a large company entails the meaning of the sentence with substitution: John works for a large organization.</S>
			<S sid ="223" ssid = "23">Hence, company lexically entails organization, but not 446 vice versa (as shown in Section 3.3), therefore the second pair is judged as not entailing.</S>
			<S sid ="224" ssid = "24">Eventually, the non-directional pair {company, organization} is considered as a correct entailment.</S>
			<S sid ="225" ssid = "25">Finally, the test set of 3,200 pairs was split into three disjoint subsets that were judged by three native English speaking assessors, each of whom possessed a Bachelors degree in English Linguistics.</S>
			<S sid ="226" ssid = "26">For each subset a different pair of assessors was assigned, each person judging the entire subset.</S>
			<S sid ="227" ssid = "27">The judges were grouped into three different pairs (i.e., JudgeI+JudgeII, JudgeII+JudgeIII, and JudgeI+JudgeIII).</S>
			<S sid ="228" ssid = "28">Each pair was assigned initially to judge all the word similarities in each subset, and the third assessor was employed in cases of disagreement between the ﬁrst two.</S>
			<S sid ="229" ssid = "29">The majority vote was taken as the ﬁnal decision.</S>
			<S sid ="230" ssid = "30">Hence, each assessor had to fully annotate two thirds of the data and for a third subset she only had to judge the pairs for which there was disagreement between the other two judges.</S>
			<S sid ="231" ssid = "31">This was done in order to measure the agreement achieved for different pairs of annotators.</S>
			<S sid ="232" ssid = "32">The output pairs from both methods were mixed so the assessors could not associate a pair with the method that proposed it.</S>
			<S sid ="233" ssid = "33">We note that this evaluation methodology, in which human assessors judge the correctness of candidate pairs by some semantic substitutability criterion, is similar to common evaluation methodologies used for paraphrase acquisition (Barzilay and McKeown 2001; Lin and Pantel 2001; Szpektor et al. 2004).</S>
			<S sid ="234" ssid = "34">Measuring human agreement level for this task, the proportions of matching decisions were 93.5% between Judge I and Judge II, 90% for Judge I and Judge III, and 91.2% for Judge II and Judge III.</S>
			<S sid ="235" ssid = "35">The corresponding kappa values are 0.83, 0.80, and 0.80, which is regarded as “very good agreement” (Landis and Koch 1997).</S>
			<S sid ="236" ssid = "36">It is interesting to note that after some discussion most of the disagreements were settled, and the few remaining mismatches were due to different understandings of word meanings.</S>
			<S sid ="237" ssid = "37">These ﬁndings seem to have a similar ﬂavor to the human agreement ﬁndings reported for the Recognizing Textual Entailment challenges (Bar-Haim et al. 2006; Dagan, Glickman, and Magnini 2006), in which entailment was judged for pairs of sentences.</S>
			<S sid ="238" ssid = "38">In fact, the kappa values obtained in our evaluation are substantially higher than reported for sentence- level textual entailment, which suggests that it is easier to make entailment judgments at the lexical level than at the full sentence level.</S>
			<S sid ="239" ssid = "39">The parameter values of the algorithms were tuned using a development set of similarity pairs generated for 10 additional nouns, distinct from the 30 nouns used for the test set.</S>
			<S sid ="240" ssid = "40">The parameters were optimized by running the algorithm systematically with various values across the parameter scales and judging a sample subset of the results.</S>
			<S sid ="241" ssid = "41">weightMI = 4 was found as the optimal MI threshold for active feature weights (features included in the feature vectors), yielding a 10% precision increase of simLIN and removing over 50% of the data relative to no feature ﬁltering.</S>
			<S sid ="242" ssid = "42">Accordingly, this value also serves as the θweight threshold in the bootstrapping scheme (Section 4).</S>
			<S sid ="243" ssid = "43">As for the θsim parameter, the best results on the development set were obtained for θsim = 0.04, θsim = 0.02, and θsim = 0.01 when bootstrapping over the initial similarity measures LIN, WJ, and COS, respectively.</S>
			<S sid ="244" ssid = "44">5.2 Evaluation Results for simB.</S>
			<S sid ="245" ssid = "45">We measured the contribution of the improved feature vectors to the resulting preci sion of simLIN and simB in predicting lexical entailment.</S>
			<S sid ="246" ssid = "46">The results are presented in Table 4, where precision and error reduction values were computed for the top 20, 30, and 40 word similarity pairs produced by each method.</S>
			<S sid ="247" ssid = "47">It can be seen that the 447 Table 4 Lexical entailment precision values for top-n similar words by the Bootstrapped LIN and the original LIN method.</S>
			<S sid ="248" ssid = "48">Top -n Wor ds C o r r e c t Entail ments (%) E r r o r R a t e R e d u c t i o n ( % ) si mLI N s i m B L I N Top 20 5 2.</S>
			<S sid ="249" ssid = "49">0 5 7.</S>
			<S sid ="250" ssid = "50">9 1 2 . 3 Top 30 4 8.</S>
			<S sid ="251" ssid = "51">2 5 6.</S>
			<S sid ="252" ssid = "52">2 1 5 . 4 Top 40 4 1.</S>
			<S sid ="253" ssid = "53">0 4 9.</S>
			<S sid ="254" ssid = "54">7 1 4 . 7 Bootstrapped LIN method outperformed the original LIN approach by 6–9 precision points at all top-n levels.</S>
			<S sid ="255" ssid = "55">As expected, the precision for the shorter top 20 list is higher for both methods, thus leaving a bit less room for improvement.</S>
			<S sid ="256" ssid = "56">Overall, the Bootstrapped LIN method extracted 104 (21%) more correct similarity pairs than the other measure and reduced the number of errors by almost 15%.</S>
			<S sid ="257" ssid = "57">We also computed the relative recall, which shows the percentage of correct word similarities found by each method relative to the joint set of similarities that were extracted by both methods.</S>
			<S sid ="258" ssid = "58">The overall relative recall of the Bootstrapped LIN was quite high (94%), exceeding LIN’s relative recall (of 78%) by 16 percentage points.</S>
			<S sid ="259" ssid = "59">We found that the bootstrapped method covers over 90% of the correct similarities learned by the original method, while also identifying many additional correct pairs.</S>
			<S sid ="260" ssid = "60">It should be noted at this point that the current limited precision levels are determined not just by the quality of the feature vectors but signiﬁcantly by the nature of the vector comparison measure itself (i.e., the LIN formula, as well weighted Jaccard and Cosine as reported in Section 5.3).</S>
			<S sid ="261" ssid = "61">It was observed in other work (Geffet and Dagan 2005) that these common types of vector comparison schemes exhibit certain ﬂaws in predicting lexical entailment.</S>
			<S sid ="262" ssid = "62">Our present work thus shows that the bootstrapping method yields a signiﬁcant improvement in feature vector quality, but future research is needed to investigate improved vector comparison schemes.</S>
			<S sid ="263" ssid = "63">An additional indication of the improved vector quality is the massive feature reduction allowed by having the most characteristic features concentrated at the top ranks of the vectors.</S>
			<S sid ="264" ssid = "64">The vectors of active features of LIN, as constructed after standard feature ﬁltering (Section 5.1), could be further reduced by the bootstrapped weighting to about one third of their size.</S>
			<S sid ="265" ssid = "65">As illustrated in Figure 1, changing the vector size signiﬁcantly affects the similarity results.</S>
			<S sid ="266" ssid = "66">In simB the best result was obtained with the top 100 features per word, while using less than 100 or more than 150 features caused a 5–10% decrease in performance.</S>
			<S sid ="267" ssid = "67">On the other hand, an attempt to cut off the lower ranked features of the MI weighting always resulted in a noticeable decrease in precision.</S>
			<S sid ="268" ssid = "68">These results show that for MI weighting many important features appear further down in the ranked vectors, while for the bootstrapped weighting adding too many features adds mostly noise, since most characteristic features are concentrated at the top ranks.</S>
			<S sid ="269" ssid = "69">Thus, in addition to better feature weighting, the bootstrapping step provides effective feature reduction, which improves vector quality and consequently the similarity results.</S>
			<S sid ="270" ssid = "70">We note that the optimal vector size we obtained conforms to previous results— for example, by Widdows (2003), Curran (2004), and Curran and Moens (2002)—who also used reduced vectors of up to 100 features as optimal for learning hyponymy and synonymy, respectively.</S>
			<S sid ="271" ssid = "71">In Widdows the known SVD method for dimension reduction 448 Figure 1 Percentage of correct entailments within the top 40 candidate pairs of each of the methods, LIN and Bootstrapped LIN (denoted as LINB in the ﬁgure), when using varying numbers of top-ranked features in the feature vector.</S>
			<S sid ="272" ssid = "72">The value of “All” corresponds to the full size of vectors and is typically in the range of 300–400 features.</S>
			<S sid ="273" ssid = "73">of LSA-based vectors is applied, whereas in Curran, and Curran and Moens, only the strongly associated verbs (direct and indirect objects of the noun) are selected as “canonical features” that are expected to be shared by true synonyms.Finally, we tried executing an additional bootstrapping iteration of weightB calcula tion over the similarity results of simB . The resulting increase in precision was.</S>
			<S sid ="274" ssid = "74">much smaller, of about 2%, showing that most of the potential beneﬁt is exploited in the ﬁrst bootstrapping iteration (which is not uncommon for natural language data).</S>
			<S sid ="275" ssid = "75">On the other hand, computing the bootstrapping weight twice increases computation time signiﬁcantly, which led us to suggest a single bootstrapping iteration as a reasonable cost-effectiveness tradeoff for our data.</S>
			<S sid ="276" ssid = "76">5.3 Evaluation for simB.</S>
			<S sid ="277" ssid = "77">and simB To further validate the behavior of the bootstrapping scheme we experimented with two additional similarity measures, weighted Jaccard (simWJ ) and Cosine (simCOS) (described in Section 2.2).</S>
			<S sid ="278" ssid = "78">For each of the additional measures the experiment repeats the main three steps described in Section 4: Initially, the basic similarity lists are calculated for each of the measures using MI weighting; then, the bootstrapped weighting, weightB , is computed based on the initial similarities, yielding new word feature vectors; ﬁnally, the similarity values are recomputed by the same vector similarity measure using the new feature vectors.</S>
			<S sid ="279" ssid = "79">To assess the effectiveness of weightB we computed the four alternative output similarity lists, using the simWJ and simCOS similarity measures, each with the weightMI 449 Table 5 Comparative precision values for the top 20 similarity lists of the three selected similarity measures, with MI and Bootstrapped feature weighting for each.</S>
			<S sid ="280" ssid = "80">Measure LIN–LINB WJ–WJB COS–COSB Correct Similarities (%) 52.0–57.9 51.0–54.8 46.1–50.9 and weightB weighting functions.</S>
			<S sid ="281" ssid = "81">The four lists were judged for lexical entailment by three assessors, according to the same procedure described in Section 5.1.</S>
			<S sid ="282" ssid = "82">To make the additional manual evaluation affordable we judged the top 20 similar words in each list for each of the 30 target nouns of Section 5.1.</S>
			<S sid ="283" ssid = "83">Table 5 summarizes the precision values achieved by LIN, WJ, and COS with both weightMI and weightB . As shown in the table, bootstrapped weighting consistently contributed between 4–6 points to the accuracy of each method in the top 20 similarity list.</S>
			<S sid ="284" ssid = "84">We view the results as quite positive, considering that improving over top 20 similarities is a much more challenging task than improving over longer similarity lists, while the improvement was achieved only by modifying the feature vectors without changing the similarity measure itself (as hinted in Section 5.2).</S>
			<S sid ="285" ssid = "85">Our results are also compatible with previous ﬁndings in the literature (Dagan, Lee, and Pereira 1999; Weeds, Weir, and McCarthy 2004) that found LIN and WJ to be more accurate for similarity acquisition than COS.</S>
			<S sid ="286" ssid = "86">Overall, the results demonstrate that the bootstrapped weighting scheme consistently produces improved results.</S>
			<S sid ="287" ssid = "87">An interesting behavior of the bootstrapping process is that the most prominent features for a given target word converge across the different initial similarity measures, as exempliﬁed in Table 6.</S>
			<S sid ="288" ssid = "88">In particular, although the initial similarity lists overlap only partly,4 the overlap of the top 30 features for our 30-word sample was ranging between 88% and 100%.</S>
			<S sid ="289" ssid = "89">This provides additional evidence that the quality of the bootstrapped weighting is quite similar for various initial similarity measures.</S>
	</SECTION>
	<SECTION title="Analyzing the Bootstrapped  Feature Vector Quality. " number = "6">
			<S sid ="290" ssid = "1">In this section we provide an in-depth analysis of the bootstrapping feature weighting quality compared to the state-of-the-art MI weighting function.</S>
			<S sid ="291" ssid = "2">6.1 Qualitative Observations.</S>
			<S sid ="292" ssid = "3">The problematic feature ranking noticed at the beginning of Section 4 can be revealed more objectively by examining the common features which contribute most to the word similarity scores.</S>
			<S sid ="293" ssid = "4">To that end, we examine the common features of the two given words and sort them by the sum of their weights in both word vectors.</S>
			<S sid ="294" ssid = "5">Table 7 shows the top 10 common features by this sorting for a pair of truly similar (lexically entailing) words (country–state), and for a pair of non-entailing words (country–party).</S>
			<S sid ="295" ssid = "6">For each common feature the table shows its two corresponding ranks in the feature vectors of the two words.</S>
			<S sid ="296" ssid = "7">4 Overlap rate was about 40% between COS and WJ or LIN, and 70% between WJ and LIN.</S>
			<S sid ="297" ssid = "8">The overlap was.</S>
			<S sid ="298" ssid = "9">computed following the procedure of Weeds, Weir, and McCarthy (2004), disregarding the order of the similar words in the lists.</S>
			<S sid ="299" ssid = "10">Interestingly, they obtained roughly similar ﬁgures, of 28% overlap for COS and WJ, 32% overlap for COS and LIN, and 81% overlap between LIN and WJ.</S>
			<S sid ="300" ssid = "11">450 Table 6 Top 30 features of town by bootstrapped weighting based on LIN, WJ, and COS as initial similarities.</S>
			<S sid ="301" ssid = "12">The three sets of words are almost identical, with relatively minor ranking differences.</S>
			<S sid ="302" ssid = "13">LINB WJB COSB sout her n so uth ern no rth er n nort her n no rth er n so uth ern ofﬁ ce of ﬁc e re mo te east ern of ﬁci al eas ter n rem ote co ast al of ﬁci al ofﬁ cial eas ter n ba se d troo p no rth eas ter n no rth eas ter n nort hea ster n re mo te ofﬁ ce peo ple tro op co ast al coa stal pe opl e no rth we ste rn atta ck ba se d pe opl e bas ed po pu lat ed att ac k pop ulat ed att ac k tro op nort hwe ster n ho me ho me bas e no rth we ste rn so ut h ho me so ut h we ste rn sou th we ste rn cit y wes t we st po pu lat ed wes tern res ide nt ba se nei ghb orin g nei gh bo rin g res ide nt resi den t ho us e no rth pla nt cit y we st poli ce ba se nei gh bo rin g hel d tri p tri p loca te ca mp sur ro un di ng trip hel d pol ice city no rth hel d site loc ate loc ate cam p sur ro un di ng ho us e surr oun din g pol ice ca m p It can be observed in Table 7 that for both word pairs the common features are scattered across the pair of feature vectors, making it difﬁcult to distinguish between the truly similar and the non-similar pairs.</S>
			<S sid ="303" ssid = "14">We suggest, on the other hand, that the desired behavior of effective feature weighting is that the common features of truly similar words would be concentrated at the top ranks of both word vectors.</S>
			<S sid ="304" ssid = "15">In other words, if the two words are semantically similar then we expect them to share their most characteristic features, which are in turn expected to appear at the higher ranks of each feature vector.</S>
			<S sid ="305" ssid = "16">The common features for non-similar words are expected to be scattered all across each of the vectors.</S>
			<S sid ="306" ssid = "17">In fact, these expectations correspond exactly to the rationale behind distributional similarity measures: Such measures are designed to assign higher similarity scores for vector pairs that share highly weighted features.</S>
			<S sid ="307" ssid = "18">Comparatively, we illustrate the behavior of the Bootstrapped LIN method relative to the observations regarding the original LIN method, using the same running example.</S>
			<S sid ="308" ssid = "19">Table 8 shows the top 10 features of country.</S>
			<S sid ="309" ssid = "20">We observe that the list now contains features that are intuitively quite indicative and reliable, while many too speciﬁc or idiomatic features, and too general ones, were demoted (compare with Table 3).</S>
			<S sid ="310" ssid = "21">Table 9 shows that most of the top 10 common features for country–state are now ranked highly 451 for both words.</S>
			<S sid ="311" ssid = "22">On the other hand, there are only two common features (among the top 100 features) for the incorrect pair country–party, both with quite low ranks (compare with Table 7), while the rest of the common features for this pair did not pass the top 100 cutoff.</S>
			<S sid ="312" ssid = "23">Consequently, Table 10 demonstrates a much more accurate similarity list for country, where many incorrect (non-entailing) word similarities, like party and company, were demoted.</S>
			<S sid ="313" ssid = "24">Instead, additional correct similarities, like kingdom and land, were promoted (compare with Table 2).</S>
			<S sid ="314" ssid = "25">In this particular case all the remaining errors correspond to words that are related quite closely to country, denoting geographic concepts.</S>
			<S sid ="315" ssid = "26">Many of these errors are context dependent entailments which might be substitutable in some cases, but they violate the word meaning entailment condition (e.g., country–neighbor, country–port).</S>
			<S sid ="316" ssid = "27">Apparently, these words tend to occur in contexts that are typical for country in the Reuters corpus.</S>
			<S sid ="317" ssid = "28">Some errors violating the substitutability condition of lexical entailment were identiﬁed as well, such as industry–product.</S>
			<S sid ="318" ssid = "29">These cases are quite hard to differentiate from correct entailments, since the two words are usually closely related to each other and also share highly ranked features, because they often appear in similar characteristic contexts.</S>
			<S sid ="319" ssid = "30">It may therefore be difﬁcult to ﬁlter out such Table 7 LIN (MI) weighting: The top 10 common features for country–state and country–party, along with their corresponding ranks in each of the two feature vectors.</S>
			<S sid ="320" ssid = "31">The features are sorted by the sum of their feature weights with both words.</S>
			<S sid ="321" ssid = "32">Country–State Ranks Country–Party Ranks Bro adc ast, pco mp in, h 2 4 5 0 Bra ss, nn, h 6 4 22 Goo ds, mod , h 14 0 1 6 Co ncl udi ng, pc om p of, h 7 3 20 Civi l serv ant, gen, h 6 4 5 4 Re pre se nta tio n, pc om p of, h 8 2 27 Bloc , gen, h 3 0 7 7 Pat ria rch , pc om p of, h 12 8 28 No nali gne d, mod , m 5 5 6 0 Fri en dly , mo d, m 5 8 83 Nei ghb orin g, mod , m 1 5 16 5 Ex pel , pc om p fro m, h 5 9 30 Stati stic, pco mp on, h 16 5 4 3 He art lan d, pc om p of, h 10 2 23 Bor der, pco mp of, h 1 0 24 7 Su rpr isi ng, pc om p of, h 11 4 38 Nor thw est, mod , h 4 1 17 4 Iss ue, pc om p bet we en, h 10 3 51 Trip, pco mp to, h 10 5 3 4 Co ntr av ent ion , pc om p in, m 12 9 43 Table 8 Top 10 features of country by the Bootstrapped feature weighting.</S>
			<S sid ="322" ssid = "33">Fea ture W eig htB Ind ustr y, gen, h 1 . 2 1 Air port , gen, h 1 . 1 6 Visi t, pco mp to, h 1 . 0 6 Nei ghb orin g, mod , m 1 . 0 4 Law , gen, h 1 . 0 2 Eco no my, gen, h 1 . 0 2 Pop ulat ion, gen, h 0 . 9 3 Stoc k mar ket, gen, h 0 . 9 2 Go ver nor, pco mp of, h 0 . 9 2 Parl iam ent, gen, h 0 . 9 1 452 Table 9 Bootstrapped weighting: top 10 common features for country–state and country–party along with their corresponding ranks in the two (sorted) feature vectors.</S>
			<S sid ="323" ssid = "34">Country–State Ranks Country–Party Ranks Nei ghb orin g, mod , m 3 1 Relation, pcomp with, h. 12 26 Ind ustr y, gen, h 1 11 Mi nis ter, pc om p fro m, h 77 49 Imp ove rish ed, mod , m 8 8 Go ver nor, pco mp of, h 10 9 Pop ulat ion, gen, h 6 16 City, gen, h 17 18 Eco no my, gen, h 5 15 Parl iam ent, gen, h 10 22 Citi zen, pco mp of, h 14 25 Law , gen, h 4 33 Table 10 Top 20 most similar words for country and their ranks in the similarity list by the Bootstrapped LIN measure.</S>
			<S sid ="324" ssid = "35">nati on 1 ter rit or y 6 *pr ovi nc e 11 zo ne 16 stat e 2 *n eig hb or 7 *ci ty 12 lan d 17 *isl and 3 col on y 8 *to wn 13 pla ce 18 regi on 4 *p ort 9 kin gd om 14 ec on om y 19 area 5 rep ubl ic 10 *di stri ct 15 *w orl d 20 Note that four of the incorrect similarities from Table 2 were replaced with correct entailments resulting in a 20% increase of precision (reaching 60%).</S>
			<S sid ="325" ssid = "36">non-substitutable similarities merely by the standard distributional similarity scheme, suggesting that additional mechanisms and data types would be required.</S>
			<S sid ="326" ssid = "37">6.2 The Average Common-Feature Rank Ratio.</S>
			<S sid ="327" ssid = "38">It should be noted at this point that these observations regarding feature weight behavior are based on subjective intuition of how characteristic features are for a word meaning, which is quite difﬁcult to assess systematically.</S>
			<S sid ="328" ssid = "39">Therefore, we next propose a quantitative measure for analyzing the quality of feature vector weights.</S>
			<S sid ="329" ssid = "40">More formally, given a pair of feature vectors for words w and v we ﬁrst deﬁne their average common-feature rank with respect to the top-n common features, denoted acfrn , as follows: acfrn (w, v) = 1 1 (7) n L.f ∈top−n(F(w)∩F(v)) 2 [rank(w, f ) + rank(v, f )] where rank(w, f ) is the rank of feature f in the vector of the word w when features are sorted by their weight, and F(w) is the set of features in w’s vector.</S>
			<S sid ="330" ssid = "41">top-n is the set of top n common features to consider, where common features are sorted by the sum of their weights in the two word vectors (the same sorting as in Table 7).</S>
			<S sid ="331" ssid = "42">In other words, acfrn (w, v) is the average rank in the two feature vectors of their top n common features.</S>
			<S sid ="332" ssid = "43">453 Using this measure, we expect that a good feature weighting function would typically yield lower values of acfrn for truly similar words (as low ranking values correspond to higher positions in the vectors) than for non-similar words.</S>
			<S sid ="333" ssid = "44">Hence, given a prejudged test set of pairs of similar and non-similar words, we deﬁne the ratio, acfr-ratio, between the average acfrn of the set of all the non-similar words, denoted as Non-Sim, and the average acfrn of the set of all the known pairs of similar words, Sim, to be an objective measure for feature weighting quality, as follows: 1 L.w,v Non Sim acfrn (w,v) acfrn − ratio = |Non−Sim| |Sim| ∈ L.w,v∈ − Sim acfrn (w,v) (8) As an illustration, the two word pairs in Table 7 yielded acfr10 (country, state) = 78 and acfr10 (country, party) = 64.</S>
			<S sid ="334" ssid = "45">Both values are quite high, showing no principal difference between the tighter lexically entailing similarity versus a pair of non-similar (or rather loosely related) words.</S>
			<S sid ="335" ssid = "46">This behavior indicates the deﬁciency of the MI feature weighting function in this case.</S>
			<S sid ="336" ssid = "47">On the other hand, the corresponding values for the two pairs produced by the Bootstrapped LIN method (for the features in Table 9) are acfr10 (country, state) = 12 and acfr10 (country, party) = 41.</S>
			<S sid ="337" ssid = "48">These ﬁgures clearly reﬂect the desired distinction between similar and non-similar words, showing that the common features of the similar words are indeed concentrated at much higher ranks in the vectors than the common features of the non-similar words.</S>
			<S sid ="338" ssid = "49">In recent work on distributional similarity (Curran 2004; Weeds and Weir 2005) a variety of alternative weighting functions were compared.</S>
			<S sid ="339" ssid = "50">However, the quality of these weighting functions was evaluated only through their impact on the performance of a particular word similarity measure, as we did in Section 5.</S>
			<S sid ="340" ssid = "51">Our acfr-ratio measure provides the ﬁrst attempt to analyze the quality of weighting functions directly, relative to a prejudged word similarity set, without reference to a concrete similarity measure.</S>
			<S sid ="341" ssid = "52">6.3 An Empirical Assessment of the acfr-ratio.</S>
			<S sid ="342" ssid = "53">In this subsection we report an empirical comparison of the acfr-ratio obtained for the MI and BootstrappedLIN weighting functions.</S>
			<S sid ="343" ssid = "54">To that end, we have run the Minipar system on the full Reuters RCV1 corpus, which contains 2.5 GB of English news stories, and then calculated the MI-weighted feature vectors.</S>
			<S sid ="344" ssid = "55">The optimized threshold on the feature weights, θweight , was set to 0.2.</S>
			<S sid ="345" ssid = "56">Further, to compute the Bootstrapped LIN feature weights a θsim of 0.02 was applied to the LIN similarity values.</S>
			<S sid ="346" ssid = "57">In this experiment we employed the full bootstrapped vectors (i.e., without applying feature reduction by the top 100 cutoff).</S>
			<S sid ="347" ssid = "58">This was done to avoid the effect of the feature vector size on the acfrn metric, which tends to naturally assign higher scores to shorter vectors.</S>
			<S sid ="348" ssid = "59">As computing the acfr-ratio requires a prejudged sample of candidate word similarity pairs, we utilized the annotated test sample of candidate pairs of word similarities described in Section 5, which contains both entailing and non-entailing pairs.</S>
			<S sid ="349" ssid = "60">First, we computed the average common-feature rank scores (acfrn ) (with varying values of n) for weightMI and for weightB over all the pairs in the test sample.</S>
			<S sid ="350" ssid = "61">Interestingly, the mean acfrn scores for weightB range within 110–264 for n = 10...100, while the corresponding range for weightMI is by an order of magnitude higher: 780–1,254, despite the insigniﬁcant differences in vector sizes.</S>
			<S sid ="351" ssid = "62">Therefore, we conclude that the common features that are relevant to establishing distributional similarity in general (regardless of entailment) are much more scattered across the vectors by MI weighting, while with bootstrapping they tend to appear at higher positions in the vectors.</S>
			<S sid ="352" ssid = "63">These ﬁgures 454 reﬂect a desired behavior of the bootstrapping function which concentrates most of the prominent common features for all the distributionally similar words (whether entailing or not) at the lower ranks of their vectors.</S>
			<S sid ="353" ssid = "64">In particular, this explains the ability of our method to perform a massive feature reduction as demonstrated in Section 5, and to produce more informative vectors, while demoting and eliminating much of the noise in the original vectors.</S>
			<S sid ="354" ssid = "65">Next, we aim to measure the discriminative power of the compared methods to distinguish between entailing and non-entailing pairs.</S>
			<S sid ="355" ssid = "66">To this end we calculated the acfr-ratio, which captures the difference in the average common feature ranks between entailing vs. non-entailing pairs, for both the MI-based and bootstrapped vectors.</S>
			<S sid ="356" ssid = "67">The obtained results are presented in Figure 2.</S>
			<S sid ="357" ssid = "68">As can be seen the acfr-ratio values are consistently higher for Bootstrapped LIN than for MI.</S>
			<S sid ="358" ssid = "69">That is, the bootstrapping method assigns much higher acfrn scores to entailing words than to non-entailing ones, whereas for MI the corresponding acfrn scores for entailing and non-entailing pairs are roughly equal.</S>
			<S sid ="359" ssid = "70">In particular, we notice that the largest gaps in acfr-ratio occur for lower numbers of top common features, whose weights are indeed the most important and inﬂuential in distributional similarity measures.</S>
			<S sid ="360" ssid = "71">Thus, these ﬁndings suggest a direct indication of an improved quality of the bootstrapped feature vectors.</S>
	</SECTION>
	<SECTION title="A Pseudo-Word  Sense  Disambiguation Evaluation" number = "7">
			<S sid ="361" ssid = "1">The lexical entailment evaluation reported herein corresponds to the lexical substitution application of distributional similarity.</S>
			<S sid ="362" ssid = "2">The other type of application, as reviewed in the Introduction, is similarity-based prediction of word co-occurrence likelihood, needed for disambiguation applications.</S>
			<S sid ="363" ssid = "3">Comparative evaluations of distributional similarity methods for this type of application were commonly conducted using a pseudo-word sense disambiguation scheme, which is replicated here.</S>
			<S sid ="364" ssid = "4">In the next subsections we ﬁrst describe how distributional similarity can help improve word sense disambiguation (WSD).</S>
			<S sid ="365" ssid = "5">Then we describe how the pseudo-word sense disambiguation task, which Figure 2 Comparison between the acfr-ratio for MI and Bootstrapped LIN methods, when using varying numbers of common top-ranked features in the words’ feature vectors.</S>
			<S sid ="366" ssid = "6">455 corresponds to the general WSD setting, was used to evaluate the co-occurrence likelihood predictions obtained by alternative similarity methods.</S>
			<S sid ="367" ssid = "7">7.1 Similarity Modeling for Word Sense Disambiguation.</S>
			<S sid ="368" ssid = "8">WSD methods need to identify the correct sense of an ambiguous word in a given context.</S>
			<S sid ="369" ssid = "9">For example, a test instance for the verb save might be presented in the context saving Private Ryan.</S>
			<S sid ="370" ssid = "10">The disambiguation method must decide whether save in this particular context means rescue, preserve, keep, lay aside, or some other alternative.</S>
			<S sid ="371" ssid = "11">Sense recognition is typically based on context features collected from a sense- annotated training corpus.</S>
			<S sid ="372" ssid = "12">For example, the system might learn from the annotated training data that the word soldier is a typical object for the rescuing sense of save, as in: They saved the soldier.</S>
			<S sid ="373" ssid = "13">In this setting, distributional similarity is used to reduce the data sparseness problem via similarity-based generalization.</S>
			<S sid ="374" ssid = "14">The general idea is to predict the likelihood of unobserved word co-occurrences based on observed co-occurrences of distributionally similar words.</S>
			<S sid ="375" ssid = "15">For example, assume that the noun private did not occur as a direct object of save in the training data.</S>
			<S sid ="376" ssid = "16">Yet, some of the words that are distributionally similar to private, like soldier or sergeant, might have occurred with save.</S>
			<S sid ="377" ssid = "17">Thus, a WSD system may infer that the co-occurrence save private is more likely for the rescuing sense of save because private is distributionally similar to soldier, which did co- occur with this sense of save in the annotated training corpus.</S>
			<S sid ="378" ssid = "18">In general terms, the WSD method estimates the co-occurrence likelihood for the target sense and a given context word based on training data for words that are distributionally similar to the context word.</S>
			<S sid ="379" ssid = "19">This idea of similarity-based estimation of co-occurrence likelihood was applied in Dagan, Marcus, and Markovitch (1995) to enhance WSD performance in machine translation and recently in Gliozzo, Giuliano, and Strapparava (2005), who employed a Latent Semantic Analysis (LSA)-based kernel function as a similarity-based representation for WSD.</S>
			<S sid ="380" ssid = "20">Other works employed the same idea for pseudo-word sense disambiguation, as explained in the next subsection.</S>
			<S sid ="381" ssid = "21">7.2 The Pseudo-Word Sense Disambiguation Setting.</S>
			<S sid ="382" ssid = "22">Sense disambiguation typically requires annotated training data, created with considerable human effort.</S>
			<S sid ="383" ssid = "23">Yarowsky (1992) suggested that when using WSD as a test bed for comparative algorithmic evaluation it is possible to set up a pseudo-word sense disambiguation scheme.</S>
			<S sid ="384" ssid = "24">This scheme was later adopted in several experiments, and was popular for comparative evaluations of similarity-based co-occurrence likelihood estimation (Dagan, Lee, and Pereira 1999; Lee 1999; Weeds and Weir 2005).</S>
			<S sid ="385" ssid = "25">We followed closely the same experimental scheme, as described subsequently.</S>
			<S sid ="386" ssid = "26">First, a list of pseudo-words is constructed by “merging” pairs of words into a single pseudo word.</S>
			<S sid ="387" ssid = "27">In our experiment each pseudo-word constitutes a pair of randomlychosen verbs, (v, v· ), where each verb represents an alternative “sense” of the pseudo word.</S>
			<S sid ="388" ssid = "28">The two verbs are chosen to have almost identical probability of occurrence, which avoids a word frequency bias on the co-occurrence likelihood predictions.</S>
			<S sid ="389" ssid = "29">Next, we consider occurrences of pairs of the form (n, (v, v’)∗ , where (v, v· ) is a pseudo-word and n is a noun representing the object of the pseudo-word.</S>
			<S sid ="390" ssid = "30">Such pairs are constructed from all co-occurrences of either v or v· with the object n in the corpus.</S>
			<S sid ="391" ssid = "31">For example, given the pseudo-word (rescue, keep) and the verb–object co-occurrence in the corpus rescue–private we construct the pair (private, (rescue, keep)∗.</S>
			<S sid ="392" ssid = "32">Given such a test 456 pair, the disambiguation task is to decide which of the two verbs is more likely to co- occur with the given object noun, aiming to recover the original verb from which this pair was constructed.</S>
			<S sid ="393" ssid = "33">In this example we would like to predict that rescue is more likely to co-occur with private as an object than keep.</S>
			<S sid ="394" ssid = "34">In our experiment 80% of the constructed pairs were used for training, providing the co-occurrence statistics for the original known verb in each pair (i.e., either (n, v∗ or (n, v’∗).</S>
			<S sid ="395" ssid = "35">From the remaining 20% of the pairs those occurring in the training corpus were discarded, leaving as a test set only pairs which do not appear in the training part.</S>
			<S sid ="396" ssid = "36">Thus, predicting the co-occurrence likelihood of the noun with each of the two verbs cannot rely on direct frequency estimation for the co-occurrences, but rather only on similarity-based information.</S>
			<S sid ="397" ssid = "37">To make the similarity-based predictions we ﬁrst compute the distributional similarity scores for all pairs of nouns based on the training set statistics, where the co- occurring verbs serve as the features in the distributional vectors of the nouns.</S>
			<S sid ="398" ssid = "38">Then, given a test pair ((v,v’), n∗ our task is to predict which of the two verbs is more likely to co-occur with n. This verb is thus predicted as being the original verb from which the pair was constructed.</S>
			<S sid ="399" ssid = "39">To this end, the noun n is substituted in turn with each of its k distributionally most similar nouns, ni , and then both of the obtained “similar ” pairs (ni , v∗ and (ni , v·∗ are sought in the training set.</S>
			<S sid ="400" ssid = "40">Next, we would like to predict that the more likely co-occurrence between (n, v∗ and (n, v’∗ is the one for which more pairs of similar words were found in the training set.</S>
			<S sid ="401" ssid = "41">Several approaches were used in the literature to quantify this decision procedure and we have followed the most recent one from Weeds and Weir (2005).</S>
			<S sid ="402" ssid = "42">Each similar noun ni is given a vote, which is equal to the difference between the frequencies of the two co-occurrences (ni , v) and (ni , v· ), and which it casts to the verb with which it co-occurs more frequently.</S>
			<S sid ="403" ssid = "43">The votes for each of the two verbs are summed over all k similar nouns ni and the one with most votes wins.</S>
			<S sid ="404" ssid = "44">The winning verb is considered correct if it is indeed the original verb from which the pair was constructed, and a tie is recorded if the votes for both verbs are equal.</S>
			<S sid ="405" ssid = "45">Finally, the overall performance of the prediction method is calculated by its error rate: error = 1 (#of incorrect choices + #of ties ) (9) T 2 where T is the number of test instances.</S>
			<S sid ="406" ssid = "46">In the experiment, we used the 1,000 most frequent nouns in our subset of the Reuters corpus (of Section 5.1).</S>
			<S sid ="407" ssid = "47">The training and test data were created as described herein, using the Minipar parser (Lin 1993) to produce verb–object co-occurrence pairs.</S>
			<S sid ="408" ssid = "48">The k= 40 most similar nouns for each test noun were computed by each of the three examined similarity measures LIN, WJ, and COS (as in Section 5), with and without bootstrapping.</S>
			<S sid ="409" ssid = "49">The six similarity lists were utilized in turn for the pseudo-word sense disambiguation task, calculating the corresponding error rate.</S>
			<S sid ="410" ssid = "50">7.3 Results.</S>
			<S sid ="411" ssid = "51">Table 11 shows the error rate improvements after applying the bootstrapped weighting for each of the three similarity measures.</S>
			<S sid ="412" ssid = "52">The largest error reduction, by over 15%, was obtained for the LIN method, with quite similar results for WJ.</S>
			<S sid ="413" ssid = "53">This result is better than the one reported by Weeds and Weir (2005), who achieved about 6% error reduction compared to LIN.</S>
			<S sid ="414" ssid = "54">457 Table 11 The comparative error rates of the pseudo-disambiguation task for the three examined similarity measures, with and without applying the bootstrapped weighting for each of them.</S>
			<S sid ="415" ssid = "55">Measure LIN–LINB WJ–WJB COS–COSB Error rate 0.157–0.133 0.150–0.132 0.155–0.145 This experiment shows that learning tighter semantic similarities, based on the improved bootstrapped feature vectors, correlates also with better similarity-based inference for co-occurrence likelihood prediction.</S>
			<S sid ="416" ssid = "56">Furthermore, we have seen once again that the bootstrapping scheme does not depend on a speciﬁc similarity measure, reducing the error rates for all three measures.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "8">
			<S sid ="417" ssid = "1">The primary contribution of this article is the proposal of a bootstrapping method that substantially improves the quality of distributional feature vectors, as needed for statistical word similarity.</S>
			<S sid ="418" ssid = "2">The main idea is that features which are common for similar words are also most characteristic for their meanings and thus should be promoted.</S>
			<S sid ="419" ssid = "3">In fact, beyond its intuitive appeal, this idea corresponds to the underlying rationale of the distributional similarity scheme: Semantically similar words are expected to share exactly those context features which are most characteristic for their meaning.</S>
			<S sid ="420" ssid = "4">The superior empirical performance of the resulting vectors was assessed in the context of the two primary applications of distributional word similarity.</S>
			<S sid ="421" ssid = "5">The ﬁrst is lexical substitution, which was represented in our work by a human gold standard for the substitutable lexical entailment relation.</S>
			<S sid ="422" ssid = "6">The second is co-occurrence likelihood prediction, which was assessed by the automatically computed scores of the common pseudo-word sense disambiguation evaluation.</S>
			<S sid ="423" ssid = "7">An additional outcome of the improved feature weighting is massive feature reduction.</S>
			<S sid ="424" ssid = "8">Experimenting with three prominent similarity measures showed that the boot- strapping scheme is robust and performs well when applied over different measures.</S>
			<S sid ="425" ssid = "9">Notably, our experiments show that the underlying assumption behind the boot- strapping scheme is valid, that is, available similarity metrics do provide a reasonable approximation of the semantic similarity space which can be then exploited via bootstrapping.</S>
			<S sid ="426" ssid = "10">The methodology of our investigation has yielded several additional contributions: 1.</S>
			<S sid ="427" ssid = "11">Utilizing a reﬁned deﬁnition of substitutable lexical entailment both as an.</S>
			<S sid ="428" ssid = "12">end goal and as an analysis vehicle for distributional similarity.</S>
			<S sid ="429" ssid = "13">It was shown that the reﬁned deﬁnition can be judged directly by human subjects with very good agreement.</S>
			<S sid ="430" ssid = "14">Overall, lexical entailment is suggested as a useful model for lexical substitution needs in semantic-oriented applications.</S>
			<S sid ="431" ssid = "15">2.</S>
			<S sid ="432" ssid = "16">A thorough error analysis of state of the art distributional similarity performance was conducted.</S>
			<S sid ="433" ssid = "17">The main observation was deﬁcient quality of the feature vectors, which reduces the eventual quality of similarity measures.</S>
			<S sid ="434" ssid = "18">458 3.</S>
			<S sid ="435" ssid = "19">Inspired by the qualitative analysis, we proposed a new analytic measure.</S>
			<S sid ="436" ssid = "20">for feature vector quality, namely average common-feature rank ratio (acfr-ratio), which is based on the common ranks of the features for pairs of words.</S>
			<S sid ="437" ssid = "21">This measure estimates the ability of a feature weighting method to distinguish between pairs of similar vs. non-similar words.</S>
			<S sid ="438" ssid = "22">To the best of our knowledge this is the ﬁrst proposed measure for direct analysis of the quality of feature weighting functions, without the need to employ them within some vector similarity measure.</S>
			<S sid ="439" ssid = "23">The ability to identify the most characteristic features of words can have additional ben- eﬁts, beyond their impact on traditional word similarity measures (as evaluated in this article).</S>
			<S sid ="440" ssid = "24">A demonstration of such potential appears in Geffet and Dagan (2005), which presents a novel feature inclusion scheme for vector comparison.</S>
			<S sid ="441" ssid = "25">That scheme utilizes our bootstrapping method to identify the most characteristic features of a word and then tests whether these particular features co-occur also with a hypothesized entailed word.</S>
			<S sid ="442" ssid = "26">The empirical success reported in that paper provides additional evidence for the utility of the bootstrapping method.</S>
			<S sid ="443" ssid = "27">More generally, our motivation and methodology can be extended in several directions by future work on acquiring lexical entailment or other lexical-semantic relations.</S>
			<S sid ="444" ssid = "28">One direction is to explore better vector comparison methods that will utilize the improved feature weighting, as shown in Geffet and Dagan (2005).</S>
			<S sid ="445" ssid = "29">Another direction is to integrate distributional similarity and pattern-based acquisition approaches, which were shown to provide largely complementary information (Mirkin, Dagan, and Geffet 2006).</S>
			<S sid ="446" ssid = "30">An additional potential is to integrate automatically acquired relationships with the information found in WordNet, which seems to suffer from several serious limitations (Curran 2005), and typically overlaps to a rather limited extent with the output of automatic acquisition methods.</S>
			<S sid ="447" ssid = "31">As a parallel direction, future research should explore in detail the impact of different lexical-semantic acquisition methods on text understanding applications.</S>
			<S sid ="448" ssid = "32">Finally, our proposed bootstrapping scheme seems to have a general appeal for improving feature vector quality in additional unsupervised settings.</S>
			<S sid ="449" ssid = "33">We thus hope that this idea will be explored further in other NLP and machine learning contexts.</S>
	</SECTION>
</PAPER>
