Learning  Semantics and Selectional Preference of Adjective-Noun  Pairs



Karl Moritz Hermann 
Department of Computer Science 
University of Oxford
Oxford OX1 3QD, UK
karl.moritz.hermann@cs.ox.ac.uk



Chris Dyer
Language Technologies Institute 
Carnegie Mellon University Pittsburgh, 
PA, 15213, USA 
cdyer@cs.cmu.edu

Phil Blunsom Department of 
Computer Science University of 
Oxford
Oxford OX1 3QD, UK
phil.blunsom@cs.ox.ac.uk


Stephen Pulman Department of 
Computer Science University of 
Oxford
Oxford OX1 3QD, 
UK
stephen.pulman@cs.ox.ac.uk

Abstract

We investigate the semantic relationship  be- 
tween a  noun and its adjectival modifiers. 
We  introduce a  class of probabilistic mod- 
els that enable us to to simultaneously cap- 
ture both the semantic similarity  of  nouns 
and modifiers, and adjective-noun selectional 
preference. Through a combination  of novel 
and existing evaluations we test the degree to 
which adjective-noun relationships can be cat- 
egorised. We analyse the effect of lexical con- 
text on these relationships, and the efficacy of 
the latent semantic representation for disam- 
biguating word meaning.


1   Introduction

Developing models of the meanings of words and 
phrases is a key challenge for computational linguis- 
tics. Distributed representations are useful in captur- 
ing such meaning for individual words (Sato et al.,
2008; Maas and Ng, 2010; Curran, 2005). How- 
ever, finding a compelling  account of semantic com- 
positionality that utilises such representations has 
proven more difficult and is an active research topic 
(Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). It is in 
this area that our paper makes its contribution.
  The dominant approaches  to distributional se- 
mantics have relied on relatively simple frequency 
counting techniques. However, such approaches fail 
to generalise to the much sparser distributions en- 
countered when modeling  compositional  processes 
and provide no account of selectional  preference. 
We  propose a probabilistic model of the semantic


tion of noun and adjective semantics together with 
their compositional probabilities. We employ this 
formulation to give  a dual view of noun-modifier 
semantics:  the induced latent variables provide an 
explicit account of selectional preference while the 
marginal distributions of the latent variables for each 
word implicitly produce a distributed representation.
  Most related work on selectional  preference uses 
class-based  probabilities to approximate (sparse) 
individual probabilities.  Relevant papers  include
O´  Se´aghdha  (2010), who evaluates  several  topic
models adapted to learning selectional  preference 
using co-occurence and Baroni and Zamparelli 
(2010), who represent nouns as vectors  and adjec- 
tives as matrices, thus treating them as  functions 
over noun meaning. Again, inference is achieved 
using co-occurrence and dimensionality  reduction.

2   Adjective-Noun Model

We hypothesize that semantic classes determine the 
semantic characteristics of nouns and adjectives, and 
that the distribution of either with respect to other 
components of the sentences they occur in is also 
mediated by these  classes (i.e., not by the words 
themselves).  We assume that in general nouns select 
for adjectives,1  and that this selection is dependent 
on both their latent semantic classes. In the next sec- 
tion, we describe a model encoding our hypotheses.

2.1   Generative Process
We  model a  corpus D  of  tuples of  the form 
(n, m, c1 . . . ck ) consisting of a noun n, an adjective 
m (modifier), and k words of context. The context 
variables (c1 . . . ck ) are treated as a bag of words and


representations for nouns and modifiers.  The foun-	 	


dation of this model is a latent variable  representa-


1 We evaluate this hypothesis  as well as its inverse.




70


First Joint Conference on Lexical and Computational  Semantics (*SEM),  pages 70–74, 
Montre´al,  Canada, June 7-8, 2012. Qc 2012 Association for Computational Linguistics



αc	 αN 	αM


2.2	Parameterization and 
Inference
We use Gibbs sampling to estimate 
the distributions of N and M , 
integrating out the multinomial 
param-
x


Ψc	 ΨN 	ΨM


eters Ψ


(Griffiths and Steyvers, 2004). The 
Dirich-


|N|



c
k


|N|



N 	M



n 	m
|D|


let parameters α 
are drawn 
independently from 
a
Γ(1, 1) distribution, 
and are resampled 
using slice sampling 
at frequent intervals 
throughout the sam- 
pling process 
(Johnson and 
Goldwater,  2009). 
This “vague” prior 
encourages  sparse  
draws from the 
Dirichlet 
distribution. The 
number of noun and 
ad- jective  classes N 
and M was set to 
50 each; other sizes 
(100,150) did not 
significantly alter 
results.





Ψn	 Ψm
|N| 	|M|


αn	 αm

Figure 1: Plate diagram illustrating our model of noun 
and modifier semantic classes (designated N and M , re- 
spectively), a modifier-noun  pair (m,n), and its context.

include the words to the left and right of the noun, 
its siblings and governing verbs. We designate the 
vocabulary Vn  for nouns, Vm  for modifiers and Vc 
for context. We use zi  to refer to the ith  tuple in D 
and refer to variables within that tuple by subscript- 
ing them with i, e.g., ni  and c3,i  are the noun and 
the third context variable of zi. The latent noun and 
adjective class variables are designated Ni  and Mi.
  The corpus D is generated according to the plate 
diagram in figure 1.  First, a set of parameters is 
drawn. A multinomial ΨN representing the distribu- 
tion of noun semantic classes in the corpus is drawn 
from a Dirichlet distribution with parameter αN. For


3   Experiments

As our model was developed on the basis of several 
hypotheses, we design the experiments and evalu- 
ation so that these hypotheses can be examined on 
their individual merit. We test the first hypothesis, 
that nouns and adjectives can be represented by se- 
mantic classes, recoverable using co-occurence, us- 
ing a sense clustering  evaluation  by Ciaramita and 
Johnson (2003). The second hypothesis, that the dis- 
tribution with respect to context and to each other is 
governed by these semantic classes is evaluated us- 
ing pseudo-disambiguation (Clark and Weir, 2002; 
Pereira et al., 1993; Rooth et al., 1999) and bigram 
plausibility (Keller and Lapata, 2003) tests.
  To test whether noun classes indeed select for ad- 
jective  classes, we also evaluate an inverse model 
(M odi), where the adjective class is drawn first, in 
turn generating both context and the noun class. In 
addition, we evaluate copies of both models ignoring 
context (M odnc and M odinc).
We use the British National Corpus (BNC), train-


each noun class i we have distributions ΨM


over


ing on 90 percent and testing on 10 percent of 
the


adjective  classes, Ψn over Vn and Ψc over Vc, also
i	i	corpus. Results are reported after 2,000 iterations


drawn from Dirichlet distributions. Finally, for each
adjective class j, we have distributions Ψm over Vm.
Next, the contents of the corpus are generated by
first drawing the length of the corpus (we do not 
parametrise this since we never generate from this 
model). Then, for each i, we generate noun class 
Ni, adjective class Mi, and the tuple zi  as follows:
Ni  | ΨN  ∼ Multi(ΨN)
Mi  | ΨM  ∼ Multi(ΨM )


including a burn-in period of 200 iterations. Classes 
are marginalised over every 10th iteration.

4   Evaluation

4.1   Supersense Tagging
Supersense tagging  (Ciaramita  and Johnson, 2003; 
Curran, 2005) evaluates a model’s  ability to clus- 
ter words by their semantics. The task of this eval-


Ni
ni  | Ψn
mi  | Ψm
Mi
∀k: ck,i  | Ψc


Ni
∼ Multi(Ψn   )
∼ Multi(Ψm   )
Mi
∼ Multi(Ψc    )


uation is to determine 
the WORDNET 
supersenses
of a given list of nouns. 
We report results on the 
WN1.6 test set as 
defined  by Ciaramita and 
John- son (2003), who 
used 755 randomly 
selected nouns with a 
unique  supersense from 
the WORDNET 1.6


corpus. As their test set was random, results weren’t 
exactly replicable. For a fair comparison, we select 
all suitable nouns from the corpus that also appeared 
in the training corpus. We report results on type and 
token level (52314 tokens with 1119 types).  The 
baseline2  chooses the most common supersense.


k
T
o
k
e
n	Type
Ba
sel
ine

.
2
4
1
	.210
Ci
ar
am
ita 
& 
Jo
hn
so
n
Cu
rra
n

.
5
2
3
	
.
5
3
4
-
 
	
.
6
8
0
M 
od
M 
od
nc
10
10
.
5
9
2
	.517
.
4
7
3
	.410
Table 1:  Supersense evaluation  results. Values are the 
percentage of correctly assigned supersenses. k indicates 
the number of nearest neighbours considered.
  We use cosine-similarity on the marginal noun 
class vectors to measure distance between nouns. 
Each noun in the test set is then assigned  a  su- 
persense by performing  a distance-weighted voting 
among its k nearest neighbours.  Results of this eval- 
uation are shown in Table 1, with Figure 2 showing 
scores for model M od across different  values for k.













Figure 2: Scores of M od on the supersense task. The up- 
per line denotes token-, the lower type-level scores. The 
y-axis is the percentage of correct assignments, the x-axis 
denotes the number of neighbours included in the vote.

  The results demonstrate that nouns can semanti- 
cally be represented  as members  of latent classes, 
while the superiority of M od over M odnc supports 
our hypothesis that context co-occurence is a key 
feature for learning these classes.

4.2   Pseudo-Disambiguation

Pseudo-disambiguation   was  introduced by  Clark 
and Weir (2002) to evaluate models of selectional 
preference.  The task is to select the more probable


predicate. For us, this is to decide which adjective,
a1 or a2, is more likely to modify a noun n.
We follow the approach by Clark and Weir (2002)
to create the test data. To improve the quality of 
the data, we filtered using bigram counts from the 
Web1T corpus, setting a lower bound on the proba- 
ble bigram (a1, n) and chosing a2 from five candi- 
dates, picking the lowest count for bigram (a2, n).
We report results for all variants of our model in
Table 2. As baseline we use unigram counts in our 
training data, chosing the more frequent adjective.

L-
bo
un
d
Siz
e
0
	
1
0
0
	
5
0
0
	
1
0
0
0
5
7
1
4	5253	3741	2789
Ba
sel
ine
.
5
4
3	.543	.539	.550
M 
od
M 
odi
.
7
8
3	.792	.810	.816
.
7
8
1	.787	.800	.810
M 
od
nc
M 
odi
nc
.
7
2
0	.728	.746	.750
.
7
2
2	.730	.747	.752
Table 2: Pseudo-disambiguation:   Percentage of correct 
choices made. L-bound denotes the Web1T lower bound 
on the (a1 , n) bigram, size the number of decisions made.
  While all models decisively beat the baseline, the 
models using context strongly outperform those that 
do not. This supports our hypothesis regarding the 
importance of context in semantic clustering.
  The similarity between the normal and inverse 
models implies that the direction of  the noun- 
adjective relationship  has negligible impact for this 
evaluation.

4.3   Bigram Plausibility
Bigram plausibility (Keller and Lapata, 2003) is a 
second evaluation for selectional preference. Unlike 
the frequency-based pseudo-disambiguation  task, it 
evaluates how well a model matches human judge- 
ment of  the plausibility  of  adjective-noun  pairs. 
Keller and Lapata (2003) demonstrated a correlation 
between frequencies and plausibility, but this does 
not sufficiently explain human judgement. An ex- 
ample taken from their unseen data set illustrates the 
dissociation between frequency and plausibility:
• Frequent, implausible: “educational water”
• Infrequent, plausible: “difficult foreigner”3
The plausibility evaluation  has two data sets of 90
adjective-noun pairs each. The first set (seen) con- 
tains random bigrams from the BNC. The second set 
(unseen) are bigrams not contained in the BNC.


of two candidate arguments to associate with a given	 	
3 At the time of writing, Google estimates 56,900 hits for


     2 The baseline results are from  Ciaramita and Johnson 
(2003). Using the majority baseline on the full test set, we only 
get .176 and .160 for token and type respectively.


“educational water” and 575 hits for “difficult foreigner”. “Ed- 
ucational water” ranks bottom in the gold standard of the unseen 
set, “difficult  foreigner” ranks in the top ten.


Recent work  (O´



Se´aghdha,  2010; 
Erk et al.,



plausibility 
judgement, which — 
as we have demon-


2010) approximated plausibility with joint probabil- 
ity (JP). We believe that for semantic plausibility 
(not probability!)  mutual information (MI), which 
factors out acutal frequencies, is a better metric.4 We 
report results using JP, MI and MIˆ2.


S
e
e
n
U
n
s
e
e
n

r
	ρ
r
	ρ
Alt
aV
ist
a
B
N
C 
(R
as
p)
.6
5
0	—
.5
4
3	.622
.4
8
0	—
.1
3
5	.102
Pa
do´ 
et 
al.
.4
7
9	.570
.1
2
0	.138
LD
A
R
O
O
T
H
-
L
D
A
 
D
U
A
L
-
L
D
A
.5
9
4	.558
.5
7
5	.599
.4
6
0	.400
.4
6
8	.459
.5
0
1	.469
.3
3
4	.278
M 
od 
(JP
)
M 
od 
(M
I)
M 
od 
(M
Iˆ2
)
.4
9
5	.413
.3
9
4	.425
.5
7
5	.501
.2
8
6	.276
.4
7
1 	.457
.4
3
0	.408
M 
od
nc 
(JP
)
M 
od
nc 
(M
I)
M 
od
nc 
(M
Iˆ2
)
.6
2
6	.505
.6
2
8	.574
.7
0
1 	.623
.3
5
7	.369
.4
2
7	.385
.4
2
3	.394
Table 3: Results (Pearson r and Spearman ρ correlations) 
on the Keller and Lapata (2003) plausibility data. Bold 
indicates  best scores, underlining our best scores. High 
values indicate high correlation with the gold standard.
  Table 3 shows the performance of our models 
compared to results reported in O´ Se´aghdha (2010). 
As before, results between the normal and the in- 
verse model (omitted due to space) are very simi- 
lar. Surprisingly, the no-context models consistently 
outperform the models using context on the seen 
data set. This suggests that the seen data set can 
quite precisely be ranked using frequency estimates, 
which the no-context models might be better at cap- 
turing without the ‘noise’ introduced by context.


S
t
a
n
d
a
r
d
I
n
v
e
r
s
e
 
(
i
)

r
	ρ
r
	ρ
M 
od 
(JP
)
M 
od 
(M
I)
M 
od 
(M
Iˆ2
)
.2
8
6	.276
.4
7
1	.457
.4
3
0	.408
.2
4
3	.245
.4
0
9	.383
.3
6
2	.347
M 
od
nc 
(JP
)
M 
od
nc 
(M
I)
M 
od
nc 
(M
Iˆ2
)
.3
5
7	.369
.4
2
7	.385
.4
2
3	.394
.1
8
1	.161
.2
2
0	.209
.2
1
8	.185
Table 4: Results on the unseen plausibility dataset.
  The results on the unseen  data set (Table 4) 
prove  interesting  as well.   The inverse no-context 
model is performing significantly poorer than any 
of the other models. To understand this result we 
must investigate the differences between the unseen 
data set and the seen data set and to the pseudo- 
disambiguation evaluation.   The key difference to 
pseudo-disambiguation is that we measure a human

4 See (Evert, 2005) for a discussion  of these metrics.


strated — only partially correlates with bigram fre- 
quencies. Our models were trained on the BNC, 
hence they could only learn frequency estimates for 
the seen data set, but not for the unseen data.
  Based on our hypothesis about the role of con- 
text, we expect M od and M odi  to learn semantic 
classes based on the distribution of context. Without 
the access to that context,  we argued that M odnc and
M odinc  would instead learn frequency estimates.5
The hypothesis that nouns generally  select for ad-
jectives rather than vice versa further suggests that 
M od and M odnc  would learn semantic properties 
that M odi  and M odinc  could not learn so well.
In summary, we hence expected M od to perform
best on the unseen data, learning semantics from 
both context and noun-adjective selection.  Also, as 
supported by the results, we expected M odinc  to 
performs poorly, as it is the model least capable of 
learning semantics according to our hypotheses.

5   Conclusion

We  have  presented  a class of probabilistic mod- 
els which successfully learn semantic clusterings of 
nouns and a representation  of adjective-noun selec- 
tional preference. These models encoded our beliefs 
about how adjective-noun pairs relate to each other 
and to the other words in the sentence. The perfor- 
mance of our models on estimating selectional pref- 
erence strongly  supported these initial hypotheses.
  We discussed plausibility judgements from a the- 
oretical perspective and argued that frequency esti- 
mates and JP are imperfect approximations for plau- 
sibility.   While models can perform well on some 
evaluations by using either frequency estimates or 
semantic knowledge,  we explained why this does 
not apply to the unseen plausibility test. The perfor- 
mance on that task demonstrates both the success of 
our model and the shortcomings of frequency-based 
approaches to human plausibility judgements.
  Finally, this paper demonstrated that it is feasi- 
ble to learn semantic representations of words while 
concurrently learning how they relate to one another.
  Future work will  explore  learning words from 
broader classes of semantic relations and the role of 
context in greater detail. Also, we will evaluate the 
system applied to higher level tasks.

  5 This  could also explain  their  weaker performance on 
pseudo-disambiguation in the previous section, where the neg- 
ative examples had zero frequency in the training corpus.


References

Marco Baroni and Roberto Zamparelli.  2010.  Nouns 
are vectors, adjectives are matrices:  representing 
adjective-noun constructions in semantic space. In 
Proceedings  of the 2010 Conference  on Empirical 
Methods in Natural Language  Processing,  EMNLP



Andrew L. Maas and Andrew Y. Ng. 2010. A probabilis- 
tic model for semantic word vectors. In Workshop on 
Deep Learning and Unsupervised Feature Learning, 
NIPS ’10.
Jeff Mitchell and Mirella Lapata. 2008.  Vector-based 
models of semantic composition.  In ACL-HLT’08, 
pages 236 – 244.


’10, pages 1183–1193, Stroudsburg,  PA, USA. Asso-


Diarmuid O´


Se´aghdha.   2010.  Latent variable models


ciation for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su- 
persense tagging of unknown nouns in wordnet.  In 
Proceedings of the 2003 conference  on Empirical 
methods in natural language processing, EMNLP ’03, 
pages 168–175, Stroudsburg,  PA, USA. Association 
for Computational Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob- 
ability estimation using a semantic hierarchy.  Comput. 
Linguist., 28:187–206, June.
James R. Curran. 2005. Supersense tagging of unknown 
nouns using semantic similarity.   In Proceedings of 
the 43rd Annual Meeting on Association for Compu- 
tational Linguistics, ACL ’05, pages 26–33, Strouds- 
burg, PA, USA. Association for Computational Lin- 
guistics.
Katrin Erk, Sebastian Pado´ , and Ulrike Pado´ .  2010. A 
flexible, corpus-driven model of regular and inverse 
selectional preferences. Computational Linguistics,
36:723–763.
Stefan Evert.   2005.   The statistics of word cooccur- 
rences:  word pairs  and collocations.    Ph.D. the- 
sis, Universita¨t  Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Edward Grefenstette and Mehrnoosh  Sadrzadeh.  2011.
Experimental support for a categorical  compositional 
distributional model of meaning.  In Proceedings of 
the Conference on Empirical Methods in Natural Lan- 
guage  Processing, EMNLP  ’11,  pages 1394–1404, 
Stroudsburg, PA, USA. Association for Computational 
Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004.  Find- 
ing scientific topics.   Proceedings  of the National 
Academy of Sciences, 101:5228–5235.
Mark Johnson and Sharon Goldwater. 2009. Improving 
nonparameteric bayesian inference: experiments on 
unsupervised word segmentation with adaptor gram- 
mars. In Proceedings of Human Language Technolo- 
gies: The 2009 Annual Conference of the North Ameri- 
can Chapter of the Association for Computational Lin- 
guistics, NAACL  ’09, pages 317–325, Stroudsburg, 
PA, USA. Association for Computational Linguistics.
Frank Keller and Mirella Lapata. 2003. Using the web to 
obtain frequencies for unseen bigrams.  Computational 
Linguistics, pages 459–484.
  

of selectional preference.  In Proceedings of the 48th 
Annual Meeting of the Association for Computational 
Linguistics, ACL  ’10, pages 435–444, Stroudsburg, 
PA, USA. Association for Computational Linguistics. 
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. 
Distributional clustering of English words.  In Pro- 
ceedings of the 31st annual meeting on Association for 
Computational Linguistics, ACL ’93, pages 183–190, 
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car- 
roll, and Franz Beil.  1999. Inducing a semantically 
annotated lexicon via EM-based clustering.  In Pro- 
ceedings of the 37th annual meeting of the Association 
for Computational Linguistics on Computational Lin- 
guistics, ACL ’99, pages 104–111,  Stroudsburg,  PA, 
USA. Association for Computational Linguistics.
Issei Sato, Minoru  Yoshida, and Hiroshi  Nakagawa.
2008. Knowledge discovery of semantic relationships 
between words using nonparametric  bayesian graph 
model. In Proceeding of the 14th ACM SIGKDD in- 
ternational conference on Knowledge discovery and 
data mining, KDD  ’08, pages 587–595, New York, 
NY, USA. ACM.

