GPLSI-IXA: Using Semantic Classes to Acquire  Monosemous Training
Examples from Domain Texts

Rube´n Izquierdo & Armando Sua´ rez
GPLSI Group
University of Alicante.  Spain
{ruben,armando}@dlsi.ua.es


German Rigau IXA NLP 
Group. EHU. Donostia, 
Spain
german.rigau@ehu.es

Abstract

This paper summarizes  our participation 
in  task #17 of  SemEval–2  (All–words 
WSD on a specific  domain)  using  a su- 
pervised class-based Word Sense Disam- 
biguation system. Basically, we use Sup- 
port Vector Machines (SVM) as learning 
algorithm  and a set of simple features to 
build three different models. Each model 
considers a different training corpus: Sem- 
Cor (SC), examples  from monosemous 
words extracted automatically from back- 
ground data (BG),  and both SC and 
BG (SCBG). Our system  explodes  the 
monosemous  words appearing   as  mem- 
bers of  a  particular WordNet semantic 
class to automatically  acquire class-based 
annotated examples from the domain text. 
We use the class-based examples gathered 
from the domain corpus to adapt our tra- 
ditional system trained on SemCor. The 
evaluation reveal that the best results are 
achieved  training with SemCor  and the 
background  examples from monosemous 
words, obtaining results  above  the first 
sense baseline  and the fifth best position 
in the competition rank.

1   Introduction

As empirically  demonstrated by the last SensEval 
and SemEval exercises, assigning the appropriate 
meaning to words in context  has resisted all at- 
tempts to be successfully addressed. In fact, super- 
vised word-based WSD systems are very depen- 
dent of the corpora used for training and testing 
the system (Escudero et al., 2000). One possible 
reason could be the use of inappropriate level of 
abstraction.


where each class corresponds to a particular synset 
of the word. But, WordNet (WN) has been widely 
criticized for being  a sense repository  that often 
provides too fine–grained  sense distinctions  for 
higher level applications like Machine Translation 
or Question & Answering. In fact, WSD at this 
level of granularity  has resisted all attempts of in- 
ferring robust broad-coverage models. It seems 
that many word–sense distinctions  are too subtle 
to be captured by automatic systems with the cur- 
rent small volumes of word–sense annotated ex- 
amples.
  Thus, some research has been focused on deriv- 
ing different word-sense groupings  to overcome 
the fine–grained distinctions of WN (Hearst and 
Schu¨ tze, 1993), (Peters et al., 1998), (Mihalcea 
and Moldovan, 2001), (Agirre and LopezDeLa- 
Calle, 2003), (Navigli, 2006) and (Snow et al.,
2007). That is, they provide methods for grouping 
senses of the same word, thus producing  coarser 
word  sense groupings for better disambiguation.
  In contrast, some research have been focused on 
using predefined sets of sense-groupings for learn- 
ing class-based classifiers for WSD (Segond et al.,
1997), (Ciaramita  and Johnson, 2003), (Villarejo 
et al., 2005), (Curran, 2005), (Kohomban and Lee,
2005) and (Ciaramita  and Altun, 2006). That is, 
grouping  senses of different words into the same 
explicit and comprehensive semantic class. Most 
of the later approaches used the original Lexico- 
graphical Files of WN (more recently called Su- 
perSenses)  as very coarse–grained  sense distinc- 
tions.
  We suspect that selecting the appropriate level 
of abstraction could be on between both levels. 
Thus, we use the semantic classes modeled by the 
Basic Level  Concepts1  (BLC) (Izquierdo et al.,
2007). Our previous research using BLC empiri- 
cally demonstrated that this automatically  derived


Most  supervised  systems  simply  model  each	 	


polysemous  word as  a  classification   problem


1 http://adimen.si.ehu.es/web/BLC




402

Proceedings of the 5th International  Workshop on Semantic Evaluation, ACL 2010, pages 402–406, 
Uppsala, Sweden, 15-16 July 2010. Qc 2010 Association for Computational Linguistics


set of meanings  groups  senses into an adequate 
level of abstraction in order to perform class-based 
Word  Sense Disambiguation (WSD) (Izquierdo et 
al., 2009). Now, we also show that class-based 
WSD allows to successfully incorporate monose- 
mous examples  from the domain text.  In fact,


3, and A has 2, so D is the first maximum).



A

2


the robustness of our class-based WSD approach 
is shown by our system that just uses the Sem-
Cor examples (SC). It performs without any kind


B	C	D

2	3


BLC 


of domain adaptation as the Most Frequent Sense
(MFS) baseline.
  This  paper describes our  participation in 
SemEval-2010  Task 17 (Agirre et al., 2010). In 
section 2 semantic classes used and selection  al- 
gorithm used to obtain them automatically  from 
WordNet are described. In section 3 the technique 
employed to extract monosemous examples from 
background data is described. Section 4 explains 
the general approach of our system, and the ex- 
periments designed, and finally, in section 5, the 
results and some analysis are shown.

2   Semantic Classes

The set of semantic  classes used in this work are 
the Basic Level  Concepts2  (BLC) (Izquierdo et 
al., 2007). These concepts are small sets of mean- 
ings representing the whole nominal and verbal 
part of WN. BLC can be obtained by a very simple 
method  that uses basic structural WordNet proper- 
ties. In fact, the algorithm only considers the rel- 
ative number of relations of each synset along the 
hypernymy  chain.  The process follows a bottom- 
up approach using the chain of hypernymy rela- 
tions. For each synset in WN, the process selects 
as its BLC the first local maximum according to 
the relative number of relations.  The local maxi- 
mum is the synset in the hypernymy chain having 
more relations than its immediate hyponym and 
immediate hypernym. For synsets having  multi- 
ple hypernyms,  the path having the local maxi- 
mum with higher number of relations is selected. 
Usually, this process finishes having a number of 
preliminary BLC. Figure 1 shows an example of 
selection of a BLC. The figure represents the hy- 
pernymy hierarchy of WordNet, with circles rep- 
resenting synsets, and links between them repre- 
senting hypernym relations. The algorithm selects 
the D synset  as BLC for J, due to D is the first 
maximum in the hypernymy chain, according to 
the number of relations (F has 2 hyponyms, D has

2 http://adimen.si.ehu.es/web/BLC


E	F	G	H

2

I	J	Synset 




Figure 1: Example of BLC selection


  Obviously, while ascending through this chain, 
more synsets are subsumed by each concept.  The 
process finishes checking  if the number of con- 
cepts subsumed by the preliminary list of BLC is 
higher  than a certain  threshold. For those BLC 
not representing enough concepts according to the 
threshold, the process selects the next local max- 
imum following the hypernymy hierarchy.  Thus, 
depending on the type of relations considered to 
be counted and the threshold established, different 
sets of BLC can be easily obtained for each WN 
version.
  We have selected the set which considers WN 
version 3.0, the total number of relations per 
synset, and a minimum threshold of 20 concepts to 
filter out not representative BLC (BLC–20). This 
set has shown to reach good performance  on previ- 
ous SensEval and SemEval exercices (Izquierdo et 
al., 2009). There are 649 different BLC for nouns 
on WordNet 3.0, and 616 for verbs. Table 2 shows 
the three most frequent BLC per POS, with the 
number of synsets subsumed by each concept, and 
its WordNet gloss.

3   Using Monosemous Examples from the
Domain

We did not applied any kind of specific domain 
adaptation technique to our class-based supervised 
system. In order to adapt our supervised system to 
the environmental domain we only increased the 
training  data with new examples of the domain. To 
acquire these examples, we used the environmen- 
tal domain background documents provided by the 
organizers.  Specifically,  we used the 122 back-



P
o
S
Nu
m.
B
L
C
Glo
ss

No
un
s
4.
79
2
1.
93
5
1.
84
6
p
er
s
o
n.
n.
0
1
a
cti
vi
ty
.n
.0
1 
ac
t.
n.
0
2
a 
hu
ma
n 
bei
ng
any 
sp
ecif
ic 
be
ha
vio
r
so
me
thin
g 
tha
t 
pe
opl
e 
do 
or 
cau
se 
to 
ha
pp
en

Ve
rb
s
1.
54
1
1.
08
5

5
1
9
ch
an
ge
.v.
01
ch
an
ge
.v.
02

m
ov
e.
v.
0
2
cau
se 
to 
ch
an
ge; 
ma
ke 
diff
ere
nt; 
cau
se 
a 
tra
nsf
or
ma
tio
n
u
n
d
er
g
o 
a 
ch
a
n
g
e; 
b
ec
o
m
e 
dif
fe
re
nt  
in 
es
se
nc
e; 
lo
si
ng  
on
e’
s 
or 
it
s 
or
ig
in
al 
n
a- 
tu
re
ca
us
e 
to 
m
ov
e 
or 
s
hi
ft 
in
to 
a 
n
e
w 
p
os
iti
o
n 
or 
pl
ac
e, 
b
ot
h 
in 
a 
co
nc
re
te 
an
d 
in 
a
n 
ab
str
ac
t 
se
ns
e

Table 1: Most frequent BLC–20 semantic classes on WordNet 3.0




ground  documents3.    TreeTagger  has been used 
to preprocess the documents, performing PoS tag- 
ging and lemmatization. Since the background 
documents are not semantically  annotated, and our 
supervised system needs labeled data, we have se- 
lected only the monosemous words occurring  in 
the documents. In this way, we have obtained au- 
tomatically  a large set of examples annotated with 
BLC. Table 3 presents the total number of training 
examples extracted from SemCor (SC) and from 
the background documents (BG). As expected, by 
this method  a large number  of monosemous ex- 
amples can be obtained for nouns and verbs. Also 
as expected, verbs are much less productive  than 
nouns. However, all these background  examples 
correspond to a reduced set of 7,646 monosemous 
words.


N
o
u
n
s
Ve
rb
s
N
+
V
S
C
B
G
8
7.
9
7
8
19
3.5
36
48.
26
7
10.
82
1
13
6.2
45
20
4.3
57
Tot
al
28
1.5
14
59.
08
8
34
0.6
02

Table 2: Number of training examples


  Table 3 lists the ten most frequent monosemous 
nouns and verbs occurring in the background doc- 
uments. Note that all these examples are monose- 
mous according to BLC–20 semantic classes.


N
o
u
n
s
V
e
r
b
s

L
e
m
m
a
# 
ex
.
Le
m
m
a
# 
ex.
1
2
3
4
5
6
7
8
9
10
bi
od
iv
er
sit
y
h
a
b
i
t
a
t 
s
p
e
c
i
e
 
c
l
i
m
a
t
e
 
e
u
r
o
p
e
a
n
 
e
c
o
s
y
s
t
e
m
 
r
i
v
e
r 
g
r
a
s
s
l
a
n
d
 
d
a
t
u
m
 
d
i
r
e
c
t
i
v
e
7.4
76
7.2
06
7.0
67
3.5
39
2.8
18
2.6
69
2.4
20
2.3
03
2.2
76
2.1
97
m
o
ni
to
r
a
c
hi
e
v
e 
ta
rg
et 
s
el
e
ct 
e
n
a
bl
e 
s
e
e
m 
pi
n
e 
e
v
al
u
at
e 
e
x
pl
or
e 
b
el
ie
v
e
78
8
78
4
48
4
34
5
33
4
28
7
28
1
24
6
20
0
17
2

Table 3: Most frequent monosemic words in BG


4   System Overview

Our system applies  a supervised  machine  learn- 
ing approach. We apply a feature  extractor  to 
represent the training examples of the examples 
acquired from SemCor and the background doc- 
uments. Then, a machine  learning  engine  uses 
the annotated examples to train a set  of classi- 
fiers. Support Vector Machines (SVM) have been 
proven to be robust and very competitive in many 
NLP tasks, and in WSD in particular (Ma`rquez et 
al., 2006). We used the SVM-Light implementa- 
tion4 (Joachims, 1998).
  We create a classifier  for each semantic  class. 
This approach has several advantages compared to 
word–based approach. The training data per clas- 
sifier is increased (we can use examples  of dif- 
ferent target words for a single  classifier,  when- 
ever all examples belong  to the same semantic 
class),  the polysemy is reduced (some different 
word senses can be collapsed into the same se- 
mantic  class), and, finally, semantic  classes pro- 
vide higher levels of abstraction.
  For each polysemous word occurring  in the test 
corpus, we obtain its potential BLC–20 classes. 
Then, we only apply the classifiers corresponding 
to the BLC-20 classes of the polysemous word. Fi- 
nally, our system simply  selects the BLC–20  class 
with the greater prediction.
  In order to obtain the correct WordNet 3.0 
synset required  by the task, we apply a  simple 
heuristic that has shown to be robust and accurate 
(Kohomban  and Lee, 2005). Our classifiers ob- 
tain first the semantic class, and then, the synset of 
the first WordNet sense that fits with the semantic 
class is assigned to the word.
  We selected a simple feature set widely used in 
many WSD systems. In particular,  we use a win- 
dow of five tokens around the target word to ex- 
tract word forms, lemmas; bigrams and trigrams
of word forms and lemmas; trigrams of PoS tags,


3 We used the documents contained on the trial data and	 	


the background.


4 
http://svmli
ght.joachim
s.org


and also the most frequent BLC–20 semantic class 
of the target word in the training corpus.
  Our system is fully described in (Izquierdo et 
al., 2009). The novelty introduced here is the use 
of semantic classes to obtain monosemous exam- 
ples from the domain corpus.
  Following the same framework   (BLC–20 se- 
mantic  architecture  and basic set of features) we 
designed three runs, each one using a  different 
training corpus.

• SC: only training examples  extracted  from
SemCor

• BG: only monosemous  examples  extracted 
from the background data

• SCBG: training examples  extracted   from
SemCor and monosemous background  data

  The first run shows the behavior of a supervised 
system trained on a general corpus, and tested in a 
specific domain. The second one analyzes the con- 
tribution of the monosemous examples extracted 
from the background data. Finally, the third run 
studies the robustness of the approach when com- 
bining the training examples from SemCor and 
from the background.

5   Results and Discussion

A total of 29 runs has been submitted  for the En- 
glish All–words  WSD on a Specific Domain.  Ta- 
ble 5 shows the ranking results of our three runs 
with respect to the other participants. The figures 
for the first sense (1sense) and random  sense (Ran- 
dom) baselines are included.
  In general,  the results  obtained  are not very 
high. The best system only achieves a precision of
0.570, and the first sense baseline reaches a preci- 
sion of 0.505. This shows that the task is hard to 
solve, and the domain adaptation of WSD systems 
is not an easy task.
  Interestingly, our worst result is obtained by the 
system using only the monosemous background 
examples (BG). This system ranks 23th with a Pre- 
cision and Recall of 0.380 (0.385 for nouns and
0.366 for verbs).  The system using only SemCor 
(SC) ranks 6th with Precision and Recall of 0.505 
(0.527 for nouns and 0.443 for verbs). This is also 
the performance of the first sense baseline.  As ex- 
pected, the best result of our three runs is obtained 
when combining  the examples from SemCor and 
the background (SCBG). This supervised system


obtains the 5th position with a Precision  and Re- 
call of 0.513 (0.534 for nouns, 0.454 for verbs) 
which is slightly above the baseline.

R
a
n
k
Pr
eci
sio
n
Re
cal
l
1
2
3
4
(S
CB
G) 
5
1se
nse
(S
C) 
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
(B
G) 
23
24
25
26
27
28
29
Ra
nd
om
0
.
5
7
0
0
.
5
5
4
0
.
5
3
4
0
.
5
2
2
0
.
5
1
3
0
.
5
0
5
0
.
5
0
5
0
.
5
1
2
0
.
5
0
6
0
.
5
0
4
0
.
4
8
1
0
.
4
9
2
0
.
4
6
1
0
.
4
4
7
0
.
4
3
6
0
.
4
4
0
0
.
4
9
6
0
.
4
9
8
0
.
4
3
3
0
.
4
2
6
0
.
4
2
4
0
.
4
3
7
0
.
3
8
4
0
.
3
8
0
0
.
3
8
1
0
.
3
5
1
0
.
3
7
0
0
.
3
2
8
0
.
3
2
1
0
.
3
1
2
0
.
2
3
0
0.
5
5
5
0.
5
4
0
0.
5
2
8
0.
5
1
6
0.
5
1
3
0.
5
0
5
0.
5
0
5
0.
4
9
5
0.
4
9
3
0.
4
9
1
0.
4
8
1
0.
4
7
9
0.
4
6
0
0.
4
4
1
0.
4
3
5
0.
4
3
4
0.
4
3
3
0.
4
3
2
0.
4
3
1
0.
4
2
5
0.
4
2
2
0.
3
9
2
0.
3
8
4
0.
3
8
0
0.
3
5
6
0.
3
5
0
0.
3
4
5
0.
3
2
2
0.
3
1
5
0.
3
0
3
0.
2
3
0

Table 4: Results of task#17

  Possibly, the reason of low performance of the 
BG system is the high correlation between the fea- 
tures of the target word and its semantic class. In 
this case, these features correspond to the monose- 
mous word while when testing corresponds to the 
target word.  However, it also seems that class- 
based systems are robust  enough  to incorporate 
large sets of monosemous examples from the do- 
main text. In fact, to our knowledge, this is the first 
time that a supervised WSD algorithm  have been 
successfully adapted to an specific domain. Fur- 
thermore, our system trained only on SemCor also 
achieves  a good performance, reaching the first 
sense baseline, showing  that class-based WSD  ap- 
proaches seem to be robust to domain variations.

Acknowledgments

This paper has  been supported  by the Euro- 
pean Union under the project KYOTO (FP7 ICT-
211423), the Valencian Region Government un- 
der PROMETEO project for excellence groups 
and the Spanish Government  under the projects


KNOW2 (TIN2009-14715-C04-04)  and TEXT- 
MESS-2 (TIN2009-13391-C04-04).


References

E. Agirre and O. LopezDeLaCalle.  2003. Clustering 
wordnet word senses. In Proceedings of RANLP’03, 
Borovets, Bulgaria.

Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell- 
baum, Shu kai Hsieh, Maurizio Tesconi, Mon- 
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific  domain. In Proceed- 
ings of the 5th International  Workshop on Semantic 
Evaluations (SemEval-2010), Association for Com- 
putational Linguistics.

M. Ciaramita and Y. Altun.  2006.  Broad-coverage 
sense disambiguation   and information extraction 
with a supersense sequence tagger.  In Proceedings 
of the Conference on Empirical  Methods in Natural 
Language Processing (EMNLP’06), pages 594–602, 
Sydney, Australia. ACL.

M. Ciaramita and M. Johnson.  2003. Supersense tag- 
ging of unknown nouns in wordnet. In Proceedings 
of the Conference on Empirical  methods in natural 
language processing (EMNLP’03),  pages 168–175. 
ACL.

J.  Curran.  2005.  Supersense tagging  of unknown 
nouns using semantic similarity. In Proceedings of 
the 43rd Annual Meeting on Association for Compu- 
tational Linguistics (ACL’05), pages 26–33. ACL.

G. Escudero, L. Ma`rquez,  and G. Rigau. 2000. An 
Empirical Study of the Domain Dependence of Su- 
pervised Word Sense Disambiguation  Systems.  In 
Proceedings of the joint SIGDAT Conference on Em- 
pirical Methods in Natural Language  Processing 
and Very Large Corpora, EMNLP/VLC, Hong Kong, 
China.

M. Hearst and H. Schu¨ tze. 1993. Customizing  a lexi- 
con to better suit a computational  task. In Proceed- 
ingns of the ACL SIGLEX Workshop on Lexical Ac- 
quisition, Stuttgart, Germany.

R. Izquierdo, A. Suarez, and G. Rigau. 2007. Explor- 
ing the automatic selection of basic level concepts. 
In Galia Angelova et al., editor, International Con- 
ference Recent Advances in Natural Language Pro- 
cessing, pages 298–302, Borovets,  Bulgaria.

Rube´n Izquierdo, Armando Sua´rez, and German Rigau.
2009. An empirical study on class-based word sense 
disambiguation. In Proceedings of the 12th Con- 
ference of the European Chapter of the ACL (EACL
2009), pages 389–397, Athens, Greece, March. As- 
sociation for Computational Linguistics.

T. Joachims. 1998.  Text categorization with sup- 
port vector machines: learning with many relevant


features.  In Claire Ne´dellec and Ce´line Rouveirol, 
editors, Proceedings of ECML-98, 10th European 
Conference on Machine Learning,  pages 137–142, 
Chemnitz, DE. Springer Verlag, Heidelberg, DE.

Upali S. Kohomban and Wee Sun Lee. 2005. Learning 
semantic classes for word  sense disambiguation. In 
ACL ’05: Proceedings of the 43rd Annual Meeting 
on Association for Computational Linguistics, pages
34–41, Morristown, NJ, USA. Association for Com- 
putational Linguistics.

Ll. Ma`rquez, G. Escudero, D. Mart´ınez, and G. Rigau.
2006. Supervised corpus-based methods for wsd. In 
E. Agirre and P. Edmonds  (Eds.)  Word Sense Disam- 
biguation: Algorithms and applications., volume 33 
of Text, Speech and Language Technology.  Springer.

R. Mihalcea and D. Moldovan. 2001. Automatic gen- 
eration of coarse grained wordnet. In Proceding of 
the NAACL workshop on WordNet and Other Lex- 
ical Resources: Applications,  Extensions and Cus- 
tomizations, Pittsburg, USA.

R. Navigli.   2006.  Meaningful clustering of senses 
helps boost word sense  disambiguation   perfor- 
mance. In ACL-44: Proceedings of the 21st Inter- 
national Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics, pages 105–112, Morris- 
town, NJ, USA. Association for Computational Lin- 
guistics.

W. Peters, I. Peters, and P. Vossen.  1998. Automatic 
sense clustering  in eurowordnet. In First Interna- 
tional  Conference on Language Resources and Eval- 
uation (LREC’98),  Granada, Spain.

F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997. An experiment in semantic tagging using hid- 
den markov model tagging. In ACL Workshop on 
Automatic Information Extraction and Building of 
Lexical  Semantic Resources for NLP Applications, 
pages 78–81. ACL, New Brunswick, New Jersey.

R. Snow, Prakash S., Jurafsky D., and Ng A.  2007.
Learning to merge word senses.  In Proceedings of 
Joint Conference on Empirical Methods in Natural 
Language  Processing and Computational Natural 
Language Learning (EMNLP-CoNLL), pages 1005–
1014.

L. Villarejo, L. Ma`rquez,  and G. Rigau. 2005. Ex- 
ploring the construction of semantic class classi- 
fiers for wsd.  In Proceedings of the 21th Annual 
Meeting of Sociedad Espaola para el Procesamiento 
del Lenguaje Natural SEPLN’05,  pages 195–202, 
Granada, Spain, September. ISSN 1136-5948.

