GPLSI: Word Coarse-grained Disambiguation aided by Basic Level
Concepts∗

Rube´n Izquierdo	Armando Sua´ rez
GPLSI Group, DLSI 
University of Alicante 
Spain
{ruben, armando}@dlsi.ua.es


German Rigau IXA 
NLP Group EHU/UPV
Donostia, Basque Country
german.rigau@ehu.es

Abstract

We present a corpus-based supervised lear- 
ning system for coarse-grained sense disam- 
biguation.  In addition to usual features for 
training in word sense disambiguation, our 
system also uses Base Level Concepts au- 
tomatically obtained from WordNet.  Base 
Level Concepts are some synsets that gene- 
ralize a hyponymy sub–hierarchy, and pro- 
vides an extra level of abstraction as well as 
relevant information about the context of a 
word to be disambiguated. Our experiments 
proved that using this type of features re- 
sults on a significant improvement of preci- 
sion. Our system has achieved almost 0.8 F1 
(fifth place) in the coarse–grained English 
all-words task using a very simple set of fea- 
tures plus Base Level Concepts annotation.

1   Introduction

The GPLSI system in SemEval’s task 7, coarse– 
grained English all-words, consists of a corpus- 
based supervised-learning method which uses lo- 
cal context information. The system uses Base Le- 
vel Concepts (BLC) (Rosch, 1977) as features.  In 
short, BLC are synsets of WordNet (WN) (Fell- 
baum, 1998) that are representative of a certain hy- 
ponymy sub–hierarchy.  The synsets that are se- 
lected to be BLC must accomplish certain condi- 
tions that will be explained in next section.  BLC
   ∗This paper has been supported by the European Union un- 
der the project QALL-ME (FP6 IST-033860) and the Spanish 
Government under the project Text-Mess (TIN2006-15265- 
C06-01) and KNOW (TIN2006-15049-C03-01)


are slightly different from Base Concepts of Eu- 
roWordNet1 (EWN) (Vossen et al., 1998), Balkanet2 
or Meaning Project3 because of the selection crite- 
ria but also because our method is capable to define 
them automatically. This type of features helps our 
system to achieve 0.79550 F1 (over the First–Sense 
baseline, 0.78889) while only four systems outper- 
formed ours being the F1 of the best one 0.83208.
  WordNet has been widely criticised for being a 
sense repository that often offers too fine–grained 
sense distinctions for higher level applications like 
Machine Translation or Question & Answering. In 
fact, WSD at this level of granularity, has resisted 
all attempts of inferring robust broad-coverage mo- 
dels. It seems that many word–sense distinctions are 
too subtle to be captured by automatic systems with 
the current small volumes of word–sense annotated 
examples. Possibly, building class-based classifiers 
would allow to avoid the data sparseness problem of 
the word-based approach.
  Thus, some research has been focused on deri- 
ving different sense groupings to overcome the fine– 
grained distinctions of WN (Hearst and Schu¨ tze,
1993) (Peters et al., 1998) (Mihalcea and Moldo- 
van, 2001) (Agirre et al., 2003) and on using predefi- 
ned sets of sense-groupings for learning class-based 
classifiers for WSD (Segond et al., 1997) (Ciaramita 
and Johnson, 2003) (Villarejo et al., 2005) (Curran,
2005) (Ciaramita and Altun, 2006). However, most 
of the later approaches used the original Lexico- 
graphical Files of WN (more recently called Super-

1 http://www.illc.uva.nl/EuroWordNet/
2 http://www.ceid.upatras.gr/Balkanet
3 http://www.lsi.upc.es/ nlp/meaning




157
Proceedings of the 4th International  Workshop on Semantic Evaluations (SemEval-2007), pages 157–160, 
Prague, June 2007. Qc 2007 Association for Computational Linguistics



senses) as very coarse–grained sense distinctions. 
However, not so much attention has been paid on 
learning class-based classifiers from other available 
sense–groupings such as WordNet Domains (Mag- 
nini and Cavaglia, 2000), SUMO labels (Niles and 
Pease, 2001), EuroWordNet Base Concepts or Top 
Concept Ontology labels (Atserias et al., 2004). Ob- 
viously, these resources relate senses at some level 
of abstraction using different semantic criteria and 
properties that could be of interest for WSD. Pos- 
sibly, their combination could improve the overall 
results since they offer different semantic perspecti- 
ves of the data. Furthermore, to our knowledge, to 
date no comparative evaluation have been performed 
exploring different sense–groupings.
  This paper is organized as follows. In section 2, 
we present a method for deriving fully automatica- 
lly a number of Base Level Concepts from any WN 
version.  Section 3 shows the details of the whole 
system and finally, in section 4 some concluding re- 
marks are provided.

2   Automatic Selection of Base Level
Concepts

The notion of Base Concepts (hereinafter BC) was 
introduced in EWN. The BC are supposed to be the 
concepts that play the most important role in the va- 
rious wordnets4  (Fellbaum, 1998) of different lan- 
guages.  This role was measured in terms of two 
main criteria:
• A high position in the semantic hierarchy;
• Having many relations to other concepts;

  Thus, the BC are the fundamental building blocks 
for establishing the relations in a wordnet and give 
information about the dominant lexicalization pat- 
terns in languages. BC are generalizations of featu- 
res or semantic components and thus apply to a ma- 
ximum number of concepts. Thus, the Lexicografic 
Files (or Supersenses) of WN could be considered 
the most basic set of BC.
  Basic Level Concepts (Rosch, 1977) should not 
be confused with Base Concepts. BLC are the result 
of a compromise between two conflicting principles 
of characterization:
4 http://wordnet.princeton.edu



#rel.
synset
18
group 1,grouping 1
19
social group 1
37
organisation 2,organization 1
10
establishment 2,institution 1
12
faith 3,religion 2
5
Christianity 2,church 1,Christian church 1
#rel.
synset
14
entity 1,something 1
29
object 1,physical object 1
39
artifact 1,artefact 1
63
construction 3,structure 1
79
building 1,edifice 1
11
place of worship 1, ...
19
church 2,church building 1
#rel.
synset
20
act 2,human action 1,human activity 1
69
activity 1
5
ceremony 3
11
religious ceremony 1,religious ritual 1
7
service 3,religious service 1,divine service 1
1
church 3,church service 1

Table 1: Possible Base Level Concepts for the noun
Church


• Represent as many concepts as possible;
• Represent as many features as possible;

  As a result of this, Basic Level Concepts typically 
occur in the middle of hierarchies and less than the 
maximum number of relations. BC mostly involve 
the first principle of the Basic Level Concepts only.
  Our work focuses on devising simple methods for 
selecting automatically an accurate set of Basic Le- 
vel Concepts from WN. In particular, our method se- 
lects the appropriate BLC of a particular synset con- 
sidering the relative number of relations encoded in 
WN of their hypernyms.
  The process follows a bottom-up approach using 
the chain of hypernym relations.  For each synset 
in WN, the process selects as its Base Level Con- 
cept the first local maximum according to the rela- 
tive number of relations. For synsets having multi- 
ple hypernyms, the path having the local maximum 
with higher number of relations is selected. Usually, 
this process finishes having a number of “fake” Base 
Level Concepts. That is, synsets having no descen- 
dants (or with a very small number) but being the 
first local maximum according to the number of re- 
lations considered.  Thus, the process finishes che- 
cking if the number of concepts subsumed by the





Table 2: Polysemy degree over SensEval–3 
preliminary list of BLC is higher than a certain th-
reshold.  For those BLC not representing enough 
concepts according to a certain threshold, the pro- 
cess selects the next local maximum following the 
hypernym hierarchy.
  An example is provided in table 1.  This table 
shows the possible BLC for the noun “church” using 
WN1.6. The table presents the hypernym chain for 
each synset together with the number of relations en- 
coded in WN for the synset. The local maxima along 
the hypernym chain of each synset appears in bold.
  Table 2 presents the polysemy degree for nouns 
and verbs of the different words when grouping its 
senses with respect the different semantic classes on 
SensEval–3. Senses stand for the WN senses, BLC 
for the Automatic BLC derived using a threshold of
20 and SuperSenses for the Lexicographic Files of
WN.

3   The GPLSI system

The GPLSI system uses a publicly available imple- 
mentation of Support Vector Machines, SVMLight5 
(Joachims, 2002), and Semcor as learning corpus. 
Semcor has been properly mapped and labelled with 
both BLC6 and sense-clusters.
  Actually, the process of training-classification has 
two phases:  first, one classifier is trained for each 
possible BLC class and then the SemEval test data 
is classified and enriched with them, and second, a 
classifier for each target word is built using as addi- 
tional features the BLC tags in Semcor and SemE- 
val’s test.
  Then, the features used for training the classifiers 
are: lemmas, word forms, PoS tags7, BLC tags, and 
first sense class of target word (S1TW). All features

5 http://svmlight.joachims.org/
    6 Because BLC are automatically defined from WN, some tu- 
ning must be performed due to the nature of the task 7. We have 
not enough room to present the complete study but threshold 20 
has been chosen, using SE N S EVA L -3 English all-words as test 
data. Moreover, our tests showed roughly 5% of improvement 
against not using these features.
7 TreeTagger (Schmid, 1994) was used


were extracted from a window [−3.. + 3] except for 
the last type (S1TW). The reason of using S1TW
features is to assure the learning of the baseline. It is 
well known that Semcor presents a higher frequency 
on first senses (and it is also the baseline of the task 
finally provided by the organizers).
  Besides, these are the same features for both first 
and second phases (obviously except for S1TW be- 
cause of the different target set of classes). Nevert- 
heless, the training in both cases are quite different: 
the first phase is class-based while the second is 
word-based. By word-based we mean that the lear- 
ning is performed using just the examples in Semcor 
that contains the target word. We obtain one classi- 
fier per polysemous word are in the SemEval test 
corpus.  The output of these classifiers is a sense- 
cluster.  In class-based learning all the examples in 
Semcor are used, tagging those ones belonging to a 
specific class (BLC in our case) as positive exam- 
ples while the rest are tagged as negatives. We ob- 
tain so many binary classifiers as BLC are in Se- 
mEval test corpus.  The output of these classifiers 
is true or f alse, “the example belongs to a class” 
or not.  When dealing with a concrete target word, 
only those BLC classifiers that are related to it are 
“activated” (i.e, “animal” classifier will be not used 
to classify “church”), ensuring that the word will be 
tagged with coherent labels. In order to avoid statis- 
tical bias because of very large set of negative exam- 
ples, the features are defined from positive examples 
only (although they are obviously used to characte- 
rize all the examples).

4   Conclusions and further work

The WSD task seems to have reached its maxi- 
mum accuracy figures with the usual framework. 
Some of its limitations could come from the sense– 
granularity of WN. In particular, SemEval’s coarse- 
grained English all-words task represents a solution 
in this direction.
  Nevertheless, the task still remains oriented to 
words rather than classes.  Then, other problems 
arise like data sparseness just because the lack of 
adequate and enough examples. Changing the set of 
classes could be a solution to enrich training corpora 
with many more examples Another option seems to 
be incorporating more semantic information.


  Base Level Concepts (BLC) are concepts that are 
representative for a set of other concepts. A simple 
method for automatically selecting BLC from WN 
based on the hypernym hierarchy and the number of 
stored relationships between synsets have been used 
to define features for training a supervised system.
  Although in our system BLC play a simple role 
aiding to the disambiguation just as additional fea- 
tures, the good results achieved with such simple 
features confirm us that an appropriate set of BLC 
will be a better semantic discriminator than senses 
or even sense-clusters.


References

E. Agirre, I. Aldezabal, and E. Pociello.  2003.  A pi- 
lot study of english selectional preferences and their 
cross-lingual compatibility with basque.   In Procee- 
dings of the International Conference on Text Speech 
and Dialogue (TSD’2003), CeskBudojovice, Czech 
Republic.

J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll, 
B. Magnini, and P. Vossen. 2004. The meaning mul- 
tilingual central repository. In Proceedings of Global 
WordNet Conference (GWC’04), Brno, Czech Repu- 
blic.

M. Ciaramita and Y. Altun.    2006.    Broad-coverage 
sense disambiguation and information extraction with 
a supersense sequence tagger.  In Proceedings of the 
Conference on Empirical Methods in Natural Lan- 
guage Processing (EMNLP’06), pages 594–602, Syd- 
ney, Australia. ACL.

M. Ciaramita and M. Johnson. 2003. Supersense tagging 
of unknown nouns in wordnet. In Proceedings of the 
Conference on Empirical methods in natural language 
processing (EMNLP’03), pages 168–175. ACL.

J. Curran. 2005. Supersense tagging of unknown nouns 
using semantic similarity. In Proceedings of the 43rd 
Annual Meeting on Association for Computational 
Linguistics (ACL’05), pages 26–33. ACL.

C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi- 
cal Database. The MIT Press.

M. Hearst and H. Schu¨ tze. 1993. Customizing a lexicon 
to better suit a computational task.  In Proceedingns 
of the ACL SIGLEX Workshop on Lexical Acquisition, 
Stuttgart, Germany.

Thorsten Joachims.   2002.   Learning to Classify Text 
Using Support Vector Machines.   Kluwer Academic 
Publishers.



B. Magnini and G. Cavaglia.  2000.  Integrating subject 
fields codes into wordnet.  In Proceedings of the Se- 
cond International Conference on Language Resour- 
ces and Evaluation (LREC’00).

R. Mihalcea and D. Moldovan.   2001.   Automatic ge- 
neration of coarse grained wordnet.  In Proceding of 
the NAACL workshop on WordNet and Other Lexical 
Resources: Applications, Extensions and Customiza- 
tions, Pittsburg, USA.

I. Niles and A. Pease.   2001.   Towards a standard up- 
per ontology. In Proceedings of the 2nd International 
Conference on Formal Ontology in Information Sys- 
tems (FOIS-2001), pages 17–19. Chris Welty and Ba- 
rry Smith, eds.

W. Peters, I. Peters, and P. Vossen.  1998.  Automatic 
sense clustering in eurowordnet.  In First Internatio- 
nal Conference on Language Resources and Evalua- 
tion (LREC’98), Granada, Spain.

E. Rosch.	1977.	Human categorisation.	Studies in
Cross-Cultural Psychology, I(1):1–49.

Helmut Schmid. 1994. Probabilistic part-of-speech tag- 
ging using decision trees. In Proceedings of NemLap-
94, pages 44–49, Manchester, England.

F. Segond, A. Schiller, G. Greffenstette, and J. Chanod.
1997.  An experiment in semantic tagging using hid- 
den markov model tagging. In ACL Workshop on Au- 
tomatic Information Extraction and Building of Lexi- 
cal Semantic Resources for NLP Applications, pages
78–81. ACL, New Brunswick, New Jersey.

L. Villarejo, L. Ma`rquez, and G. Rigau.  2005.  Explo- 
ring the construction of semantic class classifiers for 
wsd.  In Proceedings of the 21th Annual Meeting of 
Sociedad Espaola para el Procesamiento del Lenguaje 
Natural SEPLN’05, pages 195–202, Granada, Spain, 
September. ISSN 1136-5948.

P. Vossen, L. Bloksma, H. Rodriguez, S. Climent, N. Cal- 
zolari,  A. Roventini,  F. Bertagna,  A. Alonge,  and 
W. Peters. 1998. The eurowordnet base concepts and 
top ontology. Technical report, Paris, France, France.

