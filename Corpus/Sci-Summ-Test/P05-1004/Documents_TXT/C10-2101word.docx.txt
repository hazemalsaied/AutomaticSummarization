Semantic Classification of Automatically Acquired Nouns 
using Lexico-Syntactic Clues




Yugo Murawaki Graduate 
School of Informatics Kyoto 
University
murawaki@nlp.kuee.kyoto-u.ac.jp


Sadao Kurohashi Graduate School of 
Informatics Kyoto University 
kuro@i.kyoto-u.ac.jp





Abstract

In this paper, we present a two-stage  ap- 
proach to acquire  Japanese unknown mor- 
phemes from text with full POS tags as- 
signed to them. We first acquire unknown 
morphemes only making a morphology- 
level distinction, and then apply semantic 
classification to acquired nouns.  One ad- 
vantage of this approach is that, at the sec- 
ond stage, we can exploit syntactic clues 
in addition to morphological ones because 
as a result of the first stage acquisition,  we 
can rely on automatic parsing. Japanese 
semantic classification  poses an interest- 
ing challenge:   proper nouns need to be 
distinguished  from common  nouns.  It 
is because Japanese has no orthographic 
distinction  between common and proper 
nouns and no apparent morphosyntactic 
distinction between  them.  We  explore 
lexico-syntactic  clues that are  extracted 
from automatically  parsed text and inves- 
tigate their effects.

1   Introduction

A dictionary plays an important role in Japanese 
morphological analysis, or  the joint  task of 
segmentation  and part-of-speech  (POS) tag- 
ging (Kurohashi et al., 1994; Asahara and Mat- 
sumoto,  2000; Kudo et al., 2004).  Like  Chi- 
nese and Thai, Japanese does not delimit words 
by white-space.  This makes the first step of nat- 
ural language processing more ambiguous than 
simple POS tagging. Accordingly,  morphemes in 
a pre-defined dictionary  compactly  represent our 
knowledge about both segmentation and POS.
  One obvious problem with the dictionary-based 
approach  is  caused  by  unknown morphemes,


or  morphemes  not defined in  the dictionary. 
Even though, historically, extensive human re- 
sources were used to build high-coverage dictio- 
naries (Yokoi, 1995), texts other than newspa- 
per articles, in particular web pages,  contain  a 
large number of unknown morphemes. These un- 
known  morphemes often cause segmentation er- 
rors. For example, morphological  analyzer JU-
MAN 6.01 wrongly  segments the phrase “さっぽ
ろ駅” (saQporo eki, “Sapporo Station”), where “
さっぽろ” (saQporo) is an unknown morpheme,
as follows:
“さ” (sa, noun-common, “difference”),
“っ” (Q, UNK), “ぽ” (po, UNK),
“ろ” (ro, noun-common, “sumac”) and
“駅” (eki, noun-common, “station”),
where UNK refers to unknown morphemes auto- 
matically identified by the analyzer.  Such an er- 
roneous sequence has disastrous effects on appli- 
cations of morphological analysis. For example, it 
can hardly be identified as a LOCATION in named 
entity recognition.
  One solution to the unknown morpheme prob- 
lem is unknown morpheme acquisition (Mori and 
Nagao, 1996; Murawaki  and Kurohashi, 2008). It 
is the task of automatically augmenting the dictio- 
nary by acquiring unknown morphemes from text. 
In the above example, the goal is to acquire the
morpheme “さっぽろ” (saQporo) with the POS
tag “noun-location name.”   However,  unknown 
morpheme acquisition  usually adopts  a coarser 
POS tagset that only represents the morphology 
level distinction  among noun, verb and adjective.
This means that “さっぽろ” (saQporo) is acquired
as just a noun and that the semantic label “loca- 
tion name” remains to be assigned.  The reason 
only the morphology level distinction is made is

1 http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html







876
Coling  2010: Poster Volume, pages 876–884, 
Beijing,  August 2010


that the semantic level distinction  cannot easily 
be captured with morphological  clues that are ex- 
ploited in unknown morpheme acquisition.
  In this paper, we investigate  the remaining 
problem and introduce the new task of seman- 
tic classification that is to be applied to automat- 
ically acquired nouns. In this task, we can ex- 
ploit syntactic  clues in addition to morphologi- 
cal ones because,  as a result  of acquisition, we 
can now rely on automatic parsing. For exam-
ple, since text containing “さっぽろ” (saQporo,
noun-unclassified) is correctly  segmented, we can 
extract not only the phrase “saQporo station,” but 
the tree fragment “ϕ go to saQporo,” and we can 
determine its semantic label.
  Japanese semantic classification poses an inter- 
esting challenge:  proper nouns need to be distin- 
guished from common nouns.  Like Chinese and 
Thai, Japanese has no orthographic distinction be- 
tween common  and proper nouns as there is no 
such thing as  capitalization. In addition, there 
seems no morphosyntactic (i.e. grammatical) dis- 
tinction between them.
  In this paper, we explore lexico-syntactic clues 
that can be extracted from automatically  parsed 
text. We train a classification model on manually 
registered nouns and apply it to automatically ac- 
quired nouns. We then investigate the effects of 
lexico-syntactic clues.

2   Semantic Classification Task

2.1   Two-Stage Approach to Unknown
Morpheme Acquisition

Our goal is to identify  unknown morphemes in un- 
segmented text and assign POS tags to them. In 
this section, we omit the details of boundary iden- 
tification  (segmentation) and review the Japanese 
POS tagset to see why  we propose a two-stage  ap- 
proach to assign full POS tags.
  The Japanese POS  tagset derives  from tradi- 
tional grammar. It is a mixture  of several linguis- 
tic levels:  morphology,  syntax and semantics. In 
other words, information  encoded in a POS  tag 
is more than how the morpheme behaves in a se- 
quence of morphemes. In fact, POS tags given to 
pre-defined morphemes are useful for applications 
of morphological   analysis,  such as dependency


parsing (Kudo and Matsumoto, 2002), named en- 
tity recognition (Asahara and Matsumoto, 2003; 
Sasano and Kurohashi,  2008) and anaphora res- 
olution (Iida et al., 2009; Sasano and Kurohashi,
2009). In these applications,  POS tags are incor- 
porated as features for models.
  On the other hand, the mixed nature of the POS 
tagset poses a challenge  to unknown morpheme 
acquisition.   Previous approaches (Mori and Na- 
gao, 1996; Murawaki and Kurohashi,  2008) di- 
rectly or indirectly reply on morphology, or our 
knowledge on how a morpheme  behaves in a se- 
quence of morphemes. This means that semantic 
level distinction is difficult to make in these ap- 
proaches, and in fact, is left unresolved.   To be 
specific, nouns are only distinguished from verbs 
and adjectives but they have subcategories in the 
original  tagset. These are what we try to classify 
acquired nouns into in this paper.

2.2   Semantic Labels

The Japanese noun subcategories may require  an 
explanation since they are different  from the En- 
glish ones  (Marcus et al., 1993) in many re- 
spects.  Singular  and mass nouns are not distin- 
guished from plural nouns because Japanese has 
no grammatical distinction  between them. More 
importantly for this paper, proper nouns have sub- 
categories  such as person name,  location  name 
and organization  name in addition to the distinc- 
tion from common nouns. These subcategories 
provide important information to named  entity 
recognition among other applications. For proper 
nouns, we adopt these subcategories as semantic 
labels in our task.
  In contrast to proper  nouns,  common  nouns 
have  only one subcategory  “common.”    How- 
ever, we consider that subcategories of common 
nouns similar to those of proper nouns are use- 
ful for, for example, anaphora resolution (Sasano 
and Kurohashi, 2009). We adopt the “categories” 
of morphological  analyzer JUMAN, with which 
common  nouns in its dictionary are annotated. 
There are  22 “categories”  including PERSON, 
ORGANIZATION  and CONCEPT. We  collapse 
these “categories”  into coarser semantic labels 
that roughly correspond to those for proper nouns. 
To sum up, we define 9 semantic labels as shown


Table 1: List of semantic labels.

lab
els
P
/
C
s
o
u
r
c
e
s
1
m
an
ua
lly 
re
gi
st
er
ed 
n
o
u
n
s
a
ut
o
m
at
ic
al
ly 
a
c
q
ui
re
d 
n
o
u
n
s
PS
N-
P



pr
o
p
er
sub
PO
S:p
ers
on 
na
me
松
井 
(m
ats
ui,  
a 
sur
na
me
)
ジ
ョ
ー
ジ 
(joˆ 
ji, 
“G
eo
rg
e”)
佐
祐
理 
(sa
yuri
, a 
giv
en 
na
me
)
キ
ョ
ン 
(ky
oN, 
a 
nic
kn
am
e)
LO
C-
P

sub
PO
S:p
lac
e 
na
me
京
都 
(ky
out
o, 
“K
yot
o”)
ド
イ
ツ 
(do
itsu
, 
“G
er
ma
ny”
)
ア
キ
バ 
(ak
iba
, 
“A
kih
ab
ara
”)
ワ
イ
キ
キ 
(w
aiki
ki, 
“W
aik
iki”
)
O
R
G-
P

su
bP
OS
:or
ga
niz
ati
on 
na
me
日
銀 
(nic
higi
n,  
a 
ba
nk)
N
HK 
(a 
br
oa
dc
ast
er)
マ
ツ
ダ 
(m
ats
ud
a, 
“M
az
da”
)
ヤ
フ
ー 
(ya
huˆ , 
“Y
ah
oo”
)
OT
H-
P

sub
PO
S:p
rop
er 
no
un
平
成 
(hei
sei, 
an 
era 
na
me
)
ス
ラ
ブ 
(su
rab
u, 
“Sl
av”
)
ジ
プ
シ
ー 
(jip
us
hˆı, 
“G
yp
sy”
)
PS
N-
C




co
m
mo
n
cat
eg
ory
:P
ER
S
O
N
先
生 
(se
Nse
i, 
“te
ac
he
r”)
ス
タ
ッ
フ 
(su
taQ
fu, 
“st
aff
”)
メ
ル
友 
(m
eru
to
mo
, 
“ke
yp
al”)
ニ
ー
ト 
(nˆıt
o, 
“N
EE
T”)
LO
C-
C

cat
eg
ory
:PL
AC
E-
∗2
職
場 
(sh
oku
ba, 
“of
fic
e”)
カ
フ
ェ 
(ka
fe, 
“ca
fe”
)
囲
炉
裏 
(iro
ri, 
“he
art
h”)
圃
場 
(ho
jou
, 
“fa
rm 
fiel
d”)
O
R
G-
C

cat
eg
ory
:O
RG
AN
IZ
AT
IO
N
政
府 
(se
ifu, 
“g
ov
er
nm
ent
”)
チ
ー
ム 
(chˆ
ım
u, 
“te
am
”)
メ
ー
カ 
(me
ˆka
, 
“m
an
ufa
ctu
rer
”)
弊
所 
(he
ish
o, 
“ou
r 
offi
ce”
)
A
NI
-C

cat
eg
ory
:A
NI
MA
L 
an
d
cat
eg
ory
:A
NI
M
AL
-
PA
RT
犬 
(in
u, 
“d
og
”)
顔 
(ka
o, 
“fa
ce”
)
チ
ワ
ワ 
(chi
wa
wa, 
“C
hih
ua
hu
a”)
マ
ン
タ 
(m
aNt
a, 
“m
ant
a”)
OT
H-
C

oth
er 
cat
eg
ori
es
主
張 
(sh
uch
ou, 
“ar
gu
me
nt”
)
枕 
(m
ak
ura
, 
“pil
lo
w”
)
甚
平 
(jiN
bei
, a 
kin
d 
of 
clo
thi
ng)
着
メ
ロ 
(ch
aku
me
ro, 
“rin
gto
ne”
)
1 A subPOS refers to a subcategory  of noun. For example, PSN-P  corresponds to the POS tag “noun-person  name”.
2 category:PLACE-INSTITUTION, category:PLACE-INSTITUION  PART and others.



in Table 1.

2.3   Related Tasks

A line of research is dedicated to identify un- 
known morphemes with varying  degrees of identi- 
fication.  Asahara and Matsumoto (2004) only fo- 
cus on boundary identification  (segmentation) of 
unknown  morphemes.  Mori and Nagao (1996), 
Nagata  (1999) and Murawaki and Kurohashi 
(2008) assign POS tags at the morphology level. 
Uchimoto et al. (2001) assign full POS tags but 
unsurprisingly the accuracy  is low.   Nakagawa 
and Matsumoto  (2006) also assign full POS tags. 
They address the fact that local information  used 
in previous studies is inherently insufficient and 
present  a method  that uses  global information, 
in other words, takes into consideration all oc- 
currences of each unknown  word in a document. 
They report an improvement in tagging  proper 
nouns in Japanese.
  A  related task is named entity recognition 
(NER). It can handle a named entity longer than 
a single morpheme and is usually formalized  as a 
chunking problem. Since Japanese does not de- 
limit words by white-space, the unit of chunk- 
ing can be a character (Asahara  and Matsumoto,
2003; Kazama and Torisawa,  2008) or a  mor-


errors. In fact, Saito et al. (2007) report that a ma- 
jority of unknown named entities (those never ap- 
pear in a training  corpus) contain unknown mor- 
phemes as their constituents and that NER models 
perform poorly on them. A straightforward solu- 
tion to this problem would be to acquire unknown 
morphemes and to assign semantic labels to them.
  Another  related task is supersense tagging (Cia- 
ramita and Johnson,  2003; Curran, 2005; Cia- 
ramita and Altun, 2006).  A supersense corre- 
sponds to one of the 26 broad categories defined 
by WordNet (Fellbaum, 1998). Each noun synset 
is associated with a  supersense.    For example, 
“chair” has supersenses PERSON, ARTIFACT 
and ACT because it belongs to several synsets.
  Since supersense tagging is studied in English, 
it differs from our task in several respects. In En- 
glish, the distinction  between common and proper 
nouns is clear. In fact, the tagging models can use 
POS features even for unknown nouns. In addi- 
tion, the syntactic behavior of English  nouns is 
different from that of Japanese nouns (Gil, 1987). 
Definiteness is not marked in Japanese as it lacks 
determiners (e.g. “the” and “a”), and Japanese has 
no obligatory plural marking.  On the other hand, 
Japanese obligatorily uses numeral  classifiers  to 
indicate the count of nouns, as in


pheme (Sasano and Kurohashi,  2008). In either


(1)	saN


satsu no


hoN


case, NER models encode the output of morpho-


three CL


GEN book


logical analysis and therefore are affected by its


three volumes of books, or three books,


where “satsu” is a numeral classifier for books. A 
number together with its numeral classifier forms 
a numeral  quantifier. Numeral quantifiers would 
be informative  about the semantic categories of 
nouns. Note  that Japanese shares the above fea- 
tures with Chinese and Thai. Our findings in this 
paper may hold for these languages.

3   Proposed Method

3.1   Lexico-Syntactic Clues

In the task of semantic classification, we can ex- 
ploit syntactic clues in addition to morpholog- 
ical ones.  As a  result of unknown  morpheme 
acquisition, text containing acquired morphemes, 
or former unknown morphemes, is correctly seg- 
mented.  Now we can treat automatic  parsing as 
(at least partly) reliable with regard to acquired 
morphemes.
  For noun X , we use the following sets of fea- 
tures for classification.
  call:  noun phrase  Y  that appears  in a  pat- 
tern like “Y  called X ” and “Y  such as X ,” e.g. 
“call:kuni” from
  

suf: suffix or suffix-like noun that follows X , 
e.g. “suf:saN” from “X saN” (Mr./Ms. X ) and 
“suf:eki” from “X eki” (X station).
  Using automatically  parsed text to extract syn- 
tactic features has an advantage. Since no manual 
annotation is necessary, we can utilize  a huge raw 
corpus. On the other hand, parsing errors are in- 
evitable.  However, we can circumvent  this prob- 
lem by using the constraints of Japanese depen- 
dency structures:  head-final and projective.   The 
simplest example is the second last element of a 
sentence, which always depends on the last ele- 
ment. With these constraints,  we can focus on 
syntactically  unambiguous dependency pairs and 
extract syntactic features accurately. We follow 
Kawahara and Kurohashi (2001) to extract a pair 
of an argument noun and a predicate  (cf), and 
Sasano et al. (2004) to extract a pair of nouns con- 
nected with the genitive  case marker “no” (ncf1 
and ncf2).
  Noun X can be part of a compound  noun. We 
leave it for named entity recognition.  Except for 
suf, we extract features only when X alone forms
a word.  Similarly, we extract  suf features only


X to iu


kuni


when X and a 
suffix alone 
form a noun 
phrase.


X QT call country
a country called X .
  cf: predicate with a case marker  with which it 
takes X as an argument,  e.g. “cf:tooru:wo” from
  

For call, ncf1, and ncf2, we generalize 
numerals within  noun phrases.   For  “hoN” 
(book) in  example  1,  we extract  the feature 
“ncf2:<NUM>satsu.”


X wo


tooru


X ACC pass
ϕ pass through  X .
  demo: demonstrative  that modifies X ,  e.g. 
“demo:kono”   from  “kono   X ” (this  X )  and 
“demo:doNna”  from “doNna  X ” (what kind of 
X ).
  ncf1: noun phrase which X modifies with the 
genitive case marker  “no,” e.g. “ncf1:heya” from


3.2   Instances for Classification
Now that features are extracted for each noun, the 
question is how to combine them together to make 
an instance for classification. One factor we need 
to consider is polysemy:  a noun can be a person 
name in one context  and a location  name in an- 
other. If we combine features extracted from the 
whole corpus, they may represent several seman- 
tic labels.


X no


heya


Modeling a 
mixture of 
semantic 
labels 
might


X GEN room
X ’s room.
  ncf2: noun phrase that modifies X  with the 
genitive  case marker  “no,”  e.g. “ncf2:subete” 
from
subete no  X


be a solution, but we do not take this approach on 
the grounds that each occurrence of a noun corre- 
sponds to a single semantic label.
  In our strategy, we perform classification mul- 
tiple times for each noun and aggregate the results 
at the end. The features for each classification  are


all
all X .


GEN X


extracted 
from a 
relatively  
small 
subset of a 
cor- pus 
where the 
noun is 
supposedly 
consistent 
in


terms of semantic labels. In the field of named 
entity recognition, it is known that label consis- 
tency holds strongly at the level of a document 
and less strongly across different  documents (Kr- 
ishnan and Manning, 2006). Thus we start with a 
document and gradually cluster related documents 
until a sufficient  number of features are obtained. 
For the specific procedures we took in the experi- 
ments, see Section  4.1.

3.3   Training Data

Following unknown morpheme acquisition (Mu- 
rawaki and Kurohashi, 2008), we create training 
data using manually  registered nouns, for which 
we can obtain correct semantic labels. We per- 
form the same procedure  as above  to make in- 
stances of registered nouns.
  Some registered  nouns are tagged with more 
than one semantic label, which we call “explicit 
polysemy.” We drop them from the training data. 
The remaining problem is “implicit  polysemy.” 
Nouns are  sometimes  used  with an uncovered 
sense. In preliminary experiments, we found that 
a  typical case  of implicit polysemy  was that a 
proper noun derived from a basic  noun. To al- 
leviate this problem, we use an NE tagger for fil- 
tering. We run an NE tagger over a small portion 
of the corpus and extract common nouns that are 
frequently  tagged as named entities.  Then we re- 
move these nouns from the training  data.
  We also drop nouns that appear extremely fre- 
quently  such as “人” (hito, “person”), “事” (koto, 
“thing”) and “私” (watashi, “I”2). Since acquired
nouns to be classified are typically  low frequency 
morphemes, they would not behave similarly to 
these basic nouns.


the inner product of wy and x.

y = argmax⟨wy , x⟩.
y

  Several  methods have been proposed to esti- 
mate weight vector wy from training data. We use 
online algorithms  because they are easy to imple- 
ment and scale to huge instances. We try the Per- 
ceptron family of algorithms.

4   Experiments

4.1   Settings
We used JUMAN for morphological analysis and 
KNP3 for dependency parsing. The dictionary 
of JUMAN was augmented with automatically 
acquired morphemes (Murawaki  and Kurohashi,
2008). The number of manually registered mor- 
phemes  was  120 thousands  while there were
13,071 acquired  morphemes,  of which 12,615 
morphemes were nouns.
  We  used  a  web corpus that was compiled 
through the procedures  proposed  by Kawahara 
and Kurohashi (2006). It consisted of 100 million 
pages.
  We first extracted features from the web cor- 
pus.  To keep  the model size manageable,  we 
used 447,082  features that appeared more than
100 times in the corpus.
  We  constructed training data from manually 
registered nouns and test data from automatically 
acquired nouns. For each noun, we combined text 
together until the number of features grew to more 
than 100. We started with a single web page, then 
merge  pages that share a domain  name  and fi- 
nally clustered texts across different domains. We 
split the web corpus into 40 subcorpora and ap-
4


3.4   Classifier

To assign a semantic label to each instance, we use 
a multiclass discriminative  classifier. The input it 
takes is an instance that is represented by a feature 
vector x ∈ Rd. The output is one semantic label 
y ∈ Y , where Y is the set of semantic labels.
We use a linear classifier. It has a weight  vector
wy ∈ Rd for each y and outputs y that maximizes

  2 Japanese personal  pronouns  are treated as common 
nouns because they show no special morphosyntactic behav- 
ior.


plied this procedure in parallel. We used Bayon
for clustering domain texts. We sequentially read 
texts and applied the repeated bisections cluster- 
ing every time some 5,000 pages were appended. 
The vectors for clustering were nouns, both regis- 
tered and acquired, with their tf-idf scores. We ob- 
tained 4,843,085 instances for 10,613 registered 
nouns and 196,098  instances for 2,556 acquired 
nouns.
3 http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
4 http://code.google.com/p/bayon/


Table 2: Results of semantic classification.

lea
rni
ng 
alg
orit
hm
s
a
c
q
u
ir
e
d 
n
o
u
n
s
r
e
g
i
s
t
e
r
e
d
 
n
o
u
n
s
Av
era
ge
d 
Pe
rce
ptr
on
Pas
siv
e-
Ag
gre
ssi
ve
Co
nfi
de
nc
e-
We
igh
ted
86.
40
% 
(43
2 / 
50
0)
87.
00
% 
(43
5 / 
50
0)
85.
20
% 
(42
6 / 
50
0)
88.
59
% 
(12
3,1
13 
/ 
13
8,9
71)
91.
68
% 
(12
7,4
07 
/ 
13
8,9
71)
89.
66
% 
(12
4,6
04 
/ 
13
8,9
71)
ba
sel
ine
1
69.
60
% 
(34
8 / 
50
0)
79.
14
% 
(10
9,9
80 
/ 
13
8,9
71)
1 assign OTH-C to all instances.

Table 3: Examples of aggregated instances.

a
c
q
u
i
r
e
d
 
n
o
u
n
s
ins
tan
ce
s
l
a
b
e
l
s
ヒ
カ
ル 
(hik
aru
,  a 
per
son 
na
me
)
チ
ワ
ワ 
(ch
iwa
wa, 
“C
hih
ua
hu
a”)
か
み
さ
ん 
(ka
mis
aN, 
coll
oq. 
“wi
fe”
)
ラ
ス
ベ
ガ
ス 
(ra
sub
ega
su, 
“L
as 
Ve
ga
s”)
ア
ッ
プ
ル 
(aQ
pur
u, 
“A
ppl
e/a
ppl
e”)
メ
ル
マ
ガ 
(m
eru
ma
ga, 
ab
br. 
of 
“m
ail 
ma
ga
zin
e”)
84
1
2
8
1
3
1
1
3
6
1
8
7
1
,
6
2
2
PS
N-
P:5
8.3
3%
, 
PS
N-
C:
41.
67
%
AN
I-
C:
54.
69
%, 
OT
H-
C:
45.
31
%
PS
N-
C:
10
0%
LO
C-
P:9
7.0
6%
, 
LO
C-
C:
2.9
4%
OR
G-
P:6
3.1
0%
, 
PS
N-
C:3
4.7
6%
, 
OT
H-
C:
2.1
4%
OT
H-
C:9
9.3
2%
, 
LO
C-
C:0
.55
%, 
PS
N-
C:
0.0
6%




  In order to handle polysemy, we evaluated se- 
mantic classification on an instance-by-instance 
basis. We randomly selected 500 instances from 
the test data and manually  assigned the correct la- 
bels to them. For comparison purposes, we also 
classified registered nouns.  We split the training 
data: 829 nouns or 138,971 instances for testing 
and the rest for training.
  We trained the model with three online learn- 
ing algorithms, (1) the averaged version (Collins,
2002) of Perceptron (Crammer and Singer, 2003), 
(2) the Passive-Aggressive algorithm  (Crammer 
et al., 2006), and (3) the Confidence-Weighted 
algorithm (Crammer et al., 2009). For Passive- 
Aggressive algorithm, we used PA-I and set pa- 
rameter C to 1.  For Confidence-Weighted, we 
used the single-constraint  updates. All algorithms 
iterated five times through the training data.

4.2   Results

Table 2 shows the results of semantic classifica- 
tion. All algorithms significantly improved over 
the baseline. As suggested by the gap in accu- 
racy between acquired and registered nouns in the 
baseline method, the label distribution of the train- 
ing data differed from that of the test data, but the 
decrease in accuracy was smaller than expected.
  The Passive-Aggressive  algorithm performed 
best on both acquired and registered nouns. For 
the rest of this paper, we report the results of the 
Passive-Aggressive algorithm.
  Table 3 shows aggregated instances of some ac- 
quired nouns. Although classification sometimes 
failed, correct labels took the majority.  How-


ever, it is noticeable that PSN-P was frequently 
misidentified   as PSN-C while PSN-C was cor- 
rectly identified.  This phenomenon is clearly seen 
in the confusion matrix (Table 4). Half of PSN-P 
instances were misidentified   as PSN-C but the 
percentage of errors in the opposite direction was 
just above 9%. We will investigate this in the next 
section.

4.3   Discussion

Our interest is in determining what kinds of fea- 
tures are effective  in semantic classification.  We 
first performed standard ablation experiments. We 
trained a series of models on the training data af- 
ter removing  each feature set.  The training and 
test data were the same with those in Section 4.1.
  Table 5 shows the results of ablation experi- 
ments. Significant decreases in accuracy are ob- 
served in the cf dataset. This is easily explained by 
the fact that more than half of features belonged 
to cf. The ratio of ncf1 was much the same with 
that of ncf2, but the removal of ncf1 resulted in a 
worse performance in classifying registered nouns 
than that of ncf2. This means that a modifiee of a 
noun explains more about the noun than its modi- 
fier.
  The ablation experiments cannot capture inter- 
esting properties of features because each feature 
set has a great diversity  within it.  Next, we di- 
rectly examine features instead. Since we use a 
simple linear classifier, a feature has |Y | corre- 
sponding weights, each of which represents how 
likely a noun  belongs  to label y.  For example, 
features whose weights for PSN-C are the largest


Table 4: Confusion matrix of acquired nouns.


A
c
t
u
a
l

PS
N-
P	LOC-P	ORG-P	OTH-P
PS
N-
C	LOC-C	ORG-C	ANI-C 	OTH-C
P
r
e
d
i
c
t
e
d
P
S
N
-
P
L
O
C
-
P 
O
R
G
-P 
O
T
H
-
P
1
6
 
	
1

4
4	1
1

P
S
N
-
C
L
O
C
-
C 
O
R
G
-C 
A
N
I-
C 
O
T
H
-
C
1
6
2
	
2
	
1


3
	
1
	
1
39 	1	2
10 	4
2
2
8
1	13	9	338

Table 5: Results of ablation experiments.

feat
ure 
set
r
a
ti
o
1
a
c
q
u
ir
e
d 
n
o
u
n
s
r
e
g
i
s
t
e
r
e
d
 
n
o
u
n
s
-
cal
l
-cf
-
de
mo
-
ncf
1
-
ncf
2
-
suf
0.
2
3
%
54.
84
%
2.
4
0
%
19.
03
%
18.
40
%
5.
1
0
%
87.
60
% 
(43
8 / 
50
0)
84.
80
% 
(42
4 / 
50
0)
88.
00
% 
(44
0 / 
50
0)
87.
20
% 
(43
6 / 
50
0)
85.
60
% 
(42
8 / 
50
0)
87.
40
% 
(43
7 / 
50
0)
91.
58
% 
(12
7,2
76 
/ 
13
8,9
71)
88.
96
% 
(12
3,6
30 
/ 
13
8,9
71)
91.
38
% 
(12
6,9
96 
/ 
13
8,9
71)
89.
23
% 
(12
4,0
08 
/ 
13
8,9
71)
91.
54
% 
(12
7,2
20 
/ 
13
8,9
71)
91.
30
% 
(12
6,8
89 
/ 
13
8,9
71)
all

87.
00
% 
(43
5 / 
50
0)
91.
68
% 
(12
7,4
07 
/ 
13
8,9
71)
1 The proportion of each feature set that appears in the instances of the test 
data.




include:

• cf:nakusu:wo (“ϕ lose X to the disease”),


The modifiee of a numeral  expression  is not 
al- ways the noun to be counted, as 
demonstrated by the following example:


• cf:oshieru:ni (“ϕ1  teach X ϕ2”),


(2)	saN


niN no


moNdai


• ncf2:ooku (“many/much X ”), and


three CL


GEN problem



• 
nc
f2:
<N
UM
>ni
N
(X 
	is
m
od
ifi
ed
by
<
N
U
M
>
   
p
l
u
s  
a
nu
m
er
al
cla
ssi
fie
r
for
persons).
  As briefly mentioned in Section 2.3, Japanese 
numeral quantifiers received scholarly  attention 
in the fields of linguistic philosophy  and lin- 
guistics in  relation to the count/mass  distinc- 
tion (Quine, 1969; Gil, 1987).  In our feature 
sets, numeral quantifiers typically appear as ncf2, 
e.g. “ncf2:<NUM>niN.” The weights given to


matters among the three persons.
From the above, the feature “ncf2:<NUM>niN” 
is extracted although “moNdai” is OTH-C. Theis 
“noise” is attributed to the genitive  case marker 
“no” because it can denote a wide range of rela- 
tions between two nouns. We might be able to 
avoid this problem if we focus on “floating” nu- 
meral quantifiers. A floating numeral quantifier 
has no direct dependency relation to the noun to 
be counted, as in


them demonstrate their effectiveness in semantic


(3)	seito


ga	saN


niN keQseki shita


classification.  They discriminate common nouns


student NOM three CL


absence do


from proper nouns as the weights given to com- 
mon nouns are larger with wide margins. It is not 
surprising  because, say, the phrase “two Johns” is 
semantically acceptable but extremely rare in re- 
ality. They are also informative  about the distinc- 
tion among PSN, LOC and others. For example, 
the classifier “niN” for persons suggest the noun in 
question is a person while “keN” for houses would 
modify a location-like  noun. However, we found 
quite a few “noises”  about these features in data.


three students were absent,
where the numeral quantifier  modifies the verb 
phrase  instead  of the noun.   Further work is 
needed  to anchor floating numeral quantifiers 
since they bring a  different kind of ambiguity 
themselves (Bond et al., 1998).
  Closely related to numeral quantifiers are quan- 
tificational nouns  that appear  as  “ncf2:ooku” 
(“many/much”), “ncf2:subete” (“all”)  and oth- 
ers. They distinguish common nouns from proper


nouns  but does  not make  a  further classifica- 
tion. The same is true of other numeral expres- 
sions such as “cf:hueru:ga” (“X increase in num- 
ber”) and “cf:nai:ga”  (“there is no X ” or “X 
do not exist”).   We found that, other than nu- 
meral expressions, some features distinguished 
common nouns from proper nouns because they 
indicated the noun denoted an attribute. Such fea- 
tures include “cf:naru:ni” (“ϕ become X ”) and 
“cf:kaneru:wo” (“ϕ double as X ”).
  We  expected that demonstratives  (demo) 
served  similar functions to quantificational ex- 
pressions,  but it  turned out to be more com- 
plex. The distal demonstrative “ano” (“that”) of- 
ten modifies proper nouns to give emphasis.  In 
fact, the model gave  larger weights to proper 
nouns. On the other hand, interrogative demon- 
stratives  such as “dono” (“which”) and “doNna” 
(“what kind of”) are rarely used with proper nouns 
although semantically acceptable.
  As seen above,  there  is an abundant  variety 
of features that distinguish  common nouns from 
proper nouns. Also, it is not difficult to make a 
distinction among PSN, LOC and others although 
the far largest cluster OTH-C sometimes absorbs 
other instances. The remaining question is how to 
distinguish proper nouns from common nouns, or 
specifically PSN-P from PSN-C. We examined 
features that gave larger weights to PSN-P than 
to PSN-C. They generally  had smaller margins 
in weights  than those which distinguish PSN-C 
from PSN-P. Among them, features  such as 
“cf:utau:ga” (“X  sing”) and “cf:hanasu:ni” (“ϕ 
talk to X ”) have no problem with being used for 
common  nouns in terms of both semantics and 
pragmatics. They seem  to have resulted  from 
over-training.   There were seemingly appropriate 
features such as “suf:saNchi” (“X ’s house”) and 
“suf:seNshu” (honorific suffix for players), but 
they were not ubiquitous in the corpus. PSN-P in- 
stances suffered from lack of distinctive features.
  One solution to this problem is to combine ad- 
ditional  knowledge about person names. For ex- 
ample,  a Japanese family name is followed by a 
given name, and most Chinese names consist of 
three Chinese characters.  However,  quite a few 
person names in the web corpus do not follow 
the usual patterns of person names because they


are handles  (or nicknames) and names  for fic- 
tional characters. Thus it would be desirable to be 
able to classify  person names without additional 
knowledge.

5   Conclusion

In this paper, we presented the new task of seman- 
tic classification of Japanese nouns and applied  it 
to nouns automatically acquired from text. Unlike 
in unknown morpheme identification in previous 
studies, we can exploit automatically  parsed text. 
We explored  lexico-syntactic  clues and investi- 
gated their effects. We found plenty of features 
that distinguished  common  nouns from proper 
nouns, but few features worked in the opposite di- 
rection. Further work is needed to overcome this 
bias.


References

Asahara, Masayuki  and Yuji Matsumoto. 2000. Ex- 
tended models and tools for high-performance part- 
of-speech tagger. In Proc. of COLING  2000, pages
21–27.

Asahara, Masayuki and Yuji  Matsumoto.   2003.
Japanese named  entity extraction with redundant 
morphological analysis. In Proc. of HLT/NAACL
2003, pages 8–15.

Asahara, Masayuki and Yuji  Matsumoto.   2004.
Japanese unknown word identification by character- 
based chunking.  In Proc. COLING 2004, pages
459–465.

Bond, Francis, Daniela Kurz,  and Satoshi  Shirai.
1998. Anchoring floating quantifiers in Japanese- 
to-English machine translation. In Proc. of COL- 
ING 1998, pages 152–159.

Ciaramita, Massimiliano  and Yasemin Altun.  2006.
Broad-coverage sense disambiguation and informa- 
tion extraction with a supersense sequence tagger. 
In Proc. of EMNLP  2006, pages 594–602.

Ciaramita, Massimiliano and Mark Johnson. 2003.
Supersense tagging of unknown nouns in WordNet. 
In Proc. of EMNLP  2003, pages 168–175.

Collins, Michael. 2002. Discriminative training meth- 
ods for hidden markov models: Theory and ex- 
periments with perceptron algorithms. In Proc. of 
EMNLP  2002, pages 1–8.


Crammer, Koby and Yoram Singer. 2003. Ultracon- 
servative online algorithms for multiclass problems. 
Journal of Machine Learning Research, 3:951–991.

Crammer,  Koby, Ofer Dekel, Joseph  Keshet, Shai 
Shalev-Shwartz, and Yoram Singer. 2006. Online 
passive-aggressive algorithms.  Journal of Machine 
Learning Research, 7:551–585.

Crammer,  Koby, Mark Dredze, and Alex Kulesza.
2009. Multi-class confidence weighted algorithms. 
In Proc. of EMNLP  2009, pages 496–504.

Curran,  James R.  2005. Supersense tagging  of un- 
known nouns using semantic similarity. In Proc. of 
ACL 2005, pages 26–33.

Fellbaum,  Christiane, editor.  1998.  WordNet: An 
Electronic Lexical Database. The MIT Press, Cam- 
bridge, MA.

Gil, David. 1987. Definiteness, NP configurationality 
and the count-mass distinction. In Reuland, Eric J. 
and Alice G. B. ter Meulen, editors, The Representa- 
tion of (In)definiteness,  pages 254–269. MIT Press.

Iida, Ryu, Kentaro Inui, and Yuji Matsumoto. 2009.
Capturing salience with a trainable cache model for 
zero-anaphora resolution.  In Proc. of ACL/IJCNLP
2009, pages 647–655.

Kawahara,  Daisuke  and Sadao  Kurohashi.	2001.
Japanese case frame  construction   by coupling the 
verb and its closest  case component. In Proc. of 
HLT 2001, pages 204–210.

Kawahara,  Daisuke  and Sadao  Kurohashi.	2006.
Case frame compilation  from the web using high- 
performance  computing.  In Proc. of LREC-06, 
pages 1344–1347.

Kazama, Jun’ichi and Kentaro Torisawa. 2008. Induc- 
ing gazetteers for named entity recognition by large- 
scale clustering of dependency relations. In Proc. of 
ACL 2008, pages 407–415, June.

Krishnan, Vijay and Christopher  D. Manning. 2006.
An effective  two-stage model for exploiting  non- 
local dependencies in named entity recognition.  In 
Proc. of COLING-ACL 2006, pages 1121–1128.

Kudo, Taku and Yuji Matsumoto. 2002.  Japanese 
dependency analysis using cascaded chunking. In 
Proc. of CONLL  2002, pages 1–7.

Kudo, Taku, Kaoru Yamamoto, and Yuji Matsumoto.
2004.    Applying  conditional random fields to 
Japanese  morphological analysis.   In  Proc. of 
EMNLP  2004, pages 230–237.


Kurohashi, Sadao,  Toshihisa  Nakamura,  Yuji  Mat- 
sumoto, and Makoto Nagao. 1994. Improvements 
of Japanese morphological  analyzer JUMAN.  In 
Proc. of The International  Workshop on Sharable 
Natural Language Resources, pages 22–38.

Marcus, Mitchell P., Mary Ann Marcinkiewicz, and 
Beatrice Santorini. 1993. Building a large anno- 
tated corpus of English: the Penn treebank.  Com- 
putational Linguistics, 19(2):313–330.

Mori, Shinsuke and Makoto Nagao. 1996. Word ex- 
traction from corpora and its part-of-speech estima- 
tion using distributional analysis. In Proc. of COL- 
ING 1996, volume  2, pages 1119–1122.

Murawaki,  Yugo and Sadao Kurohashi.  2008. Online 
acquisition of Japanese unknown  morphemes  us- 
ing morphological constraints. In Proc. of EMNLP
2008, pages 429–437.

Nagata, Masaaki. 1999. A part of speech estimation 
method for Japanese unknown words using a statis- 
tical model of morphology and context. In Proc. of 
ACL 1999, pages 277–284.

Nakagawa, Tetsuji and Yuji Matsumoto.  2006. Guess- 
ing parts-of-speech of unknown words using global 
information. In Proc. of COLING-ACL  2006, pages
705–712.

Quine, Willard Van. 1969. Ontological Relativity and
Other Essays. Columbia University  Press.

Saito, Kuniko,  Jun Suzuki, and Kenji Imamura. 2007.
Extraction of named entities from blogs using CRF. 
In Proc. of The 13th Annual Meeting of The Associ- 
ation for Natural Language Processing, pages 107–
110. (in Japanese).

Sasano, Ryohei and Sadao Kurohashi.  2008. Japanese 
named  entity recognition using structural natural 
language processing. In Proc. of IJCNLP 2008, 
pages 607–612.

Sasano, Ryohei and Sadao Kurohashi.   2009. A prob- 
abilistic model for associative anaphora resolution. 
In Proc. of EMNLP  2009, pages 1455–1464.

Sasano, Ryohei,  Daisuke Kawahara, and Sadao Kuro- 
hashi. 2004. Automatic construction of nominal 
case frames and its application to indirect anaphora 
resolution. In Proc. of COLING 2004, pages 1201–
1207.

Uchimoto,  Kiyotaka,  Satoshi Sekine, and Hitoshi Isa- 
hara. 2001. The unknown word problem: a mor- 
phological analysis of Japanese using maximum en- 
tropy aided by a dictionary.  In Proc. of EMNLP
2001, pages 91–99.

Yokoi, Toshio. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42–44.

