Hybrid Models for Semantic Classification of
Chinese Unknown Words

Xiaofei Lu
Department of Linguistics and Applied Language Studies
Pennsylvania State University
University Park, PA 16802, USA
xxl13@psu.edu

Abstract

This paper addresses the problem of clas- 
sifying   Chinese   unknown   words   into 
fine-grained  semantic  categories  defined 
in  a  Chinese   thesaurus.   We   describe 
three novel knowledge-based models that 
capture the relationship between the se- 
mantic  categories  of  an  unknown  word 
and those of its component characters in 
three  different  ways.  We  then  combine 
two of the knowledge-based  models with 
a corpus-based model which classifies 
unknown words using contextual infor- 
mation. Experiments show that the 
knowledge-based models outperform 
previous  methods  on the same task, but 
the  use  of  contextual  information  does 
not further improve performance.

1     Introduction

Research on semantic annotation has focused 
primarily on word sense disambiguation (WSD), 
i.e., the task of determining the appropriate sense 
for each instance of a polysemous word out of a 
set of senses defined for the word in some lexi- 
con. Much less work has been done on semantic 
classification of unknown words, i.e., words that 
are not listed in the lexicon. However, real texts 
typically contain a large number of unknown 
words. Successful  classification  of unknown 
words is not only useful for lexical acquisition, 
but also necessary for natural language process- 
ing (NLP) tasks that require semantic annotation.
  This paper addresses the problem of classify- 
ing Chinese unknown words into fine-grained 
semantic categories defined in a Chinese thesau- 
rus, Cilin (Mei et al., 1984). This thesaurus clas- 
sifies over 70,000  words into 12 major catego- 
ries, including  human  (A), concrete  object (B),


time and space (C), abstract object (D), attributes 
(E), actions (F), mental activities (G), activities 
(H), physical states (I), relations (J), auxiliaries 
(K), and honorifics (L). The 12 major categories 
are further divided into 94 medium categories, 
which in turn are subdivided into 1428 small 
categories. Each small category contains syno- 
nyms  that  are  close  in  meaning.  For  example, 
under  the  major  category  D, the medium  cate- 
gory Dm groups all words that refer to institu- 
tions, and the small category Dm05 groups all 
words that refer to educational  institutions,  e.g.,
学校 xuéxiào ‘school’.  Unknown  word classifi- 
cation involves a much larger search space than
WSD. In classifying words into small categories 
in  Cilin,  the  search  space  for  a  polysemous 
known  word  consists  of  all  the  categories  the 
word belongs to, but that for an unknown word 
consists of all the 1428 small categories.
  Research on WSD has concentrated on using 
contextual  information,  which  may  be  limited 
for  infrequent  unknown  words.  On  the  other 
hand, Chinese characters carry semantic infor- 
mation that is useful for predicting the semantic 
properties of the words containing them. We pre- 
sent three novel knowledge-based models that 
capture the relationship between the semantic 
categories of an unknown word and those of its 
component  characters  in  three  different  ways, 
and combine two of them with a corpus-based 
model  that uses contextual  information  to clas- 
sify unknown words. Experiments show that the 
combined knowledge-based model achieves an 
accuracy   of   61.6%   for   classifying   unknown 
words into small categories in Cilin, but the use 
of contextual information does not further im- 
prove performance.
  The rest of the paper is organized as follows. 
Section  2  details  the  three  novel  knowledge- 
based models proposed for this task. Section 3 
describes  a  corpus-based  model.  Section  4  re- 
ports  the  experiment   results  of  the  proposed



188
Proceedings of NAACL HLT 2007, pages 188–195,
Rochester, NY, April 2007. Qc 2007 Association for Computational Linguistics


models. Section 5 compares these results with 
previous  results.  Section  6 concludes  the paper 
and points to avenues for further research.

2     Knowledge-based Models

This section describes three novel knowledge- 
based models for semantic classification of Chi- 
nese unknown words, including an overlapping- 
character model, a character-category association 
model, and a rule-based model. These models 
model  the  relationship   between   the  semantic
category  of an unknown  word and those  of its
  

In each variant, the category with the maxi- 
mum score for a target word is proposed as the 
category of the word.

2.2     Character-Category  Associations

The relationship  between  the semantic  category 
of an unknown word and those of its component 
characters can also be captured in a more sophis- 
ticated way using information-theoretical  models. 
We use two statistical measures, mutual infor- 
mation and χ2, to compute character-category 
associations    and   word-category    associations.
2


component characters in three different ways.


Chen  (2004)  used  the  χ


measure  to  compute



2.1 	The Baseline Model

The baseline  model predicts  the category  of an 
unknown word by counting the number of over-


character-character  and word-word  associations,
but   not   word-category   associations.   We   use 
word-category   associations   to  directly  predict 
the semantic categories of unknown words.
2


lapping  characters  between  the  unknown  word


The  mutual  information  and χ


measures  are



and  the  member  words  in  each  category.  As 
words in the same category are similar in mean- 
ing and the meaning of a Chinese word is gener-
ally the composition of the meanings of its char-


calculated as in (3) and (4), where Asso(c,tj) de-
notes the association between a character c and a
semantic  category  tj, and P(X)  and f(X) denote 
the probability and frequency of X respectively.


acters, it is common for words in the same cate-



(3)



Asso MI



(c, t



j ) = log


P(c, t j )
P c P t


gory to share one or more character. This model 
tests the hypothesis that speakers draw upon the


(  )  (  j )
α (c, t j )


repertoire  of characters  that relate  to a concept 
when creating new words to realize it.


(4)


Asso χ 2  (c, t j ) = max


α (c, t k )


For each semantic category in Cilin, the set of


[ f (c, t


)]2


unique  characters  in its member  words  are ex- 
tracted, and the number of times each character


(5)


α (c, t j ) =



f (c) + f (t j )


occurs in word-initial, word-middle, and word- 
final  positions  is  recorded.  With  this  informa- 
tion, we develop two variants of the baseline 
model, which differ from each other in terms of 
whether it takes into consideration  the positions 
in which the characters occur in words.
  In variant 1, the score of a category is the sum 
of the number of occurrences  of each character 
of  the  target  word  in  the  category,  as  in  (1), 
where tj  denotes a category, w denotes the target


Once  the  character-category  associations  are
calculated, the association between a word w and 
a category tj, Asso(w,tj), can be calculated as the 
sum of the weighted  associations  between each 
of the word’s characters  and the category, as in 
(6), where ci denotes the ith character of w, |w| 
denotes the length of w, and λi denotes the weight 
of Asso(ci,tj).  The λ’s add up to 1. The weights 
are  determined  empirically  based  on  the  posi- 
tions of the characters in the word.
|w|


word, ci  denotes the ith character in w, n is the
length of w, and f(ci) is the frequency of ci in tj.


(6)


Asso(w, t j ) = ∑ λi Asso(ci , t j )
i =1



(1)


n
Score
(t j , 
w) = 
∑ f 
(ci )
i
 
=
1


A
s  
in  
va
ri
an
t  
2  
of  
th
e  
ba
se
li
ne  
m
od
el,  
th
e
char
acter
-
cate
gory  
asso
ciati
on 
mod
el 
can 
also 
be


  In variant 2, the score of a category is the sum 
of the number of occurrences  of each character 
of the unknown word in the category in its corre- 
sponding position, as in (2), where pi denotes the 
position of ci in w, which could be word-initial, 
word-middle,  or word-final,  and f(ci,pi) denotes 
the frequency of ci in position pi in tj.
n


made  sensitive  to  the  positions  in  which  the
characters  occur  in the words.  To this end, we 
first need to compute the position-sensitive  asso- 
ciations  between  a category  and  a character  in 
the word-initial, word-middle, and word-final 
positions separately. The position-sensitive  asso- 
ciation  between  an unknown  word  and  a cate- 
gory  can  then  be  computed  as the  sum  of the


(2)


Score(
t j , w) 
= ∑ f 
(ci , pi 
)
i
 
=
1


wei
ght
ed 
pos
itio
n-
sen
siti
ve  
ass
oci
ati
ons 
bet
we
en 
eac
h 
of 
its 
cha
rac
ters 
and 
the 
cat
eg
ory
.


  Once the word-category associations are com- 
puted, we can propose the highest ranked cate- 
gory or a ranked list of categories for each un- 
known word.

2.3     A Rule-Based Model

The third knowledge-based  model uses linguistic 
rules to classify unknown words based on the 
syntactic and semantic categories of their com- 
ponent characters. Rule-based models have not 
been  used  for this task  before.  However,  there 
are some regularities in the relationship between 
the semantic categories of unknown words and 
those of their component characters that can be 
captured in a more direct and effective way by 
linguistic rules than by statistical models.
  A  separate  set  of  rules  are  developed   for 
words of different lengths. Rules are initially 
developed based on knowledge about Chinese 
word formation, and are then refined by examin- 
ing the development data. In general, the com- 
plete rule set takes a few hours to develop.
The rule in (7) is developed for bisyllabic un-
known words. This rule proposes the common 
category of a bisyllabic word’s two characters as 
its  category.  It  is  especially  useful  for  words 
with a parallel structure, i.e., words whose two 
characters have the same meaning and syntactic
category,  e.g.,  坍塌 tāntā  ‘collapse’,  where  坍
tān and 塌 tā both mean ‘collapse’ and share the 
category  Id05.  The thresholds  for fA  and fB  are
determined  empirically  and are both set to 1 if 
AB is a noun and to 0 and 3 respectively other- 
wise.

(7) For a bisyllabic word AB, if A and B share a cate- 
gory c1, let fA  and fB  denote the number of times 
A and B occur in word-initial and word-final po- 
sitions in c respectively. If fA and fB both surpass 
the predetermined thresholds, propose c for AB.

  A  number  of  rules  are  developed  for  trisyl- 
labic words. While most rules in the model are 
general, the first rule in this set is rather specific, 
as it handles words with three specific prefixes,
大 dà ‘big’,  小 xiăo  ‘little’,  and  老 lăo ‘old’, 
which usually do not change the category of the
root word. The other four rules again utilize the 
categories of the unknown word’s component 
characters. The rules in (8b) and (8c) are similar 
to  the  rule  in  (7).  The  ones  in  (8d)  and  (8e) 
search for neighbor  words with a similar struc- 
ture as the target word. Eligible neighbors have a

1 A and B may each belong to more than one category.


common morpheme with the target word in the 
same   position   and  a  second   morpheme   that 
shares a category with the second morpheme of 
the   target   word.   For   example,   an   eligible
neighbor  for 推销商 tuīxiāo-shāng  ‘sales-man’
is 销售商 xiāoshòu-shāng  ‘distribut-or’.  These 
two words share the morpheme 商 shāng ‘busi-
nessman’ in the word-final position, and the 
morphemes  推销 tuīxiāo  ‘to market’  and 销售
xiāoshòu  ‘distribute’  share  the  category  He03. 
The rule in (8d) therefore applies in this case.

(8) For a trisyllabic word ABC:
a.	If A equals 大 dà ‘big’, 小 xiăo ‘little’, or 老
lăo ‘old’, propose the category of AB for
ABC if C is the diminutive suffix 儿 er or the
category of BC for ABC otherwise.
b.    If A and BC share a category c, propose c for
ABC.
c.	If AB and C share a category c, propose c for
ABC.
d.    If there is a word XYC such that XY and AB 
share a category, propose the category of 
XYC for ABC.
e.	If there is a word XBC such that X and A
share  a  category, propose the  category of
XBC for ABC.

  The rules for four-character  words  are given 
in (9). Like the rules in (8d) and (8e), these rules 
also search for neighbors of the target word.

(9) For a four-character word ABCD:
a.	If  there  is  a  word  XYZD/YZD  such  that
XYZ/YZ and ABC share a category, propose 
the category of XYZ/YZ for ABCD.
b.    If there is a word ABCX such that X and D
share  a  category, propose the  category of
ABCX for ABCD.
c.	If there is a word XYCD such that XY and AB
share  a  category, propose the  category of
XYCD for ABCD.
d.    If there is a word XBCD/BCD, propose the 
category of XBCD/BCD for ABCD.

3     A Corpus-Based Model

The knowledge-based models described above 
classify unknown words using information about 
the syntactic and semantic categories of their 
component characters. Another useful source of 
information  is  the  context  in  which  unknown 
words occur. While contextual information is the 
primary source of information used in WSD re- 
search and has been used for acquiring semantic 
lexicons and classifying unknown words in other 
languages  (e.g.,  Roark  and Charniak  1998;  Ci-


aramita 2003; Curran 2005), it has been used in 
only one previous study on semantic classifica- 
tion of Chinese unknown words (Chen and Lin,
2000). Part of the goal of this study is to investi-
gate   whether   and   how   these   two   different 
sources of information can be combined to im- 
prove performance on semantic classification of 
Chinese unknown words.
To this end, we first use the knowledge-based
models to propose a list of five candidate catego- 
ries for the target word, then extract a general- 
ized context for each category in Cilin from a 
corpus, and finally compute the similarity be- 
tween the context of the target word and the gen- 
eralized context of each of its candidate catego- 
ries. Comparing  the context  of the target  word 
with  generalized  contexts  of  categories  instead 
of  contexts  of  individual  words  alleviates  the 
data-sparseness   problem,   as  infrequent   words 
have  limited  contextual  information.   Limiting 
the search space for each target word to the top 
five candidate categories reduces the computa- 
tional cost that comes with the full search space.

3.1     Context Extraction and Representation

A  generalized  context  for  each  semantic  cate- 
gory is built from the contexts of its member 
words. This is done based on the assumption that


Window  Size  For  WSD,  both  topical  context 
and   microcontext   have   been   used   (Ide   and 
Véronis 1998). Topical context includes substan- 
tive words that co-occur with the target word 
within a larger window, whereas microcontext 
includes  words  in  a  small  window  around  the 
target word. We experiment with topical context 
and microcontext with window sizes of 100 and
6 respectively  (i.e.,  50 and 3 words  to the left 
and right of the target word respectively).

Context Representation We represent the con- 
text of a category as a vector <w1, w2, ..., wn>, 
where  n is the  total  number  of context  words, 
and wi  is the weight of the ith context word. To 
arrive at this representation, we first record the 
number   of   times   each   context   word   occurs 
within a specified window of each member word 
of a category in the corpus as a vector <f1, f2, ..., 
fn>, where fi  is the number of times the ith con- 
text word co-occurs with a member word of the 
category. We then compute the weight of a con- 
text word w in context c, W(w, c), using mutual 
information and t-test, which were reported by 
Weeds and Weir (2005) to perform the best on a 
pseudo-disambiguation task. These weight func- 
tions are computed as in (10) and (11), where N 
denotes the size of the corpus.


as the words in the same category have the same
or similar meaning, they tend to occur in similar



(10)



WPMI



(w, c) = log


P(w, c)
P(w)P(c)


contexts. In terms of context extraction and rep- 
resentation, we need to consider four factors.



(11)



Wt (w, c) =


P(w, c) − P(w)P(c)
P(w, c) N



Member Words The issue here is whether to 
include   the  contexts   of  polysemous   member 
words in building the generalized context of a 
category. Including these contexts without dis- 
crimination  introduces  noise.  To  measure  the 
effect of such noise, we build two versions of 
generalized context for each category, one using 
contexts  of  unambiguous  member  words  only,
and  the  other  using  contexts   of  all  member



3.2 	Contextual Similarity Measurement

We compute  the similarity  between  the context 
vectors  of the unknown  word and its candidate

categories  using  cosine.  The  cosine  of  two  n- 
dimensional vectors  x and yr , cos( x , yr ), is com-

puted  as  in  (12),  where  xi   and  yi   denote  the 
weight of the ith context word in  xr and yr .

n


words.



(12)


cos(xr, yr) 
=  	∑i =1 xi  y i 	


n	2	n	2



Context  Words There are two issues in select- 
ing   words   for   context   representation.   First, 
words that contribute little information to the 
discrimination  of  meaning  of  other  words,  in-






4 	Results


∑i =1 xi


∑i =1 yi


cluding conjunctions, numerals, auxiliaries, and 
non-Chinese sequences, are excluded. Second, to 
model the effect of frequency on the context 
words’  contribution  to  meaning  discrimination, 
we use two sets of context words: one consists of 
the 1000 most frequent words in the corpus; the 
other consists of all words in the corpus.


4.1     Experiment Setup

The models are developed and tested using the 
Contemporary Chinese Corpus from Peking 
University  (Yu  et  al.  2002)  and  the  extended 
Cilin released by the Information  Retrieval  Lab 
at  Harbin  Institute  of  Technology.  The  corpus


contains  all  the  articles  published  in  January,
1999 in People’s Daily, a major newspaper in 
China. It contains over 1.12 million tokens and is 
word-segmented and POS-tagged. Table 1 sum- 
marizes the distribution of words in Cilin. Of the
76,029 words in Cilin, 35,151  are found in the
Contemporary Chinese Corpus.

   Length 	Unambiguous 	Polysemous 	Total 	
1	2,674	2,068	4,742
2	39,057	5,403	44,460
3	15,112	752	15,864
4	9,397	942	10,338
≥5	590	34	624
     Total 	66,830 	9,199 	76,029 	
Table 1: Word distribution in the extended Cilin

  We classify words into the third-level catego- 
ries in the extended Cilin, which are equivalent 
to the small categories in the original Cilin. The 
development and test sets consist of 3,000 words 
each, which are randomly selected from the sub- 
set of words in Cilin that are two to four charac- 
ters long, that have occurred  in the Contempo- 
rary  Chinese  Corpus,  and  that  are  tagged  as 
nouns,  verbs,  or  adjectives  in  the  corpus.  The 
words in the development and test sets are also 
controlled for frequency, with 1/3 of them occur- 
ring 1-3 times, 3-6 times, and 7 or more times in 
the corpus respectively.
  As   Chen   (2004)   noted,   excluding   all   the 
words in the development and test data in the 
testing  stage  worsens  the data-sparseness  prob- 
lem for knowledge-based models, as some cate- 
gories have few member words, and some char- 
acters appear  in few words in some categories. 
To  alleviate  this  problem,  the  remove-one 
method is used for testing the knowledge-based 
models. In other words, the models are re-trained 
for  each  test  word  using  information  about  all 
the words in Cilin except the test word. The cor- 
pus-based model is trained once using the train- 
ing data only, as the data-sparseness problem is 
alleviated by using generalized contexts of cate- 
gories. Finally, if a word is polysemous, it is 
considered  to  have  been  correctly  classified  if 
the proposed category is one of its categories.

4.2     Results of the Baseline Model

Tables 2 and 3 summarize the results of the 
baseline  model  in terms  of the accuracy  of its 
best guess and best five guesses respectively.
  The columns labeled “Non-filtered” report re- 
sults where all categories are considered for each 
unknown   word,  and  the  ones  labeled  “POS-


filtered” report results where only the categories 
that agree with the POS category of the unknown 
word are considered. In the latter case, if the tar- 
get word is a noun, only the small categories un- 
der major categories A-D are considered; other- 
wise, only those under major categories E-L are 
considered. The results show that using POS in- 
formation about the unknown word to filter cate- 
gories  improves   performance.   Variant  2  per- 
forms better when only the best guess is consid- 
ered, indicating that it is useful to model the ef- 
fect of position on a character’s contribution to 
word meaning in this case. However, it is not 
helpful to be sensitive to character position when 
the best five guesses are considered.






Table 2: Results of the baseline model: best guess






Table 3: Results of the baseline model: best 5 guesses

4.3 	Results  of the Character-Category  As- 
sociation Model

In this model, only categories that agree with the 
POS  category  of  the  unknown  word  and  that 
share  at  least  one  character  with  the  unknown 
word are considered. These filtering steps sig- 
nificantly reduce the search space for this model.
  We discussed three parameters of the model in 
Section 2.2, including the statistical measure, the 
sensitivity to character position in computing 
character-category  associations,  and the weights of 
the associations between categories and char- 
acters in different positions. In addition, the 
computation  of  the  character-category   associa- 
tions can be sensitive or insensitive to the POS 
categories of the words containing the characters. 
In the POS-sensitive  way, associations are com- 
puted  among  nouns  (words  in categories  A-D) 
and non-nouns (words in categories E-L) sepa- 
rately, whereas in the POS-insensitive  way, they 
are computed using all the words.
  Tables 4 and 5 summarize the results of the 
character-category  association model in terms of 
the  accuracy  of  its  best  guess  and  best  five 
guesses respectively. In all cases, the weights 
assigned to word-initial, word-middle, and word- 
final characters are 0.49, 0, and 0.51 respectively.


  In terms of the best guess, the model achieves 
a best accuracy of 58.2%, a 6.5% improvement 
over the baseline result. The results show that χ2 
consistently performs better than mutual infor- 
mation, and computing position-sensitive char- 
acter-category  associations  consistently  im- 
proves performance. However, computing POS- 
sensitive associations gives mixed results.
  In terms of the best five guesses, the model 
achieves  a best  accuracy  of 83.8%  on  the  test 
data, a 2.1% improvement over the best baseline 
result. Using χ2 again achieves better results. 
However,   in   this   case,   the   best   results   are 
achieved when the character-category associa- 
tions  are  insensitive  to  both  character  position 
and the POS categories of words.


recall of the model is only 21.6%. The compara- 
ble results on the development and test sets indi- 
cate  that  the  encoded  rules  are  general.  The 
model generally performs better on longer words 
than on shorter words.

4.5     Combining     the     Character-Category
Association and Rule-Based Models

Given   that  the  rule-based   model   achieves   a 
higher precision but a lower recall than the char- 
acter-category  association  model, the two mod- 
els can be combined to improve the overall per- 
formance. In general, if the rule-based model 
returns one or more categories,  these categories 
are first ranked among themselves by their asso- 
ciations with the unknown word. They are then 
followed by the other categories returned by the 
character-category  association  model.  Tables  7 
and  8  summarize  the  results  of  combining  the 
two models.





Table 4: Results of the character-category association 
model: best guess



Table 7: Results of combining the character-category 
association and rule-based models: best guess



Table 5: Results of the character-category association 
model: best 5 guesses










Table 6: Results of the rule-based model: best guess

4.4 	Results of the Rule-Based Model


Table 8: Results of combining the character-
category association and rule-based models: best 5 
guesses

  In  terms  of  the  best  guess,  the  
combined model achieves an accuracy of 
61.6%, a 3.4% improvement  over the best 
result of the charac- ter-category   association   
model  alone.  This  is
2



Table  6  summarizes  the  results  of  the  rule-


achieved  using χ


with POS-sensitive  and posi-



based model in terms of recall, precision and F- 
score. The model returns multiple categories for 
some  words,  and  it is considered  to have  cor- 
rectly classified a word only when it returns a 
single,  correct  category  for the word. Precision 
of  the  model  is  computed  over  all  the  cases 
where the model returns a single guess, and re- 
call  is  computed   over  all  cases.  The  model
achieves  an  overall  precision  of  80.3%  on  the


tion-sensitive  computation  of character-category
associations.  In terms  of the best  five  guesses, 
the  model  achieves  an  accuracy  of  85.6%,  a
1.8%  improvement  over  the  best  result  of  the 
character-category association model alone.
  To facilitate comparison with previous studies, 
the results of the combined model in terms of its 
best  guess  in  classifying  unknown  words  into 
major and medium categories are summarized in
2



test data, much higher than the accuracy  of the


Table  9. As χ


consistently  outperforms  mutual
2



other  two  knowledge-based   models.  However,


information,   results  are  reported   for  χ


only.


With POS-sensitive  and position-sensitive  com-


putation of character-category associations, the 
combined model achieves an accuracy of 83.0% 
and 69.9% for classifying unknown words into 
major and medium categories respectively.


on words with higher frequency, suggesting 
that it may benefit from a larger corpus.



S
e
n
s
i
t
i
v
i
t
y
D
ev
el
op
m
en
t
T
e
s
t
P
O
S	Position
Ma
jor	Med
Ma
jor	Med
Y
es	Yes
Y
es	No
N
o	Yes
N
o	No
0.8
40	0.705
0.8
31	0.698
0.8
32	0.692
0.8
21	0.687
0.8
30	0.699
0.8
28	0.698
0.8
25	0.692
0.8
21	0.689


Table 9: Results of the combined model for classify-
ing unknown words into major and medium catego- 
ries: best guess

4.6     Results of the Corpus-Based Model

The corpus-based  model re-ranks the five high- 
est ranked categories proposed by the combined 
knowledge-based   model.  Table  10  enumerates 
the parameters of the model and lists the labels 
used to denote the various settings in Table 11.

  Parameter    Label     Setting 	Label 	













Table 11: Results of the corpus-based model


Member 
words 
Context 
words 
Window 
size 
Weight


MW	All 
members 
words
Un
am
big
uou
s 
me
mb
ers
CW	All 
words
10
00 
mo
st 
fre
qu
ent
WS
	10
0
6
WF
	Mu
tual 
information


all un all
1000
100
6 mi







Table  
12:  
Results  
of  the  
corpus-
based  
model  
on 
words 
with 
differen
t 
frequen
cy


  function 	t-test 	t 	
Table  10:  Parameter  settings  of  the  corpus-based 
model

  Table 11 summarizes the results of 16 runs of 
the model with different parameter settings. The 
best accuracy on the test data is 37.1%, achieved 
in run 5 with the following parameter settings: 
using unambiguous member words for building 
contexts  of  categories,  using  all  words  in  the 
corpus  for context  representation,  using  a win- 
dow size of 100, and using mutual information 
as the weight function. As the combined knowl- 
edge-based  model  gives  an accuracy  of  85.6% 
for its best five guesses,  the expected  accuracy 
of a naive model that randomly picks a candidate 
for each word as its best guess is 17.1%. Com- 
pared with this baseline, the corpus-based model 
achieves a 13.0% improvement, but it performs 
much worse than the knowledge-based models.
  Table 12 summarizes the accuracy of the top 
three runs of the model on words with different 
frequency in the corpus. Each of the three groups 
consists of 1,000 words that have occurred 1-2,
3-6, and 7 or more times in the corpus respec- 
tively.  The  model  consistently  performs  better


5     Related Work

The few previous studies on semantic classifica- 
tion of Chinese unknown word have primarily 
adopted knowledge-based models. Chen (2004) 
proposed  a model  that  retrieves  the  word  with 
the  greatest  association  with  the  target  word. 
This  model  is computationally  more  expensive 
than  our  character-category   association  model, 
as  it  entails   computing   associations   between 
every character-category, category-character, 
character-character, and word-word pair. He re- 
ported an accuracy of 61.6% on bisyllabic V-V 
compounds. However, he included all the test 
words in training the model. If we also include 
the test words in computing character-category 
associations, the computationally cheaper model 
achieves an overall accuracy of 75.6%, with an 
accuracy of 75.1% on verbs.
  Chen and Chen (2000) adopted similar exem- 
plar-based models. Chen and Chen used a mor- 
phological analyzer to identify the head of the 
target word and the semantic categories of its 
modifier. They then retrieved examples with the 
same head as the target word. Finally, they com- 
puted  the  similarity  between  two  words  as the


similarity between their modifiers, using the 
concept of information load (IC) of the least 
common ancestor (LCA) of the modifiers’ se- 
mantic categories. They reported an accuracy of
81% for classifying 200 unknown nouns. Given 
the small test set of their study, it is hard to di- 
rectly compare their results with ours.
  Tseng used a morphological analyzer in the 
same way, but she also derived the morpho- 
syntactic  relationship  between  the  morphemes. 
She retrieved examples that share a morpheme 
with the target word in the same position and 
filtered those with a different morpho-syntactic 
relationship. Finally, she computed the similarity 
between  two  words  as  the  similarity  between 
their non-shared morphemes, using a similar 
concept of IC of the LCA of two categories. She 
classified unknown words into the 12 major 
categories  only,  and reported  accuracies  65.8% 
on adjectives, 71.4% on nouns, and 52.8% on 
verbs. These results are not as good as the 83.0% 
overall accuracy our combined knowledge-based 
model achieved for classifying unknown words 
into major categories.
  Chen  and  Lin  (2000)  is  the  only  study  that 
used  contextual  information  for  the  same  task. 
To  generate  candidate  categories  for  a  word, 
they looked up its translations in a Chinese- 
English dictionary and the synsets of the transla- 
tions in WordNet, and mapped the synsets to the 
categories in Cilin. They used a corpus-based 
model  similar  to  ours  to  rank  the  candidates. 
They  reported  an accuracy  of 34.4%,  which  is 
close to the 37.1% accuracy of our corpus-based 
model, but lower than the 61.6% accuracy of our 
combined knowledge-based model. In addition, 
they  could  only  classify  the  unknown  words 
listed in the Chinese-English dictionary.

6     Conclusions

  We presented three knowledge-based models 
and  a corpus-based  model  for  classifying  Chi- 
nese unknown words into fine-grained categories 
in the Chinese thesaurus Cilin, a task important 
for lexical acquisition and NLP applications that 
require   semantic   annotation.   The  knowledge- 
based models use information about the catego- 
ries of the unknown words’ component charac- 
ters, while the corpus-based model uses contex- 
tual information. By combining the character- 
category association and rule-based models, we 
achieved  an  accuracy  of  61.6%.  The  corpus- 
based model did not improve performance.
  

Several avenues can be taken for further re- 
search. First, additional resources, such as bilin- 
gual dictionaries, morphological analyzers, par- 
allel corpora, and larger corpora with richer lin- 
guistic annotation may prove useful for improv- 
ing both the knowledge-based and corpus-based 
models. Second, we only explored one way to 
combine the knowledge-based and corpus-based 
models.  Future  work  may  explore  alternative 
ways  to  combine  these  models  to  make  better 
use of contextual information.

References

C.-J. Chen. 2004. Character-sense association and 
compounding template similarity: Automatic se- 
mantic classification of Chinese compounds. In 
Proceedings of the 3rd SIGHAN Workshop on Chi- 
nese Language Processing, pages 33–40.

M. Ciaramita and M. Johnson. 2003. Supersense tag- 
ging of unknown nouns in WordNet. In Proceed- 
ings of EMNLP-2003, pages 594-602.

K.-J. Chen and C.-J. Chen. 2000. Automatic semantic 
classification for Chinese unknown compound 
nouns.  In  Proceedings  of  COLING-2000, pages
173-179.

H.-H. Chen and C.-C. Lin. 2000. Sense-tagging Chi- 
nese corpus. In Proceedings of the 2nd Chinese 
Language Processing Workshop, pages 7-14.

J. Curran. 2005. Supersense tagging of unknown 
nouns using semantic similarity. In Proceedings of 
ACL-2006, pages 26-33.

N. Ide and J. Véronis. 1998. Introduction on the spe- 
cial issue on word sense disambiguation: The state 
of the art. Computational Linguistics 24(1):2–40.

J. Mei, Y. Zhu, Y. Gao, and H. Yin. (eds.) 1984.
Tongyici Cilin [A Thesaurus of Chinese Words]. 
Commercial Press, Hong Kong.

B. Roark and E. Charniak. 1998. Noun-phrase co- 
occurrence statistics for semi-automatic semantic 
lexicon construction. In Proceedings of COL- 
ING/ACL-1998, pages 1110-1116.

H. Tseng. 2003. Semantic classification of Chinese 
unknown words. In Proceedings of ACL-2003 Stu- 
dent Research Workshop, pages 72-79.

J. Weeds and D. Weir. 2005. Co-occurrence retrieval: 
A flexible framework for lexical distributional 
similarity.  Computational Linguistics  31(4):439–
475.

S. Yu, H. Duan, X. Zhu, and B. Sun. 2002. The basic 
processing of Contemporary Chinese Corpus at 
Peking University. Journal of Chinese Information 
Processing 16(5):49–64.

