Supervised Models for Coreference Resolution


Altaf Rahman and Vincent Ng
Human Language Technology Research Institute 
University of Texas at Dallas Richardson, TX 
75083-0688
{altaf,vince}@hlt.utdallas.edu





Abstract

Traditional learning-based coreference re- 
solvers  operate  by  training  a  mention- 
pair classifier for determining whether two 
mentions are coreferent or not.  Two in- 
dependent lines of recent research have 
attempted to improve these mention-pair 
classifiers, one by learning a mention- 
ranking model to rank preceding men- 
tions for a given anaphor, and the other 
by  training  an  entity-mention  classifier 
to  determine whether a  preceding clus- 
ter  is  coreferent with  a  given  mention. 
We propose a cluster-ranking approach to 
coreference resolution that combines the 
strengths of mention rankers and entity- 
mention models.  We additionally show 
how our cluster-ranking framework natu- 
rally allows discourse-new entity detection 
to be learned jointly with coreference res- 
olution. Experimental results on the ACE 
data sets demonstrate its superior perfor- 
mance to competing approaches.
1   Introduction

Noun phrase (NP) coreference resolution is the 
task of identifying which NPs (or mentions) re- 
fer to the same real-world entity or concept. Tra- 
ditional learning-based coreference resolvers op- 
erate by training a model for classifying whether 
two mentions are co-referring or not (e.g., Soon 
et al. (2001), Ng and Cardie (2002b), Kehler et al. 
(2004), Ponzetto and Strube (2006)). Despite their 
initial successes, these mention-pair models have 
at least two major weaknesses.  First, since each 
candidate antecedent for a mention to be resolved 
(henceforth an active mention) is considered inde- 
pendently of the others, these models only deter- 
mine how good a candidate antecedent is relative 
to the active mention, but not how good a candi- 
date antecedent is relative to other candidates. In


other words, they fail to answer the critical ques- 
tion of which candidate antecedent is most prob- 
able.  Second, they have limitations in their ex- 
pressiveness: the information extracted from the 
two mentions alone may not be sufficient for mak- 
ing an informed coreference decision, especially if 
the candidate antecedent is a pronoun (which is se- 
mantically empty) or a mention that lacks descrip- 
tive information such as gender (e.g., Clinton).
  To address the first weakness, researchers have 
attempted to train a mention-ranking model for 
determining which candidate antecedent is most 
probable given an active mention (e.g., Denis and 
Baldridge (2008)).  Ranking is arguably a more 
natural reformulation of coreference resolution 
than classification, as a ranker allows all candidate 
antecedents to be considered simultaneously and 
therefore directly captures the competition among 
them. Another desirable consequence is that there 
exists a natural resolution strategy for a ranking 
approach: a mention is resolved to the candidate 
antecedent that has the highest rank.  This con- 
trasts with classification-based approaches, where 
many clustering algorithms have been employed 
to co-ordinate the pairwise coreference decisions 
(because it is unclear which one is the best).
  To address the second weakness, researchers 
have investigated the acquisition of entity-mention 
coreference models (e.g., Luo et al. (2004), Yang 
et al. (2004)). Unlike mention-pair models, these 
entity-mention models are trained to determine 
whether an active mention belongs to a preced- 
ing, possibly partially-formed, coreference cluster. 
Hence, they can employ cluster-level features (i.e., 
features that are defined over any subset of men- 
tions in a preceding cluster), which makes them 
more expressive than mention-pair models.
  Motivated in part by these recently developed 
models, we propose in this paper a cluster- 
ranking approach to coreference resolution that 
combines the strengths of mention-ranking mod-




968

Proceedings of the 2009 Conference on Empirical  Methods in Natural  Language Processing, pages 968–977, 
Singapore, 6-7 August 2009. Qc 2009 ACL and AFNLP


els and entity-mention models.  Specifically, we 
recast coreference as the problem of determining 
which of a set of preceding coreference clusters 
is the best to link to an active mention using a 
learned cluster ranker. In addition, we show how 
discourse-new detection (i.e., the task of determin- 
ing whether a mention introduces a new entity in 
a discourse) can be learned jointly with corefer- 
ence resolution in our cluster-ranking framework. 
It is worth noting that researchers typically adopt 
a pipeline coreference architecture, performing 
discourse-new detection prior to coreference res- 
olution and using the resulting information to pre- 
vent a coreference system from resolving men- 
tions that are determined to be discourse-new (see 
Poesio et al. (2004) for an overview).  As a re- 
sult, errors in discourse-new detection could be 
propagated to the resolver, possibly leading to a 
deterioration of coreference performance (see Ng 
and Cardie (2002a)).  Jointly learning discourse- 
new detection and coreference resolution can po- 
tentially address this error-propagation problem.
  In sum, we believe our work makes three main 
contributions to coreference resolution:
Proposing a simple, yet effective coreference 
model.   Our work advances the state-of-the-art 
in coreference resolution by bringing learning- 
based coreference systems to the next level of 
performance.  When evaluated on the ACE 2005 
coreference data sets, cluster rankers outperform 
three competing models — mention-pair, entity- 
mention, and mention-ranking models — by a 
large margin.  Also, our joint-learning approach 
to discourse-new detection and coreference reso- 
lution consistently yields cluster rankers that out- 
perform those adopting the pipeline architecture. 
Equally importantly, cluster rankers are conceptu- 
ally simple and easy to implement and do not rely 
on sophisticated training and inference procedures 
to make coreference decisions in dependent rela- 
tion to each other, unlike relational coreference 
models (see McCallum and Wellner (2004)).

Bridging the gap between machine-learning 
approaches and linguistically-motivated ap- 
proaches to coreference resolution.   While ma- 
chine learning approaches to coreference resolu- 
tion have received a lot of attention since the mid-
90s, popular learning-based coreference frame- 
works such as the mention-pair model are ar- 
guably rather unsatisfactory from a linguistic point 
of view.   In particular, they have not leveraged


advances in discourse-based anaphora resolution 
research in the 70s and 80s.   Our work bridges 
this gap by realizing in a new machine learn- 
ing framework ideas rooted in Lappin and Leass’s 
(1994) heuristic-based pronoun resolver, which in 
turn was motivated by classic salience-based ap- 
proaches to anaphora resolution.
Revealing the importance of adopting the right 
model.   While entity-mention models have pre- 
viously been shown to be worse or at best 
marginally better than their mention-pair counter- 
parts (Luo et al., 2004; Yang et al., 2008), our 
cluster-ranking models, which are a natural exten- 
sion of entity-mention models, significantly out- 
performed all competing approaches.  This sug- 
gests that the use of an appropriate learning frame- 
work can bring us a long way towards high- 
performance coreference resolution.
  The rest of the paper is structured as follows. 
Section 2 discusses related work.  Section 3 de- 
scribes our baseline coreference models: mention- 
pair, entity-mention, and mention-ranking.  We 
discuss our cluster-ranking approach in Section 4, 
evaluate it in Section 5, and conclude in Section 6.

2   Related Work

Heuristic-based cluster ranking.  As men- 
tioned previously, the work most related to ours is 
Lappin and Leass (1994), whose goal is to perform 
pronoun resolution by assigning an anaphoric pro- 
noun to the highest-scored preceding cluster. Nev- 
ertheless, Lappin and Leass’s work differs from 
ours in several respects.  First, they only tackle 
pronoun resolution rather than the full coreference 
task. Second, their algorithm is heuristic-based; in 
particular, the score assigned to a preceding clus- 
ter is computed by summing over the weights as- 
sociated with the factors applicable to the cluster, 
where the weights are determined heuristically, 
rather than learned, unlike ours.
  Like many heuristic-based pronoun resolvers 
(e.g., Mitkov (1998)), they first apply a set of con- 
straints to filter grammatically incompatible can- 
didate antecedents and then rank the remaining 
ones using salience factors.   As a result, their 
cluster-ranking model employs only factors that 
capture the salience of a cluster, and can therefore 
be viewed as a simple model of attentional state 
(see Grosz and Sidner (1986)) realized by coref- 
erence clusters. By contrast, our resolution strat- 
egy is learned without applying hand-coded con-


straints in a separate filtering step.  In particular, 
we attempt to determine the compatibility between 
a cluster and an active mention, using factors that 
determine not only salience (e.g., the distance be- 
tween the cluster and the mention) but also lexical 
and grammatical compatibility, for instance.

Entity-mention coreference models.   Luo et al. 
(2004) represent one of the earliest attempts to 
investigate learning-based entity-mention models. 
They use the A N Y  predicate to generate cluster- 
level features as follows: given a binary-valued 
feature X  defined over a pair of mentions, they 
introduce an A N Y-X  cluster-level feature, which 
has the value T RU E if X is true between the active 
mention and any mention in the preceding clus- 
ter under consideration. Contrary to common wis- 
dom, this entity-mention model underperforms its 
mention-pair counterpart in spite of the general- 
ization from mention-pair to cluster-level features.
  In Yang et al.’s (2004) entity-mention model, a 
training instance is composed of an active men- 
tion mk , a preceding cluster C , and a mention 
mj  in C that is closest in distance to mk  in the 
associated text.   The feature set used to repre- 
sent the instance is primarily composed of fea- 
tures that describe the relationship between mj 
and mk , as well as a few cluster-level features. 
In other words, the model still relies heavily on 
features used in a mention-pair model.   In par- 
ticular, the inclusion of mj  in the feature vector 
representation to some extent reflects the authors’ 
lack of confidence that a strong entity-mention 
model can be trained without mention-pair-based 
features. Our ranking model, on the other hand, is 
trained without such features. More recently, Yang 
et al. (2008) have proposed another entity-mention 
model trained by inductive logic programming. 
Like their previous work, the scarcity of cluster- 
level predicates (only two are used) under-exploits 
the expressiveness of entity-mention models.

Mention ranking.   The notion of ranking can- 
didate antecedents can be traced back to center- 
ing algorithms, many of which use grammatical 
roles to rank forward-looking centers (see Grosz 
et al. (1995), Walker et al. (1998), and Mitkov 
(2002)).   However, mention ranking has been 
employed in learning-based coreference resolvers 
only recently.   As mentioned before, Denis and 
Baldridge (2008) train a mention-ranking model. 
Their work can be viewed as an extension of Yang 
et al.’s (2003) twin-candidate coreference model,


which ranks only two candidate antecedents at a 
time.  Unlike ours, however, their model ranks 
mentions rather than clusters, and relies on an 
independently-trained discourse-new detector.
Discourse-new detection.  Discourse-new de- 
tection is often tackled independently of coref- 
erence resolution.  Pleonastic its have been de- 
tected using heuristics (e.g., Kennedy and Bogu- 
raev (1996)) and learning-based techniques such 
as rule learning (e.g., Mu¨ ller (2006)), kernels (e.g., 
Versley et al. (2008)), and distributional methods 
(e.g., Bergsma et al. (2008)). Non-anaphoric defi- 
nite descriptions have been detected using heuris- 
tics (e.g., Vieira and Poesio (2000)) and unsu- 
pervised methods (e.g., Bean and Riloff (1999)). 
General discourse-new detectors that are applica- 
ble to different types of NPs have been built using 
heuristics (e.g., Byron and Gegg-Harrison (2004)) 
and modeled generatively (e.g., Elsner and Char- 
niak (2007)) and discriminatively (e.g., Uryupina 
(2003)). There have also been attempts to perform 
joint inference for discourse-new detection and 
coreference resolution using integer linear pro- 
gramming (ILP), where a discourse-new classifier 
and a coreference classifier are trained indepen- 
dently of each other, and then ILP is applied as a 
post-processing step to jointly infer discourse-new 
and coreference decisions so that they are consis- 
tent with each other (e.g., Denis and Baldridge 
(2007)). Joint inference is different from our joint- 
learning approach, which allows the two tasks to 
be learned jointly and not independently.

3   Baseline Coreference Models

In this section, we describe three coreference mod- 
els that will serve as our baselines: the mention- 
pair model,  the entity-mention model,  and the 
mention-ranking model. For illustrative purposes, 
we will use the text segment shown in Figure 1. 
Each mention m in the segment is annotated as 
mid,  where mid is the mention id and cid is
the id of the cluster to which m belongs.  As we
can see, the mentions are partitioned into four sets, 
with Barack Obama, his, and he in one cluster, and 
each of the remaining mentions in its own cluster.

3.1   Mention-Pair Model

As noted before, a mention-pair model is a clas- 
sifier that decides whether or not an active men- 
tion mk  is coreferent with a candidate antecedent 
mj .  Each instance i(mj , mk ) represents mj  and


[Barack Obama]1  nominated [Hillary Rodham Clinton]2  as


an active mention m


is coreferent with a par-


1 	2
[[his]1 secretary of state]3 on [Monday]4 . [He]1 ...



tial cluster c


k
that precedes m



.   Each training


3 	4 	5 	6 	j	k




Figure 1: An illustrative example


mk  and consists of the 39 features shown in Ta- 
ble 1. These features have largely been employed 
by state-of-the-art learning-based coreference sys- 
tems (e.g., Soon et al. (2001), Ng and Cardie 
(2002b), Bengtson and Roth (2008)), and are com- 
puted automatically. As can be seen, the features 
are divided into four blocks. The first two blocks 
consist of features that describe the properties of 
mj  and mk , respectively, and the last two blocks 
of features describe the relationship between mj 
and mk . The classification associated with a train- 
ing instance is either positive or negative, depend- 
ing on whether mj  and mk are coreferent.
If one training instance were created from each
pair of mentions, the negative instances would 
significantly outnumber  the  positives,  yielding 
a  skewed  class  distribution  that  will  typically 
have an adverse effect on model training.   As 
a  result,  only  a  subset  of  mention  pairs  will 
be  generated for  training.    Following Soon  et 
al. (2001), we create (1) a positive instance for 
each discourse-old mention mk  and its closest 
antecedent mj ;  and (2) a negative instance for 
mk  paired with each of the intervening mentions, 
mj+1, mj+2, . . . , mk−1.  In our running example 
shown in Figure 1, three training instances will 
be generated for He:  i(Monday, He), i(secretary 
of state, He), and i(his, He).   The first two of 
these instances will be labeled as negative, and 
the last one will be labeled as positive. To train a 
mention-pair classifier, we use the SVM learning 
algorithm from the SVMlight  package (Joachims,
2002), converting all multi-valued features into an 
equivalent set of binary-valued features.
  After training, the resulting SVM classifier is 
used to identify an antecedent for a mention in a 
test text.  Specifically, an active mention mk  se- 
lects as its antecedent the closest preceding men- 
tion that is classified as coreferent with mk . If mk 
is not classified as coreferent with any preceding 
mention, it will be considered discourse-new (i.e., 
no antecedent will be selected for mk ).

3.2   Entity-Mention Model

Unlike a mention-pair model, an entity-mention 
model is a classifier that decides whether or not


instance, i(cj , mk ),  represents cj   and mk .   The
features for an instance can be divided into two
types:  (1) features that describe mk  (i.e, those 
shown in the second block of Table 1), and (2) 
cluster-level features, which describe the relation- 
ship between cj   and mk .   Motivated by previ- 
ous work (Luo et al., 2004; Culotta et al., 2007; 
Yang et al., 2008), we create cluster-level fea- 
tures from mention-pair features using four pred- 
icates:   N O N E,  M O S T-FA L S E,  M O S T-T RU E,  and 
A L L.   Specifically, for each feature X  shown in 
the last two blocks in Table 1, we first convert X 
into an equivalent set of binary-valued features if 
it is multi-valued. Then, for each resulting binary- 
valued feature Xb, we create four binary-valued 
cluster-level features: (1) N O N E -Xb  is true when 
Xb is false between mk and each mention in cj ; (2) 
M O S T-FA L S E -Xb  is true when Xb is true between 
mk and less than half (but at least one) of the men- 
tions in cj ; (3) M O S T-T RU E-Xb is true when Xb is 
true between mk  and at least half (but not all) of 
the mentions in cj ; and (4) A L L -Xb is true when Xb 
is true between mk and each mention in cj . Hence, 
for each Xb, exactly one of these four cluster-level 
features evaluates to true.

  Following Yang et al. (2008), we create (1) a 
positive instance for each discourse-old mention 
mk  and the preceding cluster cj  to which it be- 
longs; and (2) a negative instance for mk  paired 
with each partial cluster whose last mention ap- 
pears between mk  and its closest antecedent (i.e., 
the last mention of cj ).  Consider again our run- 
ning example.  Three training instances will be 
generated for He: i({Monday}, He), i({secretary 
of state}, He), and i({Barack Obama, his},  He). 
The first two of these instances will be labeled as 
negative, and the last one will be labeled as pos- 
itive.  As in the mention-pair model, we train an 
entity-mention classifier using the SVM learner.

  After training, the resulting classifier is used to 
identify a preceding cluster for a mention in a test 
text.  Specifically, the mentions are processed in 
a left-to-right manner.   For each active mention 
mk , a test instance is created between mk  and 
each of the preceding clusters formed so far.  All 
the test instances are then presented to the classi- 
fier. Finally, mk  will be linked to the closest pre- 
ceding cluster that is classified as coreferent with 
mk . If mk  is not classified as coreferent with any


Features describing mj , a candidate antecedent
1	P RO N O U N  1	Y if mj  is a pronoun; else N
2	S U B J E C T  1	Y if mj  is a subject; else N
3	N E S T E D  1	Y if mj  is a nested NP; else N
Features describing mk , the mention to be resolved
4	N U M B E R  2	S I N G U L A R or P L U R A L, determined using a lexicon
5	G E N D E R  2	M A L E, F E M A L E, N E U T E R, or U N K N OW N, determined using a list of common first names
6	P RO N O U N  2	Y if mk is a pronoun; else N
7	N E S T E D  2	Y if mk is a nested NP; else N
8	S E M C L A S S  2	the semantic class of mk ; can be one of P E R S O N, L O C AT I O N, O R G A N I Z AT I O N, DAT E, T I M E,
M O N E Y, P E R C E N T, O B J E C T, OT H E R S, determined using WordNet and an NE recognizer
9	A N I M AC Y  2	Y if mk is determined as H U M A N or A N I M A L by WordNet and an NE recognizer; else N
10	P RO  T Y P E  2	the nominative case of mk if it is a pronoun; else NA. E.g., the feature value for him is H E
Features describing the relationship between mj , a candidate antecedent and mk , the mention to be resolved
11	H E A D  M AT C H 	C if the mentions have the same head noun; else I
12	S T R  M AT C H 	C if the mentions are the same string; else I
13	S U B S T R  M AT C H 	C if one mention is a substring of the other; else I
14	P RO  S T R  M AT C H 	C if both mentions are pronominal and are the same string; else I
15	P N  S T R  M AT C H 	C if both mentions are proper names and are the same string; else I
16	N O N P RO  S T R  M AT C H 	C if the two mentions are both non-pronominal and are the same string; else I
17	M O D I FI E R  M AT C H 	C if the mentions have the same modifiers; NA if one of both of them don’t have a modifier;
else I
18	P RO  T Y P E  M AT C H 	C if both mentions are pronominal and are either the same pronoun or diff erent only w.r.t. 
case; NA if at least one of them is not pronominal; else I
19	N U M B E R 	C if the mentions agree in number; I if they disagree; NA if the number for one or both 
mentions cannot be determined
20	G E N D E R 	C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentions 
cannot be determined
21	AG R E E M E N T 	C if the mentions agree in both gender and number; I if they disagree in both number and 
gender; else NA
22	A N I M AC Y 	C if the mentions match in animacy; I if they don’t; NA if the animacy for one or both mentions 
cannot be determined
23	B OT H  P RO N O U N S 	C if both mentions are pronouns; I if neither are pronouns; else NA
24	B OT H  P RO P E R  N O U N S 	C if both mentions are proper nouns; I if neither are proper nouns; else NA
25	M A X I M A L N P 	C if the two mentions does not have the same maximial NP projection; else I
26	S PA N 	C if neither mention spans the other; else I
27	I N D E FI N I T E 	C if mk is an indefinite NP and is not in an appositive relationship; else I
28	A P P O S I T I V E 	C if the mentions are in an appositive relationship; else I
29	C O P U L A R 	C if the mentions are in a copular construction; else I
30	S E M C L A S S 	C if the mentions have the same semantic class; I if they don’t; NA if the semantic class 
information for one or both mentions cannot be determined
31	A L I A S 	C if one mention is an abbreviation or an acronym of the other; else I
32	D I S TA N C E 	binned values for sentence distance between the mentions
Additional features describing the relationship between mj , a candidate antecedent and mk , the mention to be resolved
33	N U M B E R’ 	the concatenation of the N U M B E R  2 feature values of mj  and mk . E.g., if mj  is Clinton and
mk is they, the feature value is S I N G U L A R-P L U R A L, since mj  is singular and mk is plural
34	G E N D E R’ 	the concatenation of the G E N D E R  2 feature values of mj  and mk
35	P RO N O U N’ 	the concatenation of the P RO N O U N  2 feature values of mj  and mk
36	N E S T E D ’	the concatenation of the N E S T E D  2 feature values of mj  and mk
37	S E M C L A S S’ 	the concatenation of the S E M C L A S S  2 feature values of mj  and mk
38	A N I M AC Y’ 	the concatenation of the A N I M AC Y  2 feature values of mj  and mk
39	P RO  T Y P E’ 	the concatenation of the P RO  T Y P E  2 feature values of mj  and mk

Table 1: The feature set for coreference resolution. Non-relational features describe a mention and in 
most cases take on a value of YE S or NO. Relational features describe the relationship between the two 
mentions and indicate whether they are CO M PAT I B L E, IN C O M PAT I B L E or NOT AP P L I C A B L E.




preceding cluster, it will be considered discourse- 
new.  Note that all partial clusters preceding mk 
are formed incrementally based on the predictions 
of the classifier for the first k − 1 mentions.

3.3   Mention-Ranking Model

As noted before, a ranking model imposes a 
ranking on all the candidate antecedents of an


active  mention  mk .      To  train  a  ranker,  
we use the SVM ranker-learning algorithm 
from the SVMlight  package. Like the mention-
pair model, each training instance i(mj , mk ) 
represents mk and a preceding mention mj .   
In fact, the fea- tures that represent the 
instance as well as the method for creating 
training instances are identi- cal to those 
employed by the mention-pair model.


The  only  difference  lies  in  the  assignment  of 
class values to training instances. Assuming that 
Sk is the set of training instances created for 
anaphoric mention mk , the class value for an in- 
stance i(mj , mk ) in Sk is the rank of mj  among 
competing candidate antecedents, which is 2 if 
mj  is the closest antecedent of mk , and 1 other- 
wise.1  To exemplify, consider our running exam- 
ple. As in the mention-pair model, three training 
instances will be generated for He: i(Monday, He), 
i(secretary of state, He), i(his, He). The third in- 
stance will have a class value of 2, and the remain- 
ing two will have a class value of 1.
  After training, the mention-ranking model is ap- 
plied to rank the candidate antecedents for an ac- 
tive mention in a test text as follows. Given an ac- 
tive mention mk , we follow Denis and Baldridge 
(2008) and use an independently-trained classifier 
to determine whether mk  is discourse-new. If so, 
mk will not be resolved. Otherwise, we create test 
instances for mk by pairing it with each of its pre- 
ceding mentions. The test instances are then pre- 
sented to the ranker, and the preceding mention 
that is assigned the largest value by the ranker is 
selected as the antecedent of mk .
The discourse-new classifier used in the resolu-
tion step is trained with 26 of the 37 features2 de- 
scribed in Ng and Cardie (2002a) that are deemed 
useful for distinguishing between anaphoric and 
non-anaphoric mentions.  These features can be 
broadly divided into two types:  (1) features that 
encode the form of the mention (e.g., NP type, 
number, definiteness), and (2) features that com- 
pare the mention to one of its preceding mentions.

4   Coreference as Cluster Ranking

In this section, we describe our cluster-ranking ap- 
proach to NP coreference.  As noted before, our 
approach aims to combine the strengths of entity- 
mention models and mention-ranking models.

4.1   Training and Applying a Cluster Ranker
For ease of exposition, we will describe in this 
subsection how to train and apply a cluster ranker 
when it is used in a pipeline architecture, where 
discourse-new detection is performed prior to 
coreference resolution. In the next subsection, we 
will show how the two tasks can be learned jointly.

1 A larger class value implies a better rank in SVMlight .
  2 The  11  features  that  we  did  not  employ  are  C O N J, 
P O S S E S S I V E, M O D I FI E R, P O S T M O D I FI E D, S P E C I A L  N O U N S, 
P O S T, S U B C L A S S, T I T L E, and the positional features.
  

Recall that a cluster ranker ranks a set of pre- 
ceding clusters for an active mention mk .  Since 
a cluster ranker is a hybrid of a mention-ranking 
model and an entity-mention model, the way it is 
trained and applied is also a hybrid of the two. 
In particular, the instance representation employed 
by a cluster ranker is identical to that used by 
an entity-mention model, where each training in- 
stance i(cj , mk ) represents a preceding cluster cj 
and a discourse-old mention mk  and consists of 
cluster-level features formed from predicates. Un- 
like in an entity-mention model, however, in a 
cluster ranker, (1) a training instance is created be- 
tween each discourse-old mention mk and each of 
its preceding clusters; and (2) since we are train- 
ing a model for ranking clusters, the assignment of 
class values to training instances is similar to that 
of a mention ranker. Specifically, the class value of 
a training instance i(cj , mk ) created for mk  is the 
rank of cj  among the competing clusters, which is
2 if mk belongs to cj , and 1 otherwise.
  Applying the learned cluster ranker to a test text 
is similar to applying a mention ranker.  Specifi- 
cally, the mentions are processed in a left-to-right 
manner.   For each active mention mk , we first 
apply an independently-trained classifier to deter- 
mine if mk is discourse-new. If so, mk will not be 
resolved.  Otherwise, we create test instances for 
mk  by pairing it with each of its preceding clus- 
ters.  The test instances are then presented to the 
ranker, and mk  is linked to the cluster that is as- 
signed the highest value by the ranker. Note that 
these partial clusters preceding mk are formed in- 
crementally based on the predictions of the ranker 
for the first k−1 mentions; no gold-standard coref- 
erence information is used in their formation.


4.2   Joint Discourse-New Detection and
Coreference Resolution

The cluster ranker described above can be used 
to determine which preceding cluster a discourse- 
old mention should be linked to, but it cannot be 
used to determine whether a mention is discourse- 
new or not. The reason is simple: all the training 
instances are generated from discourse-old men- 
tions.  Hence, to jointly learn discourse-new de- 
tection and coreference resolution, we must train 
the ranker using instances generated from both 
discourse-old and discourse-new mentions.
  Specifically, when training the ranker, we pro- 
vide each active mention with the option to start



a new cluster by creating an additional instance 
that (1) contains features that solely describe the 
active mention (i.e., the features shown in the sec- 
ond block of Table 1), and (2) has the highest rank 
value among competing clusters (i.e., 2) if it is 
discourse-new and the lowest rank value (i.e., 1) 
otherwise. The main advantage of jointly learning 
the two tasks is that it allows the ranking model 
to evaluate all possible options for an active men- 
tion (i.e., whether to resolve it, and if so, which 
preceding cluster is the best) simultaneously.
  After training, the resulting cluster ranker pro- 
cesses the mentions in a test text in a left-to-right 
manner.  For each active mention mk , we create 
test instances for it by pairing it with each of its 
preceding clusters. To allow for the possibility that 
mk  is discourse-new, we create an additional test 
instance that contains features that solely describe 
the active mention (similar to what we did in the 
training step above).  All these test instances are 
then presented to the ranker. If the additional test 
instance is assigned the highest rank value by the 
ranker, then mk is classified as discourse-new and 
will not be resolved.  Otherwise, mk  is linked to 
the cluster that has the highest rank.  As before, 
all partial clusters preceding mk are formed incre- 
mentally based on the predictions of the ranker for 
the first k − 1 mentions.

5   Evaluation

5.1   Experimental Setup

Corpus.   We use the ACE 2005 coreference cor- 
pus as released by the LDC, which consists of the
599 training documents used in the official ACE 
evaluation.3   To ensure diversity, the corpus was 
created by selecting documents from six different 
sources:  Broadcast News (bn), Broadcast Con- 
versations (bc), Newswire (nw), Webblog (wb), 
Usenet (un), and conversational telephone speech 
(cts). The number of documents belonging to each 
source is shown in Table 2. For evaluation, we par- 
tition the 599 documents into a training set and a 
test set following a 80/20 ratio, ensuring that the 
two sets have the same proportion of documents 
from the six sources.
Mention extractor.   We evaluate each corefer- 
ence model using both true mentions (i.e., gold 
standard mentions4) and system mentions (i.e., au-

  3 Since we did not participate in ACE 2005, we do not 
have access to the official test set.
4 Note that only mention boundaries are used.




Table 2: Statistics for the ACE 2005 corpus 
tomatically identified mentions).  To extract sys-
tem mentions from a test text, we trained a men- 
tion extractor on the training texts. Following Flo- 
rian et al. (2004), we recast mention extraction as 
a sequence labeling task, where we assign to each 
token in a test text a label that indicates whether it 
begins a mention, is inside a mention, or is outside 
a mention. Hence, to learn the extractor, we create 
one training instance for each token in a training 
text and derive its class value (one of b, i, and o) 
from the annotated data. Each instance represents 
wi, the token under consideration, and consists of
29 linguistic features, many of which are modeled
after the systems of Bikel et al. (1999) and Florian 
et al. (2004), as described below.
Lexical   (7):   Tokens   in   a   window   of   7:
{wi−3, . . . , wi+3}.
Capitalization	(4):		Determine	whether	wi 
IsAllCap, IsInitCap, IsCapPeriod, and 
IsAllLower (see Bikel et al. (1999)). 
Morphological (8):	wi’s prefixes and suffixes of 
length one, two, three, and four.
Grammatical  (1):   The  part-of-speech  (POS) 
tag of wi  obtained using the Stanford log-linear 
POS tagger (Toutanova et al., 2003).
Semantic (1):   The named entity (NE) tag of wi 
obtained using the Stanford CRF-based NE recog- 
nizer (Finkel et al., 2005).
Gazetteers (8):  Eight dictionaries containing 
pronouns (77 entries), common words and words 
that are not names (399.6k), person names (83.6k), 
person titles and honorifics (761), vehicle words 
(226), location names (1.8k), company names 
(77.6k), and nouns extracted from WordNet that 
are hyponyms of P E R S O N (6.3k).
We employ CRF++5, a C++ implementation of
conditional random fields, for training the mention 
detector, which achieves an F-score of 86.7 (86.1 
recall, 87.2 precision) on the test set.  These ex- 
tracted mentions are to be used as system mentions 
in our coreference experiments.
Scoring programs.  To score the output of a 
coreference model, we employ three scoring pro- 
grams: MUC (Vilain et al., 1995), B3 (Bagga and 
Baldwin, 1998), and φ3-CEAF (Luo, 2005).

5 Available from http://crfpp.sourceforge.net


  There is a complication, however. When scor- 
ing a response (i.e., system-generated) partition 
against a key (i.e., gold-standard) partition, a scor- 
ing program needs to construct a mapping between 
the mentions in the response and those in the key. 
If the response is generated using true mentions, 
then every mention in the response is mapped to 
some mention in the key and vice versa; in other 
words, there are no twinless (i.e., unmapped) men- 
tions (Stoyanov et al., 2009).   However, this is 
not the case when system mentions are used. The 
aforementioned complication does not arise from 
the construction of the mapping, but from the fact 
that Bagga and Baldwin (1998) and Luo (2005) do 
not specify how to apply B3 and CEAF to score 
partitions generated from system mentions.
  We propose a simple solution to this problem: 
we remove all and only those twinless system 
mentions that are singletons before applying B3 
and CEAF. The reason is simple: since the coref- 
erence resolver has successfully identified these 
mentions as singletons, it should not be penal- 
ized, and removing them allows us to avoid such 
penalty. Note that we only remove twinless (as op- 
posed to all) system mentions that are singletons: 
this allows us to reward a resolver for success- 
ful identification of singleton mentions that have 
twins, thus overcoming a major weakness of and 
common criticism against the MUC scorer. Also, 
we retain twinless system mentions that are non- 
singletons, as the resolver should be penalized for 
identifying spurious coreference relations. On the 
other hand, we do not remove twinless mentions 
in the key partition, as we want to ensure that the 
resolver makes the correct (non-)coreference de- 
cisions for them. We believe that our proposal ad- 
dresses Stoyanov et al.’s (2009) problem of hav- 
ing very low precision when applying the CEAF 
scorer to score partitions of system mentions.

5.2   Results and Discussions

The mention-pair baseline.   We train our first 
baseline, the mention-pair coreference classifier, 
using the SVM learning algorithm as implemented 
in the SVMlight  package (Joachims, 2002).6  Re- 
sults of this baseline using true mentions and sys- 
tem mentions, shown in row 1 of Tables 3 and 4, 
are reported in terms of recall (R), precision (P), 
and F-score (F) provided by the three scoring pro-


grams.  As we can see, this baseline achieves F- 
scores of 54.3–70.0 and 53.4–62.5 for true men- 
tions and system mentions, respectively.
The  entity-mention baseline.   Next,  we  train 
our second baseline, the entity-mention corefer- 
ence classifier, using the SVM learner. Results of 
this baseline are shown in row 2 of Tables 3 and
4. For true mentions, this baseline achieves an F- 
score of 54.8–70.7. In comparison to the mention- 
pair baseline, F-score rises insignificantly accord- 
ing to all three scorers.7 Similar trends can be ob- 
served for system mentions, where the F-scores 
between the two models are statistically indistin- 
guishable across the board.  While the insignifi- 
cant performance difference is somewhat surpris- 
ing given the improved expressiveness of entity- 
mention models over mention-pair models, similar 
trends have been reported by Luo et al. (2004).
The mention-ranking baseline.   Our third base- 
line is the mention-ranking coreference model, 
trained using the ranker-learning algorithm in 
SVMlight.   To identify discourse-new mentions, 
we employ two methods. In the first method, we 
adopt a pipeline architecture, where we train an 
SVM classifier for discourse-new detection inde- 
pendently of the mention ranker on the training set 
using the 26 features described in Section 3.3. We 
then apply the resulting classifier to each test text 
to filter discourse-new mentions prior to corefer- 
ence resolution. Results of the mention ranker are 
shown in row 3 of Tables 3 and 4.   As we can 
see, the ranker achieves F-scores of 57.8–71.2 and
54.1–65.4 for true mentions and system mentions, 
respectively, yielding a significant improvement 
over the entity-mention baseline in all but one case 
(MUC/true mentions).
  In the second method, we perform discourse- 
new detection jointly with coreference resolution 
using the method described in Section 4.2. While 
we discussed this joint learning method in the con- 
text of cluster ranking, it should be easy to see 
that the method is equally applicable to a men- 
tion ranker.  Results of the mention ranker using 
this joint architecture are shown in row 4 of Ta- 
bles 3 and 4. As we can see, the ranker achieves 
F-scores of 61.6–73.4 and 55.6–67.1 for true men- 
tions and system mentions, respectively. For both 
types of mentions, the improvements over the cor- 
responding results for the entity-mention baseline



    6 For this and subsequent uses of the SVM learner in our 
experiments, we set all parameters to their default values.



    7 We use Approximate Randomization (Noreen, 1989) for 
testing statistical significance, with p set to 0.05.




1
2
3
4
5
6

Table 3: MUC, CEAF, and B3 coreference results using true mentions.



1
2
3
4
5
6

Table 4: MUC, CEAF, and B3 coreference results using system mentions.




are significant, and suggest that mention ranking is 
a precision-enhancing device. Moreover, in com- 
parison to the pipeline architecture in row 3, we 
see that F-score rises significantly by 2.2–3.8% for 
true mentions, and improves by a smaller margin 
of 0.3–1.7% for system mentions.  These results 
demonstrate the benefits of joint modeling.
Our cluster-ranking model.   Finally, we evalu- 
ate our cluster-ranking model. As in the mention- 
ranking baseline, we employ both the pipeline ar- 
chitecture and the joint architecture for discourse- 
new detection.  Results are shown in rows 5 and
6 of Tables 3 and 4, respectively, for the two ar- 
chitectures.  When true mentions are used, the 
pipeline architecture yields an F-score of 61.8–
74.8, which represents a significant improvement 
over the mention ranker adopting the pipeline ar- 
chitecture.  With the joint architecture, the clus- 
ter ranker achieves an F-score of 63.3–76.0. This 
also represents a significant improvement over the 
mention ranker adopting the joint architecture, the 
best of the baselines, and suggests that cluster 
ranking is a better precision-enhancing model than 
mention ranking.  Moreover, comparing the re- 
sults in these two rows reveals the superiority of 
the joint architecture over the pipeline architec- 
ture, particularly in terms of its ability to enhance 
system precision. Similar performance trends can 
be observed when system mentions are used.

6   Conclusions

We have presented a cluster-ranking approach that 
recasts the mention resolution process as the prob-


lem of finding the best preceding cluster to link an 
active mention to.  Crucially, our approach com- 
bines the strengths of entity-mention models and 
mention-ranking models. Experimental results on 
the ACE 2005 corpus show that (1) jointly learn- 
ing coreference resolution and discourse-new de- 
tection allows the cluster ranker to achieve bet- 
ter performance than adopting a pipeline corefer- 
ence architecture; and (2) our cluster ranker signif- 
icantly outperforms the mention ranker, the best of 
the three baseline coreference models, under both 
the pipeline architecture and the joint architecture. 
Overall, we believe that our cluster-ranking ap- 
proach advances the state-of-the-art in coreference 
resolution both theoretically and empirically.

Acknowledgments

We thank the three anonymous reviewers for their 
invaluable comments on the paper. This work was 
supported in part by NSF Grant IIS-0812261.


References

A. Bagga and B. Baldwin.  1998.  Entity-based cross- 
document coreferencing using the vector space 
model. In Proc. of COLING-ACL, pages 79–85.
D. Bean and E. Riloff. 1999. Corpus-based identifica- 
tion of non-anaphoric noun phrases. In Proc. of the 
ACL, pages 373–380.
E. Bengtson and D. Roth.  2008.  Understanding the 
values of features for coreference resolution.   In 
Proc. of EMNLP, pages 294–303.
S. Bergsma, D. Lin, and R. Goebel.  2008.  Distribu- 
tional identification of non-referential pronouns. In 
Proc. of ACL-08:HLT, pages 10–18.


D. Bikel, R. Schwartz, and R. Weischedel.  1999.  An 
algorithm that learns what’s in a name.   Machine 
Learning, 34(1–3):211–231.
D. Byron and W. Gegg-Harrison.  2004.  Eliminating 
non-referring noun phrases from coreference resolu- 
tion. In Proc. of DAARC, pages 21–26.
A. Culotta, M. Wick, and A. McCallum. 2007. First- 
order probabilistic models for coreference resolu- 
tion. In Proc. of NAACL-HLT, pages 81–88.
P. Denis and J. Baldridge. 2007. Global, joint determi- 
nation of anaphoricity and coreference resolution us- 
ing integer programming. In Proc. of NAACL-HLT, 
pages 236–243.
P. Denis and J. Baldridge.  2008.  Specialized models 
and ranking for coreference resolution.  In Proc. of 
EMNLP, pages 660–669.
M. Elsner and E. Charniak.   2007.   A generative 
discourse-new model for text coherence. Technical 
Report CS-07-04, Brown University.
J. R. Finkel, T. Grenager, and C. Manning. 2005.  In- 
corporating non-local information into information 
extraction systems by Gibbs sampling.  In Proc. of 
the ACL, pages 363–370.
R.  Florian,   H.  Hassan,   A.  Ittycheriah,   H.  Jing, 
N. Kambhatla, X. Luo, N. Nicolov, and I. Zitouni.
2004. A statistical model for multilingual entity de-
tection and tracking. In Proc. of HLT/NAACL.
B. J. Grosz, A. K. Joshi, and S. Weinstein.	1995.
Centering: A framework for modeling the local co- 
herence of discourse.   Computational Linguistics,
21(2):203–226.
B. J. Grosz and C. L. Sidner.  1986.  Attention, inten- 
tions, and the structure of discourse. Computational 
Linguistics, 12(3):175–204.
T. Joachims.  2002.  Optimizing search engines using 
clickthrough data. In Proc. of KDD, pages 133–142.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies 
for pronoun interpretation. In Proc. of HLT/NAACL.
C. Kennedy and B. Boguraev. 1996. Anaphor for ev- 
eryone:  Pronominal anaphora resolution without a 
parser. In Proc. of COLING, pages 113–118.
S. Lappin and H. Leass.   1994.   An algorithm for 
pronominal anaphora resolution.   Computational 
Linguistics, 20(4):535–562.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and 
S. Roukos.  2004.   A mention-synchronous coref- 
erence resolution algorithm based on the Bell tree. 
In Proc. of the ACL, pages 135–142.
X. Luo. 2005. On coreference resolution performance 
metrics. In Proc. of HLT/EMNLP, pages 25–32.
A. McCallum and B. Wellner. 2004. Conditional mod- 
els of identity uncertainty with application to noun 
coreference. In Advances in NIPS.
R. Mitkov. 2002. Anaphora Resolution. Longman.
R. Mitkov. 1998. Robust pronoun resolution with lim- 
ited knowledge.  In Proc. of COLING/ACL, pages
869–875.


C. Mu¨ ller.   2006.   Automatic detection of nonrefer- 
ential it in spoken multi-party dialog.  In Proc. of 
EACL, pages 49–56.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and 
non-anaphoric noun phrases to improve coreference 
resolution. In Proc. of COLING, pages 730–736.
V. Ng and C. Cardie. 2002b. Improving machine learn- 
ing approaches to coreference resolution. In Proc. of 
the ACL, pages 104–111.
E. W. Noreen. 1989. Computer Intensive Methods for 
Testing Hypothesis: An Introduction. John Wiley & 
Sons.
M. Poesio, O. Uryupina, R. Vieira, M. Alexandrov- 
Kabadjov, and R. Goulart.  2004.  Discourse-new 
detectors for definite description resolution: A sur- 
vey and a preliminary proposal. In Proc. of the ACL 
Workshop on Reference Resolution.
S. P. Ponzetto and M. Strube. 2006. Exploiting seman- 
tic role labeling, WordNet and Wikipedia for coref- 
erence resolution.  In Proc. of HLT/NAACL, pages
192–199.
W.  M.  Soon,  H.  T.  Ng,  and  D.  Lim.    2001.    A 
machine learning approach to coreference resolu- 
tion of noun phrases.   Computational Linguistics,
27(4):521–544.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution: 
Making sense of the state-of-the-art. In Proc. of the 
ACL.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003.     Feature-rich  part-of-speech  tagging  with 
a cyclic dependency network.   In Proc. of HLT- 
NAACL, pages 252–259.
O. Uryupina.  2003.  High-precision identification of 
discourse new and unique noun phrases. In Proc. of 
the ACL Student Research Workshop.
Y. Versley,  A. Moschitti, M. Poesio,  and X. Yang.
2008.  Coreference systems based on kernel meth- 
ods. In Proc. of COLING, pages 961–968.
R. Vieira and M. Poesio. 2000. Processing definite de- 
scriptions in corpora. In Corpus-based and Compu- 
tational Approaches to Discourse Anaphora, pages
189–212. UCL Press.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and 
L. Hirschman. 1995. A model-theoretic coreference 
scoring scheme. In Proc. of MUC-6, pages 45–52.
M. Walker, A. Joshi, and E. Prince, editors.   1998.
Centering Theory in Discourse.  Oxford University
Press.
X.  Yang,  G.  Zhou,  J.  Su,  and  C.  L.  Tan.	2003.
Coreference resolution using competitive learning 
approach. In Proc. of the ACL, pages 176–183.
X. Yang, J. Su, G. Zhou, and C. L. Tan. 2004. An NP- 
cluster based approach to coreference resolution. In 
Proc. of COLING, pages 226–232.
X. Yang, J. Su, J. Lang, C. L. Tan, and S. Li.  2008.
An entity-mention model for coreference resolution 
with inductive logic programming.  In Proc. of the 
ACL, pages 843–851.

