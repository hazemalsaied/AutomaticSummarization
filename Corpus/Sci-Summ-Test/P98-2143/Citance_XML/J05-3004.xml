<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We compare two ways of obtaining lexical knowledge for antecedent selection in other-anaphora and definite noun phrase coreference.</S>
		<S sid ="2" ssid = "2">Specifically, we compare an algorithm that relies on links encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora by means of shallow lexico-semantic patterns.</S>
		<S sid ="3" ssid = "3">As corpora we use the British National Corpus (BNC), as well as the Web, which has not been previously used for this task.</S>
		<S sid ="4" ssid = "4">Our results show that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor– antecedent relations that exploit subjective or context-dependent knowledge; (b) for other- anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NP coreference, the Web-based method yields results comparable to those obtained using WordNet over the whole data set and outperforms the WordNet-based method on subsets of the data set; (d) in both case studies, the BNC-based method is worse than the other methods because of data sparseness.</S>
		<S sid ="5" ssid = "5">Thus, in our studies, the Web-based method alleviated the lexical knowledge gap often encountered in anaphora resolution and handled examples with context-dependent relations between anaphor and antecedent.</S>
		<S sid ="6" ssid = "6">Because it is inexpensive and needs no hand-modeling of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution systems.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.</S>
			<S sid ="8" ssid = "8">Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively.</S>
			<S sid ="9" ssid = "9">Less attention has been paid to nominal anaphors with full lexical heads, which cover a variety of phenomena, such as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative anaphora (Examples (3–4)).1 ∗ School of Computing, University of Leeds, Woodhouse Lane, LS2 9JT Leeds, UK.</S>
			<S sid ="10" ssid = "10">Email: markert@comp.leeds.ac.uk.</S>
			<S sid ="11" ssid = "11">† School of Informatics, University of Edinburgh, 2 Buccleuch Place, EH8 9LW Edinburgh, UK.</S>
			<S sid ="12" ssid = "12">Email: mnissim@inf.ed.ac.uk.</S>
			<S sid ="13" ssid = "13">1 In all examples presented in this article, the anaphor is typed in boldface and the correct antecedent in. italics.</S>
			<S sid ="14" ssid = "14">The abbreviation in parentheses at the end of each example specifies the corpus from which the example is taken: WSJ stands for the Wall Street Journal, Penn Treebank, release 2; BNC stands for British National Corpus (Burnard 1995), and MUC6 for the combined training/test set for the coreference task of the Sixth Message Understanding Conference (Hirschman and Chinchor 1997).</S>
			<S sid ="15" ssid = "15">Submission received: 15 December 2003; revised submission received: 21 November 2004; accepted for publication: 19 March 2005.</S>
			<S sid ="16" ssid = "16">© 2005 Association for Computational Linguistics (1) The death of Maxwell, the British publishing magnate whose empire collapsed in ruins of fraud, and who was the magazine’s publisher, gave the periodical a brief international fame.</S>
			<S sid ="17" ssid = "17">(BNC) (2) [.</S>
			<S sid ="18" ssid = "18">] you don’t have to undo the jacket to get to the map—particularly important when it’s blowing a hooley.</S>
			<S sid ="19" ssid = "19">There are elasticated adjustable drawcords on the hem, waist and on the hood.</S>
			<S sid ="20" ssid = "20">(BNC) (3) In addition to increasing costs as a result of greater financial exposure for members, these measures could have other, far-reaching repercussions.</S>
			<S sid ="21" ssid = "21">(WSJ) (4) The ordinance, in Moon Township, prohibits locating a group home for the handicapped within a mile of another such facility.</S>
			<S sid ="22" ssid = "22">(WSJ) In Example (1), the definite noun phrase (NP) the periodical corefers with the magazine.2 In Example (2), the definite NP the hood can be felicitously used because a related entity has already been introduced by the NP the jacket, and a part-of relation between the two entities can be established.</S>
			<S sid ="23" ssid = "23">Examples (3)–(4) are instances of other-anaphora.</S>
			<S sid ="24" ssid = "24">Other-anaphora are a subclass of comparative anaphora (Halliday and Hasan 1976; Webber et al. 2003) in which the anaphoric NP is introduced by a lexical modifier (such as other, such, and comparative adjectives) that specifies the relationship (such as set-complement, similarity and comparison) between the entities invoked by anaphor and antecedent.</S>
			<S sid ="25" ssid = "25">For other-anaphora, the modifiers other or another provide a set-complement to an entity already evoked in the discourse model.</S>
			<S sid ="26" ssid = "26">In Example (3), the NP other, far-reaching repercussions refers to a set of repercussions excluding increasing costs and can be paraphrased as other (far-reaching) repercussions than (increasing) costs.</S>
			<S sid ="27" ssid = "27">Similarly, in Example (4), the NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before.</S>
			<S sid ="28" ssid = "28">A large and diverse amount of lexical or world knowledge is usually necessary to understand anaphors with full lexical heads.</S>
			<S sid ="29" ssid = "29">For the examples above, we need the knowledge that magazines are periodicals, that hoods are parts of jackets, that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities.</S>
			<S sid ="30" ssid = "30">Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on handcrafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998).3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora.</S>
			<S sid ="31" ssid = "31">Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontologies; domain-specific and context-dependent knowledge; different ways of encoding information; and sense ambiguity.</S>
			<S sid ="32" ssid = "32">2 In this article, we restrict the notion of definite NPs to NPs modified by the article ‘the.’.</S>
			<S sid ="33" ssid = "33">3 These systems also use surface-level features (such as string matching), recency, and grammatical.</S>
			<S sid ="34" ssid = "34">constraints.</S>
			<S sid ="35" ssid = "35">In this article, we concentrate on the lexical and semantic knowledge employed.</S>
			<S sid ="36" ssid = "36">368 In Section 3, we discuss an alternative to the manual construction of knowledge bases, which we call the corpus-based approach.</S>
			<S sid ="37" ssid = "37">A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002).</S>
			<S sid ="38" ssid = "38">Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways.</S>
			<S sid ="39" ssid = "39">First, we suggest using the Web for anaphora resolution instead of the smaller- size, but less noisy and more balanced, corpora used previously, making available a huge additional source of knowledge.4 Second, we do not induce a fixed lexical knowledge base from the Web but use shallow lexicosyntactic patterns and their Web frequencies for anaphora resolution on the fly.</S>
			<S sid ="40" ssid = "40">This allows us to circumvent some of the above-mentioned methodological problems that occur with any fixed ontology, whether constructed manually or automatically.</S>
			<S sid ="41" ssid = "41">The core of this article consists of an empirical comparison of these different sources of lexical knowledge for the task of antecedent selection or antecedent ranking in anaphora resolution.</S>
			<S sid ="42" ssid = "42">We focus on two types of full NP anaphora: other-anaphora (Section 4) and definite NP coreference (Section 5).5 In both case studies, we compare an algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora (both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology (WordNet 1.7.1).</S>
			<S sid ="43" ssid = "43">We specifically address the following questions: 1.</S>
			<S sid ="44" ssid = "44">Can the shortcomings of using a fixed ontology that have been stipulated.</S>
			<S sid ="45" ssid = "45">by previous research on definite NPs be confirmed in our coreference study?</S>
			<S sid ="46" ssid = "46">Do they also hold for other-anaphora, a phenomenon less studied so far?</S>
	</SECTION>
	<SECTION title="How  does corpus-based knowledge acquisition compare to using. " number = "2">
			<S sid ="47" ssid = "1">manually constructed lexical hierarchies in antecedent selection?</S>
			<S sid ="48" ssid = "2">And is the use of the Web an improvement over using smaller, but manually controlled, corpora?</S>
			<S sid ="49" ssid = "3">anaphoric phenomenon addressed?</S>
			<S sid ="50" ssid = "4">In Section 6 we discuss several aspects of our findings that still need elaboration in future work.</S>
			<S sid ="51" ssid = "5">Specifically, our work is purely comparative and regards the different lexical knowledge sources in isolation.</S>
			<S sid ="52" ssid = "6">It remains to be seen how the results carry forward when the knowledge sources interact with other features (for example, grammatical preferences).</S>
			<S sid ="53" ssid = "7">A similar issue concerns the integration of the methods into anaphoricity determination in addition to antecedent selection.</S>
			<S sid ="54" ssid = "8">Additionally, future work should explore the contribution of different knowledge sources for yet other anaphora types.</S>
			<S sid ="55" ssid = "9">4 There is a growing body of research that uses the Web for NLP.</S>
			<S sid ="56" ssid = "10">As we concentrate on anaphora resolution.</S>
			<S sid ="57" ssid = "11">in this article, we refer the reader to Grefenstette (1999) and Keller and Lapata (2003), as well as the December 2003 special issue of Computational Linguistics, for an overview of the use of the Web for other NLP tasks.</S>
			<S sid ="58" ssid = "12">5 As described above, in other-anaphora the entities invoked by the anaphor are a set complement to the.</S>
			<S sid ="59" ssid = "13">entity invoked by the antecedent, whereas in definite NP coreference the entities invoked by anaphor and antecedent are identical.</S>
			<S sid ="60" ssid = "14">369 2.</S>
			<S sid ="61" ssid = "15">The Knowledge Gap and Other Problems for Lexico-semantic Resources.</S>
			<S sid ="62" ssid = "16">A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002; Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical and world knowledge for the resolution of full NP anaphora and the lack of such knowledge in existing ontologies (Section 2.1).</S>
			<S sid ="63" ssid = "17">In addition to this knowledge gap, we summarize other, methodological problems with the use of ontologies in anaphora resolution (Section 2.2).</S>
			<S sid ="64" ssid = "18">2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads.</S>
			<S sid ="65" ssid = "19">In the following, we discuss previous studies on the automatic resolution of coreference, bridging and comparative anaphora, concentrating on work that yields insights into the use of lexical and semantic knowledge.</S>
			<S sid ="66" ssid = "20">2.1.1 Coreference.</S>
			<S sid ="67" ssid = "21">The prevailing current approaches to coreference resolution are evaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treat pronominal and full NP anaphora, named-entity coreference, and non-anaphoric coreferential links that can be stipulated by appositions and copula.</S>
			<S sid ="68" ssid = "22">The performance of these approaches on definite NPs is often substantially worse than on pronouns and/or named entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ng and Cardie 2002b; Yang et al. 2003).</S>
			<S sid ="69" ssid = "23">For example, for a coreference resolution algorithm on German texts, Strube, Rapp, and Mueller (2002) report an F-measure of 33.9% for definite NPs that contrasts with 82.8% for personal pronouns.</S>
			<S sid ="70" ssid = "24">Several reasons for this performance difference have been established.</S>
			<S sid ="71" ssid = "25">First, whereas pronouns are mostly anaphoric in written text, definite NPs do not have to be so, inducing the problem of whether a definite NP is anaphoric in addition to determining an antecedent from among a set of potential antecedents (Fraurud 1990; Vieira and Poesio 2000).6 Second, the antecedents of definite NP anaphora can occur at considerable distance from the anaphor, whereas antecedents to pronominal anaphora tend to be relatively close (Preiss, Gasperin, and Briscoe 2004; McCoy and Strube 1999).</S>
			<S sid ="72" ssid = "26">An automatic system can therefore more easily restrict its antecedent set for pronominal anaphora.</S>
			<S sid ="73" ssid = "27">Third, it is in general believed that pronouns are used to refer to entities in focus, whereas entities that are not in focus are referred to by definite descriptions (Hawkins 1978; Ariel 1990; Gundel, Hedberg, and Zacharski 1993), because the head nouns of anaphoric definite NPs provide the reader with lexico-semantic knowledge.</S>
			<S sid ="74" ssid = "28">Antecedent accessibility is therefore additionally restricted via semantic compatibility and does not need to rely on notions of focus or salience to the same extent as for pronouns.</S>
			<S sid ="75" ssid = "29">Given this lexical richness of common noun anaphors, many resolution algorithms for coreference have incorporated manually controlled lexical hierarchies, such as WordNet.</S>
			<S sid ="76" ssid = "30">They use, for example, a relatively coarse-grained notion of semantic compatibility between a few high-level concepts in WordNet (Soon, Ng, and Lim 2001), or more detailed hyponymy and synonymy links between anaphor and antecedent head nouns (Vieira and Poesio 6 A two-stage process in which the first stage identifies anaphoricity of the NP and the second the antecedent for anaphoric NPs (Uryupina 2003; Ng 2004) can alleviate this problem.</S>
			<S sid ="77" ssid = "31">In this article, we focus on the second stage, namely, antecedent selection.</S>
			<S sid ="78" ssid = "32">370 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b, among others).</S>
			<S sid ="79" ssid = "33">However, several researchers have pointed out that the incorporated information is still insufficient.</S>
			<S sid ="80" ssid = "34">Harabagiu, Bunescu, and Maiorano (2001) (see also Kameyama 1997) report that evaluation of previous systems has shown that “more than 30% of the missed coreference links are due to the lack of semantic consistency information between the anaphoric noun and its antecedent noun” (page 59).</S>
			<S sid ="81" ssid = "35">Vieira and Poesio (2000) report results on anaphoric definite NPs in the WSJ that stand in a synonymy or hyponymy relation to their antecedents (as in Example (1)).</S>
			<S sid ="82" ssid = "36">Using WordNet links to retrieve the appropriate knowledge proved insufficient, as only 35.0% of synonymy relations and 56.0% of hyponymy relations needed were encoded in WordNet as direct or inherited links.7 The semantic knowledge used might also not necessarily improve on string matching: Soon, Ng, and Lim (2001) final, automatically derived decision tree does not incorporate their semantic-compatibility feature and instead relies heavily on string matching and aliasing, thereby leaving open how much information in a lexical hierarchy can improve over string matching.</S>
			<S sid ="83" ssid = "37">In this article, we concentrate on this last of the three problems (insufficient lexical knowledge).</S>
			<S sid ="84" ssid = "38">We investigate whether the knowledge gap for definite NP coreference can be overcome by using corpora as knowledge sources as well as whether the incorporation of lexical knowledge sources improves over simple head noun matching.</S>
			<S sid ="85" ssid = "39">2.1.2 Comparative Anaphora.</S>
			<S sid ="86" ssid = "40">Modjeska (2002)—one of the few computational studies on comparative anaphora—shows that lexico-semantic knowledge plays a larger role than grammatical salience for other-anaphora.</S>
			<S sid ="87" ssid = "41">In this article, we show that the semantic knowledge provided via synonymy and hyponymy links in WordNet is insufficient for the resolution of other-anaphora, although the head of the antecedent is normally a synonym or hyponym of the head of the anaphor in other-anaphora (Section 4.4).8 2.1.3 Bridging.</S>
			<S sid ="88" ssid = "42">Vieira and Poesio (2000) report that 62.0% of meronymy relations (see Example (2)) needed for bridging resolution in their corpus were not encoded in WordNet.</S>
			<S sid ="89" ssid = "43">Gardent, Manuelian, and Kow (2003) identified bridging descriptions in a French corpus, of which 187 (52%) exploited meronymic relations.</S>
			<S sid ="90" ssid = "44">Almost 80% of these were not found in WordNet.</S>
			<S sid ="91" ssid = "45">Hahn, Strube, and Markert (1996) report experiments on 109 bridging cases from German information technology reports, using a handcrafted, domain-specific knowledge base of 449 concepts and 334 relations.</S>
			<S sid ="92" ssid = "46">They state that 42 (38.5%) links between anaphor and antecedents were missing in their knowledge base, a high proportion given the domain-specific task.</S>
			<S sid ="93" ssid = "47">In this article, we will not address bridging, although we will discuss the extension of our work to bridging in Section 6.</S>
			<S sid ="94" ssid = "48">2.2 Methodological Problems for the Use of Ontologies in Anaphora Resolution.</S>
			<S sid ="95" ssid = "49">Over the years, several major problems have been identified with the use of ontologies for anaphora resolution.</S>
			<S sid ="96" ssid = "50">In the following we provide a summary of the different issues raised, using the examples in the Introduction.</S>
			<S sid ="97" ssid = "51">7 Whenever we refer to “hyponymy/meronymy (relations/links)” in WordNet, we include both direct and.</S>
			<S sid ="98" ssid = "52">inherited links.</S>
			<S sid ="99" ssid = "53">8 From this point on, we will often use the terms anaphor and antecedent instead of head of anaphor and head.</S>
			<S sid ="100" ssid = "54">of antecedent if the context is non-ambiguous.</S>
			<S sid ="101" ssid = "55">371 2.2.1 Problem 1: Knowledge Gap.</S>
			<S sid ="102" ssid = "56">As discussed above, even in large ontologies the lack of knowledge can be severe, and this problem increases for non-hyponymy relations.</S>
			<S sid ="103" ssid = "57">None of the examples in Section 1 are covered by synonymy, hyponymy, or meronymy links in WordNet; for example, hoods are not encoded as parts of jackets, and homes are not encoded as a hyponym of facilities.</S>
			<S sid ="104" ssid = "58">In addition, building, extending, and maintaining ontologies by hand is expensive.</S>
			<S sid ="105" ssid = "59">2.2.2 Problem 2: Context-Dependent Relations.</S>
			<S sid ="106" ssid = "60">Whereas the knowledge gap might be reduced as (semi)automatic efforts to enrich ontologies become available (Hearst 1992; Berland and Charniak 1999; Poesio et al. 2002), the second problem is intrinsic to fixed context-independent ontologies: How much and which knowledge should they include?</S>
			<S sid ="107" ssid = "61">Thus, Hearst (1992) raises the issue of whether underspecified, context- or point-of-view-dependent hyponymy relations (like the context-dependent link between costs and repercussions in Example (3)) should be included in a fixed ontology, in addition to universally true hyponymy relations.</S>
			<S sid ="108" ssid = "62">Some other hyponymy relations that we encountered in our studies whose inclusion in ontologies is debatable are age:(risk) factor, coffee:export, pilots:union, country:member.</S>
			<S sid ="109" ssid = "63">2.2.3 Problem 3: Information Encoding.</S>
			<S sid ="110" ssid = "64">Knowledge might be encoded in many different ways in a lexical hierarchy, and this can pose a problem for anaphora resolution (Humphreys et al. 1997; Poesio, Vieira, and Teufel 1997).</S>
			<S sid ="111" ssid = "65">For example, although magazine and periodical are not linked in WordNet via synonymy/hyponymy, the gloss records magazine as a periodic publication.</S>
			<S sid ="112" ssid = "66">Thus, the desired link might be derived through the analysis of the gloss together with derivation of periodical from periodic.</S>
			<S sid ="113" ssid = "67">However, such extensive mining of the ontology (as performed, e.g., by Harabagiu, Bunescu, and Maiorano [2001]) can be costly.</S>
			<S sid ="114" ssid = "68">In addition, different information sources must be weighed (e.g., is a hyponymy link preferred over a gloss inclusion?)</S>
			<S sid ="115" ssid = "69">and combined (should hyponyms/hyperonyms/sisters of gloss expressions be considered recursively?).</S>
			<S sid ="116" ssid = "70">Extensive combinations also increase the risk of false positives.9 2.2.4 Problem 4: Sense Proliferation.</S>
			<S sid ="117" ssid = "71">Using all senses of anaphor and potential antecedents in the search for relations might yield a link between an incorrect antecedent candidate and the anaphor due to an inappropriate sense selection.</S>
			<S sid ="118" ssid = "72">On the other hand, considering only the most frequent sense for anaphor and antecedent (as is done in Soon, Ng, and Lim [2001]) might lead to wrong antecedent assignment if a minority sense is intended in the text.</S>
			<S sid ="119" ssid = "73">So, for example, the most frequent sense of hood in WordNet is criminal, whereas the sense used in Example (2) is headdress.</S>
			<S sid ="120" ssid = "74">The alternatives are either weighing senses according to different domains or a more costly sense disambiguation procedure before anaphora resolution (Preiss 2002).</S>
	</SECTION>
	<SECTION title="The Alternative: Corpus-Based Knowledge Extraction. " number = "3">
			<S sid ="121" ssid = "1">There have been a considerable number of efforts to extract lexical relations from corpora in order to build new knowledge sources and enrich existing ones without time 9 Even without extensive mining, this risk can be high: Vieira and Poesio (2000) report a high number of.</S>
			<S sid ="122" ssid = "2">false positives for one of their data sets, although they use only WordNet-encoded links.</S>
			<S sid ="123" ssid = "3">372 consuming hand-modeling.</S>
			<S sid ="124" ssid = "4">This includes the extraction of hyponymy and synonymy relations (Hearst 1992; Caraballo 1999, among others) as well as meronymy (Berland and Charniak 1999; Meyer 2001).10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus (Hearst 1992; Berland and Charniak 1999; Caraballo 1999; Meyer 2001), and this is the approach we focus on here.</S>
			<S sid ="125" ssid = "5">As an example, the pattern NP1 and other NP2 usually expresses a hyponymy/similarity relation between the hyponym NP1 and its hypernym NP2 (Hearst 1992), and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link.</S>
			<S sid ="126" ssid = "6">Applications of the extracted relations to anaphora resolution are less frequent.</S>
			<S sid ="127" ssid = "7">However, Poesio et al.</S>
			<S sid ="128" ssid = "8">(2002) and Meyer and Dale (2002) have used patterns for the corpus-based acquisition of meronymy relations: these patterns are subsequently exploited for bridging resolution.</S>
			<S sid ="129" ssid = "9">Although automatic acquisition can help bridge the knowledge gap (see Problem 1 in Section 2.2.1), the incorporation of the acquired knowledge into a fixed ontology yields other problems.</S>
			<S sid ="130" ssid = "10">Most notably, it has to be decided which knowledge should be included in ontologies, because pattern-based acquisition will also find spurious, subjective and context-dependent knowledge (see Problem 2 in Section 2.2.2).</S>
			<S sid ="131" ssid = "11">There is also the problem of pattern ambiguity, since patterns do not necessarily have a one-to-one correspondence to lexical relations (Meyer 2001).</S>
			<S sid ="132" ssid = "12">Following our work in Markert, Nissim, and Modjeska (2003), we argue that for the task of antecedent ranking, these problems can be circumvented by not constructing a fixed ontology at all.</S>
			<S sid ="133" ssid = "13">Instead, we use the pattern-based approach to find lexical relationships holding between anaphor and antecedent in corpora on the fly.</S>
			<S sid ="134" ssid = "14">For instance, in Example (3), we do not need to know whether costs are always repercussions (and should therefore be linked via hyponymy in an ontology) but only that they are more likely to be viewed as repercussions than the other antecedent candidates.</S>
			<S sid ="135" ssid = "15">We therefore adapt the pattern-based approach in the following way for antecedent selection.</S>
			<S sid ="136" ssid = "16">Step 1: Relation Identification.</S>
			<S sid ="137" ssid = "17">We determine which lexical relation usually holds between anaphor and antecedent head nouns for a particular anaphoric phenomenon.</S>
			<S sid ="138" ssid = "18">For example, in other-anaphora, a hyponymy/ similarity relation between anaphor and antecedent is exploited (homes are facilities) or stipulated by the context (costs are viewed as repercussions).</S>
			<S sid ="139" ssid = "19">Step 2: Pattern Selection.</S>
			<S sid ="140" ssid = "20">We select patterns that express this lexical relation structurally explicitly.</S>
			<S sid ="141" ssid = "21">For example, the pattern NP1 and other NP2 usually expresses hyponymy/similarity relations between the hyponym NP1 and its hypernym NP2 (see above).</S>
			<S sid ="142" ssid = "22">Step 3: Pattern Instantiation.</S>
			<S sid ="143" ssid = "23">If the lexical relation between anaphor and antecedent head nouns is strong, then it is likely that the anaphor and antecedent also frequently co-occur in the selected explicit patterns.</S>
			<S sid ="144" ssid = "24">We extract all potential antecedents for each anaphor and instantiate the explicit 10 There is also a long history in the extraction of other lexical knowledge, which is also potentially useful.</S>
			<S sid ="145" ssid = "25">for anaphora resolution, for example, of selectional restrictions/preferences.</S>
			<S sid ="146" ssid = "26">In this article we focus on the lexical relations that can hold between antecedent and anaphor head nouns.</S>
			<S sid ="147" ssid = "27">373 for all anaphor/antecedent pairs.</S>
			<S sid ="148" ssid = "28">In Example (4) the pattern NP1 and other NP2 can be instantiated with ordinances and other facilities, Moon Township and other facilities, homes and other facilities, handicapped and other facilities, and miles and other facilities.11 Step 4: Antecedent Assignment.</S>
			<S sid ="149" ssid = "29">The instantiation of a pattern can be searched in any corpus to determine its frequency.</S>
			<S sid ="150" ssid = "30">We follow the rationale that the most frequent of these instantiated patterns determines the most likely antecedent.</S>
			<S sid ="151" ssid = "31">Therefore, should the head noun of an antecedent candidate and the anaphor co-occur in a pattern although they do not stand in the lexical relationship considered (because of pattern ambiguity, noise in the corpus, or spurious occurrences), this need not prove a problem as long as the correct antecedent candidate co-occurs more frequently with the anaphor.</S>
			<S sid ="152" ssid = "32">As the patterns can be elaborate, most manually controlled and linguistically processed corpora are too small to determine the pattern frequencies reliably.</S>
			<S sid ="153" ssid = "33">Therefore, the size of the corpora used in some previous approaches leads to data sparseness (Berland and Charniak 1999), and the extraction procedure can therefore require extensive smoothing.</S>
			<S sid ="154" ssid = "34">Thus as a further extension, we suggest using the largest corpus available, the Web, in the above procedure.</S>
			<S sid ="155" ssid = "35">The instantiation for the correct antecedent homes and other facilities in Example (4), for instance, does not occur at all in the BNC but yields over 1,500 hits on the Web.12 The competing instantiations (listed in Step 3) yield 0 hits in the BNC and fewer than 20 hits on the Web.</S>
			<S sid ="156" ssid = "36">In the remainder of this article, we present two comparative case studies on coreference and other-anaphora that evaluate the ontology- and corpus-based approaches in general and our extensions in particular.</S>
	</SECTION>
	<SECTION title="Case Study I: Other-Anaphora. " number = "4">
			<S sid ="157" ssid = "1">We now describe our first case study for antecedent selection in other-anaphora.</S>
			<S sid ="158" ssid = "2">4.1 Corpus Description and Annotation.</S>
			<S sid ="159" ssid = "3">We use Modjeska’s (2003) annotated corpus of other-anaphors from the WSJ.</S>
			<S sid ="160" ssid = "4">All examples in this section are from this corpus.</S>
			<S sid ="161" ssid = "5">Modjeska restricts the notion of other- anaphora to anaphoric NPs with full lexical heads modified by other or another (Examples (3)–(4)), thereby excluding idiomatic non-referential uses (e.g., on the other hand), reciprocals such as each other, ellipsis, and one-anaphora.</S>
			<S sid ="162" ssid = "6">The excluded cases either are non-anaphoric or do not have a full lexical head and would therefore require a mostly non-lexical approach to resolution.</S>
			<S sid ="163" ssid = "7">Modjeska’s corpus also excludes 11 These simplified instantiations serve as an example; for final instantiations, see Section 4.5.1..</S>
			<S sid ="164" ssid = "8">12 This search and all searches for the Web experiments in Section 4 were executed on August 29, 2003.</S>
			<S sid ="165" ssid = "9">All.</S>
			<S sid ="166" ssid = "10">Web searches for Section 5 were executed August 27, 2004.</S>
			<S sid ="167" ssid = "11">374 other-anaphors with structurally available antecedents: In list contexts such as Example (5), the antecedent is normally given as the left conjunct of the list: (5) [.</S>
			<S sid ="168" ssid = "12">AZT can relieve dementia and other symptoms in children [.</S>
			<S sid ="169" ssid = "13">A similar case is the construction Xs other than Ys.</S>
			<S sid ="170" ssid = "14">For a computational treatment of other-NPs with structural antecedents, see Bierner (2001).</S>
			<S sid ="171" ssid = "15">The original corpus collected and annotated by Modjeska (2003) contains 500 instances of other-anaphors with NP antecedents in a five-sentence window.</S>
			<S sid ="172" ssid = "16">In this study we use the 408 (81.6%) other-anaphors in the corpus that have NP antecedents within a two-sentence window (the current or previous sentence).13 An antecedent candidate is manually annotated as correct if it is the latest mention of the entity to which the anaphor provides the set complement.</S>
			<S sid ="173" ssid = "17">The tag lenient was used to annotate previous mentions of the same entity.</S>
			<S sid ="174" ssid = "18">In Example (6), all other bidders refers to all bidders excluding United Illuminating Co., whose latest mention is it.</S>
			<S sid ="175" ssid = "19">In this article, lenient antecedents are underlined.</S>
			<S sid ="176" ssid = "20">All other potential antecedents (e.g., offer in Example (6)), are called distractors.</S>
			<S sid ="177" ssid = "21">(6) United Illuminating Co. raised its proposed offer to one it valued at $2.29 billion from $2.19 billion, apparently topping all other bidders.</S>
			<S sid ="178" ssid = "22">The antecedent can be a set of separately mentioned entities, like May and July in Example (7).</S>
			<S sid ="179" ssid = "23">For such split antecedents (Modjeska 2003), the latest mention of each set member is annotated as correct, so that there can be more than one correct antecedent to an anaphor.14 (7) The May contract, which also is without restraints, ended with a gain of 0.45 cent to 14.26 cents.</S>
			<S sid ="180" ssid = "24">The July delivery rose its daily permissible limit of 0.50 cent a pound to 14.00 cent, while other contract months showed near-limit advances.</S>
			<S sid ="181" ssid = "25">4.2 Antecedent Extraction and Preprocessing.</S>
			<S sid ="182" ssid = "26">For each anaphor, all previously occurring NPs in the two-sentence window were automatically extracted exploiting the WSJ parse trees.</S>
			<S sid ="183" ssid = "27">NPs containing a possessive NP modifier (e.g., Spain’s economy) were split into a possessor phrase (Spain) and a possessed entity (Spain’s economy).15 Modjeska (2003) identifies several syntactic positions that cannot serve as antecedents of other-anaphors.</S>
			<S sid ="184" ssid = "28">We automatically exclude only NPs preceding an appositive other-anaphor from the candidate antecedent set.</S>
			<S sid ="185" ssid = "29">In “Mary Elizabeth Ariail, another social-studies teacher,” the NP Mary Elizabeth Ariail cannot 13 We concentrate on this majority of cases to focus on the comparison of different sources of lexical.</S>
			<S sid ="186" ssid = "30">knowledge without involving discourse segmentation or focus tracking.</S>
			<S sid ="187" ssid = "31">In Section 5 we expand the window size to allow equally high coverage for definite NP coreference.</S>
			<S sid ="188" ssid = "32">14 The occurrence of split antecedents also motivated the distinction between correct and lenient.</S>
			<S sid ="189" ssid = "33">antecedents in the annotation.</S>
			<S sid ="190" ssid = "34">Anaphors with split antecedents have several antecedent candidates annotated as correct.</S>
			<S sid ="191" ssid = "35">All other anaphors have only one antecedent candidate annotated as correct, with previous mentions of the same entity marked as lenient.</S>
			<S sid ="192" ssid = "36">15 We thank Natalia Modjeska for the extraction and for making the resulting sets of candidate antecedents.</S>
			<S sid ="193" ssid = "37">available to us.</S>
			<S sid ="194" ssid = "38">375 be the antecedent of another social-studies teacher as the two phrases are coreferential and cannot provide a set complement to each other.</S>
			<S sid ="195" ssid = "39">The resulting set of potential NP antecedents for an anaphor ana (with a unique identifier anaid) is called Aanaid .16 The final number of extracted antecedents for the whole data set is 4,272, with an average of 10.5 antecedent candidates per anaphor.</S>
			<S sid ="196" ssid = "40">After extraction, all modification was eliminated, and only the rightmost noun of compounds was retained, as modification results in data sparseness for the corpus- based methods, and compounds are often not recorded in WordNet.</S>
			<S sid ="197" ssid = "41">For the same reasons we automatically resolved named entities (NEs).</S>
			<S sid ="198" ssid = "42">They were classified into the ENAMEX MUC7 categories (Chinchor 1997) PERSON, ORGANIZATION and LOCATION, using the software ANNIE (GATE2; http://gate.ac.</S>
			<S sid ="199" ssid = "43">uk).</S>
			<S sid ="200" ssid = "44">We then automatically obtained more-fine-grained distinctions for the NE categories LOCATION and ORGANIZATION, whenever possible.</S>
			<S sid ="201" ssid = "45">We classified LOCATIONS into COUNTRY, (US ) STAT E, CITY, RIVER, LAKE, and OCEAN in the following way.</S>
			<S sid ="202" ssid = "46">First, small gazetteers for these subcategories were extracted from the Web.</S>
			<S sid ="203" ssid = "47">Second, if an entity marked as LOCATION by ANNIE occurred in exactly one of these gazetteers (e.g., Texas in the (US ) STAT E gazetteer) it received the corresponding specific label; if it occurred in none or in several of the gazetteers (e.g., Mississippi occurred in both the state and the river gazetteer), then the label was left at the LOCATION level.</S>
			<S sid ="204" ssid = "48">We further classified an ORGANIZATION entity by using its internal makeup as follows.</S>
			<S sid ="205" ssid = "49">We extracted all single-word hyponyms of the noun organization from WordNet and used the members of this set, OrgSet, as the target categories for the fine-grained distinctions.</S>
			<S sid ="206" ssid = "50">If an entity was classified by ANNIE as ORGANIZATION and it had an element &lt;ORG&gt; of OrgSet as its final lemmatized word (e.g., Deutsche Bank) or contained the pattern &lt;ORG&gt; of (for example, Bank of America), it was subclassified as &lt;ORG&gt; (here, BANK).</S>
			<S sid ="207" ssid = "51">In cases of ambiguity, again, no subclassification was carried out.</S>
			<S sid ="208" ssid = "52">No further distinctions were developed for the category PERSON.</S>
			<S sid ="209" ssid = "53">We used regular expression matching to classify numeric and time entities into DAY, MONTH, and YEAR as well as DOLLAR or simply NUMBER.</S>
			<S sid ="210" ssid = "54">This subclassification of the standard categories provides us with additional lexical information for antecedent selection.</S>
			<S sid ="211" ssid = "55">Thus, in Example (8), for instance, a finer-grained classification of South Carolina into STAT E provides more useful information than resolving both South Carolina and Greenville County as LOCATION only: (8) Use of Scoring High is widespread in South Carolina and common in Greenville County....</S>
			<S sid ="212" ssid = "56">Experts say there isn’t another state in the country where . . .</S>
			<S sid ="213" ssid = "57">Finally, all antecedent candidates and anaphors were lemmatized.</S>
			<S sid ="214" ssid = "58">The procedure of extraction and preprocessing results in the following antecedent sets and anaphors for Examples (3) and (4): A3 = {..., addition, cost, result, exposure, member, measure} and ana = repercussion and A4 = {..., ordinance, Moon Township [= location], home, handicapped, mile} and ana = facility.</S>
			<S sid ="215" ssid = "59">Table 1 shows the distribution of antecedent NP types in the other-anaphora data set.17 NE resolution is clearly important as 205 of 468 (43.8%) of correct antecedents are NEs.</S>
			<S sid ="216" ssid = "60">16 In this article the anaphor ID corresponds to the example numbers..</S>
			<S sid ="217" ssid = "61">17 Note that there are more correct antecedents than anaphors because the data include split antecedents..</S>
			<S sid ="218" ssid = "62">376 Table 1 Distribution of antecedent NP types in the other-anaphora data set.</S>
			<S sid ="219" ssid = "63">Co rre ct Le nie nt Di str act ors A ll Pro nou ns 4 9 1 9 3 2 9 3 9 7 Na me d enti ties 2 0 5 5 6 8 0 6 1,0 67 Co mm on nou ns 2 1 4 1 0 4 2 , 4 9 0 2,8 08 Tota l 4 6 8 1 7 9 3 , 6 2 5 4,2 72 4.3 Evaluation Measures and Baselines.</S>
			<S sid ="220" ssid = "64">For each anaphor, each algorithm selects at most one antecedent as the correct one.</S>
			<S sid ="221" ssid = "65">If this antecedent provides the appropriate set complement to the anaphor (i.e., is marked in the gold standard as correct or lenient), the assignment is evaluated as correct.18 Otherwise, it is evaluated as wrong.</S>
			<S sid ="222" ssid = "66">We use the following evaluation measures: Precision is the number of correct assignments divided by the number of assignments, recall is the number of correct assignments divided by the number of anaphors, and F-measure is based on equal weighting of precision and recall.</S>
			<S sid ="223" ssid = "67">In addition, we also give the coverage of each algorithm as the number of assignments divided by the number of anaphors.</S>
			<S sid ="224" ssid = "68">This last measure is included to indicate how often the algorithm has any knowledge to go on, whether correct or false.</S>
			<S sid ="225" ssid = "69">For algorithms in which the coverage is 100%, precision, recall, and F-measure all coincide.</S>
			<S sid ="226" ssid = "70">We developed two simple rule-based baseline algorithms.</S>
			<S sid ="227" ssid = "71">The first, a recency-based baseline (baselineREC), always selects the antecedent candidate closest to the anaphor.</S>
			<S sid ="228" ssid = "72">The second (baselineSTR) takes into account that the lemmatized head of an other- anaphor is sometimes the same as that of its antecedent, as in the pilot’s claim . . .</S>
			<S sid ="229" ssid = "73">other bankruptcy claims.</S>
			<S sid ="230" ssid = "74">For each anaphor, baselineSTR string-compares its last (lemmatized) word with the last (lemmatized) word of each of its potential antecedents.</S>
			<S sid ="231" ssid = "75">If the strings match, the corresponding antecedent is chosen as the correct one.</S>
			<S sid ="232" ssid = "76">If several antecedents produce a match, the baseline chooses the most recent one among them.</S>
			<S sid ="233" ssid = "77">If no antecedent produces a match, no antecedent is assigned.</S>
			<S sid ="234" ssid = "78">We tested two variations of this baseline.19 The algorithm baselineSTRv1 uses only the original antecedents for string matching, disregarding named-entity resolution.</S>
			<S sid ="235" ssid = "79">If string-comparison returns no match, a back-off version (baselineSTR∗ ) chooses the antecedent closest to the anaphor among all antecedent candidates, thereby yielding a 100% coverage.</S>
			<S sid ="236" ssid = "80">The second variation (baselineSTRv2 ) uses the replacements for named entities for string matching; again a back-off version (baselineSTR∗ ) uses a recency back-off.</S>
			<S sid ="237" ssid = "81">This baseline performs slightly better, as now cases such as that in Example (8) (South Carolina . . .</S>
			<S sid ="238" ssid = "82">another state, in which South Carolina is resolved to STAT E) can also be resolved.</S>
			<S sid ="239" ssid = "83">The results of all baselines are summarized in Table 2.</S>
			<S sid ="240" ssid = "84">Results of the 100% coverage backoff algorithms are indicated by Precision∗ in all tables.</S>
			<S sid ="241" ssid = "85">The sets of anaphors covered by the string-matching baselines baselineSTRv1 and baselineSTRv2 18 This does not hold for anaphors with split antecedents, for which all antecedents marked as correct need.</S>
			<S sid ="242" ssid = "86">to be found in order to provide the complete set complement.</S>
			<S sid ="243" ssid = "87">Therefore, all our algorithms’ assignments in these cases are evaluated as wrong, as they select at most one antecedent.</S>
			<S sid ="244" ssid = "88">19 Different versions of the same prototype algorithm are indicated via an index of v1, v2, ....</S>
			<S sid ="245" ssid = "89">The general.</S>
			<S sid ="246" ssid = "90">prototype algorithm is referred to without indices.</S>
			<S sid ="247" ssid = "91">377 Table 2 Overview of the results for all baselines for other-anaphora.</S>
			<S sid ="248" ssid = "92">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ bas eline REC 1 . 0 0 0 0 . 1 7 8 0.</S>
			<S sid ="249" ssid = "93">17 8 0 . 1 7 8 0 . 1 7 8 bas eline STR v1 0 . 2 8 2 0 . 6 8 6 0.</S>
			<S sid ="250" ssid = "94">19 4 0 . 3 0 4 0 . 3 3 3 bas eline STR v2 0 . 3 0 9 0 . 6 9 8 0.</S>
			<S sid ="251" ssid = "95">21 6 0 . 3 2 9 0 . 3 5 0 will be called StrSetv1 and StrSetv2 , respectively.</S>
			<S sid ="252" ssid = "96">These sets do not include the cases assigned by the recency back-off in baselineSTR∗ and baselineSTR∗ . For our WordNet and corpus-based algorithms we additionally deleted pronouns from the antecedent sets, since they are lexically not very informative and are also not encoded in WordNet.</S>
			<S sid ="253" ssid = "97">This removes 49 (10.5%) of the 468 correct antecedents (see Table 1); however, we can still resolve some of the anaphors with pronoun antecedents if they also have a lenient non-pronominal antecedent, as in Example (6).</S>
			<S sid ="254" ssid = "98">After pronoun deletion, the total number of antecedents in our data set is 3,875 for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296 are distractors.</S>
			<S sid ="255" ssid = "99">4.4 Wordnet as a Knowledge Source for Other-Anaphora Resolution.</S>
			<S sid ="256" ssid = "100">4.4.1 Descriptive Statistics.</S>
			<S sid ="257" ssid = "101">As most antecedents are hyponyms or synonyms of their anaphors in other-anaphora, for each anaphor ana, we look up which elements of its antecedent set Aanaid are hyponyms/synonyms of ana in WordNet, considering all senses of anaphor and candidate antecedent.</S>
			<S sid ="258" ssid = "102">In Example (4), for example, we look up whether ordinance, Moon Township, home, handicapped, and mile are hyponyms or synonyms of facility in WordNet.</S>
			<S sid ="259" ssid = "103">Similarly, in Example (9), we look up whether Will Quinlan [= PERSON], gene, and risk are hyponyms/synonyms of child.</S>
			<S sid ="260" ssid = "104">(9) Will Quinlan had not inherited a damaged retinoblastoma supressor gene and, therefore, faced no more risk than other children ...</S>
			<S sid ="261" ssid = "105">As proper nouns (e.g., Will Quinlan) are often not included in WordNet, we also look up whether the NE category of an NE antecedent is a hyponym/synonym of the anaphor (e.g., whether person is a synonym/hyponym of child) and vice versa (e.g., whether child is a synonym/hyponym of person).</S>
			<S sid ="262" ssid = "106">This last inverted look-up is necessary, as the NE category of the antecedent is often too general to preserve the normal hyponymy relationship to the anaphor.</S>
			<S sid ="263" ssid = "107">Indeed, in Example (9), it is the inverted look-up that captures the correct hyponymy relation between person and child.</S>
			<S sid ="264" ssid = "108">If the single look-up for common nouns or any of the three look-ups for proper nouns is successful, we say that a hyp/syn relation between candidate antecedent and anaphor holds in WordNet.</S>
			<S sid ="265" ssid = "109">Note that each noun in WordNet stands in a hyp/syn relation to itself.</S>
			<S sid ="266" ssid = "110">Table 3 summarizes how many correct/ lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in WordNet.</S>
			<S sid ="267" ssid = "111">Correct/lenient antecedents stand in a hyp/syn relation to their anaphor significantly more often than distractors do (p &lt; 0.001, t-test).</S>
			<S sid ="268" ssid = "112">The use of WordNet hyponymy/synonymy relations to distinguish between correct/lenient antecedents and distractors is therefore plausible.</S>
			<S sid ="269" ssid = "113">However, Table 3 also shows two limitations 378 Table 3 Descriptive statistics for WordNet hyp/syn relations for other-anaphora.</S>
			<S sid ="270" ssid = "114">Hyp/syn relation No hyp/syn relation Total t o a n a p h o r t o a n a p h o r Corr ect ante ced ents 1 8 0 ( 4 3.</S>
			<S sid ="271" ssid = "115">0 % ) 2 3 9 ( 5 7 . 0 % ) 4 1 9 ( 1 0 0 % ) Leni ent ante ced ents 6 8 ( 4 2 . 5 % ) 9 2 ( 5 7 . 5 % ) 1 6 0 ( 1 0 0 % ) Dist ract ors 2 9 6 ( 9.</S>
			<S sid ="272" ssid = "116">0 % ) 3 , 0 0 0 ( 9 1 . 0 % ) 3 , 2 9 6 ( 1 0 0 % ) All ante ced ents 5 4 4 ( 1 4.</S>
			<S sid ="273" ssid = "117">0 % ) 3 , 3 3 1 ( 8 6 . 0 % ) 3 , 8 7 5 ( 1 0 0 % ) of relying on WordNet in resolution algorithms.</S>
			<S sid ="274" ssid = "118">First, 57% of correct and lenient antecedents are not linked via a hyp/syn relation to their anaphor in WordNet.</S>
			<S sid ="275" ssid = "119">This will affect coverage and recall (see also Section 2.2.1).</S>
			<S sid ="276" ssid = "120">Examples from our data set that are not covered are home:facility, cost:repercussion, age:(risk) factor, pension:beneﬁt, coffee:export, and pilot(s):union, including both missing universal hyponymy links and context-stipulated ones.</S>
			<S sid ="277" ssid = "121">Second, the raw frequency (296) of distractors that stand in a hyp/syn relation to their anaphor is higher than the combined raw frequency for correct/lenient antecedents (248) that do so, which can affect precision.</S>
			<S sid ="278" ssid = "122">This is due to both sense proliferation (Section 2.2.4) and anaphors that require more than just lexical knowledge about antecedent and anaphor heads to select a correct antecedent over a distractor.</S>
			<S sid ="279" ssid = "123">In Example (10), the distractor product stands in a hyp/syn relationship to the anaphor commodity and—disregarding other factors—is a good antecedent candidate.20 (10) . . .</S>
			<S sid ="280" ssid = "124">the move is designed to more accurately reflect the value of products and to put steel on a more equal footing with other commodities.</S>
			<S sid ="281" ssid = "125">4.4.2 The WordNet-Based Algorithm.</S>
			<S sid ="282" ssid = "126">The WordNet-based algorithm resolves each anaphor ana to a hyponym or synonym in Aanaid , if possible.</S>
			<S sid ="283" ssid = "127">If several antecedent candidates are hyponyms or synonyms of ana, it uses a tiebreaker based on string match and recency.</S>
			<S sid ="284" ssid = "128">When no candidate antecedent is a hyponym or synonym of ana, string match and recency can be used as a possible back-off.21 String comparison for tiebreaker and back-off can again use the original or the replaced antecedents, yielding two versions, algoWNv1 (original antecedents) and algoWNv2 (replaced antecedents).</S>
			<S sid ="285" ssid = "129">The exact procedure for the version algoWNv1 given an anaphor ana is as follows:22 (i) for each antecedent a in Aanaid , look up whether a hyp/syn relation between a and ana holds in WordNet; if this is the case, push a into a set hyp/syn Aanaid 20 This problem is not WordNet-specific but affects all algorithms that rely on lexical knowledge only..</S>
			<S sid ="286" ssid = "130">21 Because each noun is a synonym of itself, anaphors in StrSetv1 /StrSetv2 that do have a string-matching.</S>
			<S sid ="287" ssid = "131">antecedent candidate will already be covered by the WordNet look-up prior to back-off in almost all cases: Back-off string matching will take effect only if the anaphor/antecedent head noun is not in WordNet at all.</S>
			<S sid ="288" ssid = "132">Therefore, the described back-off will most of the time just amount to a recency back-off.</S>
			<S sid ="289" ssid = "133">22 The algorithm algoWNv2 follows the same procedure apart from the variation in string matching..</S>
			<S sid ="290" ssid = "134">379 (ii) if Ahyp/syn contains exactly one element, choose this element and stop; (iii) otherwise, if Ahyp/syn contains more than one element, string-compare each antecedent in Aanaid with ana (using original antecedents only).</S>
			<S sid ="291" ssid = "135">If exactly one element of Ahyp/syn matches ana, select this one and stop; if several match ana, select the closest to ana within these matching antecedents and stop; if none match, select the closest to ana within Aanaid and stop; (iv) otherwise, if Ahyp/syn is empty, make no assignment and stop.</S>
			<S sid ="292" ssid = "136">The back-off algorithm algoWN∗ uses baselineSTR∗ as a back-off (iv’) if no antecedent can be assigned: (iv’) otherwise, if Ahyp/syn is empty, use baselineSTR∗ to assign an antecedent to ana and stop; Both algoWNv1 and algoWNv2 achieved the same results, namely, a coverage of 65.2%, precision of 56.8%, and recall of 37.0%, yielding an F-measure of 44.8%.</S>
			<S sid ="293" ssid = "137">The low coverage and recall confirm our predictions in Section 4.4.1.</S>
			<S sid ="294" ssid = "138">Using backoff algoWN∗ /algoWN∗ achieves a coverage of 100% and a precision/recall/F measure v1 v2 of 44.4%.</S>
			<S sid ="295" ssid = "139">4.5 Corpora as Knowledge Sources for Other-Anaphora Resolution.</S>
			<S sid ="296" ssid = "140">In Section 3 we suggested the use of shallow lexico-semantic patterns for obtaining anaphor–antecedent relations from corpora.</S>
			<S sid ="297" ssid = "141">In our first experiment we use the Web, which with its approximately 8,058M pages23 is the largest corpus available to the NLP community.</S>
			<S sid ="298" ssid = "142">In our second experiment we use the same technique on the BNC, a smaller (100 million words) but virtually noise-free and balanced corpus of contemporary English.</S>
			<S sid ="299" ssid = "143">4.5.1 Pattern Selection and Instantiation.</S>
			<S sid ="300" ssid = "144">The list-context Xs and other Ys explicitly expresses a hyponymy/synonymy relationship with X being hyponyms/synonyms of Y (see also Example (5) and [Hearst 1992]).</S>
			<S sid ="301" ssid = "145">This is only one of the possible structures that express hyponymy/synonymy.</S>
			<S sid ="302" ssid = "146">Others involve such, including, and especially (Hearst 1992) or appositions and coordination.</S>
			<S sid ="303" ssid = "147">We derive our patterns from the list-context because it corresponds relatively unambigously to hyponymy/synonymy relations (in contrast to coordination, which often links sister concepts instead of a hyponym and its hyperonym, as in tigers and lions, or even completely unrelated concepts).</S>
			<S sid ="304" ssid = "148">In addition, it is quite frequent (for example, and other occurs more frequently on the Web than such as and other than).</S>
			<S sid ="305" ssid = "149">Future work has to explore which patterns have the highest precision and/or recall and how different patterns can be combined effectively without increasing the risk of false positives (see also Section 2.2.3).</S>
			<S sid ="306" ssid = "150">23 Google (http://www.google.com), estimate from November 2004..</S>
			<S sid ="307" ssid = "151">380 Table 4 Patterns and instantiations for other-anaphora.</S>
			<S sid ="308" ssid = "152">Common-noun patterns Common-noun instantiations W1: (N1 {sg} OR N1 {pl}) and other N2 {pl} WIc : (home OR homes) and other facilities .........................................................................................................</S>
			<S sid ="309" ssid = "153">B1: (...)</S>
			<S sid ="310" ssid = "154">and D* other A* N2 {pl} BIc : (home OR homes) and D* other A* facilities Proper-noun patterns Proper-noun instantiations W1: (N {sg} OR N {pl}) and other N {pl} WIp : (person OR persons) and other children WIp : (child OR children) and other persons p W2: N and other N {pl} WI : Will Quinlan and other children .........................................................................................................</S>
			<S sid ="311" ssid = "155">p B1: (...)</S>
			<S sid ="312" ssid = "156">and D* other A* N {pl} BI : (person OR persons) and D* other A* children BIp : (child OR children) and D* other A* persons p B2: N and D* other A* N {pl} BI : Will Quinlan and D* other A* children Web.</S>
			<S sid ="313" ssid = "157">For the Web algorithm (algoWeb), we use the following pattern:24 (W1) (N1 {sg} OR N1 {pl}) and other N2 {pl} Given an anaphor ana and a common-noun antecedent candidate a in Aanaid , we instantiate (W1) by substituting a for N1 and ana for N2 . An instantiated pattern for Example (4) is (home OR homes) and other facilities (WIc in Table 4).25 This pattern instantiation is parallel to the WordNet hyp/syn relation look-up for common nouns.</S>
			<S sid ="314" ssid = "158">For NE antecedents we instantiate (W1) by substituting the NE category of the antecedent for N1 , and ana for N2 . An instantiated pattern for Example (9) is (person OR persons) and other children (WIp in Table 4).</S>
			<S sid ="315" ssid = "159">In this instantiation, N1 (person) is not a hyponym of N2 (child); instead N2 is a hyponym of N1 (see the discussion on inverted queries in Section 4.4.1).</S>
			<S sid ="316" ssid = "160">Therefore, we also instantiate (W1) by substituting ana for N1 and the NE type of the antecedent for N2 (WIp in Table 4).</S>
			<S sid ="317" ssid = "161">Finally, for NE antecedents, we use an additional pattern: (W2) N1 and other N2 {pl} which we instantiate by substituting the original NE antecedent for N1 and ana for N2 (WIp in Table 4).</S>
			<S sid ="318" ssid = "162">The three instantiations for NEs are parallel to the three hyp/syn relation look-ups in the WordNet experiment in Section 4.4.1.</S>
			<S sid ="319" ssid = "163">We submit these instantiations as queries to the Google search engine, making use of the Google API technology.</S>
			<S sid ="320" ssid = "164">BNC.</S>
			<S sid ="321" ssid = "165">For BNC patterns and instantiations, we exploit the BNC’s part-of-speech tagging.</S>
			<S sid ="322" ssid = "166">On the one hand, we restrict the instantiation of N1 and N2 to nouns to avoid noise, and on the other hand, we allow occurrence of modification to improve coverage.</S>
			<S sid ="323" ssid = "167">We 24 In all patterns and instantiations in this article, OR is the boolean operator, N1 and N2 are variables, and.</S>
			<S sid ="324" ssid = "168">and and other are constants.</S>
			<S sid ="325" ssid = "169">25 All common-noun instantiations are marked by a superscript c and all proper-noun instantiations by a. superscript p. 381 therefore extend (W1) and (W2) to the patterns (B1) and (B2).26 An instantiation for (B1), for example, also matches “homes and the other four facilities.” Otherwise the instantiations are produced parallel to the Web (see Table 4).</S>
			<S sid ="326" ssid = "170">We search the instantiations in the BNC using the IMS Corpus Query Workbench (Christ 1995).</S>
			<S sid ="327" ssid = "171">(B1) (N1 {sg} OR N1 {pl}) and D* other A* N2 {pl} (B2) N1 and D* other A* N2 {pl} For both algoWeb and algoBNC, each antecedent candidate a in Aanaid is assigned a score.</S>
			<S sid ="328" ssid = "172">The procedure, using the notation for the Web, is as follows.</S>
			<S sid ="329" ssid = "173">We obtain the raw frequencies of all instantiations in which a occurs (WIc for common nouns, or WIp , WIp , 1 1 2 and WIp for proper names) from the Web, yielding freq(WIc ), or freq(WIp ), freq(WIp ), 3 1 1 2 and freq(WIp ).</S>
			<S sid ="330" ssid = "174">The maximum WM over these frequencies is the score associated with each antecedent (given an anaphor ana), which we will also simply refer to as the antecedent’s Web score.</S>
			<S sid ="331" ssid = "175">For the BNC, we call the corresponding maximum score BMa and refer to it as the antecedent’s BNC score.</S>
			<S sid ="332" ssid = "176">This simple maximum score is biased toward antecedent candidates whose head nouns occur more frequently overall.</S>
			<S sid ="333" ssid = "177">In a previous experiment we used mutual information to normalize Web scores (Markert, Nissim, and Modjeska 2003).</S>
			<S sid ="334" ssid = "178">However, the results achieved with normalized and non-normalized scores showed no significant difference.</S>
			<S sid ="335" ssid = "179">Other normalization methods might yield significant improvements over simple maximum scoring and can be explored in future work.</S>
			<S sid ="336" ssid = "180">4.5.2 Descriptive Statistics.</S>
			<S sid ="337" ssid = "181">Table 5 gives descriptive statistics for the Web and BNC score distributions for correct/lenient antecedents and distractors, including the minimum and maximum score, mean score and standard deviation, median, and number of zero scores, scores of one, and scores greater than one.</S>
			<S sid ="338" ssid = "182">Web scores resulting from simple pattern-based search produce on average significantly higher scores for correct/lenient antecedents (mean: 2,416.68/807.63; median: 68/68.5) than for distractors (mean: 290.97; median: 1).</S>
			<S sid ="339" ssid = "183">Moreover, the method produces significantly fewer zero scores for correct/lenient antecedents (19.6%/22.5%) than for distractors (42.3%).27 Therefore the pattern-based Web method is a good candidate for distinguishing correct/lenient antecedents and distractors in anaphora resolution.</S>
			<S sid ="340" ssid = "184">In addition, the median for correct/lenient antecedents is relatively high (68/68.5), which ensures a relatively large amount of data upon which to base decisions.</S>
			<S sid ="341" ssid = "185">Only 19.6% of correct antecedents have scores of zero, which indicates that the method might have high coverage (compared to the missing 57% of hyp/syn relations for correct antecedents in WordNet; Section 4.4).</S>
			<S sid ="342" ssid = "186">Although the means of the BNC score distributions of correct/lenient antecedents are significantly higher than that of the distractors, this is due to a few outliers; more interestingly, the median for the BNC score distributions is zero for all antecedent groups.</S>
			<S sid ="343" ssid = "187">This will affect precision for a BNC-based algorithm because of the small amount of data decisions are based on.</S>
			<S sid ="344" ssid = "188">In addition, although the number of zero scores 26 The star operator indicates zero or more occurrences of a variable.</S>
			<S sid ="345" ssid = "189">The variable D can be instantiated by.</S>
			<S sid ="346" ssid = "190">any determiner; the variable A can be instantiated by any adjective or cardinal number.</S>
			<S sid ="347" ssid = "191">27 Difference in means was calculated via a t-test; for medians we used chi-square, and for zero counts a. t-test for proportions.</S>
			<S sid ="348" ssid = "192">The significance level used was 5%.</S>
			<S sid ="349" ssid = "193">382 Table 5 Descriptive statistics for Web scores and BNC scores for other-anaphora.</S>
			<S sid ="350" ssid = "194">Min–Max Mean SD Med 0 scores 1 scores scores &gt; 1 All possible antecedents (Total: 3,875) BN C 0– 22 0 . 0 7 0 . 6 0 0 3,7 14 (95 .8 %) 10 9 (2.</S>
			<S sid ="351" ssid = "195">8% ) 5 2 ( 1 . 4 % ) We b 0– 28 3,0 00 5 4 2 . 1 5 8, 3 5 2.</S>
			<S sid ="352" ssid = "196">4 6 2 1,5 13 (39 .0 %) 27 0 (7.</S>
			<S sid ="353" ssid = "197">0% ) 2, 09 2 (5 4.</S>
			<S sid ="354" ssid = "198">0 %) C or re ct a nt e c e d e nt s (T ot al : 4 1 9) BN C 0– 22 0 . 3 2 1 . 6 2 0 3 6 0 ( 8 5 . 9 % ) 3 9 (9 .3 % ) 2 0 ( 4 . 8 % ) We b 0– 28 3,0 00 2,4 16.</S>
			<S sid ="355" ssid = "199">68 15, 94 7.9 3 6 8 8 2 ( 1 9 . 6 % ) 1 1 (2 .6 % ) 3 2 6 ( 7 7 . 8 % ) L en ie nt a nt ec e d e nt s (T ot al: 16 0) BN C 0– 4 0 . 2 1 0 . 6 2 0 1 3 9 ( 8 6 . 9 % ) 1 3 (8 .1 % ) 8 ( 5 . 0 % ) We b 0– 8,8 40 8 0 7 . 6 3 1, 7 1 8.</S>
			<S sid ="356" ssid = "200">1 3 6 8.</S>
			<S sid ="357" ssid = "201">5 3 6 ( 2 2 . 5 % ) 3 ( 1 . 9 % ) 1 2 1 ( 7 5 . 6 % ) D i s t r a c t o r s ( T o t a l : 3 , 2 9 6 ) BN C 0– 6 0 . 0 3 0 . 2 5 0 3,2 15 (97 .5 %) 5 7 (1 .7 % ) 2 4 ( 0 . 8 % ) We b 0– 28 3,0 00 2 9 0 . 9 7 7, 0 1 0.</S>
			<S sid ="358" ssid = "202">0 7 1 1,3 95 (42 .3 %) 25 6 (7.</S>
			<S sid ="359" ssid = "203">8% ) 1, 64 5 (4 9.</S>
			<S sid ="360" ssid = "204">9 %) for correct/lenient antecedents (85.9%/86.9%) is significantly lower than for distractors (97.5%), the number of zero scores is well above 80% for all antecedent groups.</S>
			<S sid ="361" ssid = "205">Thus, the coverage and recall of a BNC-based algorithm will be very low.</S>
			<S sid ="362" ssid = "206">Although the BNC scores are in general much lower than Web scores and although the Web scores distinguish better between correct/lenient antecedents and distractors, we observe that Web and BNC scores still correlate significantly, with correlation coefficients between 0.20 and 0.35, depending on antecedent group.28 To summarize, the pattern-based method yields correlated results on different corpora, but it is expected to depend on large corpora to be really successful.</S>
			<S sid ="363" ssid = "207">4.5.3 The Corpus-Based Algorithms.</S>
			<S sid ="364" ssid = "208">The prototype Web-based algorithm resolves each anaphor ana to the antecedent candidate in Aanaid with the highest Web score above zero.</S>
			<S sid ="365" ssid = "209">If several potential antecedents achieve the same Web score, it uses a tiebreaker based on string match and recency.</S>
			<S sid ="366" ssid = "210">If no antecedent candidate achieves a Web score above zero, string match and recency can be used as a back-off.</S>
			<S sid ="367" ssid = "211">String comparison for tiebreaker and back-off can again use the original or the replaced antecedents, yielding two versions, algoWebv1 (original antecedents) and algoWebv2 (replaced antecedents).</S>
			<S sid ="368" ssid = "212">The exact procedure for the version algoWebv1 for an anaphor ana is as follows:29 (i) for each antecedent a in Aanaid , compute its Web score WMa . Compute the maximum WM of all Web scores over all antecedents in Aanaid . If WMa is equal to WM and bigger than zero, push a into a set AWM ; 28 Correlation significance was measured by both a t-test for the correlation coefficient and also by the.</S>
			<S sid ="369" ssid = "213">nonparametric paired Kendall rank correlation test, both yielding significance at the 1% level.</S>
			<S sid ="370" ssid = "214">29 The algorithm algoWebv2 follows the same basic procedure apart from the variation regarding.</S>
			<S sid ="371" ssid = "215">original/replaced antecedents in string matching.</S>
			<S sid ="372" ssid = "216">383 (ii) if AWM contains exactly one element, select this element and stop; (iii) otherwise, if AWM contains more than one element, string-compare each antecedent in AWM one element of AWM with ana (using original antecedents).</S>
			<S sid ="373" ssid = "217">If exactly , select this one and stop; if several match anaid matches ana ana, select the closest to ana within these matching antecedents and stop; if none match, select the closest to ana within AWM and stop; (iv) otherwise, if AWM is empty, make no assigment and stop.</S>
			<S sid ="374" ssid = "218">The back-off algorithm algoWeb∗ uses baselineSTR∗ as a back-off (iv’) if no antecedent can be assigned (parallel to the back-off in algoWN∗ ): (iv’) otherwise, if AWM is empty, use baselineSTR∗ to assign an antecedent to ana and stop; algoWebv1 and algoWebv2 can overrule string matching for anaphors in StrSetv1 /StrSetv2 . This happens when the Web score of an antecedent candidate that does not match the anaphor is higher than the Web scores of matching antecedent candidates.</S>
			<S sid ="375" ssid = "219">In particular, there is no guarantee that matching antecedent candidates are included in AWM . In that respect, algoWebv1 and algoWebv2 differ from the corresponding WordNet algorithms: Matching antecedent candidates are always synonyms of the anaphor (as each noun is a synonym of itself) and therefore always included in Ahyp/syn . Therefore the WordNet algorithms can be seen as a direct extension of baselineSTR; that is, they achieve the same results as the string-matching baseline on the sets StrSetv1 /StrSetv2 . Given the high precision of baselineSTR, we might want to exclude the possibility that the Web algorithms overrule string matching.</S>
			<S sid ="376" ssid = "220">Instead we can use string matching prior to Web scoring, use the Web scores only when there are no matching antecedent candidates, and use recency as the final back-off.</S>
			<S sid ="377" ssid = "221">This variation then achieves the same results on the sets StrSetv1 /StrSetv2 as the WordNet algorithms and the string-matching baselines.</S>
			<S sid ="378" ssid = "222">In combination with the possibility of using original or replaced antecedents for string matching this yields four algorithm variations overall (see Table 6).</S>
			<S sid ="379" ssid = "223">The results (see Table 7) do not show any significant differences according to the variation explored.</S>
			<S sid ="380" ssid = "224">The BNC-based algorithms follow the same procedures as the Web-based algorithms, using the BNC scores instead of Web scores.</S>
			<S sid ="381" ssid = "225">The results (see Table 8) are disappointing because of data sparseness (see above).</S>
			<S sid ="382" ssid = "226">No variation yields considerableimprovement over baselineSTRv2 in the final precision∗; in fact, in most cases the varia Table 6 Properties of the variations for the corpus-based algorithms for other-anaphora.</S>
			<S sid ="383" ssid = "227">Re pla ce d/o rig ina l ant ec ed ent Ov err ule stri ng ma tch ing ? v1 v2 v3 v4 o r i g i n a l r e p l a c e d o r i g i n a l r e p l a c e d y e s y e s n o n o 384 Table 7 Web results for other-anaphora.</S>
			<S sid ="384" ssid = "228">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ algo Web v1 0 . 9 5 0 0 . 5 2 0 0.</S>
			<S sid ="385" ssid = "229">49 5 0 . 5 0 7 0 . 5 1 2 algo Web v2 0 . 9 5 0 0 . 5 1 8 0.</S>
			<S sid ="386" ssid = "230">49 3 0 . 5 0 5 0 . 5 0 9 algo Web v3 0 . 9 5 8 0 . 5 3 4 0.</S>
			<S sid ="387" ssid = "231">51 2 0 . 5 2 3 0 . 5 1 9 algo Web v4 0 . 9 6 1 0 . 5 3 8 0.</S>
			<S sid ="388" ssid = "232">51 7 0 . 5 2 7 0 . 5 2 4 Table 8 BNC results for other-anaphora.</S>
			<S sid ="389" ssid = "233">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ algo BNC v1 0 . 2 1 0 0 . 4 8 8 0.</S>
			<S sid ="390" ssid = "234">10 3 0 . 1 7 0 0 . 3 5 5 algo BNC v2 0 . 2 1 0 0 . 4 8 8 0.</S>
			<S sid ="391" ssid = "235">10 3 0 . 1 7 0 0 . 3 6 0 algo BNC v3 0 . 4 1 7 0 . 6 1 8 0.</S>
			<S sid ="392" ssid = "236">25 7 0 . 3 6 3 0 . 3 7 0 algo BNC v4 0 . 4 1 9 0 . 6 2 6 0.</S>
			<S sid ="393" ssid = "237">26 2 0 . 3 6 9 0 . 3 7 5 tions just apply a string-matching baseline, either as a back-off or prior to checking BNC scores, depending on the variation used.</S>
			<S sid ="394" ssid = "238">4.6 Discussion and Error Analysis.</S>
			<S sid ="395" ssid = "239">The performances of the best versions of all algorithms for other-anaphora are summarized in Table 9.</S>
			<S sid ="396" ssid = "240">4.6.1 Algorithm Comparison.</S>
			<S sid ="397" ssid = "241">Algorithms are compared on their final precision∗ using two tests throughout this article.</S>
			<S sid ="398" ssid = "242">We used a t-test to measure the difference between two algorithms in the proportion of correctly resolved anaphors.</S>
			<S sid ="399" ssid = "243">However, there are many examples which are easy (for example, string-matching examples) and that therefore most or all algorithms will resolve correctly, as well as many that are too hard for all algorithms.</S>
			<S sid ="400" ssid = "244">Therefore, we also compare two algorithms using McNemar ’s test, which only relies on the part of the data set in which the algorithms do not give the same answer.30 If not otherwise stated, all significance claims hold at the 5% level for both the t-test and McNemar ’s test.</S>
			<S sid ="401" ssid = "245">The algorithm baselineSTR significantly outperforms baselineREC in precision∗, showing that the “same predicate match” is quite accurate even though not very frequent (coverage is only 30.9%).</S>
			<S sid ="402" ssid = "246">The WordNet-based and Web-based algorithms achieve a final precision that is significantly better than the baselines’ as well as algoBNC’s. Most interestingly, the Web-based algorithms significantly outperform the WordNet-based algorithms, confirming our predictions based on the descriptive statistics.</S>
			<S sid ="403" ssid = "247">The Web approach, for example, resolves Examples (3), (4), (6), and (11) (which WordNet could not resolve) in addition to Examples (8) and (9), which both the Web and WordNet algorithms could resolve.</S>
			<S sid ="404" ssid = "248">30 We thank an anonymous reviewer for suggesting the use of McNemar ’s test for this article..</S>
			<S sid ="405" ssid = "249">385 Table 9 Overview of the results for the best algorithms for other-anaphora.</S>
			<S sid ="406" ssid = "250">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ bas eline REC 1 . 0 0 0 0 . 1 7 8 0.</S>
			<S sid ="407" ssid = "251">17 8 0 . 1 7 8 0 . 1 7 8 bas eline STR v2 0 . 3 0 9 0 . 6 9 8 0.</S>
			<S sid ="408" ssid = "252">21 6 0 . 3 2 9 0 . 3 5 0 algo BNC v4 0 . 4 1 9 0 . 6 2 6 0.</S>
			<S sid ="409" ssid = "253">26 2 0 . 3 6 9 0 . 3 7 5 algo WNv 2 0 . 6 5 2 0 . 5 6 8 0.</S>
			<S sid ="410" ssid = "254">37 0 0 . 4 4 8 0 . 4 4 4 algo Web v4 0 . 9 6 1 0 . 5 3 8 0.</S>
			<S sid ="411" ssid = "255">51 7 0 . 5 2 7 0 . 5 2 4 As expected, the WordNet-based algorithms suffer from the problems discussed in Section 2.2.</S>
			<S sid ="412" ssid = "256">In particular, Problem 1 proved to be quite severe, as algoWN achieved a coverage of only 65.2%.</S>
			<S sid ="413" ssid = "257">Missing links in WordNet also affect precision if a good distractor has a link to the anaphor in WordNet, whereas the correct antecedent does not (Example (10)).</S>
			<S sid ="414" ssid = "258">Missing links are both universal relations that should be included in an ontology (such as home:facility) and context-dependent links (e.g., age:(risk) factor, costs:repercussions; see Problem 2 in Section 2.2.2).</S>
			<S sid ="415" ssid = "259">Further mining of WordNet beyond following hyponymy/synonymy links might alleviate Problem 1 but is more costly and might lead to false positives (Problem 3).</S>
			<S sid ="416" ssid = "260">To a lesser degree, the WordNet algorithms also suffer from sense proliferation (Problem 4), as all senses of both anaphor and antecedent candidates were considered.</S>
			<S sid ="417" ssid = "261">Therefore, some hyp/syn relations based on a sense not intended in the text were found, leading to wrong-antecedent selection and lowering precision.</S>
			<S sid ="418" ssid = "262">In Example (11), for instance, there is no hyponymy link between the head noun of the correct antecedent (question) and the head noun of the anaphor (issue), whereas there is a hyponymy link between issue and person = [Mr. Dallara] (using the sense of issue as offspring) as well as a synonymy link between number and issue.</S>
			<S sid ="419" ssid = "263">While in this case considering the most frequent sense of the anaphor issue as indicated in WordNet would help, this would backfire in other cases in our data set in which issue is mostly used in the minority sense of stock, share.</S>
			<S sid ="420" ssid = "264">Obviously, prior word sense disambiguation would be the most principled but also a more costly solution.</S>
			<S sid ="421" ssid = "265">(11) While Mr. Dallara and Japanese officials say the question of investors access to the U.S. and Japanese markets may get a disproportionate share of the public’s attention, a number of other important economic issues [...]</S>
			<S sid ="422" ssid = "266">The Web-based method does not suffer as much from these problems.</S>
			<S sid ="423" ssid = "267">The linguistically motivated patterns we use reduce long-distance dependencies between anaphor and antecedent to local dependencies.</S>
			<S sid ="424" ssid = "268">By looking up these patterns on the Web we make use of a large amount of data that is very likely to encode strong semantic links via these local dependencies and to do so frequently.</S>
			<S sid ="425" ssid = "269">This holds both for universal hyponymy relations (addressing Problem 1) and relations that are not necessarily to be included in an ontology (addressing Problem 2).</S>
			<S sid ="426" ssid = "270">The problem of whether to include subjective and context-dependent relations in an ontology (Problem 2) is circumvented by using Web scores only in comparison to Web scores of other antecedent candidates.</S>
			<S sid ="427" ssid = "271">In addition, the Web-based algorithm needs no hand-processing or hand-modeling whatsoever, thereby avoiding the manual effort of building ontologies.</S>
			<S sid ="428" ssid = "272">Moreover, the local dependencies we use reduce the need for prior word sense disambiguation (Problem 4), as the anaphor and the antecedent constrain each other ’s sense within the 386 Figure 1 Decision tree for error classification.</S>
			<S sid ="429" ssid = "273">context of the pattern.</S>
			<S sid ="430" ssid = "274">Furthermore, the Web scores are based on frequency, which biases the Web-based algorithms toward frequent senses as well as sense pairs that occur together frequently.</S>
			<S sid ="431" ssid = "275">Thus, the Web algorithm has no problem resolving issue to question in Example (11) because of the high frequency of the query question OR questions and other issues.</S>
			<S sid ="432" ssid = "276">Problem 3 is still not addressed, however, as any corpus can encode the same semantic relations via different patterns.</S>
			<S sid ="433" ssid = "277">Combining patterns might therefore yield problems similar to those presented by combining information sources in an ontology.</S>
			<S sid ="434" ssid = "278">Our pattern-based method, though, seems to work on very large corpora only.</S>
			<S sid ="435" ssid = "279">Unlike the Web-based algorithms, the BNC-based ones make use of POS tagging and observe sentence boundaries, thus reducing the noise intrinsic to an unprocessed corpus like the Web.</S>
			<S sid ="436" ssid = "280">Moreover, the instantiations used in algoBNC allow for modification to occur (see Table 4), thus increasing chances of a match.</S>
			<S sid ="437" ssid = "281">Nevertheless, the BNC-based algorithms performed much worse than the Web-based ones: Only 4.2% of all pattern instantiations were found in the BNC, yielding very low coverage and recall (see Table 5).</S>
			<S sid ="438" ssid = "282">4.6.2 Error Analysis.</S>
			<S sid ="439" ssid = "283">Although the Web algorithms perform best, algoWEBv4 still incurs 194 errors (47.6% of 408).</S>
			<S sid ="440" ssid = "284">Because in several cases there is more than one reason for a wrong assignment, we use the decision tree in Figure 1 for error classification.</S>
			<S sid ="441" ssid = "285">By using this decision tree, we can, for example, exclude from further analysis those cases that none of the algorithms could resolve because of their intrinsic design.</S>
			<S sid ="442" ssid = "286">As can be seen in Table 10, quite a large number of errors result from deleting pronouns as well as not dealing with split antecedents (44 cases, or 22.7% of all mistakes).31 Out of these 44, 30 involve split antecedents.</S>
			<S sid ="443" ssid = "287">In 19 of these 30 cases, one of the several correct antecedents has indeed been chosen by our algorithm, but all the correct antecedents need to be found to allow for the resolution to be counted as correct.</S>
			<S sid ="444" ssid = "288">Given the high number of NE antecedents in our corpus (43.8% of correct, 25% of all antecedents; see Table 1), NE resolution is crucial.</S>
			<S sid ="445" ssid = "289">In 11.3% of the cases, the algorithm selects a distractor instead of the correct antecedent because the NER module 31 Percentages of errors are rounded to the first decimal; rounding errors account for the coverage of 99.9%.</S>
			<S sid ="446" ssid = "290">of errors instead of 100%.</S>
			<S sid ="447" ssid = "291">387 Table 10 Occurrences of error types for the best other-anaphora algorithm algoWebv4 . Erro r typ e Nu mb er of cas es Per ce nta ge of cas es Des ign 4 4 2 2 . 7 Na me d enti ty 2 2 1 1 . 3 Stri ng mat chi ng 1 9 9 . 8 Zero scor e 4 8 2 4 . 7 Tieb reak er 1 3 6 . 7 Oth er 4 8 2 4 . 7 Tota l 1 9 4 9 9 . 9 either leaves the correct antecedent unresolved (which could then lead to very few or zero hits in Google) or resolves the named entity to the wrong NE category.</S>
			<S sid ="448" ssid = "292">String matching is a minor cause of errors (under 10%).</S>
			<S sid ="449" ssid = "293">This is because, apart from its being generally reliable, there is also a possible string match only in just about 30% of the cases (see Table 2).Many mistakes, instead, occur because other-anaphora can express heavily context dependent and very unconventional relations, such as the description of dolls as winners in Example (12).</S>
			<S sid ="450" ssid = "294">(12) Coleco bounced back with the introduction of the Cabbage Patch dolls.</S>
			<S sid ="451" ssid = "295">But as the craze died, Coleco failed to come up with another winner.</S>
			<S sid ="452" ssid = "296">In such cases, the relation between the anaphor and antecedent head nouns is not frequent enough to be found in a corpus even as large as the Web.32 This is mirrored in the high percentage of zero-score errors (24.7% of all mistakes).</S>
			<S sid ="453" ssid = "297">Although the Web algorithm suffers from a knowledge gap to a smaller degree than WordNet, there is still a substantial number of cases in which we cannot find the right lexical relation.</S>
			<S sid ="454" ssid = "298">Errors of type other are normally due to good distractors that achieve higher Web scores than the correct antecedent.</S>
			<S sid ="455" ssid = "299">A common reason is that the wished-for relation is attested but rare and therefore other candidates yield higher scores.</S>
			<S sid ="456" ssid = "300">This is similar to zero-score errors.</S>
			<S sid ="457" ssid = "301">Furthermore, the elimination of modification, although useful to reduce data sparseness, can sometimes lead to the elimination of information that could help disambiguate among several candidate antecedents.</S>
			<S sid ="458" ssid = "302">Lastly, lexical information, albeit crucial and probably more important than syntactic information (Modjeska 2002), is not sufficient for the resolution of other-anaphora.</S>
			<S sid ="459" ssid = "303">The integration of other features, such as grammatical function, NP form, and discourse structure, could probably help when very good distractors cannot be ruled out by purely lexical methods (Example (10)).</S>
			<S sid ="460" ssid = "304">The integration of the Web feature in a machine-learning algorithm using several other features has yielded good results (Modjeska, Markert, and Nissim 2003).</S>
			<S sid ="461" ssid = "305">32 Using different or simply more patterns might yield some hits for anaphor–antecedent pairs that return a. zero score when instantiated in the pattern we use in this article.</S>
			<S sid ="462" ssid = "306">388</S>
	</SECTION>
	<SECTION title="Case Study II: Definite NP Coreference. " number = "5">
			<S sid ="463" ssid = "1">The Web-based method we have described outperforms WordNet as a knowledge source for antecedent selection in other-anaphora resolution.</S>
			<S sid ="464" ssid = "2">However, it is not clear how far the method and the achieved comparative results generalize to other kinds of full NP anaphora.</S>
			<S sid ="465" ssid = "3">In particular, we are interested in the following questions: • Is the knowledge gap encountered in WordNet for other-anaphora equally severe for other kinds of full NP anaphora?</S>
			<S sid ="466" ssid = "4">A partial (mostly affirmative) answer to this is given by previous researchers, who put the knowledge gap for coreference at 30–50% and for bridging at 38–80%, depending on language, domain, and corpus (see Section 2).</S>
			<S sid ="467" ssid = "5">• Do the Web-based method and the specific search patterns we use generalize to other kinds of anaphora?</S>
			<S sid ="468" ssid = "6">• Do different anaphoric phenomena require different lexical knowledge sources?</S>
			<S sid ="469" ssid = "7">As a contribution, we investigate the performance of the knowledge sources discussed for other-anaphora in the resolution of coreferential NPs with full lexical heads, concentrating on definite NPs (see Example (1)).</S>
			<S sid ="470" ssid = "8">The automatic resolution of such anaphors has been the subject of quite significant interest in the past years, but results are much less satisfactory than those obtained for the resolution of pronouns (see Section 2).</S>
			<S sid ="471" ssid = "9">The relation between the head nouns of coreferential definite NPs and their antecedents is again, in general, one of hyponymy or synonymy, making an extension of our approach feasible.</S>
			<S sid ="472" ssid = "10">However, other-anaphors are especially apt at conveying context-specific or subjective information by forcing the reader via the other-expression to accommodate specific viewpoints.</S>
			<S sid ="473" ssid = "11">This might not hold for definite NPs.33 5.1 Corpus Collection.</S>
			<S sid ="474" ssid = "12">We extracted definite NP anaphors and their candidate antecedents from the MUC6 coreference corpus, including both the original training and test material, for a total of 60 documents.</S>
			<S sid ="475" ssid = "13">The documents were automatically preprocessed in the following way: All meta-information about each document indicated in XML (such as WSJ category and date) was discarded, and the headline was included and counted as one sentence.</S>
			<S sid ="476" ssid = "14">Whenever headlines contained three dashes, everything after the dashes was discarded.</S>
			<S sid ="477" ssid = "15">We then converted the MUC coreference chains into an anaphor–antecedent annotation concentrating on anaphoric definite NPs.</S>
			<S sid ="478" ssid = "16">All definite NPs which are in, but not at the beginning of, a coreference chain are potential anaphors.</S>
			<S sid ="479" ssid = "17">We excluded definite NPs with proper noun heads (such as the United States) from this set, since these do not depend on an antecedent for interpretation and are therefore not truly anaphoric.34 We also excluded appositives, which provide coreference structurally and are therefore 33 We thank an anonymous reviewer for pointing out that this role for coreference is more likely to be.</S>
			<S sid ="480" ssid = "18">provided by demonstratives than definite NPs.</S>
			<S sid ="481" ssid = "19">34 Proper-noun heads are approximated by capitalization in the exclusion procedure..</S>
			<S sid ="482" ssid = "20">389 not anaphoric.</S>
			<S sid ="483" ssid = "21">Otherwise, we strictly followed the MUC annotation for coreference in our extraction, although it is not entirely consistent and not necessarily comprehensive (van Deemter and Kibble 2000).</S>
			<S sid ="484" ssid = "22">This extraction method yielded a set of 565 anaphoric definite NPs.</S>
			<S sid ="485" ssid = "23">For each extracted anaphor in a coreference chain C we regard the NP in C that is closest to the anaphor as the correct antecedent, whereas all other previous mentions in C are regarded as lenient.</S>
			<S sid ="486" ssid = "24">NPs that occur before the anaphor but are not marked as being in the same coreference chain are distractors.</S>
			<S sid ="487" ssid = "25">Since anaphors with split antecedents are not annotated in MUC, anaphors cannot have more than one correct antecedent.</S>
			<S sid ="488" ssid = "26">In Example (13), the NPs with the head nouns Pact, contract, and settlement are marked as coreferent in MUC: In our annotation, the settlement is an anaphor with a correct antecedent headed by contract and a lenient antecedent Pact.</S>
			<S sid ="489" ssid = "27">Other NPs prior to the anaphor (e.g., Canada or the IWACanada union) are distractors.35 (13) Forest Products Firms Tentatively Agree On Pact in Canada.</S>
			<S sid ="490" ssid = "28">A group of large British Columbia forest products companies has reached a tentative, three-year labor contract with about 18,000 members of the IWACanada union, ...</S>
			<S sid ="491" ssid = "29">The settlement involves . . ..</S>
			<S sid ="492" ssid = "30">With respect to other-anaphora, we expanded our window size from two to five sentences (the current and the four previous sentences) and excluded all anaphors with no correct or lenient antecedent within this window size, thus yielding a final set of 477 anaphors (84.4% of 565).</S>
			<S sid ="493" ssid = "31">This larger window size is motivated by the fact that a window size of two would cover only 62.3% of all anaphors (352 out 565).</S>
			<S sid ="494" ssid = "32">5.2 Antecedent Extraction, Preprocessing, and Baselines.</S>
			<S sid ="495" ssid = "33">All NPs prior to the anaphor within the five-sentence window were extracted as antecedent candidates.36 We further processed anaphors and antecedents as in Case Study I (see Section 4.2): Modification was stripped and all NPs were lemmatized.</S>
			<S sid ="496" ssid = "34">In this experiment, named entities were resolved using Curran and Clark’s (2003) NE tagger rather than GATE.37 The identified named entities were further subclassified into finer-grained entities, as described for Case Study I. The final number of extracted antecedents for the whole data set of 477 anaphors is 14,233, with an average of 29.84 antecedent candidates per anaphor.</S>
			<S sid ="497" ssid = "35">This figure is much higher than the average number of antecedent candidates for other-anaphors (10.5) because of the larger window size used.</S>
			<S sid ="498" ssid = "36">The data set includes 473 correct antecedents, 803 lenient antecedents, and 12,957 distractors.</S>
			<S sid ="499" ssid = "37">Table 11 shows the distribution of NP types for correct and lenient antecedents and for distractors.</S>
			<S sid ="500" ssid = "38">There are fewer correct antecedents (473) than anaphors (477) because the MUC annotation also includes anaphors whose antecedent is not an NP but, for example, a nominal modifier in a compound.</S>
			<S sid ="501" ssid = "39">Thus, in Example (14), the bankruptcy code is annotated in MUC as coreferential to bankruptcy-law, a modifier in bankruptcy-law protection.</S>
			<S sid ="502" ssid = "40">35 All examples in the coreference study are from the MUC6 corpus..</S>
			<S sid ="503" ssid = "41">36 This extraction was conducted manually, to put this study on an equal footing with Case Study I. It.</S>
			<S sid ="504" ssid = "42">presupposes perfect NP chunking.</S>
			<S sid ="505" ssid = "43">A further discussion of this issue can be found in Section 6.</S>
			<S sid ="506" ssid = "44">37 Curran and Clark’s (2003) tagger was not available to us during the first case study.</S>
			<S sid ="507" ssid = "45">Both NE taggers are.</S>
			<S sid ="508" ssid = "46">state-of-the-art taggers trained on newspaper text.</S>
			<S sid ="509" ssid = "47">390 Table 11 Distribution of antecedent NP types for definite NP anaphora.</S>
			<S sid ="510" ssid = "48">Co rre ct Le nie nt Di str act ors A ll Pro nou ns 7 0 1 4 5 1 , 0 7 8 1, 2 9 3 Na me d enti ties 1 2 3 3 1 6 3 , 1 0 8 3, 5 4 7 Co mm on nou ns 2 8 0 3 4 2 8 , 7 7 1 9, 1 3 3 Tota l 4 7 3 8 0 3 1 2 , 9 5 7 14, 23 3 (14) All legal proceedings against Eastern, a unit of Texas Air Corp., were put on hold when Eastern filed for bankruptcy-law protection March 9....</S>
			<S sid ="511" ssid = "49">If it doesn’t go quickly enough, the judge said he may invoke a provision of the bankruptcy code [...]</S>
			<S sid ="512" ssid = "50">In our scheme we extract the bankruptcy code as anaphoric but our method of extracting candidate antecedents does not include bankruptcy-law.</S>
			<S sid ="513" ssid = "51">Therefore, there are four anaphors in our data set with no correct/lenient antecedent extracted.</S>
			<S sid ="514" ssid = "52">These cannot be resolved by any of the suggested approaches.</S>
			<S sid ="515" ssid = "53">We use the same evaluation measures as for other-anaphora as well as the same significance tests for precision∗.</S>
			<S sid ="516" ssid = "54">We also use the same baseline variations baselineREC, baselineSTRv1 , and baselineSTRv2 (see Table 12 and cf.</S>
			<S sid ="517" ssid = "55">Table 2).</S>
			<S sid ="518" ssid = "56">The recency baseline performs worse than for other-anaphora.</S>
			<S sid ="519" ssid = "57">String matching improves dramatically on simple recency.</S>
			<S sid ="520" ssid = "58">It also seems to be more relevant than for our other-anaphora data set, achieving higher coverage, precision, and recall.</S>
			<S sid ="521" ssid = "59">This confirms the high value of string matching that has been assigned to coreference resolution by previous researchers (Soon, Ng, and Lim 2001; Strube, Rapp, and Mueller 2002, among others).</S>
			<S sid ="522" ssid = "60">As the MUC data set does not include split antecedents, an anaphor ana usually agrees in number with its antecedent.</S>
			<S sid ="523" ssid = "61">Therefore, we also explored variations of all algorithms that as a first step delete from Aanaid all candidate antecedents that do not agree in number with ana.38 The algorithms then proceed as usual.</S>
			<S sid ="524" ssid = "62">Algorithms that use number checking are marked with an additional n in the subscript.</S>
			<S sid ="525" ssid = "63">Using number checking leads to small but consistent gains for all baselines.</S>
			<S sid ="526" ssid = "64">As in Case Study I, we deleted pronouns for the WordNet- and corpus-based methods, thereby removing 70 of 473 (14.8%) of correct antecedents (see Table 11).</S>
			<S sid ="527" ssid = "65">After pronoun deletion, the total number of antecedents in our data set is 12,940 for 477 anaphors, of which 403 are correct antecedents, 658 are lenient antecedents, and 11,879 are distractors.</S>
			<S sid ="528" ssid = "66">38 The number feature can have the values singular, plural, or unknown.</S>
			<S sid ="529" ssid = "67">All NE antecedent candidates.</S>
			<S sid ="530" ssid = "68">received the value singular, as this was by far the most common occurrence in the data set.</S>
			<S sid ="531" ssid = "69">Information about the grammatical number of anaphors and common-noun antecedent candidates was calculated and retained as additional information during the lemmatization process.</S>
			<S sid ="532" ssid = "70">If lemmatization to both a plural and a singular noun (as determined by WordNet and CELEX) was possible (for example, the word talks could be lemmatized to talk or talks), the value unknown was used.</S>
			<S sid ="533" ssid = "71">An anaphor and an antecedent candidate were said to agree in number if they had the same value or if at least one of the two values was unknown.</S>
			<S sid ="534" ssid = "72">391 Table 12 Overview of the results for all baselines for coreference.</S>
			<S sid ="535" ssid = "73">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ bas eline REC 1 . 0 0 0 0 . 0 3 1 0.</S>
			<S sid ="536" ssid = "74">03 1 0 . 0 3 1 0 . 0 3 1 bas eline STR v1 0 . 6 3 7 0 . 8 0 3 0.</S>
			<S sid ="537" ssid = "75">51 1 0 . 6 2 5 0 . 5 3 2 bas eline STR v2 0 . 7 1 7 0 . 7 7 5 0.</S>
			<S sid ="538" ssid = "76">55 5 0 . 6 4 7 0 . 5 7 0 W i t h n u m b e r c h e c k i n g bas eline REC n 1 . 0 0 0 0 . 0 8 6 0.</S>
			<S sid ="539" ssid = "77">08 6 0 . 0 8 6 0 . 0 8 6 bas eline STR v1n 0 . 6 1 4 0 . 8 3 3 0.</S>
			<S sid ="540" ssid = "78">51 1 0 . 6 3 4 0 . 5 4 9 bas eline STR v2n 0 . 6 9 4 0 . 8 0 9 0.</S>
			<S sid ="541" ssid = "79">56 2 0 . 6 6 4 0 . 5 9 1 5.3 WordNet for Antecedent Selection in Definite NP Coreference.</S>
			<S sid ="542" ssid = "80">We hypothesize that again most antecedents are hyponyms or synonyms of their anaphors in definite NP coreference (see Examples (1) and (13)).</S>
			<S sid ="543" ssid = "81">Therefore we use the same look-up for hyp/syn relations that was used for other-anaphora (see Section 4.4), including the specifications for common noun and proper name look-ups.</S>
			<S sid ="544" ssid = "82">Parallel to Table 3, Table 13 summarizes how many correct and lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in WordNet.</S>
			<S sid ="545" ssid = "83">As already observed for other-anaphora, correct and lenient antecedents stand in a hyp/syn relation to their anaphor significantly more often than distractors do (t-test, p &lt; 0.001).</S>
			<S sid ="546" ssid = "84">Hyp/syn relations in WordNet might be better at capturing the relation between antecedent and anaphors for definite NP coreference than for other-anaphora:39 A higher percentage of correct and lenient antecedents of definite NP coreference (71.96%/67.78%) stand in a hyp/syn relation to their anaphors than is the case for other-anaphora (43.0%/42.5%).</S>
			<S sid ="547" ssid = "85">At the same time, though, there is no difference in the percentage of distractors that stand in a hyp/syn relation to their anaphors (9% for other- anaphora, 8.80% for definite NP coreference).</S>
			<S sid ="548" ssid = "86">For our WordNet algorithms, this is likely to translate directly into higher coverage and recall and potentially into higher precision than in Case Study I. Still, about 30% of correct antecedents are not in a hyp/syn relation to their anaphor in the current case study, confirming results by Harabagiu, Bunescu, and Maiorano (2001), who also look at MUC-style corpora.40 This gap, though, is alleviated by a quite high number of lenient antecedents, whose resolution can make up for a missing link between anaphor and correct antecedent.41 The WordNet-based algorithms are defined exactly as in Section 4.4, with the additional two algorithms that include number checking.</S>
			<S sid ="549" ssid = "87">Results are summarized in Table 14.</S>
			<S sid ="550" ssid = "88">All variations of the WordNet algorithms perform significantly better than the corresponding versions of the string-matching baseline (i.e., algoWNv1 is better than baselineSTRv1 , ..., algoWNv2n is better than baselineSTRv2n ), showing that they add 39 Some of this difference might be due to the corpus used instead of the phenomenon as such..</S>
			<S sid ="551" ssid = "89">40 Harabagiu, Bunescu, and Maiorano (2001) include all common-noun coreference links in their countings,.</S>
			<S sid ="552" ssid = "90">whereas we concentrate on definite NPs only, so that the results are not exactly the same.</S>
			<S sid ="553" ssid = "91">41 The possibility of resolving to lenient antecedents follows a similar approach as that of Ng and Cardie.</S>
			<S sid ="554" ssid = "92">(2002b), who suggest a “best-first” coreference resolution approach instead of a “most recent first” approach.</S>
			<S sid ="555" ssid = "93">392 Table 13 Descriptive statistics for WordNet hyp/syn relations on the coreference data set.</S>
			<S sid ="556" ssid = "94">Hyp/syn relation to anaphor No hyp/syn relation Total Corr ect ante ced ents 2 9 0 ( 7 1 . 9 6 % ) 1 1 3 ( 2 8 . 0 4 % ) 4 0 3 ( 1 0 0 % ) Leni ent ante ced ents 4 4 6 ( 6 7 . 7 8 % ) 2 1 2 ( 3 2 . 2 2 % ) 6 5 8 ( 1 0 0 % ) Dist ract ors 1 , 0 4 6 ( 8 . 8 0 % ) 1 0 , 8 3 3 ( 9 1 . 2 0 % ) 1 1, 8 7 9 (1 0 0 % ) All ante ced ents 1 , 7 8 2 ( 1 3 . 7 7 % ) 1 1 , 1 5 8 ( 8 6 . 2 3 % ) 1 2, 9 4 0 (1 0 0 % ) additional lexical knowledge to string matching.</S>
			<S sid ="557" ssid = "95">As expected from the descriptive statistics discussed above, the results are better than those obtained by the WordNet algorithms for other-anaphora, even if we disregard the additional morphosyntactic number constraint.</S>
			<S sid ="558" ssid = "96">5.4 The Corpus-Based Approach for Definite NP Coreference.</S>
			<S sid ="559" ssid = "97">Following the assumption that most antecedents are hyponyms or synonyms of their anaphors in definite NP coreference, we use the same list-context pattern and instantiations that were used for other-anaphora, allowing us to evaluate whether they are transferrable.</S>
			<S sid ="560" ssid = "98">The corpora we use are again the Web and the BNC.</S>
			<S sid ="561" ssid = "99">As with other-anaphora, the Web scores do well in distinguishing between correct/lenient antecedents and distractors, with significantly higher means/medians for correct/lenient antecedents (median 472/617 vs. 2 for distractors), as well as significantly fewer zero scores (8% for correct/lenient vs. 41% for distractors).</S>
			<S sid ="562" ssid = "100">This indicates transferability of the Web-based approach to coreference.</S>
			<S sid ="563" ssid = "101">Compared to other-anaphora, the number of zero-scores is lower for correct/lenient antecedent types, so that we expect better overall results, similar to our expectations for the WordNet algorithm.</S>
			<S sid ="564" ssid = "102">The BNC scores can also distinguish between correct/lenient antecedents and distractors, since the number of zero scores for correct/lenient antecedents (68.98%/ 58.05%) is significantly lower than for distractors (96.97%).</S>
			<S sid ="565" ssid = "103">Although more than 50% of correct/lenient antecedents receive a zero score, there are fewer zero scores than for other-anaphora (for which more than 80% of correct/lenient antecedents received zero scores).</S>
			<S sid ="566" ssid = "104">However, BNC scores are again in general much lower than Web scores, as measured by means, medians, and zero scores.</S>
			<S sid ="567" ssid = "105">Nevertheless, Web scores and BNC scores correlate significantly, with the correlations reaching higher coeffi Table 14 Overview of the results for all WordNet algorithms for coreference.</S>
			<S sid ="568" ssid = "106">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ algo WNv 1 0 . 8 7 4 0 . 7 1 5 0.</S>
			<S sid ="569" ssid = "107">62 5 0 . 6 6 6 0 . 6 3 1 algo WNv 2 0 . 8 7 4 0 . 7 2 4 0.</S>
			<S sid ="570" ssid = "108">63 3 0 . 6 7 6 0 . 6 3 9 W ith n u m be r ch ec ki ng algo WNv 1n 0 . 8 6 6 0 . 7 3 4 0.</S>
			<S sid ="571" ssid = "109">63 5 0 . 6 8 1 0 . 6 4 8 algo WNv 2n 0 . 8 6 6 0 . 7 5 1 0.</S>
			<S sid ="572" ssid = "110">64 9 0 . 6 9 7 0 . 6 6 2 393 cients (0.53 to 0.65, depending on antecedent group) than they did in the case study for other-anaphora.</S>
			<S sid ="573" ssid = "111">The corpus-based algorithms for coreference resolution are parallel to those described for other-anaphora and are marked by the same subscripts.</S>
			<S sid ="574" ssid = "112">The variations that include number checking are again marked by a subscript n. Tables 15 and 16 report the results for all the Web and BNC algorithms, respectively.</S>
			<S sid ="575" ssid = "113">5.5 Discussion and Error Analysis.</S>
			<S sid ="576" ssid = "114">5.5.1 Algorithm Comparison.</S>
			<S sid ="577" ssid = "115">Using the original or the replaced antecedent for string matching (versions v1 vs. v2, v1n vs. v2n, v3 vs. v4, and v3n vs. v4n) never results in interesting differences in any of the approaches discussed.</S>
			<S sid ="578" ssid = "116">Also, number matching provides consistent improvements.</S>
			<S sid ="579" ssid = "117">Therefore, from this point on, our discussion will disregard those variations, that use original antecedents only (v1, v1n, v3, and v3n) as well as algorithms that do not use number matching (v2, v4).</S>
			<S sid ="580" ssid = "118">We will also concentrateon the final precision∗ of the full-coverage algorithms.</S>
			<S sid ="581" ssid = "119">The set of anaphors that are cov ered by the best string-matching baseline, prior to recency back-off, will again be denoted by StrSetv2n . Again, both a t-test and McNemar ’s test will be used, when statements about significance are made.</S>
			<S sid ="582" ssid = "120">The results for the string-matching baselines and for the lexical methods are higher for definite coreferential NPs than for other-anaphora.</S>
			<S sid ="583" ssid = "121">This is largely a result of the higher number of string-matching antecedent/anaphor pairs in coreference, the higher precision of string matching, and to a lesser degree, the lower number of unusual redescriptions.</S>
			<S sid ="584" ssid = "122">Similar to the results for other-anaphora, the WordNet-based algorithms beat the corresponding baselines.</S>
			<S sid ="585" ssid = "123">The first striking result is that the Web algorithm variation algoWebv2n , which relies only on the highest Web scores and is therefore allowed to overrule string matching, does not outperform the corresponding string-matching baseline baselineSTRv2n and performs significantly worse than the corresponding WordNet algorithm algoWNv2n . This contrasts with the results for other-anaphora.</S>
			<S sid ="586" ssid = "124">When the results were examined in detail, it emerged that for a considerable number of anaphors in StrSetv2n , the highest Web score was indeed achieved by a distractor with a high-frequency head noun when the correct or lenient antecedent could be instead found by a simple string match to the anaphor.</S>
			<S sid ="587" ssid = "125">This problem is much more severe than Table 15 Overview of the results for all Web algorithms for coreference.</S>
			<S sid ="588" ssid = "126">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ algo Web v1 0 . 9 9 4 0 . 5 6 1 0.</S>
			<S sid ="589" ssid = "127">55 8 0 . 5 5 9 0 . 5 6 2 algo Web v2 0 . 9 9 4 0 . 5 5 3 0.</S>
			<S sid ="590" ssid = "128">54 9 0 . 5 5 0 0 . 5 5 4 algo Web v3 0 . 9 9 8 0 . 6 7 4 0.</S>
			<S sid ="591" ssid = "129">67 3 0 . 6 7 3 0 . 6 7 3 algo Web v4 0 . 9 9 8 0 . 6 7 9 0.</S>
			<S sid ="592" ssid = "130">67 7 0 . 6 7 8 0 . 6 7 7 W ith n u m be r ch ec ki ng algo Web v1n 0 . 9 9 2 0 . 6 1 3 0.</S>
			<S sid ="593" ssid = "131">60 8 0 . 6 1 0 0 . 6 1 2 algo Web v2n 0 . 9 9 2 0 . 6 0 7 0.</S>
			<S sid ="594" ssid = "132">60 2 0 . 6 0 4 0 . 6 0 6 algo Web v3n 0 . 9 9 6 0 . 7 0 5 0.</S>
			<S sid ="595" ssid = "133">70 2 0 . 7 0 3 0 . 7 0 3 algo Web v4n 0 . 9 9 6 0 . 7 1 6 0.</S>
			<S sid ="596" ssid = "134">71 3 0 . 7 1 4 0 . 7 1 3 394 Table 16 Overview of the results for all BNC algorithms for coreference.</S>
			<S sid ="597" ssid = "135">Alg orit hm Co ver ag e Pre cis ion Re cal lF me as ure Pr eci sio n∗ algo BNC v1 0 . 4 3 8 0 . 5 5 9 0.</S>
			<S sid ="598" ssid = "136">24 5 0 . 3 4 1 0 . 5 2 4 algo BNC v2 0 . 4 3 8 0 . 5 5 9 0.</S>
			<S sid ="599" ssid = "137">24 5 0 . 3 4 1 0 . 5 2 6 algo BNC v3 0 . 7 6 9 0 . 7 4 9 0.</S>
			<S sid ="600" ssid = "138">57 6 0 . 6 5 1 0 . 5 8 9 algo BNC v4 0 . 7 7 7 0 . 7 5 7 0.</S>
			<S sid ="601" ssid = "139">58 9 0 . 6 6 3 0 . 5 9 9 W it h n u m b er ch ec ki n g algo BNC v1n 0 . 4 1 1 0 . 6 1 2 0.</S>
			<S sid ="602" ssid = "140">25 1 0 . 3 5 6 0 . 5 6 2 algo BNC v2n 0 . 4 1 1 0 . 6 2 2 0.</S>
			<S sid ="603" ssid = "141">25 6 0 . 3 6 9 0 . 5 7 0 algo BNC v3n 0 . 7 5 3 0 . 7 6 9 0.</S>
			<S sid ="604" ssid = "142">57 9 0 . 6 6 1 0 . 6 1 0 algo BNC v4n 0 . 7 6 1 0 . 7 8 5 0.</S>
			<S sid ="605" ssid = "143">59 7 0 . 6 7 8 0 . 6 2 7 for other-anaphora because of (1) the larger window size that includes more distractors and (2) the higher a priori precision of the string-matching baseline, which means that overruling string matching leads to wrong results more frequently.</S>
			<S sid ="606" ssid = "144">Typical examples involve named-entity recognition and inverted queries.</S>
			<S sid ="607" ssid = "145">Thus, in Example (15), the anaphor the union is coreferent with the first occurrence of the union, a case easily resolved by string matching.</S>
			<S sid ="608" ssid = "146">However, the distractor organization [= Chrysler Canada] achieves a higher Web score, because of the score of the inverted query union OR unions and other organizations.42 (15) [.</S>
			<S sid ="609" ssid = "147">] The union struck Chrysler Canada Tuesday after rejecting a company offer on pension adjustments.</S>
			<S sid ="610" ssid = "148">The union said the size of the adjustments was inadequate.</S>
			<S sid ="611" ssid = "149">Several potential solutions exist to this problem, such as normalization of Web scores or penalizing of inverted queries.</S>
			<S sid ="612" ssid = "150">The solution we have adopted in algoWebv4n is to use Web scores only after string matching, thereby making the Web-based approach more comparable to the WordNet approach.</S>
			<S sid ="613" ssid = "151">Therefore, baselineSTRv2n , algoWebv4n , and algoWNv2n (as well as algoBNCv4n ) all coincide in their decisions for anaphors in StrSetv2n and only differ in the decisions made for anaphors that do not have a matching antecedent candidate.</S>
			<S sid ="614" ssid = "152">Indeed, algoWebv4n performs significantly better than the base lines at the 1% level, and results rise from a precision∗ of 60.6% for algoWebv2n to 71.3% for algoWebv4n . It also significantly outperforms the best BNC results, thus showing that overcoming data sparseness is more important than working with a controlled, tagged, and representative corpus.</S>
			<S sid ="615" ssid = "153">Furthermore, shows better performance than WordNet in the final algorithm variation (71.3% vs. 66.2%).43 According to results of a t-test, however, this last difference is not significant.</S>
			<S sid ="616" ssid = "154">McNemar ’s test, concentrating on the part of the data in which the methods differ, shows instead significance at the 1% level.</S>
			<S sid ="617" ssid = "155">Indeed, one of the problems in comparing algorithm results for coreference is that such a large number of anaphors are covered by simple string matching, leaving only 42 Remember that this problem does not affect the WordNet-based algorithm, which always achieves the.</S>
			<S sid ="618" ssid = "156">same results as the string-matching baseline on StrSetv2n . Both the correct antecedent and the organization [= Chrysler Canada] distractor stand in a hyp/syn relation to the anaphor, and then string matching is used as a tiebreaker.</S>
			<S sid ="619" ssid = "157">43 In general, the WordNet methods achieve higher precision, with the Web method achieving higher recall..</S>
			<S sid ="620" ssid = "158">395 a small data set on which the lexical methods can differ.</S>
			<S sid ="621" ssid = "159">Thus, StrSetv2n contains 331 of 477 cases (268 of which are assigned correctly by baselineStrv2n ), so that improvements by the other methods are confined to the set of the remaining 146 anaphors.</S>
			<S sid ="622" ssid = "160">Of these 146, baselineStr∗ assigns the correct antecedent to 13 (8.9%) anaphors by using a recency back-off, the best WordNet method to 55 anaphors (37.67%), and the best Web method to 72 anaphors (49.31%).</S>
			<S sid ="623" ssid = "161">Therefore the Web-based method is a better complement to string matching than WordNet, which is reflected in the results of McNemar ’s test.</S>
			<S sid ="624" ssid = "162">Anaphor–antecedent relations that were not covered in WordNet but that did not prove a problem for the Web algorithm were again both general hyponymy relations, such as retailer:organization, bill:legislation and month:time, and more subjective relations like (wage) cuts:concessions and legislation:attack.</S>
			<S sid ="625" ssid = "163">5.5.2 Error Analysis.</S>
			<S sid ="626" ssid = "164">The best-performing Web-based algorithm, algoWebv4n , still selects the wrong antecedent for a given anaphor in 137 of 477 cases (28.7%).</S>
			<S sid ="627" ssid = "165">Again, we use the decision tree in Figure 1 to classify errors.</S>
			<S sid ="628" ssid = "166">Design errors now do not include split antecedents but do include errors that occur because the condition of number agreement was violated, pronoun deletion errors, and the four cases in which the antecedent is a non-NP antecedent and therefore not extracted in the first place (see Section 5.1 and Example (14)).</S>
			<S sid ="629" ssid = "167">Table 17 reports the frequency of each error type.</S>
			<S sid ="630" ssid = "168">Differently from other-anaphora, the design and NE errors together account for under 15% of the mistakes.</S>
			<S sid ="631" ssid = "169">Also rare are zero-score errors (only 8%).</S>
			<S sid ="632" ssid = "170">When compared to the number of zero-score errors in other anaphora (24.7%), this low figure suggests that other-anaphora is more prone to exploit rare, unusual, and context-dependent redescriptions than full NP coreference.</S>
			<S sid ="633" ssid = "171">Nevertheless, it is yet possible to find nonstandard redescriptions in coreference as well which yield zero scores, such as the use of transaction to refer to move in Example (16).</S>
			<S sid ="634" ssid = "172">(16) Conseco Inc., in a move to generate about $200 million in tax deductions, said it induced five of its top executives to exercise stock options to purchase about 3.6 million common shares of the financial-services concern.</S>
			<S sid ="635" ssid = "173">As a result of the transaction, ...</S>
			<S sid ="636" ssid = "174">Much more substantial is the weight of errors due to string matching, tiebreaker decisions, and the presence of good distractors (the main reason for errors of type other), which together account for over three-quarters of all mistakes.</S>
			<S sid ="637" ssid = "175">String matching is quite successful for coreference (baselineSTRv2n covers nearly70% of the cases with a precision of 80.9%).</S>
			<S sid ="638" ssid = "176">However, because algoWebv4n never over Table 17 Occurrences of error types for the best coreference algorithm algoWebv4n . Error type Number of cases Percentage of cases Des ign 1 2 8 . 7 Na me d enti ty 7 5 . 1 Stri ng mat chi ng 3 3 2 4 . 1 Zero scor es 1 1 8 . 0 Tieb reak er 3 4 2 4 . 8 Oth er 4 0 2 9 . 2 Tota l 1 3 7 9 9 . 9 396 rules string matching, the errors of baselineSTRv2n are preserved here and account for24.1% of all mistakes.44 Tiebreaker errors are quite frequent too (24.8%), as our far-from sophisticated tiebreaker was needed in nearly half of the cases (224 times; 47.0%).</S>
			<S sid ="639" ssid = "177">The remaining errors (29.2%) are due to the presence of good distractors that score higher than the correct/lenient antecedent.</S>
			<S sid ="640" ssid = "178">In Example (17), for instance, a distractor with a higher Web score (comment) prevents the algorithm from selecting the correct antecedent (investigation) for the anaphor the inquiry.</S>
			<S sid ="641" ssid = "179">(17) Mr. Adams couldn’t be reached for comment.</S>
			<S sid ="642" ssid = "180">Though the investigation has barely begun, persons close to the board said Messrs.</S>
			<S sid ="643" ssid = "181">Lavin and Young will get a “hard look” as to whether they were involved, and are both considered a “natural focus” of the inquiry.</S>
			<S sid ="644" ssid = "182">Example (18) shows how stripping modification might have eliminated information crucial to identifying the correct antecedent: Only the head process was retained of the anaphor arbitration process, so that the surface link between anaphor and antecedent (arbitration) was lost and the distractor securities industry, reduced to industry, was instead selected.</S>
			<S sid ="645" ssid = "183">(18) The securities industry has favored arbitration because it keeps brokers and dealers out of court.</S>
			<S sid ="646" ssid = "184">But consumer advocates say customers sometimes unwittingly sign away their right to sue.</S>
			<S sid ="647" ssid = "185">”We don’t necessarily have a beef with the arbitration process,” says Martin Meehan, [...]</S>
	</SECTION>
	<SECTION title="Open Issues. " number = "6">
			<S sid ="648" ssid = "1">6.1 Preprocessing and Prior Assumptions.</S>
			<S sid ="649" ssid = "2">Our algorithms build on two main preprocessing assumptions.</S>
			<S sid ="650" ssid = "3">First, we assume perfect base-NP chunking and expect results to be lower with automatic chunking.</S>
			<S sid ="651" ssid = "4">Nevertheless, since automatic chunking will affect all algorithms in the same way, we do expect comparative results to stand.</S>
			<S sid ="652" ssid = "5">We are not, however, dependent on full parsing, as no parsing-dependent grammatical features are used by the algorithms.</S>
			<S sid ="653" ssid = "6">Second, the anaphoricity of the definite NPs in Case Study II has de facto been manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent.</S>
			<S sid ="654" ssid = "7">One of the reasons why pronoun resolution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2).</S>
			<S sid ="655" ssid = "8">In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a two- stage process which we also follow in this article.</S>
			<S sid ="656" ssid = "9">Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors.</S>
			<S sid ="657" ssid = "10">Future work will explore the extent of such a decrease in performance.</S>
			<S sid ="658" ssid = "11">44 Some of the errors incured by baselineSTRv2n are here classified as design, NE, or tiebreaker errors..</S>
			<S sid ="659" ssid = "12">397 6.2 Directions for Improvement.</S>
			<S sid ="660" ssid = "13">All algorithms we have described can be considered blueprints for more complex versions.</S>
			<S sid ="661" ssid = "14">Specifically, the WordNet-based algorithms could be improved by exploiting information encoded in WordNet beyond explicitly encoded links (glosses could be mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]).</S>
			<S sid ="662" ssid = "15">The Web- based algorithms could similarly benefit from the exploration of different patterns and their combination, as well as from using non-pattern-based approaches for hyponymy detection (Shinzato and Torisawa 2004).</S>
			<S sid ="663" ssid = "16">In addition, we have evaluated the contribution of lexical resources in isolation rather than within a more sophisticated system that integrates additional non-lexical features.</S>
			<S sid ="664" ssid = "17">It is unclear whether integrating such knowledge sources in a full-resolution system might even out the differences between the Web-based and the WordNet-based algorithms or exacerbate them.</S>
			<S sid ="665" ssid = "18">Modjeska, Markert, and Nissim (2003) included a feature based on Web scores in a naive Bayes model for other-anaphora resolution that also used grammatical features and showed that the addition of the Web feature yielded an 11.4-percentage-point improvement over using a WordNet-based feature.</S>
			<S sid ="666" ssid = "19">This gives some indication that additional grammatical features might not be able to compensate fully for the knowledge gap encountered in WordNet.</S>
			<S sid ="667" ssid = "20">6.3 Extension to Yet Other Anaphora Types.</S>
			<S sid ="668" ssid = "21">Using the Web for antecedent selection in anaphora resolution is novel and needs further study for other types of full NP anaphora than the ones studied in this article.</S>
			<S sid ="669" ssid = "22">If an anaphora type exploits hyponymy/synonymy relationships between anaphor and antecedent head nouns, it can in principle be treated with the exact same pattern we used in this article.</S>
			<S sid ="670" ssid = "23">This holds, for example, for demonstratives and such-anaphors.</S>
			<S sid ="671" ssid = "24">The latter, in particular, are similar to other-anaphora in that they establish a comparison between the entity they invoke and that invoked by the antecedent and are also easily used to accommodate subjective viewpoints.</S>
			<S sid ="672" ssid = "25">They should therefore benefit especially from not relying wholly on standard taxonomic links.</S>
			<S sid ="673" ssid = "26">Different patterns can be developed for anaphora types that build on non- hyponymy relations.</S>
			<S sid ="674" ssid = "27">For example, bridging exploits meronymy and/or causal relations (among others).</S>
			<S sid ="675" ssid = "28">Therefore, patterns that express “part-of” links, for example, such as X of Y and genitives, would be appropriate.</S>
			<S sid ="676" ssid = "29">Indeed, these patterns have been recently used in Web search for antecedent selection for bridging anaphora by Poesio et al.</S>
			<S sid ="677" ssid = "30">(2004).</S>
			<S sid ="678" ssid = "31">They compare accuracy in antecedent selection for a method that integrates Web hits and focusing techniques with a method that uses WordNet and focusing, achieving comparable results for both methods.</S>
			<S sid ="679" ssid = "32">This strenghtens our hypothesis that antecedent selection for full NP anaphora without hand-modeled lexical knowledge has become feasible.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "7">
			<S sid ="680" ssid = "1">We have explored two different ways of exploiting lexical knowledge for antecedent selection in other-anaphora and definite NP coreference.</S>
			<S sid ="681" ssid = "2">Specifically, we have compared a handcrafted and -structured source of information such as WordNet and a simple and inexpensive pattern-based method operating on corpora.</S>
			<S sid ="682" ssid = "3">As corpora we have used the BNC and also suggested the Web as the biggest corpus available.</S>
			<S sid ="683" ssid = "4">398 We confirmed results by other researchers that show that a substantial number of lexical links often exploited in coreference are not included in WordNet.</S>
			<S sid ="684" ssid = "5">We have also shown the presence of an even more severe knowledge gap for other-anaphora (see also Question 1 in Section 1).</S>
			<S sid ="685" ssid = "6">Largely because of this knowledge gap, the novel Web-based method that we proposed proved better than WordNet at resolving other-anaphora.</S>
			<S sid ="686" ssid = "7">Although the gains for coreference are not as high, the Web-based method improves more substantially on string-matching techniques for coreference than WordNet does (see the success rate beyond StrSetv2n for coreference, Section 5.5).</S>
			<S sid ="687" ssid = "8">In both studies, the Web-based method clearly outperformed the BNC-based one.</S>
			<S sid ="688" ssid = "9">This shows that, for our tasks, overcoming data sparseness was more important than working with a manually controlled, virtually noise-free, but relatively small corpus, which addresses Question 2 in Section 1: Corpus-induced knowledge can indeed rival and even outperform the knowledge obtained via lexical hierarchies, as long as the corpus is large enough.</S>
			<S sid ="689" ssid = "10">Corpus-based methods can therefore be a very useful complement to resolution algorithms for languages for which handcrafted taxonomies have not yet been created but for which large corpora do exist.</S>
			<S sid ="690" ssid = "11">In answer to Question 3 in Section 1, our results suggest that different anaphoric phenomena suffer in varying degrees from missing knowledge and that the Web-based method performs best when used to deal with phenomena that standard taxonomy links do not capture that easily or that frequently exploit subjective and context-dependent knowledge.</S>
			<S sid ="691" ssid = "12">In addition, the Web-based method that we propose does not suffer from some of the intrinsic limitations of ontologies, specifically, the problem of what knowledge should be included (see Section 2.2).</S>
			<S sid ="692" ssid = "13">It is also inexpensive and does not need any postprocessing of the Web pages returned or any hand-modeling of lexical knowledge.</S>
			<S sid ="693" ssid = "14">To summarize, antecedent selection for other-anaphora and definite NP coreference without handcrafted lexical knowledge is feasible.</S>
			<S sid ="694" ssid = "15">This might also be the case for yet other full NP anaphora types with similar properties—an issue that we will explore in future work.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="695" ssid = "16">We especially thank Natalia Modjeska for providing us with her annotated corpus of other-anaphors as well as with the extracted and partially preprocessed sets of candidate antecedents for Case Study I. She also collaborated on previous related work on other-anaphora (Markert, Nissim, and Modjeska 2003; Modjeska, Markert, and Nissim 2003) on which this article builds.</S>
			<S sid ="696" ssid = "17">We would also like to thank Johan Bos, James Curran, Bonnie Webber, and four anonymous reviewers for helpful comments, which allowed us to greatly improve this article.</S>
			<S sid ="697" ssid = "18">Malvina Nissim was partially supported by Scottish Enterprise Stanford-Link Grants R36766 (Paraphrase Generation) and R36759 (SEER).</S>
	</SECTION>
</PAPER>
