<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes a Natural Language Learning method that extractsknowledge in the form of semantic patterns with ontology elements associated to syntactic components in the text.</S>
		<S sid ="2" ssid = "2">Themethod combines the use of EuroWord-Net’s ontological concepts and the correct sense of each word assigned by a Word Sense Disambiguation(WSD)module to extract three sets of patterns: subject-verb, verb-direct objectand verb-indirect object.</S>
		<S sid ="3" ssid = "3">These sets define the semantic behaviour of the maintextual elements based on their syntactic role.</S>
		<S sid ="4" ssid = "4">On the one hand, it is shown that Maximum Entropy models applied to WSD tasks provide good results.</S>
		<S sid ="5" ssid = "5">Theevaluation of the WSD module has revealed a accuracy rate of 64% in a preliminary test.</S>
		<S sid ="6" ssid = "6">On the other hand, we explain how an adequate set of semantic or ontological patterns can improve thesuccess rate of NLP tasks such us pronoun resolution.</S>
		<S sid ="7" ssid = "7">We have implemented both modules in C++ and although theevaluation has been performed for English, their general features allow thetreatment of other languages like Spanish.</S>
		<S sid ="8" ssid = "8">This paper has been partially supported by the SpanishGovernment (CICYT) project number TIC20000664-C0202.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Semantic patterns, as defined in this method, con figure a system to add a new information source to Natural Language Processing (NLP) tasks.</S>
			<S sid ="10" ssid = "10">To obtain these semantic patterns, it is necessary to count on different tools.</S>
			<S sid ="11" ssid = "11">On the one hand, a full parser must make a syntactic analysis of the text.This parsing will allow the selection of the differ ent syntactic functional elements such as subject, direct object (DObj) and indirect object (IObj).</S>
			<S sid ="12" ssid = "12">On the other hand, a WSD tool must provide the correct sense in order to ensure the appropriate selection of the ontological concept associated toeach word.</S>
			<S sid ="13" ssid = "13">Finally, with the parsing and the cor rect sense of each word, the pattern extraction method will form and store ontological pairs that define the semantic behaviour of each sentence.</S>
	</SECTION>
	<SECTION title="Full parsing. " number = "2">
			<S sid ="14" ssid = "1">The analyzer used for this work is the Conexor’s FDG Parser (Pasi Tapanainen and Timo J¬arvinen,1997).</S>
			<S sid ="15" ssid = "2">This parser tries to provide a build depen dency tree from the sentence.</S>
			<S sid ="16" ssid = "3">When this is not possible, the parser tries to build partial trees thatoften result from unresolved ambiguity.</S>
			<S sid ="17" ssid = "4">One vi sual example of this dependency trees is shown in Figure 1 where the parsing tree of sentence (1) is illustrated.</S>
			<S sid ="18" ssid = "5">(1) The minister gave explanations to the Government.</S>
			<S sid ="19" ssid = "6">As seen in Figure 2, the analyzer assigns to each word a text token (second column), a base form (third column) and functional link 0 1 The the det:&amp;gt;2 @DN&amp;gt; DET SG/PL. 2 minister minister subj:&amp;gt;3 @SUBJ N NOM SG 3 gave give main:&amp;gt;0 @+FMAINV V PAST 4 explanations explanation obj:&amp;gt;3 @OBJ N NOM PL 5 to to dat:&amp;gt;3 @ADVL PREP 6 the the det:&amp;gt;7 @DN&amp;gt; DET SG/PL 7 Government government pcomp:&amp;gt;5 @&amp;lt;P N NOM SG/PL. . . Figure 2: FDG Analyser’s output example Figure 1: Parsing tree names, lexico-syntactic function labels and parts of speech (fourth column).</S>
			<S sid ="20" ssid = "7">Figure 1 shows the parsing tree related to this output.</S>
			<S sid ="21" ssid = "8">These elements are enough for the pattern extraction method to be applied to NLP tasks.</S>
			<S sid ="22" ssid = "9">Regarding to the evaluation of the parser, the authors report an average precision and recall of 95% and 88% respectively in the detection of the correct head.</S>
			<S sid ="23" ssid = "10">Furthermore, they report a precisionrate between 89% and 95% and a recall rate between 83% and 96% in the selection of the func tional dependencies.</S>
	</SECTION>
	<SECTION title="WSD based on Maximum Entropy. " number = "3">
			<S sid ="24" ssid = "1">A WSD module is applied to this parser’s output, in order to select the correct sense of each entry.Maximum Entropy(ME) modeling is a framework for integrating information from many het erogeneous information sources for classification(Manning and Sch¬utze, 1999).</S>
			<S sid ="25" ssid = "2">This WSD system is based on conditional ME probability models.</S>
			<S sid ="26" ssid = "3">The system implements a supervised learn ing method consisting of the building of wordsense classifiers through training on a semanti cally tagged corpus.</S>
			<S sid ="27" ssid = "4">A classifier obtained by means of a ME technique consist of a set ofparameters or coefficients estimated by an opti mization procedure.</S>
			<S sid ="28" ssid = "5">Each coefficient associates a weight to one feature observed in the trainingdata.</S>
			<S sid ="29" ssid = "6">A feature is a function that gives information about some characteristic in a context as sociated to a class.</S>
			<S sid ="30" ssid = "7">The basic idea is to obtainthe probability distribution that maximizes the en tropy, that is, maximum ignorance is assumed and nothing apart of training data is considered.</S>
			<S sid ="31" ssid = "8">As advantages of ME framework, knowledge-poor features applying and accuracy can be mentioned; ME framework allows a virtually unrestricted ability to represent problem-specific knowledge in the form of features (Ratnaparkhi, 1998).</S>
			<S sid ="32" ssid = "9">Let us assume a set of contexts and a set of classes . The functionthat performs the classification in a condi tional probability model chooses the class with the highest conditional probability: . The features have the form ex-.</S>
			<S sid ="33" ssid = "10">pressed in equation (1), where is some observable characteristic1.</S>
			<S sid ="34" ssid = "11">The conditional proba bility is defined as in equation (2) where are the parameters or weights of each feature, and is a constant to ensure that the sum of probabilities for each possible class in this context is equal to 1.</S>
			<S sid ="35" ssid = "12">The features defined on the present system are, 1This is the kind of features used in the system due to it is required by the parameter estimation procedure, but the ME approach is not limited to binary funtions.</S>
			<S sid ="36" ssid = "13">if and otherwise basically, collocations of content words and POS tags of function words around the target word.With only this information the system obtains re sults comparable to other well known methods or systems.</S>
			<S sid ="37" ssid = "14">For training, DSO sense tagged English corpus (Hwee Tou Ng and Hian Beng Lee, 1996)is used.</S>
			<S sid ="38" ssid = "15">The DSO corpus is structured in files con taining tagged examples of some word.</S>
			<S sid ="39" ssid = "16">The tags correspond to the correct sense in WordNet 1.5 (FellBaum, 1998).</S>
			<S sid ="40" ssid = "17">The examples were extracted from articles of the Brown Corpus and Wall Street Journal.The implemented system has three main modules: the Feature Extractor (FE), the Generalized Iterative Scaling (GIS), and the Classifica tion module.</S>
			<S sid ="41" ssid = "18">Each word has its own ME model, that is, there will be a distinct classifier for each one.</S>
			<S sid ="42" ssid = "19">The FE module automatically defines thefeatures to be observed on the training corpus depending on the classes (senses) defined in Word Net for a word.</S>
			<S sid ="43" ssid = "20">The GIS module performs the parameter estimation.</S>
			<S sid ="44" ssid = "21">Finally, the Classificationmodule uses this set of parameters in order to dis ambiguate new occurrences of the word.</S>
			<S sid ="45" ssid = "22">3.1 Evaluation and results.</S>
			<S sid ="46" ssid = "23">Some evaluation results over a few terms of the aforementioned corpus are presented in Table 1.</S>
			<S sid ="47" ssid = "24">The system was trained with features that inform of content words in the sentence context ( , , , , ,).</S>
			<S sid ="48" ssid = "25">For each word, the training set is divided in 10 folds, 9 for training and 1 for evaluation; ten tests were accomplished using a different fold for evaluation in each one (10-fold cross-validation).</S>
			<S sid ="49" ssid = "26">The accuracy results are the average accuracy on the ten tests for a word.Results comparison with previous work is dif ficult because there is different approaches to the WSD task (knowledge based methods, supervisedand unsupervised statistical methods...)</S>
			<S sid ="50" ssid = "27">(Mihal cea and Moldovan, 1999) and many of them focus on a different set of words and sense definitions.Furthermore, the training corpus seems to be crit ical to the application of the learning to a specific ocurrences accuracy standard deviation age,N 48,2 0,584 0,134 art,N 38,0 0,623 0,090 car,N 136,7 0,963 0,048 child,N 105,1 0,809 0,073 church,N 35,8 0,625 0,126 cost,N 143,2 0,895 0,051 fall,V 143,7 0,759 0,242 head,N 83,3 0,714 0,125 interest,N 147,8 0,619 0,173 know,V 143,3 0,421 0,087 line,N 132,8 0,529 0,154 set,V 126,1 0,537 0,139 speak,V 51,1 0,729 0,080 take,V 138,0 0,264 0,042 work,N 118,9 0,530 0,175 Overall 0,637 Table 1: Evaluation results from DSOWSJ domain (Escudero et al., 2000b).</S>
			<S sid ="51" ssid = "28">In the experiment presented here, the selection of the target words and the corpus used are thesame that (Escudero et al., 2000a) where a Boosting method is proposed.</S>
			<S sid ="52" ssid = "29">In this paper a com parison between some WSD methods is shown.</S>
			<S sid ="53" ssid = "30">Boosting is the most successful method with a68.1 % accuracy.</S>
			<S sid ="54" ssid = "31">Our method obtains lower accuracy but this is a first implementation and a bet ter feature selection is expected to improve our results.</S>
	</SECTION>
	<SECTION title="Semantic Pattern Learning. " number = "4">
			<S sid ="55" ssid = "1">Once the WSD phase has been performed, thesemantic pattern extraction module can be ex ecuted.</S>
			<S sid ="56" ssid = "2">This module extracts head word pairs with subject-verb, verb-DObj and verb-IObj roles in the sentence and convert them into patterns formed by ontological concepts extracted from EuroWordNet.</S>
			<S sid ="57" ssid = "3">4.1 EuroWordNet’s ontology.</S>
			<S sid ="58" ssid = "4">EuroWordNet (Vossen, 2000) is a multilingual lexical database representing semantic relationsamong basic concepts for West European lan guages.</S>
			<S sid ="59" ssid = "5">In our case, we are going to work withisolated WordNets, it means, we won’t take ad vantage of its multilingual feature, although we will use the ontology defined on it.EuroWordnet’s ontology consists of 63 higherlevel concepts and distinguishes three types of en tities:1stOrderEntity: any concrete entity (pub licly) perceivable by the senses and located at any point in time, in a three-dimensional ,, , ,), multi-word expres sions ( , , , , , , , , , , , ), and POS tags (space, e.g.: vehicle, animal, substance, ob ject.2ndOrderEntity: any Static Situation (prop erty, relation) or Dynamic Situation, whichcannot be grasped, heard, seen, felt as an independent physical thing.</S>
			<S sid ="60" ssid = "6">They can be lo cated in time and occur or take place rather than exist, e.g.: happen, be, have, begin, end, cause, result, continue, occur..3rdOrderEntity: any unobservable proposi tion which exists independently of time and space.</S>
			<S sid ="61" ssid = "7">They can be true or false rather thanreal.</S>
			<S sid ="62" ssid = "8">They can be asserted or denied, remembered or forgotten, e.g.: idea, thought, infor mation, theory, plan.</S>
			<S sid ="63" ssid = "9">These ontological concepts, associated to eachsynset from EuroWordNet, give semantic proper ties to these synsets that can be used, as we willsee in the nexts sections, for improving the in formation source in Natural Language Processing tasks.</S>
			<S sid ="64" ssid = "10">4.2 The Learning Process.</S>
			<S sid ="65" ssid = "11">From each clause, the module extracts the verb and (if exists) its subject, its direct object and itsindirect object.</S>
			<S sid ="66" ssid = "12">With these elements, three pos sible pairs can be formed using the verb and thenoun head of the aforementioned syntactic com ponents.</S>
			<S sid ="67" ssid = "13">The verb head and the noun head are looked up in EuroWordNet’s ontology using thecorrect sense previously selected.</S>
			<S sid ="68" ssid = "14">This query gen erates three possible ontological pairs that define, for each clause, the semantic concept associated to the main syntactic elements.Sentence (2) corresponds to a fragment ex tracted from a training corpus in English.</S>
			<S sid ="69" ssid = "15">As shown in section 2, the output of the parser generates the next functional entities: Verb: give Subject head: minister D.Obj.</S>
			<S sid ="70" ssid = "16">head: explanations I.Obj.</S>
			<S sid ="71" ssid = "17">head: Government The superscripts indicate the correct sense in EuroWordNet for each word.</S>
			<S sid ="72" ssid = "18">After consulting EuroWordNet the semantic patterns formed are: Subj|V: Human,Occupation|Communication V|DObj: Communication|Agentive,Mental V|IObj: Communication|Group,HumanThese patterns will be stored in their corre sponding files in order to be consulted later by the NLP task.</S>
			<S sid ="73" ssid = "19">This process is completely automatic and the error rate in the pattern extraction come from the aforementioned errors in the WSD and parsing phases.</S>
			<S sid ="74" ssid = "20">This strategy defined just as it has been done is, in principle, a little bit naive.</S>
			<S sid ="75" ssid = "21">Obviously, thisis the single basis for the approach, but depend ing on the application, it can be combined withmore sophisticated methods to improve its effec tiveness.</S>
			<S sid ="76" ssid = "22">In this way, it is possible to make more elaborated combinations of ontological concepts to form new branches in the ontology defined by EuroWordNet.</S>
	</SECTION>
	<SECTION title="Applying the method to anaphora. " number = "5">
			<S sid ="77" ssid = "1">resolutionSince the aforementioned semantic patterns re veal the semantic behaviour of the main textual elements, this Natural Language learning processcan be applied to any task that involves text un derstanding.</S>
			<S sid ="78" ssid = "2">One possible application in this way could be the anaphora resolution problem, one of the mostactive research areas in Natural Language Pro cessing.The comprehension of anaphora is an impor tant process in any NLP system, and it is among the toughest problems to solve in Computational Linguistics and NLP.</S>
			<S sid ="79" ssid = "3">According to Hirst (Hirst, 1981): “Anaphora, in discourse, is a device for making an abbreviated reference (containing fewer bits of disambiguating information, rather than being lexically or phonetically shorter) to some entity (or entities) in the expectation thatthe receiver of the discourse will be able to dis abbreviate the reference and, thereby, determine the identity of the entity.” (2) The minister gave explanations to the Government . The reference to an entity is generally called an anaphor (e.g. a pronoun), and the entity to which the anaphor refers is its referent or antecedent.</S>
			<S sid ="80" ssid = "4">For instance, in the sentence “John ate an apple.</S>
			<S sid ="81" ssid = "5">He was hungry”, the pronoun he is the anaphor and it refers to the antecedent John.Traditionally, some of the most relevant ap proaches to solve anaphora have been those called poor-knowledge approaches.</S>
			<S sid ="82" ssid = "6">They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.</S>
			<S sid ="83" ssid = "7">These proposals have report high success rates for English (89.7%) (Mitkov, 1998) and for Spanish (83%) (Ferr«andez et al., 1999).Taking this basis, it is possible to improve the re sults of a resolution method adding other sources such us semantic, pragmatic, world-knowledge or indeed statistical information.We have explored the use of semantic information extracted from an ontology and its application to the anaphora resolution proccess.</S>
			<S sid ="84" ssid = "8">This ad ditional source has give good results on restrictedtexts (Azzam et al., 1998).</S>
			<S sid ="85" ssid = "9">Nevertheless, its application on unrestricted texts has not been so sat isfactory, mainly due to the lack of adequate andavailable lexical resources.</S>
			<S sid ="86" ssid = "10">Due to this, we con sider that the pattern learning can complement the semantic source in order to establish additional criteria in the antecedent selection.</S>
			<S sid ="87" ssid = "11">In addition, we believe that an adequate selection of patternscan improve the success rate in anaphora resolu tion on unrestricted texts.</S>
			<S sid ="88" ssid = "12">Each pattern contributes a compatibility feature between two syntactic elements.</S>
			<S sid ="89" ssid = "13">The whole setof patterns is a knowledge tool that can be con sulted in order to deÞne the compatibility betweena pronoun and a candidate according to their syntactic role (subject, direct object and indirect ob ject) and their relation with the verb.</S>
			<S sid ="90" ssid = "14">So, looking up the concepts associated to the antecedents ofthe pronoun and the verb, and using the syntac tic relation between the pronoun and its verb, thesemantic patterns can provide a compatibility de gree to help the selection of the antecedent.</S>
			<S sid ="91" ssid = "15">A method oriented to anaphora resolution that usesthese kinds of patterns extracted from two on tologies is detailed in (SaizNoeda and Palomar, 2000).The beneÞt of this approach is shown in a clas sical example shown in (3).</S>
			<S sid ="92" ssid = "16">(3) [The monkey] climbed [the tree] to get [a coconut] when [the sun] was rising.</S>
			<S sid ="93" ssid = "17">It was ripe.In this example, there are four possible antecedents of the pronoun ’it’.</S>
			<S sid ="94" ssid = "18">Basing the reso lution only in morpho-syntactic information, it isnot possible to solve it correctly.</S>
			<S sid ="95" ssid = "19">None of the candidates would be rejected regarding to their mor phological features (all of them are masculine andsingular).</S>
			<S sid ="96" ssid = "20">The classical approaches would determine that ’the monkey’, for having the same sub ject role as the pronoun, or ’the sun’, for being the closest to the pronoun, could be the correctantecedent.</S>
			<S sid ="97" ssid = "21">Nevertheless, it is clear that the correct one in this case is ’the coconut’.</S>
			<S sid ="98" ssid = "22">Only a se mantic pattern applied to this method could give additional information to solve it correctly.</S>
			<S sid ="99" ssid = "23">If we would extract ontological concepts for all the candidates, we would be able to compare thecompatibility degree with the pronoun.</S>
			<S sid ="100" ssid = "24">One pos sible output could be the one in next table: Subject concept verb monkey animal be ripe tree plant be ripe coconut fruit be ripe sun star be ripe Examining this table it is easy to notice that, when applying this additional information, thesuggestion of the system would be the correct an tecedent, mainly based on a good previous pattern learning.</S>
			<S sid ="101" ssid = "25">This pronoun resolution system with additional information provided by the semantic patterns has been evaluated on a corpus formed by a set oftexts containing news regarding the common top ics in a newspaper (national, international, sports, society, economy, ...</S>
			<S sid ="102" ssid = "26">Results obtained in the preliminary evaluation of this pronoun resolution reveal a success rate of 79.3% anaphors correctlysolved.</S>
			<S sid ="103" ssid = "27">Although it has not been mentioned be fore, it is very important to have in mind that thismethod provides a fully automatic anaphora res olution process.</S>
			<S sid ="104" ssid = "28">Methods previously mentioned apply the resolution process over supervised steps to achieve such high rates.</S>
			<S sid ="105" ssid = "29">When the process is automated, the success rate decrease dramatically up to less than 55% (Mitkov, 2001).</S>
	</SECTION>
	<SECTION title="Conclusions and outstanding work. " number = "6">
			<S sid ="106" ssid = "1">In this paper we have presented a semantic pat tern learning system driven by a WSD methodbased on Maximum Entropy models.</S>
			<S sid ="107" ssid = "2">These se mantic patterns have been applied to the anaphora resolution through the construction of ontologicalpatterns.</S>
			<S sid ="108" ssid = "3">The adding of this pattern learning im prove, as it can be seen, the anaphora resolutionprocess.</S>
			<S sid ="109" ssid = "4">We have pointed out the main advan tages of this approach comparing it with other.The WSD method is based on conditional Maximum Entropy probability models.</S>
			<S sid ="110" ssid = "5">It is a super vised learning method that uses a semantically annotated corpus for training.</S>
			<S sid ="111" ssid = "6">ME models are used in order to estimate functions that performsa sense classiÞcation of nouns, verbs and adjec tives.</S>
			<S sid ="112" ssid = "7">The learning phase has been made withsimple features with no deep linguistic knowledge.</S>
			<S sid ="113" ssid = "8">Preliminary results indicate that the accu racy of the model is comparable to other learning methods.</S>
			<S sid ="114" ssid = "9">The main problem in the addition of this kind of knowledge is the lack of appropriate resources to deal with these tasks.</S>
			<S sid ="115" ssid = "10">In our research work weare trying to apply these techniques both in En glish and Spanish.</S>
			<S sid ="116" ssid = "11">The WSD method have been mainly developed in English, but one of our maingoals is the design of a complete anaphora reso lution system for Spanish.</S>
			<S sid ="117" ssid = "12">In this way, the main problem is the short available resources regarding to semantically tagged corpora in Spanish (unlikein English).</S>
			<S sid ="118" ssid = "13">This lack affects the correct devel opment of tasks belonging to the research lineshown in this paper, such us the pattern learn ing and the anaphora resolution.</S>
			<S sid ="119" ssid = "14">Nevertheless, this shortage opens the door to new research linesthat join English resources and multilingual techniques for the generation of patterns in other lan guages from the learned English patterns.</S>
	</SECTION>
</PAPER>
