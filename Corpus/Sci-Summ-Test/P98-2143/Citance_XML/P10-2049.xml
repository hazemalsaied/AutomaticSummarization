<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Current work on automatic opinion mining has ignored opinion targets expressed by anaphorical pronouns, thereby missing a significant number of opinion targets.</S>
		<S sid ="2" ssid = "2">In this paper we empirically evaluate whether using an off-the-shelf anaphora resolution algorithm can improve the performance of a baseline opinion mining system.</S>
		<S sid ="3" ssid = "3">We present an analysis based on two different anaphora resolution systems.</S>
		<S sid ="4" ssid = "4">Our experiments on a movie review corpus demonstrate, that an unsupervised anaphora resolution algorithm significantly improves the opinion target extraction.</S>
		<S sid ="5" ssid = "5">We furthermore suggest domain and task specific extensions to an off-the-shelf algorithm which in turn yield significant improvements.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Over the last years the task of opinion mining (OM) has been the topic of many publications.</S>
			<S sid ="7" ssid = "7">It has been approached with different goals in mind: Some research strived to perform subjectivity analysis at the document or sentence level, without focusing on what the individual opinions uttered in the document are about.</S>
			<S sid ="8" ssid = "8">Other approaches focused on extracting individual opinion words or phrases and what they are about.</S>
			<S sid ="9" ssid = "9">This aboutness has been referred to as the opinion target or opinion topic in the literature from the field.</S>
			<S sid ="10" ssid = "10">In this work our goal is to extract opinion target - opinion word pairs from sentences from movie reviews.</S>
			<S sid ="11" ssid = "11">A challenge which is frequently encountered in text mining tasks at this level of granularity is, that entities are being referred to by anaphora.</S>
			<S sid ="12" ssid = "12">In the task of OM, it can therefore also be necessary to analyze more than the content of one individual sentence when extracting opinion targets.</S>
			<S sid ="13" ssid = "13">Consider this example sentence: “Simply put, it’s unfathomable that this movie cracks the Top 250.</S>
			<S sid ="14" ssid = "14">It is absolutely awful.”.</S>
			<S sid ="15" ssid = "15">If one wants to extract what the opinion in the second sentence is about, an algorithm which resolves the anaphoric reference to the opinion target is required.</S>
			<S sid ="16" ssid = "16">The extraction of such anaphoric opinion targets has been noted as an open issue multiple times in the OM context (Zhuang et al., 2006; Hu and Liu, 2004; Nasukawa and Yi, 2003).</S>
			<S sid ="17" ssid = "17">It is not a marginal phenomenon, since Kessler and Nicolov (2009) report that in their data, 14% of the opinion targets are pronouns.</S>
			<S sid ="18" ssid = "18">However, the task of resolving anaphora to mine opinion targets has not been addressed and evaluated yet to the best of our knowledge.</S>
			<S sid ="19" ssid = "19">In this work, we investigate whether anaphora resolution (AR) can be successfully integrated into an OM algorithm and whether we can achieve an improvement regarding the OM in doing so.</S>
			<S sid ="20" ssid = "20">This paper is structured as follows: Section 2 discusses the related work on opinion target identification and OM on movie reviews.</S>
			<S sid ="21" ssid = "21">Section 3 outlines the OM algorithm we employed by us, while in Section 4 we discuss two different algorithms for AR which we experiment with.</S>
			<S sid ="22" ssid = "22">Finally, in Section 5 we present our experimental work including error analysis and discussion, and we conclude in Section 6.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="23" ssid = "1">We split the description of the related work in two parts: In Section 2.1 we discuss the related work on OM with a focus on approaches for opinion target identification.</S>
			<S sid ="24" ssid = "2">In Section 2.2 we elaborate on findings from related OM research which also worked with movie reviews as this is our target domain in the present paper.</S>
			<S sid ="25" ssid = "3">2.1 Opinion Target Identification.</S>
			<S sid ="26" ssid = "4">The extraction of opinions and especially opinion targets has been performed with quite diverse 263 Proceedings of the ACL 2010 Conference Short Papers, pages 263–268, Uppsala, Sweden, 1116 July 2010.</S>
			<S sid ="27" ssid = "5">Qc 2010 Association for Computational Linguistics approaches.</S>
			<S sid ="28" ssid = "6">Initial approaches combined statistical information and basic linguistic features such as part-of-speech tags.</S>
			<S sid ="29" ssid = "7">The goal was to identify the opinion targets, here in form of products and their attributes, without a pre-built knowledge base which models the domain.</S>
			<S sid ="30" ssid = "8">For the target candidate identification, simple part-of-speech patterns were employed.</S>
			<S sid ="31" ssid = "9">The relevance ranking and extraction was then performed with different statistical measures: Pointwise Mutual Information (Popescu and Etzioni, 2005), the Likelihood Ratio Test (Yi et al., 2003) and Association Mining (Hu and Liu, 2004).</S>
			<S sid ="32" ssid = "10">A more linguistically motivated approach was taken by Kim and Hovy (2006) through identifying opinion holders and targets with semantic role labeling.</S>
			<S sid ="33" ssid = "11">This approach was promising, since their goal was to extract opinions from professionally edited content i.e. newswire.</S>
			<S sid ="34" ssid = "12">Zhuang et al.</S>
			<S sid ="35" ssid = "13">(2006) present an algorithm for the extraction of opinion target - opinion word pairs.</S>
			<S sid ="36" ssid = "14">The opinion word and target candidates are identified in the annotated corpus and their extraction is then performed by applying possible paths connecting them in a dependency graph.</S>
			<S sid ="37" ssid = "15">These paths are combined with part-of-speech information and also learned from the annotated corpus.</S>
			<S sid ="38" ssid = "16">To the best of our knowledge, there is currently only one system which integrates coreference information in OM.</S>
			<S sid ="39" ssid = "17">The algorithm by Stoyanov and Cardie (2008) identifies coreferring targets in newspaper articles.</S>
			<S sid ="40" ssid = "18">A candidate selection or extraction step for the opinion targets is not required, since they rely on manually annotated targets and focus solely on the coreference resolution.</S>
			<S sid ="41" ssid = "19">However they do not resolve pronominal anaphora in order to achieve that.</S>
			<S sid ="42" ssid = "20">2.2 Opinion Mining on Movie Reviews.</S>
			<S sid ="43" ssid = "21">There is a huge body of work on OM in movie reviews which was sparked by the dataset from Pang and Lee (2005).</S>
			<S sid ="44" ssid = "22">This dataset consists of sentences which are annotated as expressing positive or negative opinions.</S>
			<S sid ="45" ssid = "23">An interesting insight was gained from the document level sentiment analysis on movie reviews in comparison to documents from other domains: Turney (2002) observes that the movie reviews are hardest to classify since the review authors tend to give information about the storyline of the movie which often contain charac ions of the reviewers regarding the movie.</S>
			<S sid ="46" ssid = "24">Zhuang et al.</S>
			<S sid ="47" ssid = "25">(2006) also observe that movie reviews are different from e.g. customer reviews on Amazon.com.</S>
			<S sid ="48" ssid = "26">This is reflected in their experiments, in which their system outperforms the system by Hu and Liu (2004) which attributes an opinion target to the opinion word which is closest regarding word distance in a sentence.</S>
			<S sid ="49" ssid = "27">The sentences in the movie reviews tend to be more complex, which can also be explained by their origin.</S>
			<S sid ="50" ssid = "28">The reviews were taken from the Internet Movie Database1, on which the users are given a set of guidelines on how to write a review.</S>
			<S sid ="51" ssid = "29">Due to these insights, we are confident that the overall textual quality of the movie reviews is high enough for linguistically more advanced technologies such as parsing or AR to be successfully applied.</S>
	</SECTION>
	<SECTION title="Opinion Target Identification. " number = "3">
			<S sid ="52" ssid = "1">3.1 Dataset.</S>
			<S sid ="53" ssid = "2">Currently the only freely available dataset annotated with opinions including annotated anaphoric opinion targets is a corpus of movie reviews by Zhuang et al.</S>
			<S sid ="54" ssid = "3">(2006).</S>
			<S sid ="55" ssid = "4">Kessler and Nicolov (2009) describe a collection of product reviews in which anaphoric opinion targets are also annotated, but it is not available to the public (yet).</S>
			<S sid ="56" ssid = "5">Zhuang et al.</S>
			<S sid ="57" ssid = "6">(2006) used a subset of the dataset they published (1829 documents), namely 1100 documents, however they do not state which documents comprise this subset used in their evaluation.</S>
			<S sid ="58" ssid = "7">In our experiments, we therefore use the complete dataset available, detailed in Table 1.</S>
			<S sid ="59" ssid = "8">As shown, roughly 9.5% of the opinion targets are referred to by pronouns.</S>
			<S sid ="60" ssid = "9">Table 2 outlines detailed statistics on which pronouns occur as opinion targets.</S>
			<S sid ="61" ssid = "10">Table 1: Dataset Statistics # Documents # Sentences # Tokens 1829 24918 273715 # Target + Opinion Pairs # Targets which are Pronouns # Pronouns 5298 504 &gt; 11000 3.2 Baseline Opinion Mining.</S>
			<S sid ="62" ssid = "11">We reimplemented the algorithm presented by Zhuang et al.</S>
			<S sid ="63" ssid = "12">(2006) as the baseline for our terizations, such as “bad guy” or “violent scene”.</S>
			<S sid ="64" ssid = "13">These statements however do not reflect any opin 1 http://www.imdb.com (IMDB) Table 2: Pronouns as Opinion Targets it 274 thi s 77 h e 58 h is 26 him 15 she 22 he r 10 th ey 22 experiments.</S>
			<S sid ="65" ssid = "14">Their approach is a supervised one.</S>
			<S sid ="66" ssid = "15">The annotated dataset is split in five folds, of which four are used as the training data.</S>
			<S sid ="67" ssid = "16">In the first step, opinion target and opinion word candidates are extracted from the training data.</S>
			<S sid ="68" ssid = "17">Frequency counts of the annotated opinion targets and opinion words are extracted from four training folds.</S>
			<S sid ="69" ssid = "18">The most frequently occurring opinion targets and opinion words are selected as candidates.</S>
			<S sid ="70" ssid = "19">Then the annotated sentences are parsed and a graph containing the words of the sentence is created, which are connected by the dependency relations between them.</S>
			<S sid ="71" ssid = "20">For each opinion target - opinion word pair, the shortest path connecting them is extracted from the dependency graph.</S>
			<S sid ="72" ssid = "21">A path consists of the part-of-speech tags of the nodes and the dependency types of the edges.</S>
			<S sid ="73" ssid = "22">In order to be able to identify rarely occurring opinion targets which are not in the candidate list, they expand it by crawling the cast and crew names of the movies from the IMDB.</S>
			<S sid ="74" ssid = "23">How this crawling and extraction is done is not explained.</S>
	</SECTION>
	<SECTION title="Algorithms for Anaphora Resolution. " number = "4">
			<S sid ="75" ssid = "1">As pointed out by Charniak and Elsner (2009) there are hardly any freely available systems for AR.</S>
			<S sid ="76" ssid = "2">Although Charniak and Elsner (2009) present a machine-learning based algorithm for AR, they evaluate its performance in comparison to three non machine-learning based algorithms, since those are the only ones available.</S>
			<S sid ="77" ssid = "3">They observe that the best performing baseline algorithm (OpenNLP) is hardly documented.</S>
			<S sid ="78" ssid = "4">The algorithm with the next-to-highest results in (Char- niak and Elsner, 2009) is MARS (Mitkov, 1998) from the GuiTAR (Poesio and Kabadjov, 2004) toolkit.</S>
			<S sid ="79" ssid = "5">This algorithm is based on statistical analysis of the antecedent candidates.</S>
			<S sid ="80" ssid = "6">Another promising algorithm for AR employs a rule based approach for antecedent identification.</S>
			<S sid ="81" ssid = "7">The Cog- NIAC algorithm (Baldwin, 1997) was designed for high-precision AR.</S>
			<S sid ="82" ssid = "8">This approach seems like an adequate strategy for our OM task, since in the dataset used in our experiments only a small fraction of the total number of pronouns are ac tual opinion targets (see Table 1).</S>
			<S sid ="83" ssid = "9">We extended the CogNIAC implementation to also resolve “it” and “this” as anaphora candidates, since off-the-shelf it only resolves personal pronouns.</S>
			<S sid ="84" ssid = "10">We will refer to this extension with [id].</S>
			<S sid ="85" ssid = "11">Both algorithms follow the common approach that noun phrases are antecedent candidates for the anaphora.</S>
			<S sid ="86" ssid = "12">In our experiments we employed both the MARS and the CogNIAC algorithm, for which we created three extensions which are detailed in the following.</S>
			<S sid ="87" ssid = "13">4.1 Extensions of CogNIAC.</S>
			<S sid ="88" ssid = "14">We identified a few typical sources of errors in a preliminary error analysis.</S>
			<S sid ="89" ssid = "15">We therefore suggest three extensions to the algorithm which are on the one hand possible in the OM setting and on the other hand represent special features of the target discourse type: [1.] We observed that the Stanford Named Entity Recognizer (Finkel et al., 2005) is superior to the Person detection of the (MUC6 trained) CogNIAC implementation.</S>
			<S sid ="90" ssid = "16">We therefore filter out Person antecedent candidates which the Stanford NER detects for the impersonal and demonstrative pronouns and Location &amp; Organization candidates for the personal pronouns.</S>
			<S sid ="91" ssid = "17">This way the input to the AR is optimized.</S>
			<S sid ="92" ssid = "18">[2.] The second extension exploits the fact that reviews from the IMDB exhibit certain contextual properties.</S>
			<S sid ="93" ssid = "19">They are gathered and to be presented in the context of one particular entity (=movie).</S>
			<S sid ="94" ssid = "20">The context or topic under which it occurs is therefore typically clear to the reader and is therefore not explicitly introduced in the discourse.</S>
			<S sid ="95" ssid = "21">This is equivalent to the situational context we often refer to in dialogue.</S>
			<S sid ="96" ssid = "22">In the reviews, the authors often refer to the movie or film as a whole by a pronoun.</S>
			<S sid ="97" ssid = "23">We exploit this by an additional rule which resolves an impersonal or demonstrative pronoun to “movie” or “film” if there is no other (matching) antecedent candidate in the previous two sentences.</S>
			<S sid ="98" ssid = "24">[3.] The rules by which CogNIAC resolves anaphora were designed so that anaphora which have ambiguous antecedents are left unresolved.</S>
			<S sid ="99" ssid = "25">This strategy should lead to a high precision AR, but at the same time it can have a negative impact on the recall.</S>
			<S sid ="100" ssid = "26">In the OM context, it happens quite frequently that the authors comment on the entity they want to criticize in a series of arguments.</S>
			<S sid ="101" ssid = "27">In such argument chains, we try to solve cases of antecedent ambiguity by analyzing the opinions: If there are ambiguous antecedent candidates for a pronoun, we check whether there is an opinion uttered in the previous sentence.</S>
			<S sid ="102" ssid = "28">If this is the case and if the opinion target matches the pronoun regarding gender and number, we resolve the pronoun to the antecedent which was the previous opinion target.</S>
			<S sid ="103" ssid = "29">In the results of our experiments in Section 5, we will refer to the configurations using these extensions with the numbers attributed to them above.</S>
	</SECTION>
	<SECTION title="Experimental Work. " number = "5">
			<S sid ="104" ssid = "1">To integrate AR in the OM algorithm, we add the antecedents of the pronouns annotated as opinion targets to the target candidate list.</S>
			<S sid ="105" ssid = "2">Then we extract the dependency paths connecting pronouns and opinion words and add them to the list of valid paths.</S>
			<S sid ="106" ssid = "3">When we run the algorithm, we extract anaphora which were resolved, if they occur with a valid dependency path to an opinion word.</S>
			<S sid ="107" ssid = "4">In such a case, the anaphor is substituted for its antecedent and thus extracted as part of an opinion target - opinion word pair.</S>
			<S sid ="108" ssid = "5">To reproduce the system by Zhuang et al.</S>
			<S sid ="109" ssid = "6">(2006), we substitute the cast and crew list employed by them (see Section 3.2), with a NER component (Finkel et al., 2005).</S>
			<S sid ="110" ssid = "7">One aspect regarding the extraction of opinion target - opinion word pairs remains open in Zhuang et al.</S>
			<S sid ="111" ssid = "8">(2006): The dependency paths only identify connections between pairs of single words.</S>
			<S sid ="112" ssid = "9">However, almost 50% of the opinion target candidates are multiword expressions.</S>
			<S sid ="113" ssid = "10">Zhuang et al.</S>
			<S sid ="114" ssid = "11">(2006) do not explain how they extract multiword opinion targets with the dependency paths.</S>
			<S sid ="115" ssid = "12">In our experiments, we require a dependency path to be found to each word of a multiword target candidate for it to be extracted.</S>
			<S sid ="116" ssid = "13">Furthermore, Zhuang et al.</S>
			<S sid ="117" ssid = "14">(2006) do not state whether in their evaluation annotated multiword targets are treated as a single unit which needs to be extracted, or whether a partial matching is employed in such cases.</S>
			<S sid ="118" ssid = "15">We require all individual words of a multiword expression to be extracted by the algorithm.</S>
			<S sid ="119" ssid = "16">As mentioned above, the dependency path based approach will only identify connections between pairs of single words.</S>
			<S sid ="120" ssid = "17">We therefore employ a merging step, in which we combine adjacent opinion targets to a multiword expression.</S>
			<S sid ="121" ssid = "18">We have compiled two result sets: Table 3 shows the results of the overall OM in a five-fold cross-validation.</S>
			<S sid ="122" ssid = "19">Table 4 gives a detailed overview of the AR for opinion target identification summed up over all folds.</S>
			<S sid ="123" ssid = "20">In Table 4, a true positive refers to an extracted pronoun which was annotated as an opinion target and is resolved to the correct antecedent.</S>
			<S sid ="124" ssid = "21">A false positive subsumes two error classes: A pronoun which was not annotated as an opinion target but extracted as such, or a pronoun which is resolved to an incorrect antecedent.</S>
			<S sid ="125" ssid = "22">As shown in Table 3, the recall of our reimplementation is slightly higher than the recall reported in Zhuang et al.</S>
			<S sid ="126" ssid = "23">(2006).</S>
			<S sid ="127" ssid = "24">However, our precision and thus f-measure are lower.</S>
			<S sid ="128" ssid = "25">This can be attributed to the different document sets used in our experiments (see Section 3.1), or our substitution of the list of peoples’ names with the NER component, or differences regarding the evaluation strategy as mentioned above.</S>
			<S sid ="129" ssid = "26">We observe that the MARS algorithm yields an improvement regarding recall compared to the baseline system.</S>
			<S sid ="130" ssid = "27">However, it also extracts a high number of false positives for both the personal and impersonal / demonstrative pronouns.</S>
			<S sid ="131" ssid = "28">This is due to the fact that the MARS algorithm is designed for robustness and always resolves a pronoun to an antecedent.</S>
			<S sid ="132" ssid = "29">CogNIAC in its off-the-shelf configuration already yields significant improvements over the baseline regarding f-measure2.</S>
			<S sid ="133" ssid = "30">Our CogNIAC extension [id] improves recall slightly in comparison to the off-the-shelf system.</S>
			<S sid ="134" ssid = "31">As shown in Table 4, the algorithm extracts impersonal and demonstrative pronouns with lower precision than personal pronouns.</S>
			<S sid ="135" ssid = "32">Our error analysis shows that this is mostly due to the Person / Location / Organization classification of the CogNIAC implementation.</S>
			<S sid ="136" ssid = "33">The names of actors and movies are thus often misclassified.</S>
			<S sid ="137" ssid = "34">Extension [1] mitigates this problem, since it increases precision (Table 3 row 6), while not affecting recall.</S>
			<S sid ="138" ssid = "35">The overall improvement of our extensions [id] + [1] is however not statistically significant in comparison to off-the-shelf CogNIAC.</S>
			<S sid ="139" ssid = "36">Our extensions [2] and [3] in combination with [id] each increase recall at the expense of precision.</S>
			<S sid ="140" ssid = "37">The improvement in f-measure of CogNIAC [id] + [3] over the off-the-shelf system is statistically significant.</S>
			<S sid ="141" ssid = "38">The best overall results regarding f-measure are reached if we combine all our extensions of the CogNIAC algorithm.</S>
			<S sid ="142" ssid = "39">The results of this configuration show that the positive effects of extensions [2] and [3] are complemen 2 Significance of improvements was tested using a paired two-tailed t-test and p ≤ 0.05 (∗) and p ≤ 0.01 (∗∗) Table 3: Op.</S>
			<S sid ="143" ssid = "40">Target - Op.</S>
			<S sid ="144" ssid = "41">Word Pair Extraction C o n f i g u r a t i o n Re ca.</S>
			<S sid ="145" ssid = "42">Pr ec.F Me as.</S>
			<S sid ="146" ssid = "43">R e s u l t s i n Z h u a n g e t a l . 0.</S>
			<S sid ="147" ssid = "44">54 8 0.6 54 0.</S>
			<S sid ="148" ssid = "45">59 6 O ur R ei m pl e m e nt at io n M A R S off the s h el f C o g N IA C off the s h el f C o g N IA C +[ id ] C o g N IA C +[ id ]+ [1 ] C o g N IA C +[ id ]+ [2 ] C o g N IA C +[ id ]+ [3 ] C o g N IA C +[ id ]+ [1 ]+ [2 ]+ [3 ] 0.</S>
			<S sid ="149" ssid = "46">55 4 0.</S>
			<S sid ="150" ssid = "47">59 5 0.</S>
			<S sid ="151" ssid = "48">58 6 0.</S>
			<S sid ="152" ssid = "49">59 4 0.</S>
			<S sid ="153" ssid = "50">59 4 0.</S>
			<S sid ="154" ssid = "51">60 3 0.</S>
			<S sid ="155" ssid = "52">61 3 0.</S>
			<S sid ="156" ssid = "53">61 4 0.5 23 0.4 67 0.5 34 0.5 16 0.5 33 0.5 01 0.5 21 0.5 31 0.</S>
			<S sid ="157" ssid = "54">53 8 0.</S>
			<S sid ="158" ssid = "55">52 3 0.</S>
			<S sid ="159" ssid = "56">55 9∗ ∗ 0.</S>
			<S sid ="160" ssid = "57">55 2 0.</S>
			<S sid ="161" ssid = "58">56 1 0.</S>
			<S sid ="162" ssid = "59">54 7 0.</S>
			<S sid ="163" ssid = "60">56 3∗ 0.</S>
			<S sid ="164" ssid = "61">56 9∗ Table 4: Results of AR for Opinion Targets 1 personal, impersonal &amp; demonstrative pronouns 2 true positives, false positives tary regarding the extraction of impersonal and demonstrative pronouns.</S>
			<S sid ="165" ssid = "62">This configuration yields statistically significant improvements regarding f- measure over the off-the-shelf CogNIAC configuration, while also having the overall highest recall.</S>
			<S sid ="166" ssid = "63">5.1 Error Analysis.</S>
			<S sid ="167" ssid = "64">When extracting opinions from movie reviews, we observe the same challenge as Turney (2002): The users often characterize events in the storyline or roles the characters play.</S>
			<S sid ="168" ssid = "65">These characterizations contain the same words which are also used to express opinions.</S>
			<S sid ="169" ssid = "66">Hence these combinations are frequently but falsely extracted as opinion target - opinion word pairs, negatively affecting the precision.</S>
			<S sid ="170" ssid = "67">The algorithm cannot distinguish them from opinions expressing the stance of the author.</S>
			<S sid ="171" ssid = "68">Overall, the recall of the baseline is rather low.</S>
			<S sid ="172" ssid = "69">This is due to the fact that the algorithm only learns a subset of the opinion words and opinion targets annotated in the training data.</S>
			<S sid ="173" ssid = "70">Currently, it cannot discover any new opinion words and targets.</S>
			<S sid ="174" ssid = "71">This could be addressed by integrating a component which identifies new opinion targets by calculating the relevance of a word in the corpus based on statistical measures.</S>
			<S sid ="175" ssid = "72">The AR introduces new sources of errors regarding the extraction of opinion targets: Errors in gender and number identification can lead to an incorrect selection of antecedent candidates.</S>
			<S sid ="176" ssid = "73">Even if the gender and number identification is correct, the algorithm might select an incorrect antecedent if there is more than one possible candidate.</S>
			<S sid ="177" ssid = "74">A non-robust algorithm as CogNIAC might leave a pronoun which is an actual opinion target unresolved, due to the ambiguity of its antecedent candidates.</S>
			<S sid ="178" ssid = "75">The upper bound for the OM with perfect AR on top of the baseline would be recall: 0.649, precision: 0.562, f-measure: 0.602.</S>
			<S sid ="179" ssid = "76">Our best configuration reaches ∼ 50% of the improvements which are theoretically possible with perfect AR.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="180" ssid = "1">We have shown that by extending an OM algorithm with AR for opinion target extraction significant improvements can be achieved.</S>
			<S sid ="181" ssid = "2">The rule based AR algorithm CogNIAC performs well regarding the extraction of opinion targets which are personal pronouns.</S>
			<S sid ="182" ssid = "3">The algorithm does not yield high precision when resolving impersonal and demonstrative pronouns.</S>
			<S sid ="183" ssid = "4">We present a set of extensions which address this challenge and in combination yield significant improvements over the off-the-shelf configuration.</S>
			<S sid ="184" ssid = "5">A robust AR algorithm does not yield any improvements regarding f-measure in the OM task.</S>
			<S sid ="185" ssid = "6">This type of algorithm creates many false positives, which are not filtered out by the dependency paths employed in the algorithm by Zhuang et al.</S>
			<S sid ="186" ssid = "7">(2006).</S>
			<S sid ="187" ssid = "8">AR could also be employed in other OM algorithms which aim at identifying opinion targets by means of a statistical analysis.</S>
			<S sid ="188" ssid = "9">Vicedo and Ferra´ndez (2000) successfully modified the relevance ranking of terms in their documents by replacing anaphora with their antecedents.</S>
			<S sid ="189" ssid = "10">The approach can be taken for OM algorithms which select the opinion target candidates with a relevance ranking (Hu and Liu, 2004; Yi et al., 2003).</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="190" ssid = "11">The project was funded by means of the German Federal Ministry of Economy and Technology under the promotional reference “01MQ07012”.</S>
			<S sid ="191" ssid = "12">The authors take the responsibility for the contents.</S>
			<S sid ="192" ssid = "13">This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No.</S>
			<S sid ="193" ssid = "14">I/82806.</S>
	</SECTION>
</PAPER>
