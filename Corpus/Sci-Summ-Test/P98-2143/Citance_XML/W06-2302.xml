<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we will present an evaluation of current state-of-the-art algorithms for Anaphora Resolution based on a segment of Susanne corpus (itself a portion of Brown Corpus), a much more comparable text type to what is usually required at an international level for s u c h a p p l i c a t i o n d o m a i n s a s Question/Answering, Information Extraction, Text Understanding, Language Learning.</S>
		<S sid ="2" ssid = "2">The portion of text chosen has an adequate size which lends itself to significant statistical measurements: it is portion A, counting 35,000 tokens and some 1000 third person pronominal expressions.</S>
		<S sid ="3" ssid = "3">The algorithms will then be compared to our system, GETARUNS, which incorporates an AR algorithm at the end of a pipeline of interconnected modules that instantiate standard architectures for NLP.</S>
		<S sid ="4" ssid = "4">F- measure values reached by our system are significantly higher (75%) than the other ones.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">The problem of anaphora resolution (hence AR) looms more and more as a prominent one in unrestricted text processing due to the need to recover semantically consistent information in most current NLP applications.</S>
			<S sid ="6" ssid = "6">This problem does not lend itself easily to a statistical approach so that rule-based approaches seem the only viable solution.</S>
			<S sid ="7" ssid = "7">We present a new evaluation of three state-of-the-art algorithms for anaphora resolution – GuiTAR, JavaRAP, MARS – on the basis of a portion of Susan Corpus (derived from Brown Corpus) a much richer testbed than the ones previously used for evaluation, and in any case a much more comparable source with such texts as newspaper articles and stories.</S>
			<S sid ="8" ssid = "8">Texts used previously ranged from scientific manuals to descriptive scientific texts and were generally poor on pronouns and rich on nominal descriptions.</S>
			<S sid ="9" ssid = "9">Two of the algorithms – GuiTAR and JavaRAP - use Charniak’s parser output, which contributes to the homogeneity of the type of knowledge passed to the resolution procedure.</S>
			<S sid ="10" ssid = "10">MARS, on the contrary, uses a more sophisticated input, the one provided by Connexor FDG-parser.</S>
			<S sid ="11" ssid = "11">The algorithms will then be compared to our system, GETARUNS, which incorporated an AR algorithm at the end of a pipeline of interconnected modules that instantiate standard architectures for NLP.</S>
			<S sid ="12" ssid = "12">The version of the algorithm presented here is a newly elaborated one, and is devoted to unrestricted text processing.</S>
			<S sid ="13" ssid = "13">It is an upgraded version from the one discussed in Delmonte (1999;2002a;2002b) and tries to incorporate as much as possible of the more sophisticated version implemented in the complete GETARUN (see Delmonte 1990;1991;1992;1994; 2003;2004).</S>
			<S sid ="14" ssid = "14">The paper is organized as follows: in section 2 below we briefly discuss architectures and criteria for AR of the three algorithms evaluated.</S>
			<S sid ="15" ssid = "15">In section 3 we present our system.</S>
			<S sid ="16" ssid = "16">Section 4 is dedicated to a compared evaluation and a general discussion.</S>
	</SECTION>
	<SECTION title="The Anaphora Resolution Algorithms. " number = "2">
			<S sid ="17" ssid = "1">We start by presenting a brief overview of three state-of-the-art algorithms for anaphora resolution – GuiTAR, JavaRAP, MARS.</S>
			<S sid ="18" ssid = "2">2.1 JavaRAP.</S>
			<S sid ="19" ssid = "3">As reported by the authors (Long Qiu, Min-Yen Kan, Tat-Seng Chua, 2004) of the JAVA implementation, head-dependent relations required by RAP are provided by looking into the structural “argument domain” for arguments and into the structural “adjunct domain” for adjuncts.</S>
			<S sid ="20" ssid = "4">Domain information is important to establish disjunction relations, i.e. to tell whether a third person pronoun can look for antecedents within a certain structural domain or not.</S>
			<S sid ="21" ssid = "5">According to Binding Principles, Anaphors (i.e. reciprocal and reflexive pronouns), must be bound – search for their binder-antecedent – in their same binding domain – roughly corresponding to the notion of structural “argument/adjunct domain”.</S>
			<S sid ="22" ssid = "6">Within the same domains, Pronouns must be free.</S>
			<S sid ="23" ssid = "7">Head-argument or head-adjunct relation is determined whenever two or more NPs are sibling of the same VP.</S>
			<S sid ="24" ssid = "8">Additional information is related to agreement features, which in the case of pronominal expressions are directly derived.</S>
			<S sid ="25" ssid = "9">As for nominal expressions, features are expressed in case they are either available on the verb – for SUBJect NPs– or else if they are expressed on the noun and some other tricks are performed for conjoined nouns.</S>
			<S sid ="26" ssid = "10">Gender is looked up in the list of names available on the web.</S>
			<S sid ="27" ssid = "11">This list is also used to provide the semantic feature of animacy.</S>
			<S sid ="28" ssid = "12">RAP is also used to find pleonastic pronouns, i.e. pronouns which have no referents.</S>
			<S sid ="29" ssid = "13">To detect conditions for pleonastic pronouns a list of patterns is indicated, which used both lexical and structural information.</S>
			<S sid ="30" ssid = "14">Salience weight is produced for each candidate antecedent from a set of salience factors.</S>
			<S sid ="31" ssid = "15">These factors include main Grammatical Relations, Headedness, non Adverbiality, belonging to the same sentence.</S>
			<S sid ="32" ssid = "16">The information is computed again by RAP, directly on the syntactic structure.</S>
			<S sid ="33" ssid = "17">The weight computed for each noun phrase is divided by two in case the distance from the current sentence increases.</S>
			<S sid ="34" ssid = "18">Only NPs contained within a distance of three sentences preceding the anaphor are considered by JavaRAP.</S>
			<S sid ="35" ssid = "19">2.2 GuiTAR.</S>
			<S sid ="36" ssid = "20">The authors (Poesio, M. and Mijail A. Kabadjov 2004) present their algorithm as an attempt at providing a domain independent anaphora resolution module, “that developers of NLE applications can pick off the shelf in the way of tokenizers, POS taggers, parsers, or Named Entity classifiers”.</S>
			<S sid ="37" ssid = "21">For these reasons, GuiTAR has been designed to be as independent as possible from other modules, and to be as modular as possible, thus “allowing for the possibility of replacing specific components (e.g., the pronoun resolution component)”.</S>
			<S sid ="38" ssid = "22">The authors have also made an attempt at specifying what they call the Minimal Anaphoric Syntax (MAS) and have devised a markup language based on GNOME markup scheme.</S>
			<S sid ="39" ssid = "23">In MAS, Nominal Expressions constitute the main processing units, and are identified with the tag NE &lt;ne&gt;, which have a CAT attribute, specifying the NP type: the-np, pronoun etc., as well as Person, Number and Gender attributes for agreement features.</S>
			<S sid ="40" ssid = "24">Also the internal structure of the NP is marked with Mod and NPHead tags.</S>
			<S sid ="41" ssid = "25">The pre-processing phase uses a syntactic guesser which is a chunker of NPs based on heuristics.</S>
			<S sid ="42" ssid = "26">All NEs add up to a discourse model – or better History List - which is then used as the basic domain where Discourse Segments are contained.</S>
			<S sid ="43" ssid = "27">Each Discourse Segment in turn may be constituted by one or more Utterances.</S>
			<S sid ="44" ssid = "28">Each Utterance in turn contains a list of forward looking centers Cfs.</S>
			<S sid ="45" ssid = "29">The Anaphora Resolution algorithm implemented is the one proposed by MARS which will be commented below.</S>
			<S sid ="46" ssid = "30">The authors also implemented a simple algorithm for resolving Definite Descriptions on the basis of the History List by a same head matching approach.</S>
			<S sid ="47" ssid = "31">2.3 MARS.</S>
			<S sid ="48" ssid = "32">The approach is presented as a knowledge poor anaphora resolution algorithm (Mitkov R. [1995;1998]), which makes use of POS and NP chunking, it tries to individuate pleonastic “it” occurrences, and assigns animacy.</S>
			<S sid ="49" ssid = "33">The weighting algorithm seems to contain the most original approach.</S>
			<S sid ="50" ssid = "34">It is organized with a filtering approach by a series of indicators that are used to boost or reduce the score for antecedenthood to a given NP.</S>
			<S sid ="51" ssid = "35">The indicators are the following ones: FNP (First NP); INDEF (Indefinite NP); IV (Indicating Verbs); REI (Lexical Reiteration); SH (Section Heading Preference); CM (Collocation Match); PNP (Prepositional Noun Phrases); IR (Immediate Reference); SI (Sequential Instructions); RD (Referential Distance); TP (Term Preference), As the author comments, antecedent indicators (preferences) play a decisive role in tracking down the antecedent from a set of possible candidates.</S>
			<S sid ="52" ssid = "36">Candidates are assigned a score (-1, 0, 1 or 2) for each indicator; the candidate with the highest aggregate score is proposed as the antecedent.</S>
			<S sid ="53" ssid = "37">The authors comment is that antecedent indicators have been identified empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, &quot;non- prepositional&quot; noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.</S>
			<S sid ="54" ssid = "38">However it is clear that most of the indicators have been suggested for lack of better information, in particular no syntactic constituency was available.</S>
			<S sid ="55" ssid = "39">In a more recent paper (Mitkov et al., 2003) MARS has been fully reimplemented and the indicators updated.</S>
			<S sid ="56" ssid = "40">The authors seem to acknowledge the fact that anaphora resolution is a much more difficult task than previous work had suggested, In unrestricted text analysis, the tasks involved in the anaphora resolution process contribute a lot of uncertainty and errors that may be the cause for low performance measures.</S>
			<S sid ="57" ssid = "41">The actual algorithm uses the output of Connexor’s FDG Parser, filters instances of “it” and eliminates pleonastic cases, then produces a list of potential antecedents by extracting nominal and pronominal heads from NPs preceding the pronoun.</S>
			<S sid ="58" ssid = "42">Constraints are then applied to this list in order to produce the “set of competing candidates” to be considered further, i.e. those candidates that agree in number and gender with the pronoun, and also obey syntactic constraints.</S>
			<S sid ="59" ssid = "43">They also introduced the use of Genetic Algorithms in the evaluation phase.</S>
			<S sid ="60" ssid = "44">The new version of MARS includes three new indicators which seem more general and applicable to any text, so we shall comment on them.</S>
			<S sid ="61" ssid = "45">Frequent Candidates (FC) – this is a boosting score for most frequent three NPs; Syntactic Parallelism (SP) – this is a boosting score for NPs with the same syntactic role as the pronoun, roles provided by the FDG-Parser; Boost Pronoun (BP) – pronoun candidates are given a bonus (no indication of conditions for such a bonus).</S>
			<S sid ="62" ssid = "46">The authors also reimplemented in a significant way the indicator First NPs which has been renamed, “Obliqueness (OBL) – score grammatical functions, SUBJect &gt; OBJect &gt; IndirectOBJect &gt; Undefined”.</S>
			<S sid ="63" ssid = "47">MARS has a procedure for automatically identifying pleonastic pronouns: the classification is done by means of 35 features organized into 6 types and are expressed by a mixture of lexical and grammatical heuristics.</S>
			<S sid ="64" ssid = "48">The output should be a fine-grained characterization of the phenomenon of the use of pleonastic pronouns which includes, among others, discourse anaphora, clause level anaphora and idiomatic cases.</S>
			<S sid ="65" ssid = "49">In the same paper, the authors deal with two more important topics: syntactic constraints and animacy identification.</S>
	</SECTION>
	<SECTION title="GETARUNS. " number = "3">
			<S sid ="66" ssid = "1">In a number of papers (Delmonte 1990;1991; 1992;1994; 2003;2004) and in a book (Delmonte 1992) we described our algorithms and the theoretical background which inspired it.</S>
			<S sid ="67" ssid = "2">Whereas the old version of the system had a limited vocabulary and was intended to work only in limited domains with high precision, the current version of the system has been created to cope with unrestricted text.</S>
			<S sid ="68" ssid = "3">In Delmonte (2002), we reported preliminary results obtained on a corpus of anaphorically annotated texts made available by R.Mitkov on his website.</S>
			<S sid ="69" ssid = "4">Both definite descriptions and pronominal expressions were considered, success rate was at 75% F-measure.</S>
			<S sid ="70" ssid = "5">In those case we used a very shallow and robust parser which produced only NP chunks which were then used to fire anaphoric processes.</S>
			<S sid ="71" ssid = "6">However the texts making up the corpus were technical manuals, where the scope and usage of pronominal expressions is very limited.</S>
			<S sid ="72" ssid = "7">The current algorithm for anaphora resolution works on the output of a complete deep robust parser which builds an indexed linear list of dependency structures where clause boundaries are clearly indicated; differently from Connexor, our system elaborates both grammatical relations and semantic roles information for arguments and adjuncts.</S>
			<S sid ="73" ssid = "8">Semantic roles are very important in the weighting procedures.</S>
			<S sid ="74" ssid = "9">Our system also produces implicit grammatical relations which are either controlled SUBJects of untensed clauses, arguments or adjuncts of relative clauses.</S>
			<S sid ="75" ssid = "10">As to the anaphoric resolution algorithm, it is based on the original Sidner’s (1983:Chapter 5) and Webber’s (1983:Chapter 6) intuitions on Focussing in Discourse.</S>
			<S sid ="76" ssid = "11">We find distributed, local approaches to anaphora resolution more efficient than monolithic, global ones.</S>
			<S sid ="77" ssid = "12">In particular we believe that due to the relevance of structural constraints in the treatment of locally restricted classes of pronominal expressions, it is more appropriate to activate different procedures which by dealing separately with non-locally restricted classes also afford separate evaluation procedures.</S>
			<S sid ="78" ssid = "13">There are also at least two principled reasons for the separation into two classes.</S>
			<S sid ="79" ssid = "14">The first reason is a theoretical one.</S>
			<S sid ="80" ssid = "15">Linguistic theory has long since established without any doubt the existence in most languages of the world of at least two classes: the class of pronouns which must be bound locally in a given domain and the class of pronouns which must be left free in the same domain – as a matter of fact, English also has a third class of pronominals, the so-called long-distance subject-of-consciousness bound pronouns (see Zribi-Hertz A., 1989); The second reason is empirical.</S>
			<S sid ="81" ssid = "16">Anaphora resolution is usually carried out by searching antecedents backward w.r.t. the position of the current anaphoric expression.</S>
			<S sid ="82" ssid = "17">In our approach, we proceed in a clause by clause fashion, weighting each candidate antecedent w.r.t. that domain, trying to resolve it locally.</S>
			<S sid ="83" ssid = "18">Weighting criteria are amenable on the one hand to linear precedence constraints, with scores assigned on a functional/semantic basis.</S>
			<S sid ="84" ssid = "19">On the other hand, these criteria may be overrun by a functional ranking of clauses which requires to treat main clauses differently from secondary clauses, and these two differently from complement clauses.</S>
			<S sid ="85" ssid = "20">On the contrary, global algorithms neglect altogether such requirements: they weight each referring expression w.r.t. the utterance, linear precedence is only physically evaluated, no functional correction is introduced.</S>
			<S sid ="86" ssid = "21">3.1 Referential Policies and Algorithms.</S>
			<S sid ="87" ssid = "22">There are also two general referential policy assumption that we adopt in our approach: The first one is related to pronominal expressions, the second one to referring expressions or entities to be asserted in the History List, and are expressed as follows: - no more than two pronominal expressions are allowed to refer back in the previous discourse portion; - at discourse level, referring expressions are stored in a push-down stack according to Persistence principles.</S>
			<S sid ="88" ssid = "23">Persistence principles respond to psychological principles and limit the topicality space available to user w.r.t. a given text.</S>
			<S sid ="89" ssid = "24">It has a bidimensional nature: it is determined both in relation to an overall topicality frequency value and to an utterance number proximity value.</S>
			<S sid ="90" ssid = "25">Only “persistent” referring expressions are allowed to build up the History List, where persistence is established on the basis of the frequency of topicality for each referring expression which must be higher than 1.</S>
			<S sid ="91" ssid = "26">All referring expression asserted as Topic (Secondary, Potential) only once are discarded in case they appeared at a distance measured in 5 previous utterances.</S>
			<S sid ="92" ssid = "27">Proximate referring expressions are allowed to be asserted in the History List.</S>
			<S sid ="93" ssid = "28">In particular, if Mitkov considers the paragraph as the discourse unit most suitable for coreferring and cospecifying operation at discourse level, we prefer to adopt a parameterized procedure which is definable by the user and activated automatically: it can be fired within a number that can vary from every 10 up to 50 sentences.</S>
			<S sid ="94" ssid = "29">Our procedure has the task to prune the topicality space and reduce the number of perspective topic for Main and Secondary Topic.</S>
			<S sid ="95" ssid = "30">Thus we garbage-collect all non- relevant entities.</S>
			<S sid ="96" ssid = "31">This responds to the empirically validated fact that as the distance between first and second mention of the same referring expression increases, people are obliged to repeat the same linguistic description, using a definite expression or a bare NP.</S>
			<S sid ="97" ssid = "32">Indefinites are unallowed and may only serve as first mention; they can also be used as bridging expression within opaque propositions.</S>
			<S sid ="98" ssid = "33">The first procedure is organized as follows: A. For each clause, 1.</S>
			<S sid ="99" ssid = "34">we collect all referential expressions and weight them (see B below for criteria) – this is followed by an automatic ranking; 2.</S>
			<S sid ="100" ssid = "35">then we subtract pronominal expressions; 3.</S>
			<S sid ="101" ssid = "36">at clause level, we try to bind personal and possessive pronouns obeying specific structural properties; we also bind reflexive pronouns and reciprocals if any, which must be bound obligatorily in this domain; 4.</S>
			<S sid ="102" ssid = "37">when binding a pronoun, we check for disjointness w.r.t. a previously bound pronoun if any; 5.</S>
			<S sid ="103" ssid = "38">all unbound pronouns and all remaining p e r s o n a l p r o n o u n s a r e a s s e r t e d a s “externals”, and are passed up to the higher clause levels; B. Weighting is carried out by taking into account the following linguistic properties associated to each referring expression: 1.</S>
			<S sid ="104" ssid = "39">Grammatical Function with usual hierarchy.</S>
			<S sid ="105" ssid = "40">(SUBJ &gt; ARG_MOD &gt; OBJ &gt; OBJ2 &gt; IOBJ &gt; NCMOD); 2.</S>
			<S sid ="106" ssid = "41">Semantic Roles, as they have been labelled in. FrameNet, and in our manually produced frequency lexicon of English; 3.</S>
			<S sid ="107" ssid = "42">Animacy: we use 75 semantic features derived.</S>
			<S sid ="108" ssid = "43">from WordNet, and reward Human and I n s t i t u t i o n / C o m p a n y l a b e l l e d r e f e r r i n g expressions;</S>
	</SECTION>
	<SECTION title="Functional Clause Type is further used to. " number = "4">
			<S sid ="109" ssid = "1">introduce penalties associated to those referring expressions which don’t belong to main clause.</S>
			<S sid ="110" ssid = "2">C. Then we turn at the higher level – if any -, and we proceed as in A., in addition 1.</S>
			<S sid ="111" ssid = "3">we try to bind pronouns passed up by the lower clause levels o if successful, this will activate a retract of the “ e x t e r n a l ” l a b e l a n d a l a b e l o f “antecedenthood” for the current pronoun with a given antecedent; o the best antecedent is chosen by recursively trying to match features of the pronoun with the first available antecedent previously ranked by weighting; o here again whenever a pronoun is bound we check for disjointness at utterance level.</S>
			<S sid ="112" ssid = "4">D. This is repeated until all clauses are examined and all pronouns are scrutinised and bound or left free.</S>
			<S sid ="113" ssid = "5">E. Pronouns left free – those asserted as externals – will be matched tentatively with the best candidates provided this time by a “centering-like” algorithm.</S>
			<S sid ="114" ssid = "6">Step A. is identical and is recursively repeated until all clauses are processed.</S>
			<S sid ="115" ssid = "7">Then, we move to step B. which in this case will use all referring expressions present in the utterance, rather than only those available locally.</S>
			<S sid ="116" ssid = "8">Fig.</S>
			<S sid ="117" ssid = "9">1 GETARUNS AR algorithm 3.2 Focussing Revisited.</S>
			<S sid ="118" ssid = "10">Our version of the focussing algorithm follows Sidner’s proposal (Sidner C., 1983; Grosz B., Sidner C., 1986), to use a Focus Stack, a certain Focus Algorithm with Focus movements and data structures to allow for processing simple inferential relations between different linguistic descriptions co-specifying or coreferring to a given entity.</S>
			<S sid ="119" ssid = "11">Our Focus Algorithm is organized as follows: for each utterance, we assert three “centers” that we call Main, Secondary and the first Potential Topic, which represent the best three referring expressions as they have been weighted in the candidate list used for pronominal binding; then we also keep a list of Potential Topics for the remaining best candidates.</S>
			<S sid ="120" ssid = "12">These three best candidates repositories are renovated at each new utterance, and are used b o t h t o r e s o l v e p r o n o m i n a l a n d n o m i n a l cospecification and coreference: this is done both in case of strict identity of linguistic description and of non-identity.</S>
			<S sid ="121" ssid = "13">The second case may occur either when derivational morphological properties allow the two referring expressions to be matched successfully, or when a simple hyponym/hypernym relation is entertained by two terms, one of which is contained in the list of referring expressions collected from the current sentence, and the other is among one of the entities stored in the focus list.</S>
			<S sid ="122" ssid = "14">The Main Topic may be regarded the Forward Looking Center in the centering terminology or the Current Focus.</S>
			<S sid ="123" ssid = "15">All entities are stored in the History List (HL) which is a stack containing their morphological and semantic features: this is not to be confused with a Discourse Model - what we did in the deep complete system anaphora resolution module – which is a highly semantically wrought elaboration of the current text.</S>
			<S sid ="124" ssid = "16">In the HL every new entity is assigned a semantic index which identifies it uniquely.</S>
			<S sid ="125" ssid = "17">To allow for Persistence evaluation, we also assert rhetorical properties associated to each entity, i.e. we store the information of topicality (i.e. whether it has been evaluated as Main, Secondary or Potential Topic), together with the semantic ID and the number of the current utterance.</S>
			<S sid ="126" ssid = "18">This is subsequently used to measure the degree of Persistence in the overall text of a given entity, as explained below.</S>
			<S sid ="127" ssid = "19">In order to decide which entity has to become Main, Secondary or Potential Topic we proceed as follows: - we collect all entities present in the History List with their semantic identifier and feature list and proceed to an additional weighting procedure; - nominal expressions, they are divided up into four semantic types: definite, indefinite, bare NPs, quantified NPs.</S>
			<S sid ="128" ssid = "20">Both definite and indefinite NP may be computed as new or old entity according to contextual conditions as will be discussed below and are given a rewarding score; - we enumerate for each entity its persistence in the previous text, and keep entities which have frequency higher than 1, we discard the others; - we recover entities which have been asserted in the HL in proximity to the current utterance, up to four utterances back; - we use this list to “resolve” referring expressions contained in the current utterance; - if this succeeds, we use the “resolved” entities as new Main, Secondary, and Potential Topics and assert the rest in the Potential Topics stack; - if this fails – also partially – we use the best candidates in the weighted list of referring expressions to assert the new Topics.</S>
			<S sid ="129" ssid = "21">It may be the case that both resolved and current best candidates are used, and this is by far the most common case.</S>
			<S sid ="130" ssid = "22">4.</S>
			<S sid ="131" ssid = "23">Evaluation and General Discussion.</S>
			<S sid ="132" ssid = "24">Evaluating anaphora resolution systems calls for a reformulation of the usual parameters of Precision and Recall as introduced in IR/IE field: in that case, there are two levels that are used as valuable results; a first stage where systems are measured for their capacity to retrieve/extract relevant items from the corpus/web (coverage-recall).</S>
			<S sid ="133" ssid = "25">Then a second stage follows in which systems are evaluated for their capacity to match the content of the query (accuracy-precision).</S>
			<S sid ="134" ssid = "26">In the field of IR/IE items to be matched are usually constituted by words/phrases and pattern-matching procedures are the norm.</S>
			<S sid ="135" ssid = "27">However, for AR systems this is not sufficient and NLP heavy techniques are used to get valuable results.</S>
			<S sid ="136" ssid = "28">As Mitkov also notes, this phase jeopardizes the capacity of AR systems to reach satisfactory accuracy scores simply because of its intrinsic weakness: none of the off-the-shelf parsers currently available overcomes 90% accuracy.</S>
			<S sid ="137" ssid = "29">To clarify these issues, we present here below two Tables: in the first one we report data related to the vexed question of whether pleonastic “it” should be regarded as part of the task of anaphora resolution or rather part of a separate classification task – as suggested in a number of papers by Mitkov.</S>
			<S sid ="138" ssid = "30">In the former case, they should contribute to the overall anaphora resolution evaluation metrics; in the latter case they should be compute separately as a case of classification over all occurrences of “it” in the current dataset and discarded from the overall count.</S>
			<S sid ="139" ssid = "31">Even though we don’t agree fully with Mitkov’s position, we find it useful to deal with “it” separate, due to its high inherent ambiguity.</S>
			<S sid ="140" ssid = "32">Besides, it is true that the AR task is not like any Information Retrieval task.</S>
			<S sid ="141" ssid = "33">In Table 1 below we reported figures for “it” in order to evaluate the three algorithms in relation to the classification task.</S>
			<S sid ="142" ssid = "34">Then in Table 2.</S>
			<S sid ="143" ssid = "35">we report general data where we computed the two types of accuracy reported in the literature.</S>
			<S sid ="144" ssid = "36">In Table 1 we split results for “it” into Wrong Reference vs Wrong Classification: following Mitkov, in case we only computed anaphora related cases and disregarded those cases of “it” which were wrongly classified as expletives.</S>
			<S sid ="145" ssid = "37">Expletive “it” present in the text are 189: so at first we computed coverage and accuracy with the usual formula that we report below.</S>
			<S sid ="146" ssid = "38">Then we subtracted wrongly classified cases from the number of total “it” found in one case (following Mitkov who claims that wrongly classified “it” found by the system should not count; in another case, this number is subtracted from the total number of “it” to be found in the text.</S>
			<S sid ="147" ssid = "39">Only for MARS we then computed different measures of Coverage and Accuracy.</S>
			<S sid ="148" ssid = "40">If we regard this approach worth pursuing, we come up with two Adjusted Accuracy measures which are related to the revised total numbers of anaphors by the two subtractions indicated above.</S>
			<S sid ="149" ssid = "41">We computed manually all third person pronominal expressions and came up with a figure 982 which is Table 1.</S>
			<S sid ="150" ssid = "42">Expletive “it” compared results M A R S J a v a R A P G u i T A R G E T A R U N S C o v e r a g e 1 6 3 ( 8 6 . 2 % ) 1 8 8 ( 9 9 . 5 % ) 1 8 8 ( 9 9 . 5 % ) 1 7 1 ( 9 1 % ) A c c u r a c y 1 6 3 ( 3 3 . 3 % ) 7 3 ( 3 8 . 6 % ) 7 5 ( 3 9 . 7 % ) 8 7 ( 4 6 % ) W ro ng Cl as sif ic ati on 4 4 1 6 3 4 4 = 1 1 9 1 8 9 4 4 = 1 4 5 4 9 1 8 9 4 9 = 1 4 0 6 4 1 8 9 6 4 = 1 2 5 5 3 1 8 9 5 3 = 1 3 6 W r o n g R e f e r e n c e 5 6 6 6 4 9 3 2 A c c u r a c y 2 6 3 ( 3 8 . 6 % ) A dj us te d A cc ur ac y 2 6 3 ( 5 2 . 9 % ) A dj us te d A cc ur ac y 3 6 3 ( 4 3 . 4 % ) 7 3 ( 5 2 . 1 % ) 7 5 ( 6 0 % ) 8 7 ( 6 4 % ) only confirmed by one of the three systems considered: JavaRAP.</S>
			<S sid ="151" ssid = "43">Pronouns considered are the following one, lower case and upper case included: Possessives – his, its, her, hers, their, theirs Personals – he, she, it, they, him, her, it, them (where “it” and “her” have to be disambiguated) Reflexives – himself, itself, herself, themselves There are 16 different wordforms.</S>
			<S sid ="152" ssid = "44">As can be seen from the table below, apart from JavaRAP, none of the other systems considered comes close to 100% coverage.</S>
			<S sid ="153" ssid = "45">Computing general measures for Precision and Recall we have three quantities (see also Poesio &amp; Kabadjov): • total number of anaphors present in the text; • anaphors identified by the system; • correctly resolved anaphors.</S>
			<S sid ="154" ssid = "46">Formulas related to Accuracy/Success Rate or Precision are as follows: Accuracy1 = number of successfully resolved anaphors/number of all anaphors; Accuracy2 = number of successfully resolved anaphors/number of anaphors found (attempted to be resolved).</S>
			<S sid ="155" ssid = "47">Recall - which should correspond to Coverage - we come up with formula: R= number of anaphors found /number of all anaphors to be resolved (present in the text).</S>
			<S sid ="156" ssid = "48">Finally the formula for F-measure is as follows: 2*P*R/(P+R) where P is chosen as Accuracy 2.</S>
			<S sid ="157" ssid = "49">Table 2.</S>
			<S sid ="158" ssid = "50">Overall results Coverage/Accuracy C O V E R A G E A C C U R A C Y 1 A C C U R A C Y 2 F m e a s u r e M A RS 93 6 (9 5.3 %) 40 3/9 82 (4 1.5 %) 40 3/9 03 (4 3 %) 5 9 . 2 6 % Ja va R A P 98 1 (1 00 %) 49 0/9 82 (4 9.9 %) 49 0/9 81 (5 0 %) 6 6 . 7 % G UI T A R 82 4 (8 4.8 %) 44 5/9 82 (4 5.8 %) 44 5/8 24 (5 4 %) 6 5 . 9 8 % G ET A R U N S 88 5 (9 0.1 %) 55 5/9 82 (5 6.5 %) 55 5/8 85 (6 2.7 %) 7 3 . 9 4 % In absolute terms best accuracy figures have been obtained by GETARUNS, followed by JavaRAP.</S>
			<S sid ="159" ssid = "51">So it is still thanks to the classic Recall formula that this result stands out clearly.</S>
			<S sid ="160" ssid = "52">We also produced another table which can however only be worked out for our system, which uses a distributed approach.</S>
			<S sid ="161" ssid = "53">We managed to separate pronominal expressions in relation to their contribution at the different levels of anaphora resolution considered: clause level, utterance level, discourse level.</S>
			<S sid ="162" ssid = "54">At clause level, only those pronouns which must be bound locally are checked, as is the case with reflexive pronouns, possessives, some cases of expletive ‘it’: both arguments and adjuncts may contribute the appropriate antecedent.</S>
			<S sid ="163" ssid = "55">At utterance level, in case the sentence is complex or there is more than one clause, also personal subject/object pronouns may be bound (if only preferentially so).</S>
			<S sid ="164" ssid = "56">Eventually, those pronouns which do not find an antecedent are regarded discourse level pronouns.</S>
			<S sid ="165" ssid = "57">We collapsed under CLAUSE all pronouns bound at clause and utterance level; DISCOURSE contains only sentence external pronouns.</S>
			<S sid ="166" ssid = "58">Expletives have b e e n c o m p u t e d i n a s e p a r a t e c o l u m n . Table 3.</S>
			<S sid ="167" ssid = "59">GETARUNS pronouns collapsed at structural level C L A U S E D I S C O U R S E E X P L E T I V E S T O T A L S Pr on ou ns fo un d 4 1 0 3 6 6 1 0 9 8 8 5 C o r r e c t 2 6 6 2 2 2 6 7 5 5 5 E r r o r s m a d e 1 4 4 1 4 4 4 2 3 3 0 As can be noticed easily, the highest percentage of pronouns found is at Clause level: this is not however the best performance of the system, which on the contrary performs better at discourse level.</S>
			<S sid ="168" ssid = "60">Expletives contribute by far the highest correct result.</S>
			<S sid ="169" ssid = "61">We also found correctly 47 ‘there’ expletives and 6 correctly classified pronominal ‘there’ which however have been left unbound.</S>
			<S sid ="170" ssid = "62">The system also found 48 occurrences of deictic discourse bound “this” and “that”, which corresponds to the full coverage.</S>
			<S sid ="171" ssid = "63">Finally, nominal expressions: the History List (HL) has been incremented up to 2243 new entities.</S>
			<S sid ="172" ssid = "64">The system identified 2773 entities from the HL by matching their linguistic description.</S>
			<S sid ="173" ssid = "65">The overall number of resolution actions taken by the Discourse Level algorithm is 1861: this includes both cases of nominal and pronominal expressions.</S>
			<S sid ="174" ssid = "66">However, since only 366 can be pronouns, the remaining 1500 resolution actions have been carried out on nominal expressions present in the HL.</S>
			<S sid ="175" ssid = "67">If we compare these results to the ones computed by GuiTAR, which a s s i g n s e m a n t i c i n d i c e s t o N a m e d E n t i t i e s disregarding their status of anaphora, we can see that the whole text is made up of 12731 NEs.</S>
			<S sid ="176" ssid = "68">GuiTAR finds 1585 cases of identity relations between a NE and an antecedent.</S>
			<S sid ="177" ssid = "69">However, GuiTAR introduces always new indices and creates local antecedent-referring expression chains rather than repeating the same index of the chain head.</S>
			<S sid ="178" ssid = "70">In this way, it is difficult if not impossible to compute how many times the text corefers/cospecifies to the same referring expressions.</S>
			<S sid ="179" ssid = "71">On the contrary, in our case, this can be easily computed by counting how many times the same semantic index is being repeated in a “resolution” or “identity” action of the anaphora resolution algorithm.</S>
			<S sid ="180" ssid = "72">For instance, the Jury is coreferred/cospecified 12 times; Price Daniel also 12 times and so on.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "5">
			<S sid ="181" ssid = "1">The error rate of both Charniak’s and Connexor’s as reported in the literature, is approximately the same, 20%; this notwithstanding, MARS has a slightly reduced coverage when compared with JavaRAP, 96%.</S>
			<S sid ="182" ssid = "2">GuiTAR has the worst coverage, 85%.</S>
			<S sid ="183" ssid = "3">As to accuracy, none of the three algorithms overruns 50%: JavaRAP has the best score 49.9%.</S>
			<S sid ="184" ssid = "4">However GETARUNS has 63% correct score, with 90% coverage.</S>
			<S sid ="185" ssid = "5">There are at least three reasons why our system has a better performance: one is the presence of a richer functional and semantic information as explained above, which comes with augmented head- dependent structures.</S>
			<S sid ="186" ssid = "6">Second reason is the decision to split the referential process into two and treat utterance level pronominal expressions separately from discourse level ones.</S>
			<S sid ="187" ssid = "7">Third reason is the way in which discourse level anaphora resolution is organized: our version of the Centering algorithm hinges on a record of a list of best antecedents weighted on the basis of their behaviour in History List and on their intrinsic semantic properties.</S>
			<S sid ="188" ssid = "8">These three properties of our AR algorithm can be dubbed the Knowledge Rich approach.</S>
			<S sid ="189" ssid = "9">F-measures approximates very closely what we obtained in a previous experiment: however, as a whole it is an insufficient score to insure adequate confidence in semantic substitution of anaphoric items by the head of the antecedent.</S>
			<S sid ="190" ssid = "10">Improvements need to come from parsing and the lexical component.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="191" ssid = "11">Thanks to three anonymous reviewers who helped us improve the overall layout of the paper.</S>
	</SECTION>
</PAPER>
