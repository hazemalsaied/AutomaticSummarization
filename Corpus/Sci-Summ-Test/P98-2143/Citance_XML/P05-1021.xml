<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we focus on how to improve pronoun resolution using the statistics- based semantic compatibility information.</S>
		<S sid ="2" ssid = "2">We investigate two unexplored issues that influence the effectiveness of such information: statistics source and learning framework.</S>
		<S sid ="3" ssid = "3">Specifically, we for the first time propose to utilize the web and the twin-candidate model, in addition to the previous combination of the corpus and the single-candidate model, to compute and apply the semantic information.</S>
		<S sid ="4" ssid = "4">Our study shows that the semantic compatibility obtained from the web can be effectively incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Semantic compatibility is an important factor for pronoun resolution.</S>
			<S sid ="6" ssid = "6">Since pronouns, especially neutral pronouns, carry little semantics of their own, the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphor’s context, based on the statistics that the corresponding predicate-argument tuples occur in a particular large corpus.</S>
			<S sid ="7" ssid = "7">Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2.</S>
			<S sid ="8" ssid = "8">For anaphor it1, the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus.</S>
			<S sid ="9" ssid = "9">A similar pattern could also be observed for it2.</S>
			<S sid ="10" ssid = "10">So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems.</S>
			<S sid ="11" ssid = "11">Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution.</S>
			<S sid ="12" ssid = "12">It determined the preference of candidates based on predicate-argument frequencies.</S>
			<S sid ="13" ssid = "13">Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicate- arguments from a large corpus using a bootstrapping method.</S>
			<S sid ="14" ssid = "14">However, the utility of the corpus-based semantics for pronoun resolution is often argued.</S>
			<S sid ="15" ssid = "15">Kehler et al.</S>
			<S sid ="16" ssid = "16">(2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution.</S>
			<S sid ="17" ssid = "17">Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al.</S>
			<S sid ="18" ssid = "18">(2001), Strube and Muller (2003)).</S>
			<S sid ="19" ssid = "19">Could the relatively noisy semantic knowledge give us further system improvement?</S>
			<S sid ="20" ssid = "20">In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information.</S>
			<S sid ="21" ssid = "21">We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source.</S>
			<S sid ="22" ssid = "22">Corpus-based knowledge usually suffers from data sparseness problem.</S>
			<S sid ="23" ssid = "23">That is, many predicate-argument tuples would be unseen even in a large corpus.</S>
			<S sid ="24" ssid = "24">A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, Ann Arbor, June 2005.</S>
			<S sid ="25" ssid = "25">Qc 2005 Association for Computational Linguistics web.</S>
			<S sid ="26" ssid = "26">It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003).</S>
			<S sid ="27" ssid = "27">So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair.</S>
			<S sid ="28" ssid = "28">However, to our knowledge, using the web to help pronoun resolution still remains unexplored.</S>
			<S sid ="29" ssid = "29">Learning framework.</S>
			<S sid ="30" ssid = "30">Commonly, the predicate- argument statistics is incorporated into anaphora resolution systems as a feature.</S>
			<S sid ="31" ssid = "31">What kind of learning framework is suitable for this feature?</S>
			<S sid ="32" ssid = "32">Previous approaches to anaphora resolution adopt the single- candidate model, in which the resolution is done on an anaphor and one candidate at a time (Soon et al., 2001; Ng and Cardie, 2002).</S>
			<S sid ="33" ssid = "33">However, as the purpose of the predicate-argument statistics is to evaluate the preference of the candidates in semantics, it is possible that the statistics-based semantic feature could be more effectively applied in the twin- candidate (Yang et al., 2003) that focusses on the preference relationships among candidates.</S>
			<S sid ="34" ssid = "34">In our work we explore the acquisition of the semantic compatibility information from the corpus and the web, and the incorporation of such semantic information in the single-candidate model and the twin-candidate model.</S>
			<S sid ="35" ssid = "35">We systematically evaluate the combinations of different statistics sources and learning frameworks in terms of their effectiveness in helping the resolution.</S>
			<S sid ="36" ssid = "36">Results on the MUC data set show that for neutral pronoun resolution in which an anaphor has no specific semantic category, the web-based semantic information would be the most effective when applied in the twin-candidate model: Not only could such a system significantly improve the baseline without the semantic feature, it also outperforms the system with the combination of the corpus and the single-candidate model (by 11.5% success).</S>
			<S sid ="37" ssid = "37">The rest of this paper is organized as follows.</S>
			<S sid ="38" ssid = "38">Section 2 describes the acquisition of the semantic com and finally, Section 5 gives the conclusion.</S>
	</SECTION>
	<SECTION title="Computing the Statistics-based Semantic. " number = "2">
			<S sid ="39" ssid = "1">Compatibility In this section, we introduce in detail how to compute the semantic compatibility, using the predicate- argument statistics obtained from the corpus or the web.</S>
			<S sid ="40" ssid = "2">2.1 Corpus-Based Semantic Compatibility.</S>
			<S sid ="41" ssid = "3">Three relationships, possessive-noun, subject-verb and verb-object, are considered in our work.</S>
			<S sid ="42" ssid = "4">Before resolution a large corpus is prepared.</S>
			<S sid ="43" ssid = "5">Documents in the corpus are processed by a shallow parser that could generate predicate-argument tuples of the above three relationships1.</S>
			<S sid ="44" ssid = "6">To reduce data sparseness, the following steps are applied in each resulting tuple, automatically: • Only the nominal or verbal heads are retained.• Each Named-Entity (NE) is replaced by a com mon noun which corresponds to the semantic category of the NE (e.g. “IBM” → “com pany”) 2 . • All words are changed to their base morpho- logic forms (e.g. “companies → company”).</S>
			<S sid ="45" ssid = "7">During resolution, for an encountered anaphor, each of its antecedent candidates is substituted with the anaphor . According to the role and type of the anaphor in its context, a predicate-argument tuple is extracted and the above three steps for data-sparse reduction are applied.</S>
			<S sid ="46" ssid = "8">Consider the sentence (1), for example.</S>
			<S sid ="47" ssid = "9">The anaphors “it1” and “it2” indicate a subject verb and verb object relationship, respectively.</S>
			<S sid ="48" ssid = "10">Thus, the predicate-argument tuples for the two candidates “government” and “money” would be (collect (subject government)) and (collect (subject money)) for “it1”, and (collect (object government)) and (collect (object money)) for “it2”.</S>
			<S sid ="49" ssid = "11">Each extracted tuple is searched in the prepared tuples set of the corpus, and the times the tuple occurs are calculated.</S>
			<S sid ="50" ssid = "12">For each candidate, its semantic patibility information from the corpus and the web.</S>
			<S sid ="51" ssid = "13">1 The possessive-noun relationship involves the forms like.</S>
			<S sid ="52" ssid = "14">Section 3 discusses the application of the statistics in the single-candidate and twin-candidate learning models.</S>
			<S sid ="53" ssid = "15">Section 4 gives the experimental results, “NP2 of NP1 ” and “NP1 ’s NP2 ”.</S>
			<S sid ="54" ssid = "16">2 In our study, the semantic category of a NE is identified automatically by the pre-processing NE recognition component.</S>
			<S sid ="55" ssid = "17">compatibility with the anaphor could be represented simply in terms of frequency StatSem(candi, ana) = count(candi, ana) (1) where count(candi, ana) is the count of the tuple formed by candi and ana, or alternatively, in terms of conditional probability (P (candi, ana|candi)), where the count of the tuple is divided by the count of the single candidate in the corpus.</S>
			<S sid ="56" ssid = "18">That is empty determiners (for bare plurals) would be added (e.g., “the|a business of the company|companies”).</S>
			<S sid ="57" ssid = "19">Queries are submitted to a particular web search engine (Google in our study).</S>
			<S sid ="58" ssid = "20">All queries are performed as exact matching.</S>
			<S sid ="59" ssid = "21">Similar to the corpus- based statistics, the compatibility for each candidate and anaphor pair could be represented using either frequency (Eq. 1) or probability (Eq. 2) metric.</S>
			<S sid ="60" ssid = "22">In such a situation, count(candi, ana) is the hit number of the inflected queries returned by the search StatSem(candi, ana) = count(candi, ana) count(candi) (2) engine, while count(candi) is the hit number of the query formed with only the head of the candidate (i.e.,“the + candi”).</S>
			<S sid ="61" ssid = "23">In this way, the statistics would not bias candidates that occur frequently in isolation.</S>
			<S sid ="62" ssid = "24">2.2 Web-Based Semantic Compatibility.</S>
			<S sid ="63" ssid = "25">Unlike documents in normal corpora, web pages could not be preprocessed to generate the predicate- argument reserve.</S>
			<S sid ="64" ssid = "26">Instead, the predicate-argument statistics has to be obtained via a web search engine like Google and Altavista.</S>
			<S sid ="65" ssid = "27">For the three types of predicate-argument relationships, queries are constructed in the forms of “NPcandi VP” (for subject- verb), “VP NPcandi” (for verb-object), and “NPcandi ’s NP” or “NP of NPcandi” (for possessive-noun).</S>
			<S sid ="66" ssid = "28">Consider the following sentence: (2) Several experts suggested that IBM’s accounting grew much more liberal since the mid 1980s as its business turned sour.</S>
			<S sid ="67" ssid = "29">For the pronoun “its” and the candidate “IBM”, the two generated queries are “business of IBM” and “IBM’s business”.</S>
			<S sid ="68" ssid = "30">To reduce data sparseness, in an initial query only the nominal or verbal heads are retained.</S>
			<S sid ="69" ssid = "31">Also, each NE is replaced by the corresponding common noun.</S>
			<S sid ="70" ssid = "32">(e.g, “IBM’s business” → “company’s business” and “business of IBM” → “business of company”).A set of inflected queries is generated by ex panding a term into all its possible morphological forms.</S>
			<S sid ="71" ssid = "33">For example, in Sentence (1), “collect money” becomes “collected|collecting|... money”, and in (2) “business of company” becomes “business of company|companies”).</S>
			<S sid ="72" ssid = "34">Besides, determiners are inserted for every noun.</S>
			<S sid ="73" ssid = "35">If the noun is the candidate under consideration, only the definite article the is inserted.</S>
			<S sid ="74" ssid = "36">For other nouns, instead, a/an, the and the</S>
	</SECTION>
	<SECTION title="Applying the Semantic Compatibility. " number = "3">
			<S sid ="75" ssid = "1">In this section, we discuss how to incorporate the statistics-based semantic compatibility for pronoun resolution, in a machine learning framework.</S>
			<S sid ="76" ssid = "2">3.1 The Single-Candidate Model.</S>
			<S sid ="77" ssid = "3">One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002).</S>
			<S sid ="78" ssid = "4">In such a learning model, each training or testing instance takes the form of i{C, ana}, where ana is the possible anaphor and C is its antecedent candidate.</S>
			<S sid ="79" ssid = "5">An instance is associated with a feature vector to describe their relationships.</S>
			<S sid ="80" ssid = "6">During training, for each anaphor in a given text, a positive instance is created by pairing the anaphor and its closest antecedent.</S>
			<S sid ="81" ssid = "7">Also a set of negative instances is formed by pairing the anaphor and each of the intervening candidates.</S>
			<S sid ="82" ssid = "8">Based on the training instances, a binary classifier is generated using a certain learning algorithm, like C5 (Quinlan, 1993) in our work.</S>
			<S sid ="83" ssid = "9">During resolution, given a new anaphor, a test instance is created for each candidate.</S>
			<S sid ="84" ssid = "10">This instance is presented to the classifier, which then returns a positive or negative result with a confidence value indicating the likelihood that they are co-referent.</S>
			<S sid ="85" ssid = "11">The candidate with the highest confidence value would be selected as the antecedent.</S>
			<S sid ="86" ssid = "12">3.2 Features.</S>
			<S sid ="87" ssid = "13">In our study we only consider those domain- independent features that could be obtained with low Feature Description DefNp 1 if the candidate is a definite NP; else 0 Pron 1 if the candidate is a pronoun; else 0 NE 1 if the candidate is a named entity; else 0 SameSent 1 if the candidate and the anaphor is in the same sentence; else 0 NearestNP 1 if the candidate is nearest to the anaphor; else 0 ParalStuct 1 if the candidate has an parallel structure with ana; else 0 FirstNP 1 if the candidate is the first NP in a sentence; else 0 Reflexive 1 if the anaphor is a reflexive pronoun; else 0 Type Type of the anaphor (0: Single neuter pronoun; 1: Plural neuter pronoun; 2: Male personal pronoun; 3: Female personal pronoun) StatSem∗ the statistics-base semantic compatibility of the candidate SemMag∗∗ the semantic compatibility difference between two competing candidates Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) computational cost but with high reliability.</S>
			<S sid ="88" ssid = "14">Table 1 summarizes the features with their respective possible values.</S>
			<S sid ="89" ssid = "15">The first three features represent the lexical properties of a candidate.</S>
			<S sid ="90" ssid = "16">The POS properties could indicate whether a candidate refers to a hearer- old entity that would have a higher preference to be selected as the antecedent (Strube, 1998).</S>
			<S sid ="91" ssid = "17">SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978).</S>
			<S sid ="92" ssid = "18">FirstNP aims to capture the salience of the it2.</S>
			<S sid ="93" ssid = "19">The consequence is that the learning algorithm would think such a feature is not that ”indicative” and reduce its salience in the resulting classifier.</S>
			<S sid ="94" ssid = "20">One way to tackle this problem is to normalize the feature by the frequencies of the anaphor’s context, e.g., “count(collected)” and “count(said)”.</S>
			<S sid ="95" ssid = "21">This, however, would require extra calculation.</S>
			<S sid ="96" ssid = "22">In fact, as candidates of a specific anaphor share the same anaphor context, we can just normalize the semantic feature of a candidate by that of its competitor: StatSem(C, ana) candidate in the local discourse segment.</S>
			<S sid ="97" ssid = "23">ParalStuctmarks whether a candidate and an anaphor have sim StatSemN (C, ana) = c max StatSem(ci , ana) (ana) ilar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998).</S>
			<S sid ="98" ssid = "24">Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section.</S>
			<S sid ="99" ssid = "25">If a candidate is a pronoun, this feature value would be set to that of its closest nominal antecedent.</S>
			<S sid ="100" ssid = "26">As described, the semantic compatibility of a candidate is computed under the context of the current anaphor.</S>
			<S sid ="101" ssid = "27">Consider two occurrences of anaphors “.</S>
			<S sid ="102" ssid = "28">it1 collected . . .</S>
			<S sid ="103" ssid = "29">” and “.</S>
			<S sid ="104" ssid = "30">it2 said . . .</S>
			<S sid ="105" ssid = "31">”.</S>
			<S sid ="106" ssid = "32">As “NP collected” should occur less frequently than “NP said”, the candidates of it1 would generally have predicate-argument statistics lower than those of it2.</S>
			<S sid ="107" ssid = "33">That is, a positive instance for it1 might bear a lower semantic feature value than a negative instance for i ∈candi set The value (0 ∼ 1) represents the rank of the semantic compatibility of the candidate C among candi set(ana), the current candidates of ana.</S>
			<S sid ="108" ssid = "34">3.3 The Twin-Candidate Model.</S>
			<S sid ="109" ssid = "35">Yang et al.</S>
			<S sid ="110" ssid = "36">(2003) proposed an alternative twin- candidate model for anaphora resolution task.</S>
			<S sid ="111" ssid = "37">The strength of such a model is that unlike the single- candidate model, it could capture the preference relationships between competing candidates.</S>
			<S sid ="112" ssid = "38">In the model, candidates for an anaphor are paired and features from two competing candidates are put together for consideration.</S>
			<S sid ="113" ssid = "39">This property could nicely deal with the above mentioned training problem of different anaphor contexts, because the semantic feature would be considered under the current candidate set only.</S>
			<S sid ="114" ssid = "40">In fact, as semantic compatibility is a preference-based factor for anaphor resolution, it would be incorporated in the twin-candidate model more naturally.</S>
			<S sid ="115" ssid = "41">In the twin-candidate model, an instance takes a form like i{C1, C2, ana}, where C1 and C2 are two candidates.</S>
			<S sid ="116" ssid = "42">We stipulate that C2 should be closer to ana than C1 in distance.</S>
			<S sid ="117" ssid = "43">The instance is labelled as “10” if C1 the antecedent, or “01” if C2 is. During training, for each anaphor, we find its closest antecedent, Cante.</S>
			<S sid ="118" ssid = "44">A set of “10” instances, i{Cante, C, ana}, is generated by pairing Cante and each of the interning candidates C. Also a set of “01” instances, i{C, Cante, ana}, is created by pairing Cante with each candidate before Cante until another antecedent, if any, is reached.</S>
			<S sid ="119" ssid = "45">The resulting pairwise classifier would return “10” or “01” indicating which candidate is preferred to the other.</S>
			<S sid ="120" ssid = "46">During resolution, candidates are paired one by one.</S>
			<S sid ="121" ssid = "47">The score of a candidate is the total number of the competitors that the candidate wins over.</S>
			<S sid ="122" ssid = "48">The candidate with the highest score would be selected as the antecedent.</S>
			<S sid ="123" ssid = "49">Features The features for the twin-candidate model are similar to those for the single-candidate model except that a duplicate set of features has to be prepared for the additional candidate.</S>
			<S sid ="124" ssid = "50">Besides, a new feature, SemMag, is used in place of StatSem to represent the difference magnitude between the semantic compatibility of two candidates.</S>
			<S sid ="125" ssid = "51">Let mag = StatSem(C1 , ana)/StatSem(C2 , ana), feature SemMag is defined as follows, mag − 1 : mag &gt;= 1 from MUC6 coreference data set, while the testing was on the 50 formal-test documents of MUC6 (30) and MUC7 (20).</S>
			<S sid ="126" ssid = "52">Throughout the experiments, default learning parameters were applied to the C5 algorithm.</S>
			<S sid ="127" ssid = "53">The performance was evaluated based on success, the ratio of the number of correctly resolved anaphors over the total number of anaphors.</S>
			<S sid ="128" ssid = "54">An input raw text was preprocessed automatically by a pipeline of NLP components.</S>
			<S sid ="129" ssid = "55">The noun phrase identification and the predicate-argument extraction were done based on the results of a chunk tagger, which was trained for the shared task of CoNLL2000 and achieved 92% accuracy (Zhou et al., 2000).</S>
			<S sid ="130" ssid = "56">The recognition of NEs as well as their semantic categories was done by a HMM based NER, which was trained for the MUC NE task and obtained high F-scores of 96.9% (MUC6) and 94.3% (MUC7) (Zhou and Su, 2002).</S>
			<S sid ="131" ssid = "57">For each anaphor, the markables occurring within the current and previous two sentences were taken as the initial candidates.</S>
			<S sid ="132" ssid = "58">Those with mismatched number and gender agreements were filtered from the candidate set.</S>
			<S sid ="133" ssid = "59">Also, pronouns or NEs that disagreed in person with the anaphor were removed in advance.</S>
			<S sid ="134" ssid = "60">For the training set, there are totally 645 neutral pronouns and 385 personal pronouns with nonempty candidate set, while for the testing set, the number is 245 and 197.</S>
			<SUBSECTION>4.2 The Corpus and the Web.</SUBSECTION>
			<S sid ="135" ssid = "61">The corpus for the predicate-argument statistics computation was from the TIPSTER’s Text Re SemM ag(C1 , C2 , ana) = 1 − mag−1 : mag &lt; 1 search Collection (v1994).</S>
			<S sid ="136" ssid = "62">Consisting of 173,252 The positive or negative value marks the times that the statistics of C1 is larger or smaller than C2.</S>
	</SECTION>
	<SECTION title="Evaluation and Discussion. " number = "4">
			<S sid ="137" ssid = "1">4.1 Experiment Setup.</S>
			<S sid ="138" ssid = "2">In our study we were only concerned about the third- person pronoun resolution.</S>
			<S sid ="139" ssid = "3">With an attempt to examine the effectiveness of the semantic feature on different types of pronouns, the whole resolution was divided into neutral pronoun (it &amp; they) resolution and personal pronoun (he &amp; she) resolution.</S>
			<S sid ="140" ssid = "4">The experiments were done on the newswire domain, using MUC corpus (Wall Street Journal articles).</S>
			<S sid ="141" ssid = "5">The training was done on 150 documents Wall Street Journal articles from the year 1988 to 1992, the data set contained about 76 million words.</S>
			<S sid ="142" ssid = "6">The documents were preprocessed using the same POS tagging and NE-recognition components as in the pronoun resolution task.</S>
			<S sid ="143" ssid = "7">Cass (Abney, 1996), a robust chunker parser was then applied to generate the shallow parse trees, which resulted in 353,085 possessive-noun tuples, 759,997 verb-object tuples and 1,090,121 subject-verb tuples.</S>
			<S sid ="144" ssid = "8">We examined the capacity of the web and the corpus in terms of zero-count ratio and count number.</S>
			<S sid ="145" ssid = "9">On average, among the predicate-argument tuples that have nonzero corpus-counts, above 93% have also nonzero web-counts.</S>
			<S sid ="146" ssid = "10">But the ratio is only around 40% contrariwise.</S>
			<S sid ="147" ssid = "11">And for the predicate L ea rn in g M o de l S y s t e m Neutr al Pron Corpu s Web Person al Pron Corpu s Web O v e r a l l Corpu s Web b a s e l i n e 6 5 . 7 8 6 . 8 7 5 . 1 + f r e q u e n c y 6 7 . 3 69.</S>
			<S sid ="148" ssid = "12">9 8 6 . 8 86.</S>
			<S sid ="149" ssid = "13">8 7 6 . 0 76.</S>
			<S sid ="150" ssid = "14">9 Si ngle Ca nd id ate + n or m al iz ed fr eq ue nc y 6 6 . 9 67.</S>
			<S sid ="151" ssid = "15">8 8 6 . 8 86.</S>
			<S sid ="152" ssid = "16">8 7 5 . 8 76.</S>
			<S sid ="153" ssid = "17">2 + p r o b a b i l i t y 6 5 . 7 65.</S>
			<S sid ="154" ssid = "18">7 8 6 . 8 86.</S>
			<S sid ="155" ssid = "19">8 7 5 . 1 75.</S>
			<S sid ="156" ssid = "20">1 +n or m ali ze d pr ob ab ilit y 6 7 . 7 70.</S>
			<S sid ="157" ssid = "21">6 8 6 . 8 86.</S>
			<S sid ="158" ssid = "22">8 7 6 . 2 77.</S>
			<S sid ="159" ssid = "23">8 b a s e l i n e 7 3 . 9 9 1 . 9 8 1 . 9 T w in C a n di d at e + f r e q u e n c y 7 6 . 7 79.</S>
			<S sid ="160" ssid = "24">2 9 1 . 4 91.</S>
			<S sid ="161" ssid = "25">9 8 3 . 3 84.</S>
			<S sid ="162" ssid = "26">8 + p r o b a b i l i t y 7 5 . 9 78.</S>
			<S sid ="163" ssid = "27">0 9 1 . 4 92.</S>
			<S sid ="164" ssid = "28">4 8 2 . 8 84.</S>
			<S sid ="165" ssid = "29">4 Table 2: The performance of different resolution systems Relationship N-Pron P-Pron Possessive-Noun 0.508 0.517 Verb-Object 0.503 0.526 Subject-Verb 0.619 0.676 Table 3: Correlation between web and corpus counts on the seen predicate-argument tuples argument tuples that could be seen in both data sources, the count from the web is above 2000 times larger than that from the corpus.</S>
			<S sid ="166" ssid = "30">Although much less sparse, the web counts are significantly noisier than the corpus count since no tagging, chunking and parsing could be carried out on the web pages.</S>
			<S sid ="167" ssid = "31">However, previous study (Keller and Lapata, 2003) reveals that the large amount of data available for the web counts could outweigh the noisy problems.</S>
			<S sid ="168" ssid = "32">In our study we also carried out a correlation analysis3 to examine whether the counts from the web and the corpus are linearly related, on the predicate-argument tuples that can be seen in both data sources.</S>
			<S sid ="169" ssid = "33">From the results listed in Table 3, we observe moderately high correlation, with coefficients ranging from 0.5 to 0.7 around, between the counts from the web and the corpus, for both neutral pronoun (N-Pron) and personal pronoun (P- Pron) resolution tasks.</S>
			<S sid ="170" ssid = "34">4.3 System Evaluation.</S>
			<S sid ="171" ssid = "35">Table 2 summarizes the performance of the systems with different combinations of statistics sources and learning frameworks.</S>
			<S sid ="172" ssid = "36">The systems without the se 3 All the counts were log-transformed and the correlation coefficients were evaluated based on Pearsons’ r. mantic feature were used as the baseline.</S>
			<S sid ="173" ssid = "37">Under the single-candidate (SC) model, the baseline system obtains a success of 65.7% and 86.8% for neutral pronoun and personal pronoun resolution, respectively.</S>
			<S sid ="174" ssid = "38">By contrast, the twin-candidate (TC) modelachieves a significantly (p ≤ 0.05, by two-tailed ttest) higher success of 73.9% and 91.9%, respec tively.</S>
			<S sid ="175" ssid = "39">Overall, for the whole pronoun resolution, the baseline system under the TC model yields a success 81.9%, 6.8% higher than SC does4.</S>
			<S sid ="176" ssid = "40">The performance is comparable to most state-of-the-art pronoun resolution systems on the same data set.</S>
			<S sid ="177" ssid = "41">Web-based feature vs. Corpus-based feature The third column of the table lists the results using the web-based compatibility feature for neutral pronouns.</S>
			<S sid ="178" ssid = "42">Under both SC and TC models, incorporation of the web-based feature significantly boosts the performance of the baseline: For the best system in the SC model and the TC model, the success rate is improved significantly by around 4.9% and 5.3%, respectively.</S>
			<S sid ="179" ssid = "43">A similar pattern of improvement could be seen for the corpus-based semantic feature.</S>
			<S sid ="180" ssid = "44">However, the increase is not as large as using the web-based feature: Under the two learning models, the success rate of the best system with the corpus-based feature rises by up to 2.0% and 2.8% respectively, about 2.9% and 2.5% less than that of the counterpart systems with the web-based feature.</S>
			<S sid ="181" ssid = "45">The larger size and the better counts of the web against the corpus, as reported in Section 4.2, 4 The improvement against SC is higher than that reported in (Yang et al., 2003).</S>
			<S sid ="182" ssid = "46">It should be because we now used 150 training documents rather than 30 ones as in the previous work.</S>
			<S sid ="183" ssid = "47">The TC model would benefit from larger training data set as it uses more features (more than double) than SC.</S>
			<S sid ="184" ssid = "48">sho uld con trib ute to the bett er per for ma nce . F e a t u r e G r o u p Is ol ate d Co m bi ne d S i n g l e c a n d i d a t e m o d e l v s . Twin-Candidate.</S>
			<S sid ="185" ssid = "49">S e m M a g ( W e b b a s e d ) 6 1 . 2 6 1 . 2 mo del Th e diff ere nce bet we en the SC and the TC T y p e + R e f l e x i v e 5 3 . 1 6 1 . 2 mo del is obv iou s fro m the tabl e. For the N Pro n P a r a S t r u c t 5 3 . 1 6 1 . 2 and P Pro n res olu tio n, the sys tem s und er TC cou ld Pr on + De fN P+ In De fN P+ N E 5 7 . 1 6 7 . 8 out per for m the cou nte rpa rt sys tem s und er SC by N e a r e s t N P + S a m e S e n t 5 3 . 1 7 0 . 2 abo ve 5% and 8% suc ces s, res pec tive ly.</S>
			<S sid ="186" ssid = "50">In add itio n, F i r s t N P 6 5 . 3 7 9 . 2 the utility of the statistics-based semantic feature is more salient under TC than under SC for N-Pron resolution: the best gains using the corpus-based and the web-based semantic features under TC are 2.9% and 5.3% respectively, higher than those under the SC model using either un-normalized semantic features (1.6% and 3.3%), or normalized semantic features (2.0% and 4.9%).</S>
			<S sid ="187" ssid = "51">Although under SC, the normalized semantic feature could result in a gain close to under TC, its utility is not stable: with metric frequency, using the normalized feature performs even worse than using the un-normalized one.</S>
			<S sid ="188" ssid = "52">These results not only affirm the claim by Yang et al.</S>
			<S sid ="189" ssid = "53">(2003) that the TC model is superior to the SC model for pronoun resolution, but also indicate that TC is more reliable than SC in applying the statistics-based semantic feature, for N-Pron resolution.</S>
			<S sid ="190" ssid = "54">Web+TC vs. Other combinations The above analysis has exhibited the superiority of the web over the corpus, and the TC model over the SC model.</S>
			<S sid ="191" ssid = "55">The experimental results also reveal that using the the web-based semantic feature together with the TC model is able to further boost the resolution performance for neutral pronouns.</S>
			<S sid ="192" ssid = "56">The system with such a Web+TC combination could achieve a high success of 79.2%, defeating all the other possible combinations.</S>
			<S sid ="193" ssid = "57">Especially, it considerably outperforms (up to 11.5% success) the system with the Corpus+SC combination, which is commonly adopted in previous work (e.g., Kehler et al.</S>
			<S sid ="194" ssid = "58">(2004)).</S>
			<S sid ="195" ssid = "59">Personal pronoun resolution vs. Neutral pronoun resolution Interestingly, the statistics-based semantic feature has no effect on the resolution of personal pronouns, as shown in the table 2.</S>
			<S sid ="196" ssid = "60">We found in the learned decision trees such a feature did not occur (SC) or only occurred in bottom nodes (TC).</S>
			<S sid ="197" ssid = "61">This should be because personal pronouns have strong restriction on the semantic category (i.e., human) of the candidates.</S>
			<S sid ="198" ssid = "62">A nonhuman candidate, even with a high predicate-argument statistics, could Table 4: Results of different feature groups under the TC model for N-pron resolution SameSent_1 = 0: :..SemMag &gt; 0: : :..Pron_2 = 0: 10 (200/23) : : Pron_2 = 1: ...</S>
			<S sid ="199" ssid = "63">: SemMag &lt;= 0: : :..Pron_2 = 1: 01 (75/1) : Pron_2 = 0: : :..SemMag &lt;= -28: 01 (110/19) : SemMag &gt; -28: ...</S>
			<S sid ="200" ssid = "64">SameSent_1 = 1: :..SameSent_2 = 0: 01 (1655/49) SameSent_2 = 1: :..FirstNP_2 = 1: 01 (104/1) FirstNP_2 = 0: :..ParaStruct_2 = 1: 01 (3) ParaStruct_2 = 0: :..SemMag &lt;= -151: 01 (27/2) SemMag &gt; -151:...</S>
			<S sid ="201" ssid = "65">Figure 1: Top portion of the decision tree learned under TC model for N-pron resolution (features ended with “ 1” are for the first candidate C1 and those with “ 2” are for C2 .) not be used as the antecedent (e.g. company said in the sentence “.</S>
			<S sid ="202" ssid = "66">the company . . .</S>
			<S sid ="203" ssid = "67">he said . . .</S>
			<S sid ="204" ssid = "68">”).</S>
			<S sid ="205" ssid = "69">In fact, our analysis of the current data set reveals that most P-Prons refer back to a P-Pron or NE candidate whose semantic category (human) has been determined.</S>
			<S sid ="206" ssid = "70">That is, simply using features NE and Pron is sufficient to guarantee a high success, and thus the relatively weak semantic feature would not be taken in the learned decision tree for resolution.</S>
			<S sid ="207" ssid = "71">4.4 Feature.</S>
			<S sid ="208" ssid = "72">Analysis In our experiment we were also concerned about the importance of the web-based compatibility feature (using frequency metric) among the feature set.</S>
			<S sid ="209" ssid = "73">For this purpose, we divided the features into groups, and then trained and tested on one group at a time.</S>
			<S sid ="210" ssid = "74">Table 4 lists the feature groups and their respective results for N-Pron resolution under the TC model.</S>
			<S sid ="211" ssid = "75">The second column is for the systems with only the current feature group, while the third column is with the features combined with the existing feature set.</S>
			<S sid ="212" ssid = "76">We see that used in isolation, the semantic compatibility feature is able to achieve a success up to 61% around, just 4% lower than the best indicative feature FirstNP.</S>
			<S sid ="213" ssid = "77">In combination with other features, the performance could be improved by as large as 18% as opposed to being used alone.</S>
			<S sid ="214" ssid = "78">Figure 1 shows the top portion of the pruned decision tree for N-Pron resolution under the TC model.</S>
			<S sid ="215" ssid = "79">We could find that: (i) When comparing two candidates which occur in the same sentence as the anaphor, the web-based semantic feature would be examined in the first place, followed by the lexical property of the candidates.</S>
			<S sid ="216" ssid = "80">(ii) When two non- pronominal candidates are both in previous sentences before the anaphor, the web-based semantic feature is still required to be examined after FirstNP and ParaStruct.</S>
			<S sid ="217" ssid = "81">The decision tree further indicates that the web-based feature plays an important role in N-Pron resolution.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="218" ssid = "1">Our research focussed on improving pronoun resolution using the statistics-based semantic compatibility information.</S>
			<S sid ="219" ssid = "2">We explored two issues that affect the utility of the semantic information: statistics source and learning framework.</S>
			<S sid ="220" ssid = "3">Specifically, we proposed to utilize the web and the twin-candidate model, in addition to the common combination of the corpus and single-candidate model, to compute and apply the semantic information.</S>
			<S sid ="221" ssid = "4">Our experiments systematically evaluated different combinations of statistics sources and learning models.</S>
			<S sid ="222" ssid = "5">The results on the newswire domain showed that the web-based semantic compatibility could be the most effectively incorporated in the twin-candidate model for the neutral pronoun resolution.</S>
			<S sid ="223" ssid = "6">While the utility is not obvious for personal pronoun resolution, we can still see the improvement on the overall performance.</S>
			<S sid ="224" ssid = "7">We believe that the semantic information under such a configuration would be even more effective on technical domains where neutral pronouns take the majority in the pronominal anaphors.</S>
			<S sid ="225" ssid = "8">Our future work would have a deep exploration on such domains.</S>
	</SECTION>
</PAPER>
