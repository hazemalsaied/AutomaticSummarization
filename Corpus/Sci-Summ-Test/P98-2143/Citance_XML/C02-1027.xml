<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes LINGUA - an architecture for text processing in Bulgarian.</S>
		<S sid ="2" ssid = "2">First, the pre-processing modules for tokenisation, sentence splitting, paragraph segmentation, part- of-speech tagging, clause chunking and noun phrase extraction are outlined.</S>
		<S sid ="3" ssid = "3">Next, the paper proceeds to describe in more detail the anaphora resolution module.</S>
		<S sid ="4" ssid = "4">Evaluation results are reported for each processing task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">The state of the art of today’s full parsing and knowledge-based automatic analysis still falls short of providing a reliable processing framework for robust, real-world applications such as automatic abstracting or information extraction.</S>
			<S sid ="6" ssid = "6">The problem is especially acute for languages which do not benefit from a wide range of processing programs such as Bulgarian.</S>
			<S sid ="7" ssid = "7">There have been various projects which address different aspects of the automatic analysis in Bulgarian such as morphological analysis (Krushkov, 1997), (Simov et al., 1992), morphological disambiguation (Simov et al., 1992) and parsing (Avgustinova et al., 1989), but no previous work has pursued the development of a knowledge-poor, robust processing environment with a high level of component integrity.</S>
			<S sid ="8" ssid = "8">This paper reports the development and implementation of a robust architecture for language processing in Bulgarian referred to as LINGUA, which includes modules for POS tagging, sentence splitting, clause segmentation, parsing and anaphora resolution.</S>
			<S sid ="9" ssid = "9">Our text processing framework builds on the basis of considerably shallower linguistic analysis of the input, thus trading off depth of interpretation for breadth of coverage and workable, robust solution.</S>
			<S sid ="10" ssid = "10">LINGUA uses knowledge poor, heuristi cally based algorithms for language analysis, in this way getting round the lack of resources for Bulgarian.</S>
	</SECTION>
	<SECTION title="LINGUA - an architecture for. " number = "2">
			<S sid ="11" ssid = "1">language processing in Bulgarian LINGUA is a text processing framework for Bulgarian which automatically performs tokenisation, sentence splitting, part-of-speech tagging, parsing, clause segmentation, section- heading identification and resolution for third person personal pronouns (Figure 1).</S>
			<S sid ="12" ssid = "2">All modules of LINGUA are original and purpose- built, except for the module for morphological analysis which uses Krushkov’s morphological analyser BULMORPH (Krushkov, 1997).</S>
			<S sid ="13" ssid = "3">The anaphora resolver is an adaptation for Bulgarian of Mitkovs knowledge-poor pronoun resolution approach (Mitkov, 1998).</S>
			<S sid ="14" ssid = "4">LINGUA was used in a number of projects covering automatic text abridging, word semantic extraction (Totkov and Tanev, 1999) and term extraction.</S>
			<S sid ="15" ssid = "5">The following sections outline the basic language processing functions, provided by the language engine.</S>
			<S sid ="16" ssid = "6">2.1 Text segmentation: tokenisation,.</S>
			<S sid ="17" ssid = "7">sentence splitting and paragraph identification The first stage of every text processing task is the segmentation of text in terms of tokens, sentences and paragraphs.</S>
			<S sid ="18" ssid = "8">LINGUA performs text segmentation by operating within an input window of 30 tokens, applying rule-based algorithm for token synthesis, sentence splitting and paragraph identification.</S>
			<S sid ="19" ssid = "9">2.1.1 Tokenisation and token stapling Tokens identified from the input text serve as input to the token stapler.</S>
			<S sid ="20" ssid = "10">The token stapler forms more complex tokens on the basis of a Figure 1: General architecture of LINGUA token grammar.</S>
			<S sid ="21" ssid = "11">With a view to improving to- kenisation, a list of abbreviations has been incorporated into LINGUA.</S>
			<S sid ="22" ssid = "12">2.1.2 Sentence splitting LINGUA’s sentence splitter operates to identify sentence boundaries on the basis of 9 main end-of-sentence rules and makes use of a list of abbreviations.</S>
			<S sid ="23" ssid = "13">Some of the rules consist of several finer sub-rules.</S>
			<S sid ="24" ssid = "14">The evaluation of the performance of the sentence splitter on a text of 190 sentences reports a precision of 92% and a recall of 99%.</S>
			<S sid ="25" ssid = "15">Abbreviated names such as J.S.Simpson are filtered by special constraints.</S>
			<S sid ="26" ssid = "16">The sentence splitting and tokenising rules were adapted for English.</S>
			<S sid ="27" ssid = "17">The resulting sentence splitter was then employed for identifying sentence boundaries in the Wolverhampton Corpus of Business English project.</S>
			<S sid ="28" ssid = "18">2.1.3 Paragraph identification Paragraph identification is based on heuristics such as cue words, orthography and typographical markers.</S>
			<S sid ="29" ssid = "19">The precision of the paragraph splitter is about 94% and the recall is 98% (Table 3).</S>
			<S sid ="30" ssid = "20">2.2 Morphological analysis and.</S>
			<S sid ="31" ssid = "21">part-of-speech tagging 2.2.1 Morphological analysis Bulgarian morphology is complex, for example the paradigm of a verb has over 50 forms.</S>
			<S sid ="32" ssid = "22">Krushkov’s morphological analyser BULMORPH (Krushkov, 1997) is integrated in 2.2.2 Morphological disambiguation The level of morphological ambiguity for Bulgarian is not so high as it is in other languages.</S>
			<S sid ="33" ssid = "23">As a guide, we measured the ratio: N umber of all tags/N umber of all words.</S>
			<S sid ="34" ssid = "24">The results show that this ratio is comparatively low and for a corpus of technical texts of 9000 words the ratio tags per word is 1,26, whereas for a 13000-word corpus from the genre of fiction this ratio is 1,32.</S>
			<S sid ="35" ssid = "25">For other languages such as Turkish this ratio is about 1,9 and for certain English corpora 2,0 1.</S>
			<S sid ="36" ssid = "26">We used 33 handcrafted rules for disambiguation.</S>
			<S sid ="37" ssid = "27">Since large tagged corpora in Bulgarian are not widely available, the development of a corpus-based probabilistic tagger was an unrealistic goal for us.</S>
			<S sid ="38" ssid = "28">However, as some studies suggest (Voutilainen, 1995), the precision of rule-based taggers may exceed that of the probabilistic ones.</S>
			<S sid ="39" ssid = "29">2.3 Parsing.</S>
			<S sid ="40" ssid = "30">Seeking a robust flexible solution for parsing we implemented two alternative approaches in LINGUA: a fast-working NP extractor and more general parser, which works more slowly, but delivers better results both in accuracy and coverage.</S>
			<S sid ="41" ssid = "31">As no syntactically annotated Bulgarian corpora were available to us, using statistical data to implement probabilistic algorithm was not an option.</S>
			<S sid ="42" ssid = "32">The NP extraction algorithm is capable of analysing nested NPs, NPs which contain left the language engine with a view to processing Bulgarian texts at morphological level.</S>
			<S sid ="43" ssid = "33">1 Kemal Oflazer, personal communication.</S>
			<S sid ="44" ssid = "34">modifiers, prepositional phrases and coordinating phrases.</S>
			<S sid ="45" ssid = "35">The NP extractor is based on a simple unification grammar for NPs and APs.</S>
			<S sid ="46" ssid = "36">The recall of NP extraction, measured against 352 NPs from software manuals, was 77% and.</S>
			<S sid ="47" ssid = "37">the precision - 63.5%.</S>
			<S sid ="48" ssid = "38">A second, better coverage parser was implemented which employs a feature grammar based on recent formal models for Bulgarian, (Penchev, 1993), (Barkalova, 1997).</S>
			<S sid ="49" ssid = "39">All basic types of phrases such as NP, AP, PP, VP and AdvP are described in this grammar.</S>
			<S sid ="50" ssid = "40">The parser is supported by a grammar compiler, working on grammar description language for representation of non context unification grammars.</S>
			<S sid ="51" ssid = "41">For example one of the rules for synthesis of NP phrases has the form: N P (def : Y f ull art : F ext : + rex : − nam : −) →AP (gender :X def :Y f ull art:F number: L ) N P (ext:− def:− number:L gender:X rex:−) The features and values in the rules are not fixed sets and can be changed dynamically.</S>
			<S sid ="52" ssid = "42">The flexibility of this description allows the grammar to be extended easily.</S>
			<S sid ="53" ssid = "43">The parser uses a chart bottom-up strategy, which allows for partial parsing in case full syntactic tree cannot be built over the sentence.</S>
			<S sid ="54" ssid = "44">There are currently about 1900 syntactic rules in the grammar which are encoded through 70 syntactic formulae.</S>
			<S sid ="55" ssid = "45">Small corpus of 600 phrases was syntactically annotated by hand.</S>
			<S sid ="56" ssid = "46">We used this corpus to measure the precision of the parsing algorithm (Table 3).</S>
			<S sid ="57" ssid = "47">We found that the precision of NP extraction performed by the chart parser is higher than the precision of the standalone NP extraction 74.8% vs. 63.5% while the recall improves by only 0.9% - 77.9% vs. 77% . The syntactic ambiguity is resolved using syntactic verb frames and heuristics, similar to the ones described in (Allen, 1995).</S>
			<S sid ="58" ssid = "48">The parser reaches its best performance for NPs (74.8% precision and 77.9% recall) and lowest for VPs (33% precision, 26% recall) and Ss (20% precision and 5.9% recall) (Table 3).</S>
			<S sid ="59" ssid = "49">The overall (measured on all the 600 syntactic phrases) precision and recall, are 64.9% and 60.5% respectively.</S>
			<S sid ="60" ssid = "50">This is about 20% lower, compared with certain English parsers (Murat and Charniak, 1995), which is due to the insufficient grammar coverage, as well as the lack of reliable disambiguation algorithm.</S>
			<S sid ="61" ssid = "51">However the bracket crossing accuracy is 80%, which is comparable to some probabilistic approaches.</S>
			<S sid ="62" ssid = "52">It should be noted that in our experiments we restricted the maximal number of arcs up to 35000 per sentence to speed up the parsing.</S>
	</SECTION>
	<SECTION title="Anaphora resolution in Bulgarian. " number = "3">
			<S sid ="63" ssid = "1">3.1 Adaptation of Mitkovs.</S>
			<S sid ="64" ssid = "2">knowledge-poor approach for Bulgarian The anaphora resolution module is implemented as the last stage of the language processing architecture (Figure 1).</S>
			<S sid ="65" ssid = "3">This module resolves third-person personal pronouns and is an adaptation of Mitkov’s robust, knowledge-poor multilingual approach (Mitkov, 1998) whose latest implementation by R. Evans is referred to as MARS 2 (Orasan et al., 2000).</S>
			<S sid ="66" ssid = "4">MARS does not make use of parsing, syntactic or semantic constraints; nor does it employ any form of non-linguistic knowledge.</S>
			<S sid ="67" ssid = "5">Instead, the approach relies on the efficiency of sentence splitting, part-of-speech tagging, noun phrase identification and the high performance of the antecedent indicators; knowledge is limited to a small noun phrase grammar, a list of (indicating) verbs and a set of antecedent indicators.</S>
			<S sid ="68" ssid = "6">The core of the approach lies in activating the antecedent indicators after filtering candidates (from the current and two preceding sentences) on the basis of gender and number agreement and the candidate with the highest composite score is proposed as antecedent 3.</S>
			<S sid ="69" ssid = "7">Before that, the text is pre-processed by a sentence split- ter which determines the sentence boundaries, a part-of-speech tagger which identifies the parts of speech and a simple phrasal grammar which detects the noun phrases.</S>
			<S sid ="70" ssid = "8">In the case of complex sentences, heuristic ’clause identification’ rules track the clause boundaries.</S>
			<S sid ="71" ssid = "9">LINGUA performs the pre-processing, needed as an input to the anaphora resolution algorithm: sentence, paragraph and clause splitters, NP grammar, part-of-speech tagger, 2 MARS stands for Mitkov’s Anaphora Resolution.</S>
			<S sid ="72" ssid = "10">System.</S>
			<S sid ="73" ssid = "11">3 For a detailed procedure how candidates are handled in the event of a tie, see (Mitkov, 1998).</S>
			<S sid ="74" ssid = "12">T e x t Pr on ou ns W e i g h t s e t St an da rd O pti mi se d B a s e l i n e m o st re c e nt S oft w ar e m a n u al s Su cc es s rat e 2 2 1 7 5 . 0 % 7 8 . 8 % 5 8 . 0 % Cr iti ca l su cc . rat e 7 0 . 0 % 7 3 . 0 % 5 4 . 0 % N on tri vi al su cc . rat e 7 0 . 0 % 7 8 . 8 % 5 8 . 0 % T o ur ist g ui d es Su cc es s rat e 1 1 6 6 8 . 1 % 6 9 . 8 % 6 5 . 0 % Cr iti ca l su cc . rat e 6 3 . 3 % 6 4 . 4 % 5 8 . 8 % N on tri vi al su cc . rat e 6 7 . 2 % 6 9 . 0 % 6 5 . 0 % Al l te xt s Su cc es s rat e 3 3 7 7 2 . 6 % 7 5 . 7 % 6 0 . 4 % Cr iti ca l su cc . rat e 6 7 . 7 % 7 0 . 0 % 5 5 . 7 % N on tri vi al su cc . rat e 7 2 . 3 % 7 5 . 4 % 6 0 . 4 % Table 1: Success rate of anaphora resolution section heading identification heuristics.</S>
			<S sid ="75" ssid = "13">Since one of the indicators that Mitkov’s approach uses is term preference, we manually developed4 a small term bank containing 80 terms from the domains of programming languages, word processing, computer hardware and operating systems 5.</S>
			<S sid ="76" ssid = "14">This bank additionally featured 240 phrases containing these terms.</S>
			<S sid ="77" ssid = "15">The antecedent indicators employed in MARS are classified as boosting (such indicators when pointing to a candidate, reward it with a bonus since there is a good probability of it being the antecedent) or impeding (such indicators penalise a candidate since it does not appear to have high chances of being the antecedent).</S>
			<S sid ="78" ssid = "16">The majority of indicators are genre- independent and are related to coherence phenomena (such as salience and distance) or to structural matches, whereas others are genre- specific (e.g. term preference, immediate reference, sequential instructions).</S>
			<S sid ="79" ssid = "17">Most of the indicators have been adopted in LINGUA without modification from the original English version (see (Mitkov, 1998) for more details).</S>
			<S sid ="80" ssid = "18">However, we have added 3 new indicators for Bulgarian: selectional restriction pattern, adjectival NPs and name preference.</S>
			<S sid ="81" ssid = "19">The boosting indicators are First Noun Phrases: A score of +1 is assigned to the first NP in a sentence, since it is deemed 4 This was done for experimental purposes.</S>
			<S sid ="82" ssid = "20">In future applications, we envisage the incorporation of automatic term extraction techniques.</S>
			<S sid ="83" ssid = "21">5 Note that MARS obtains terms automatically using.</S>
			<S sid ="84" ssid = "22">TF.IDF.</S>
			<S sid ="85" ssid = "23">to be a good candidate for the antecedent.</S>
			<S sid ="86" ssid = "24">Indicating verbs: A score of +1 is assigned to those NPs immediately following the verb which is a member of a previously defined set such as discuss, present, summarise etc. Lexical reiteration: A score of +2 is assigned those NPs repeated twice or more in the paragraph in which the pronoun appears, a score of +1 is assigned to those NP, repeated once in the paragraph.</S>
			<S sid ="87" ssid = "25">Section heading preference: A score of +1 is assigned to those NPs that also appear in the heading of the section.</S>
			<S sid ="88" ssid = "26">Collocation match: A score of +2 is assigned to those NPs that have an identical collocation pattern to the pronoun.</S>
			<S sid ="89" ssid = "27">Immediate reference: A score of +2 is assigned to those NPs appearing in constructions of the form “ ...V1 NP &lt; CB &gt; V2 it ” , where &lt; CB &gt; is a clause boundary.</S>
			<S sid ="90" ssid = "28">Sequential instructions: A score of +2 is applied to NPs in the N P1 position of constructions of the form: “To V1 N P1 ...</S>
			<S sid ="91" ssid = "29">To V2 it ...” Term preference: a score of +1 is applied to those NPs identified as representing domain terms.</S>
			<S sid ="92" ssid = "30">Selectional restriction pattern: a score of T ex t Pr on ou ns Int ra se nt en tia l: I n t e r s e n t e n t i a l a n a p h o r s Av er ag e n u m b e r o f c a n di d a t e s p e r a n a p h o r Av er ag e di st a n c e fr o m t h e a nt e c e d e n t in cl a u s e s Av er ag e di st a n c e fr o m t h e a nt e c e d e n t in s e nt e n c e s Av er ag e d is t a n c e fr o m t h e a n t e c e d e n t i n N P S of w ar e m a n u al s 22 1 10 6 : 11 5 3.</S>
			<S sid ="93" ssid = "31">29 1.</S>
			<S sid ="94" ssid = "32">10 0.</S>
			<S sid ="95" ssid = "33">62 3.</S>
			<S sid ="96" ssid = "34">30 T o ur ist g ui d es 11 6 17 : 99 3.</S>
			<S sid ="97" ssid = "35">35 1.</S>
			<S sid ="98" ssid = "36">74 0.</S>
			<S sid ="99" ssid = "37">98 5.</S>
			<S sid ="100" ssid = "38">13 Table 2: Complexity of the evaluation data +2 is applied to noun phrases occurring in collocation with the verb preceding or following the anaphor.</S>
			<S sid ="101" ssid = "39">This preference is different from the collocation match preference in that it operates on a wider range of ’selectional restriction patterns’ associated with a specific verb 6 and not on exact lexical matching.</S>
			<S sid ="102" ssid = "40">If the verb preceding or following the anaphor is identified to be in a legitimate collocation with a certain candidate for antecedent, that candidate is boosted accordingly.</S>
			<S sid ="103" ssid = "41">As an illustration, assume that ’Delete file’ has been identified as a legitimate collocation being a frequent expression in a domain specific corpus and consider the example ’Make sure you save the file in the new directory.</S>
			<S sid ="104" ssid = "42">You can now delete it.</S>
			<S sid ="105" ssid = "43">’ Whereas the ’standard’ collocation match will not be activated here, the selectional restriction pattern will identify ’delete file’ as an acceptable construction and will reward the candidate ’the file’.</S>
			<S sid ="106" ssid = "44">Adjectival NP: a score of +1 is applied to NPs which contain adjectives modifying the head.</S>
			<S sid ="107" ssid = "45">Empirical analysis shows that Bulgarian constructions of that type are more salient than NPs consisting simply of a noun.</S>
			<S sid ="108" ssid = "46">Recent experiments show that the success rate of the anaphora resolution is improved by 2.20%, using this indicator.</S>
			<S sid ="109" ssid = "47">It would be interesting to establish if this indicator is applicable for English.</S>
			<S sid ="110" ssid = "48">Name preference: a score +2 is applied to names of entities (person, organisation, product 6 At the moment these patterns are extracted from a list of frequent expressions involving the verb and domain terms in a purpose-built term bank but in generally they are automatically collected from large domain- specific corpora.</S>
			<S sid ="111" ssid = "49">n a m e s ) . The impeding indicator is Prepositional Noun Phrases: NPs appearing in prepositional phrases are assigned a score of -1.</S>
			<S sid ="112" ssid = "50">Two indicators, Referential distance and Indefiniteness may increase or decrease a candidate’s score.</S>
			<S sid ="113" ssid = "51">Referential distance gives scores of +2 and +1 for the NPs in the same and in the previous sentence respectively, and -1 for the NPs two sentences back.</S>
			<S sid ="114" ssid = "52">This indicator has strong influence on the anaphora resolution performance, especially in the genre of technical manuals.</S>
			<S sid ="115" ssid = "53">Experiments show that its switching off can decrease the success rate by 26% . Indefiniteness assigns a score of -1 to indefinite NPs, 0 to the definite (not full article) and +1 to these which are definite, containing the definite ’full’ article in Bulgarian.</S>
	</SECTION>
	<SECTION title="Evaluation  of the. " number = "4">
			<S sid ="116" ssid = "1">anaphora resolution module The precision of anaphora resolution measured on corpus of software manuals containing 221 anaphors, is 75.0%.</S>
			<S sid ="117" ssid = "2">Given that the anaphora resolution system operates in a fully automatic mode, this result could be considered very satisfactory.</S>
			<S sid ="118" ssid = "3">It should be noted that some of the errors arise from inaccuracy of the pre- processing modules such as clause segmentation and NP extraction (see Table 3).</S>
			<S sid ="119" ssid = "4">We also evaluated the anaphora resolution system in the genre of tourist texts.</S>
			<S sid ="120" ssid = "5">As expected, the success rate dropped to 68.1% which, however, can still be regarded as a very La ng ua ge pr oc es si ng m o d ul e Pr ec isi on % R ec all % Ev al ua tio n da ta se nt en ce s pli tt er 9 2 . 0 0 9 9 . 0 0 1 9 0 s e n t e n c e s pa ra gr ap h s pli tt er 9 4 . 0 0 9 8 . 0 0 2 6 8 p a r a g r a p h s cl au se ch u nk er 9 3 . 5 0 9 3 . 1 0 2 3 2 c l a u s e s P O S ta g g er 9 5 . 0 0 9 5 . 0 0 3 0 3 P O S t a g s N P ex tr ac to r 6 3 . 5 0 7 7 . 0 0 3 5 2 N P s ch ar t p ar si n g N P A P A d v P V P P P S T o t a l Br ac ke t cr os si ng ac cu ra cy 7 4 . 8 4 6 5 . 1 5 3 7 . 1 4 3 3 . 3 3 7 0 . 0 0 2 0 . 0 0 6 4 . 9 3 8 0 . 3 3 7 7 . 8 9 6 7 . 1 9 5 0 . 0 0 2 6 . 3 9 6 0 . 2 1 5 . 8 8 6 0 . 5 0 2 9 4 N P s 6 4 A P s 2 6 A d v P s 7 2 V P s 9 3 P P s 5 1 S s 6 0 0 p h r a s e s 6 0 0 p h r a s e s A n a p h or a re so lut io n 7 2 . 6 0 3 3 7 a n a p h o r s Table 3: Summary of LINGUA performance good result, given the fact that neither manual pre-editing of the input text, nor any post- editing of the output of the pre-processing tools were undertaken.</S>
			<S sid ="121" ssid = "6">The main reason for the decline of performance is that some of the original indicators such as term preference, immediate reference and sequential instructions of the knowledge-poor approach, are genre specific.</S>
			<S sid ="122" ssid = "7">The software manuals corpus featured 221 anaphoric third person pronouns, whereas the tourist text consisted of 116 such pronouns.</S>
			<S sid ="123" ssid = "8">For our evaluation we used the measures success rate, critical success rate and nontrivial success rate (Mitkov, 2001).</S>
			<S sid ="124" ssid = "9">Success rate is the ratio SR = AC/A, where AC is the number of correctly resolved and A is the number of all anaphors.</S>
			<S sid ="125" ssid = "10">Critical success rate is the success rate for the anaphors which have more than one candidates for antecedent after the gender and number agreement filter is applied.</S>
			<S sid ="126" ssid = "11">Nontrivial success rate is calculated for those anaphors which have more than one candidates for antecedent before the gender and number agreement is applied.</S>
			<S sid ="127" ssid = "12">We also compared our approach with the typical baseline model Baseline most recent which takes as antecedent the most recent NP matching the anaphor in gender and number.</S>
			<S sid ="128" ssid = "13">The results are shown in the Table 1.</S>
			<S sid ="129" ssid = "14">These results show that the performance of LINGUA in anaphora resolution is comparable to that of MARS (Orasan et al., 2000).</S>
			<S sid ="130" ssid = "15">An opti mised version 7 of the indicator weights scored a success rate of 69,8% on the tourist guide texts, thus yielding an improvement of 6,1%.</S>
			<S sid ="131" ssid = "16">Table 2 illustrates the complexity of the evaluation data by providing simple quantifying measures such as average number of candidates per anaphor, average distance from the anaphor to the antecedent in terms of sentences, clauses, intervening NPs, number of intrasentential anaphors as opposed to intersentential ones etc.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="132" ssid = "1">This paper outlines the development of the first robust and shallow text processing framework in Bulgarian LINGUA which includes modules for tokenisation, sentence splitting, paragraph segmentation, part-of-speech tagging, clause chunking, noun phrases extraction and anaphora resolution (Figure 1).</S>
			<S sid ="133" ssid = "2">Apart from the module on pronoun resolution which was adapted from Mitkov’s knowledge-poor approach for English and the incorporation of BULMORPH in the part-of-speech tagger, all modules were specially built for LINGUA.</S>
			<S sid ="134" ssid = "3">The evaluation shows promising results for each of the modules.</S>
			<S sid ="135" ssid = "4">7 The optimisation made use of genetic algorithms in a manner similar to that described in (Orasan et al., 2000).</S>
	</SECTION>
</PAPER>
