<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The paper aims at a deeper understanding of several well-known algorithms and proposes ways to optimize them.</S>
		<S sid ="2" ssid = "2">It describes and discusses factors and strategies of factor interaction used in the algorithms.</S>
		<S sid ="3" ssid = "3">The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al., 1997).</S>
		<S sid ="4" ssid = "4">A common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">In recent years, a variety of approaches to pronoun resolution have been proposed.</S>
			<S sid ="6" ssid = "6">Some of them are based on centering theory (Strube, 1998; Strube and Hahn, 1999; Tetreault, 2001), others on Machine Learning (Aone and Bennett, 1995; Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003).</S>
			<S sid ="7" ssid = "7">They supplement older heuristic approaches (Hobbs, 1978; Lappin and Leass, 1994).</S>
			<S sid ="8" ssid = "8">Unfortunately, most of these approaches were evaluated on different corpora making different assumptions so that direct comparison is not possible.</S>
			<S sid ="9" ssid = "9">Appreciation of the new insights is quite hard.</S>
			<S sid ="10" ssid = "10">Evaluation differs not only with regard to size and genre of corpora but also along the following lines.</S>
			<S sid ="11" ssid = "11">Scope of application: Some approaches only deal with personal and possessive pronouns (centering and heuristic), while others consider coreference links in general (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003).</S>
			<S sid ="12" ssid = "12">A drawback of this latter view is that it mixes problems on different lev els of difficulty.</S>
			<S sid ="13" ssid = "13">It remains unclear how much of the success is due to the virtues of the approach and how much is due to the distribution of hard and easy problems in the corpus.</S>
			<S sid ="14" ssid = "14">In this paper, we will only deal with coreferential pronouns (i.e. possessive, demonstrative, and third person pronouns).</S>
			<S sid ="15" ssid = "15">My thanks go to Melvin Wurster for help in annotation and to Ciprian Gerstenberger for discussion.</S>
			<S sid ="16" ssid = "16">Quality of linguistic input: Some proposals were evaluated on hand annotated (Strube and Hahn, 1999) or tree bank input (Ge et al., 1998; Tetreault, 2001).</S>
			<S sid ="17" ssid = "17">Other proposals provide a more realistic picture in that they work as a backend to a parser (Lappin and Leass, 1994) or noun chunker (Mitkov, 1998; Soon et al., 2001; Ng and Cardie, 2002)).</S>
			<S sid ="18" ssid = "18">In evaluation of applications presupposing parsing, it is helpful to separate errors due to parsing from intrinsic errors.</S>
			<S sid ="19" ssid = "19">On the other hand, one would also like to gauge the end-to-end performance of a system.</S>
			<S sid ="20" ssid = "20">Thus we will provide performance figures for both ideal (hand-annotated) input and realistic (automatically generated) input.</S>
			<S sid ="21" ssid = "21">Language: Most approaches were evaluated on English where large resources are available, both in terms of pre-annotated data (MUC6 and MUC7 data) and lexical information (WordNet).</S>
			<S sid ="22" ssid = "22">This paper deals with German.</S>
			<S sid ="23" ssid = "23">Arguably, the free word-order of German arguably leads to a clearer distinction between grammatical function, surface order, and information status (Strube and Hahn, 1999).</S>
			<S sid ="24" ssid = "24">The paper is organized as follows.</S>
			<S sid ="25" ssid = "25">Section 2 describes the evaluation corpus.</S>
			<S sid ="26" ssid = "26">Section 3 describes several factors relevant to pronoun resolution.</S>
			<S sid ="27" ssid = "27">It assesses these factors against the corpus, measuring their precision and restrictiveness.</S>
			<S sid ="28" ssid = "28">Section 4 describes and evaluates six algorithms on the basis of these factors.</S>
			<S sid ="29" ssid = "29">It also captures the algorithms as para- metric systems and proposes parameter settings optimal on the evaluation data.</S>
			<S sid ="30" ssid = "30">Section 5 concludes.</S>
	</SECTION>
	<SECTION title="Evaluation Corpus. " number = "2">
			<S sid ="31" ssid = "1">We chose as an evaluation base the NEGRA tree bank, which contains about 350,000 tokens of German newspaper text.</S>
			<S sid ="32" ssid = "2">The same corpus was also processed with a finite-state parser, performing at 80% dependency f-score (Schiehlen, 2003).</S>
			<S sid ="33" ssid = "3">All personal pronouns (PPER), possessive pronouns (PPOSAT), and demonstrative pronouns (PDS) in Negra were annotated in a format geared to the MUC7 guidelines (MUC7, 1997).</S>
			<S sid ="34" ssid = "4">Proper names were annotated automatically by a named entity recognizer.</S>
			<S sid ="35" ssid = "5">In a small portion of the corpus (6.7%), all coreference links were annotated.</S>
			<S sid ="36" ssid = "6">Thus the size of the annotated data (3,115 personal pronouns1 , 2,198 possessive pronouns, 928 demonstrative pronouns) compares favourably with the size of evaluation data in other proposals (619 German pronouns in (Strube and Hahn, 1999), 2,477 English pronouns in (Ge et al., 1998), about 5,400 English coreferential expressions in (Ng and Cardie, 2002)).</S>
			<S sid ="37" ssid = "7">In the experiments, systems only looked for single NP antecedents.</S>
			<S sid ="38" ssid = "8">Hence, propositional or predicative antecedents (8.4% of the pronouns annotated) and split antecedents (0.2%) were inaccessible, which reduced optimal success rate to 91.4%.</S>
	</SECTION>
	<SECTION title="Factors in Pronoun Resolution. " number = "3">
			<S sid ="39" ssid = "1">Pronoun resolution is conditioned by a wide range of factors.</S>
			<S sid ="40" ssid = "2">Two questions arise: Which factors are the most effective?</S>
			<S sid ="41" ssid = "3">How is interaction of the factors modelled?</S>
			<S sid ="42" ssid = "4">The present section deals with the first question, while the second question is postponed to section 4.</S>
			<S sid ="43" ssid = "5">Many approaches distinguish two classes of resolution factors: filters and preferences.</S>
			<S sid ="44" ssid = "6">Filters express linguistic rules, while preferences are merely tendencies in interpretation.</S>
			<S sid ="45" ssid = "7">Logically, filters are monotonic inferences that select a certain subset of possible antecedents, while preferences are non- monotonic inferences that partition the set of antecedents and impose an order on the cells.</S>
			<S sid ="46" ssid = "8">In the sequel, factors proposed in the literature are discussed and their value is appraised on evaluation data.</S>
			<S sid ="47" ssid = "9">Every factor narrows the set of antecedents and potentially discards correct antecedents.</S>
			<S sid ="48" ssid = "10">Table 1 lists both the success rate maximally achievable (broken down according to different types of pronouns) and the average number of antecedents remaining after applying each factor.</S>
			<S sid ="49" ssid = "11">Figures are also given for parsed input.</S>
			<S sid ="50" ssid = "12">Preferences are evaluated on filtered sets of antecedents.</S>
			<S sid ="51" ssid = "13">3.1 Filters.</S>
			<S sid ="52" ssid = "14">Agreement.</S>
			<S sid ="53" ssid = "15">An important filter comes from morphology: Agreement in gender and number is generally regarded as a prerequisite for coreference.</S>
			<S sid ="54" ssid = "16">Exceptions are existant but few (2.5%): abstract pronouns (such as that in English) referring to non- neuter or plural NPs, plural pronouns co-referring with singular collective NPs (Ge et al., 1998), antecedent and anaphor matching in natural gender 1 Here, we only count anaphoric pronouns, i.e. third person pronouns not used expletively.</S>
			<S sid ="55" ssid = "17">rather than grammatical gender.</S>
			<S sid ="56" ssid = "18">All in all, a maximal performance of 88.9% is maintained.</S>
			<S sid ="57" ssid = "19">The filter is very restrictive, and cuts the set of possible antecedents in half.</S>
			<S sid ="58" ssid = "20">See Table 1 for details.</S>
			<S sid ="59" ssid = "21">Binding.</S>
			<S sid ="60" ssid = "22">Binding constraints have been in the focus of linguistic research for more than thirty years.</S>
			<S sid ="61" ssid = "23">They provide restrictions on coindexation of pronouns with clause siblings, and therefore can only be applied with systems that determine clause boundaries, i.e. parsers (Mitkov, 1998).</S>
			<S sid ="62" ssid = "24">Empirically, binding constraints are rules without exceptions, hence they do not lead to any loss in achievable performance.</S>
			<S sid ="63" ssid = "25">The downside is that their restrictive power is quite bad as well (0.3% in our corpus, cf.</S>
			<S sid ="64" ssid = "26">Table 1).</S>
			<S sid ="65" ssid = "27">Sortal Constraints.</S>
			<S sid ="66" ssid = "28">More controversial are sortal constraints.</S>
			<S sid ="67" ssid = "29">Intuitively, they also provide a hard filter: The correct antecedent must fit into the environment of the pronoun (Carbonell and Brown, 1988).</S>
			<S sid ="68" ssid = "30">In general, however, the required knowledge sources are lacking, so they must be hand-coded and can only be applied in restricted domains (Strube and Hahn, 1999).</S>
			<S sid ="69" ssid = "31">Selectional restrictions can also be modelled by collocational data extracted by a parser, which have, however, only a very small impact on overall performance (Kehler et al., 2004).</S>
			<S sid ="70" ssid = "32">We will neglect sortal constraints in this paper.</S>
			<S sid ="71" ssid = "33">3.2 Preferences.</S>
			<S sid ="72" ssid = "34">Preferences can be classified according to their requirements on linguistic processing.</S>
			<S sid ="73" ssid = "35">Sentence Re- cency and Surface Order can be read directly off the surface.</S>
			<S sid ="74" ssid = "36">NP Form presupposes at least tagging.</S>
			<S sid ="75" ssid = "37">A range of preferences (Grammatical Roles, Role Parallelism, Depth of Embedding, Common Path), as well as all filters, presuppose full syntactic analysis.</S>
			<S sid ="76" ssid = "38">Mention Count and Information Status are based on previous decisions of the anaphora resolution module.</S>
			<S sid ="77" ssid = "39">Sentence Recency (SR).</S>
			<S sid ="78" ssid = "40">The most important criterion in pronoun resolution (Lappin and Leass, 1994) is the textual distance between anaphor and antecedent measured in sentences.</S>
			<S sid ="79" ssid = "41">Lappin and Le ass (1994) motivate this preference as a dynamic expression of the attentional state of the human hearer:Memory capability for storage of discourse refer ents degrades rapidly.</S>
			<S sid ="80" ssid = "42">Several implementations are possible.</S>
			<S sid ="81" ssid = "43">Perhaps most obvious is the strategy implicit in Lappin and Leass (1994)’s algorithm: The antecedent issearched in a sentence that is as recent as possi ble, beginning with the already uttered part of the current sentence, continuing in the last sentence, in the one but last sentence, and so forth.</S>
			<S sid ="82" ssid = "44">In case no Co nst rai nt Uppe r Boun d n u m b e r o f a nt e c. P a r s e r tot al PP E R PP O S A T P D S U pp er B an tec . no V P 91 .6 9 8 . 4 1 0 0 . 0 48 .5 1 2 3 . 2 8 5 . 5 12 8.</S>
			<S sid ="83" ssid = "45">4 no spl it 91 .4 9 8 . 3 1 0 0 . 0 47 .8 1 2 3 . 2 ag re e m en t 88 .9 9 6 . 8 9 9 . 5 37 .6 5 3 . 0 7 9 . 1 6 1 . 8 bi nd in g 88 .9 5 2 . 7 7 8 . 7 6 1 . 4 se nt en ce re ce nc y SR 78 .8 8 4 . 6 9 0 . 2 32 .3 2 . 4 6 6 . 2 2 . 7 gr a m m ati cal rol e GR 74 .0 82 .3 2 8 7 . 9 13 .0 1 4 . 5 5 1 . 2 9 . 0 rol e pa ral lel is m RP 64 .3 7 7 . 4 – 20 .0 1 2 . 5 4 7 . 0 1 0 . 3 su rfa ce or de r LR 53 .5 6 2 . 8 5 6 . 6 15 .3 1 4 2 . 6 1 su rfa ce or de r RL 45 .9 4 5 . 9 5 5 . 7 22 .7 1 3 5 . 2 1 de pt h of e m be dd in g DE 51 .6 5 1 . 3 6 7 . 7 14 .1 2 . 4 4 1 . 7 4 . 0 co m m on pa th CP 51 .7 5 2 . 3 6 4 . 2 19 .9 5 . 3 4 6 . 8 1 1 . 3 eq ui va le nc e cla ss es EQ 63 .6 6 7 . 5 7 8 . 4 15 .7 1 . 3 5 1 . 3 1 . 5 m en tio n co un t MC 32 .9 4 0 . 3 3 4 . 0 4 . 6 5 . 5 3 5 . 7 7 . 1 inf or m ati on sta tus IS 65 .3 7 1 . 1 7 7 . 4 16 .7 1 6 . 6 4 9 . 7 1 6 . 3 N P for m NF 42 .4 4 9 . 9 4 4 . 4 12 .8 7 . 4 2 0 . 6 8 . 3 N P for m (pr on ou n) NP 73 .7 8 2 . 4 7 9 . 8 30 .2 2 9 . 7 5 9 . 7 3 6 . 6 Table 1: Effect of Factors antecedent is found in the previous context, subsequent sentences are inspected (cataphora), also ordered by proximity to the pronoun.</S>
			<S sid ="84" ssid = "46">10000 all PPER age only 2.4 such antecedents.</S>
			<S sid ="85" ssid = "47">However, the benefit also comes at a cost: The upper ceiling of performance is lowered to 82.0% in our corpus: In many cases an incorrect antecedent is found in a more recent sentence.Similarly, we can assess other strategies of sen 1000 100 10 1 P P O S A T P D S -2-1 0 1 2 3 4 5 6 7 8 9 12 19 Fi gu re 1: Se nte nc e Re ce nc y tence orderi ng that have been propo sed in the litera ture.</S>
			<S sid ="86" ssid = "48">Hard core center ing appro aches only deal with the last senten ce (Bren nan et al., 1987).</S>
			<S sid ="87" ssid = "49">In Negra, these appro aches can conse quentl y have at most a succes s rate of 44.2% . Perfor mance is partic ularly low with posses sive prono uns which often only have antece dents in the curren t senten ce.</S>
			<S sid ="88" ssid = "50">Strube (1998) ’s centeri ng appro ach (whos e senten ce orderi ng is designate d as SR2 in Table 2) also deals with and even prefer s intrase ntenti al anaph ora, which raises the upper limit to a more accept able 80.2% . Strube and Hahn (1999) extend the contex t to more than the last senten ce, but switch prefer ence order betwe en the last and the curren t senten ce so that an antece dent is deter mined in the last senten ce, whene ver possiFigure 1 shows the absolute frequencies of sen tence recency values when only the most recent antecedent (in the order just stated) is considered.</S>
			<S sid ="89" ssid = "51">In Negra, 55.3% of all pronominal anaphora can be resolved intrasententially, and 97.6% within the last three sentences.</S>
			<S sid ="90" ssid = "52">Since only 1.6% of all pronouns are cataphoric, it seems reasonable to neglect cat- aphora, as is mostly done (Strube and Hahn, 1999; Hobbs, 1978).</S>
			<S sid ="91" ssid = "53">Table 1 underscores the virtues of Sentence Recency: In the most recent sentence with antecedents satisfying the filters, there are on aver ble.</S>
			<S sid ="92" ssid = "54">In Negra, this ordering imposes an upper limit of 51.2%.</S>
			<S sid ="93" ssid = "55">Grammatical Roles (GR).</S>
			<S sid ="94" ssid = "56">Another important factor in pronoun resolution is the grammatical role of the antecedent.</S>
			<S sid ="95" ssid = "57">The role hierarchy used in centering (Brennan et al., 1987; Grosz et al., 1995) ranks subjects over direct objects over indirect objects over others.</S>
			<S sid ="96" ssid = "58">Lappin and Leass (1994) provide a more elaborate model which ranks NP complements and NP adjuncts lowest.</S>
			<S sid ="97" ssid = "59">Two other distinctions in their model express a preference of rhematic2 over thematic arguments: Existential subjects, which follow the verb, rank very high, between subjects and direct objects.</S>
			<S sid ="98" ssid = "60">Topic adjuncts in pre-subject position separated by a comma rank very low, between adjuncts and NP complements.</S>
			<S sid ="99" ssid = "61">Both positions are not clearly demarcated in German.</S>
			<S sid ="100" ssid = "62">When the Lap- pin&amp;Leass hierarchy is adopted to German without changes, a small drop in performance results as compared with the obliqueness hierarchy used in centering.</S>
			<S sid ="101" ssid = "63">So we will use the centering hierarchy.</S>
			<S sid ="102" ssid = "64">Table 1 shows the effect of the role-based preference on our data.</S>
			<S sid ="103" ssid = "65">The factor is both less restrictive and less precise than sentence recency.</S>
			<S sid ="104" ssid = "66">The definition of a grammatical role hierarchy is more involved in case of automatically derived input, as the parser cannot always decide on the grammatical role (determining grammatical roles in German may require world knowledge).</S>
			<S sid ="105" ssid = "67">It proposes a syntactically preferred role, however, which we will adopt.</S>
			<S sid ="106" ssid = "68">Role Parallelism (RP).</S>
			<S sid ="107" ssid = "69">Carbonell and Brown (1988) argue that pronouns prefer antecedents in the same grammatical roles.</S>
			<S sid ="108" ssid = "70">Lappin and Leass (1994) also adopt such a principle.</S>
			<S sid ="109" ssid = "71">The factor is, however, not applicable to possessive pronouns.</S>
			<S sid ="110" ssid = "72">Again, role ambiguities make this factor slightly problematic.</S>
			<S sid ="111" ssid = "73">Several approaches are conceivable: Antecedent and pronoun are required to have a common role in one reading (weak match).</S>
			<S sid ="112" ssid = "74">Antecedent and pronoun are required to have the same role in the reading preferred by surface order (strong match).</S>
			<S sid ="113" ssid = "75">Antecedent and pronoun must display the same role ambiguity (strongest match).</S>
			<S sid ="114" ssid = "76">Weak match restricted performance to 49.9% with 12.1 antecedents on average.</S>
			<S sid ="115" ssid = "77">Strong match gave an upper limit of 47.0% but with only 10.3 antecedents on average.</S>
			<S sid ="116" ssid = "78">Strongest match lowered the upper limit to 43.1% but yielded only 9.3 antecedents.</S>
			<S sid ="117" ssid = "79">In interaction, strong match performed best, so we adopt it.</S>
			<S sid ="118" ssid = "80">Surface Order (LR, RL).</S>
			<S sid ="119" ssid = "81">Surface Order is usually used to bring down the number of available antecedents to one, since it is the only factor that produces a unique discourse referent.</S>
			<S sid ="120" ssid = "82">There is less consensus on the preference order: (sentence-wise) left-to-right (Hobbs, 1978; Strube, 1998; Strube and Hahn, 1999; Tetreault, 1999) or right-to-left (recency) (Lappin and Leass, 1994).</S>
			<S sid ="121" ssid = "83">Furthermore, something has to be said about antecedents which embed other antecedents (e.g. conjoined NPs and their conjuncts).</S>
			<S sid ="122" ssid = "84">We registered performance gains 2 Carbonell and Brown (1988) also argue that clefted or fronted arguments should be preferred.</S>
			<S sid ="123" ssid = "85">(of up to 3%) by ranking embedding antecedents higher than embedded ones (Tetreault, 2001).</S>
			<S sid ="124" ssid = "86">Left-to-right order is often used as a surrogate for grammatical role hierarchy in English.</S>
			<S sid ="125" ssid = "87">The most notable exception to this equivalence are fronting constructions, where grammatical roles outperform surface order (Tetreault, 2001).</S>
			<S sid ="126" ssid = "88">A comparison of the lines for grammatical roles and for surface order in Table 1 shows that the same is true in German.</S>
			<S sid ="127" ssid = "89">Left-to-right order performs better (upper limit 56.8%) than right-to-left order (upper limit 49.2%).</S>
			<S sid ="128" ssid = "90">The gain is largely due to personal pronouns; demonstrative pronouns are better modelled by right-to-left order.</S>
			<S sid ="129" ssid = "91">It is well-known that German demonstrative pronouns contrast with personal pronouns in that they function as topic-shifting devices.</S>
			<S sid ="130" ssid = "92">Another effect of this phenomenon is the poor performance of the role preferences in connection with demonstrative pronouns.</S>
			<S sid ="131" ssid = "93">Depth of Embedding (DE).</S>
			<S sid ="132" ssid = "94">A prominent factor in Hobbs (1978)’s algorithm is the level of phrasal embedding: Hobbs’s algorithm performs a breadth- first search, so antecedents at higher levels of embedding are preferred.</S>
			<S sid ="133" ssid = "95">Common Path (CP).</S>
			<S sid ="134" ssid = "96">The syntactic version of Hobbs (1978)’s algorithm also assumes maximization of the common path between antecedents and anaphors as measured in NP and S nodes.</S>
			<S sid ="135" ssid = "97">Accordingly, intrasentential antecedents that are syntactically nearer to the pronoun are preferred.</S>
			<S sid ="136" ssid = "98">The factor only applies to intrasentential anaphora.</S>
			<S sid ="137" ssid = "99">The anaphora resolution module itself generates potentially useful information when processing a text.</S>
			<S sid ="138" ssid = "100">Arguably, discourse entities that have been often referred to in the previous context are topical and more likely to serve as antecedents again.</S>
			<S sid ="139" ssid = "101">This principle can be captured in different ways.</S>
			<S sid ="140" ssid = "102">Equivalence Classes (EQ).</S>
			<S sid ="141" ssid = "103">Lappin and Leass (1994) make use of a mechanism based on equivalence classes of discourse referents which manages the attentional properties of the individual entities referred to.</S>
			<S sid ="142" ssid = "104">The mechanism stores and provides information on how recently and in which grammatical role the entities were realized in the discourse.</S>
			<S sid ="143" ssid = "105">The net effect of the storage mechanism is that discourse entities are preferred as antecedents if they recently came up in the discourse.</S>
			<S sid ="144" ssid = "106">But the mechanism also integrates the preferences Role Hierarchy and Role Parallelism.</S>
			<S sid ="145" ssid = "107">Hence, it is one of the best- performing factors on our data.</S>
			<S sid ="146" ssid = "108">Since the equivalence class scheme is tightly integrated in the parser, the problem of ideal anaphora resolution data does not arise.</S>
			<S sid ="147" ssid = "109">Mention Count (MC).</S>
			<S sid ="148" ssid = "110">Ge et al.</S>
			<S sid ="149" ssid = "111">(1998) try to factorize the same principle by counting the number of times a discourse entities has been mentioned in the discourse already.</S>
			<S sid ="150" ssid = "112">However, they do not only train but also test on the manually annotated counts, and hence presuppose an optimal anaphora resolution system.</S>
			<S sid ="151" ssid = "113">In our implementation, we did not bother with intrasentential mention count, which depends on the exact traversal.</S>
			<S sid ="152" ssid = "114">Rather, mention count was computed only from previous sentences.</S>
			<S sid ="153" ssid = "115">Information Status (IS).</S>
			<S sid ="154" ssid = "116">Strube (1998) and Strube and Hahn (1999) argue that the information status of an antecedent is more important than the grammatical role in which it occurs.</S>
			<S sid ="155" ssid = "117">They distinguish three levels of information status: entities known to the hearer (as expressed by coreferential NPs, unmodified proper names, appositions, relative pronouns, and NPs in titles), entities related to such hearer-old entities (either overtly via modifiers or by bridging), and entities new to the hearer.</S>
			<S sid ="156" ssid = "118">Like (Ge et al., 1998), Strube (1998) evaluates on ideal hand annotated data.</S>
			<S sid ="157" ssid = "119">NP Form (NF, NP).</S>
			<S sid ="158" ssid = "120">A cheap way to model information status is to consider the form of an antecedent (Tetreault, 2001; Soon et al., 2001; Strube and Müller, 2003).</S>
			<S sid ="159" ssid = "121">Personal and demonstrative pronouns are necessarily context-dependent, and proper nouns are nearly always known to the hearer.</S>
			<S sid ="160" ssid = "122">Definite NPs may be coreferential or interpreted by bridging, while indefinite NPs are in their vast majority new to the hearer.</S>
			<S sid ="161" ssid = "123">We considered two proposals for orderings of form: preferring pronouns and proper names over other NPs over indefinite NPs (Tetreault, 2001) (NF) or preferring pronouns over all other NPs (Tetreault, 2001) (NP).</S>
	</SECTION>
	<SECTION title="Algorithms and Evaluation. " number = "4">
			<S sid ="162" ssid = "1">In this section, we consider the individual approaches in more detail, in particular we will look at their choice of factors and their strategy to model factor interaction.</S>
			<S sid ="163" ssid = "2">According to interaction potential, we distinguish three classes of approaches: Serialization, Weighting, and Machine Learning.</S>
			<S sid ="164" ssid = "3">We re-implemented some of the algorithms described in the literature and evaluated them on syntactically ideal and realistic German3 input.</S>
			<S sid ="165" ssid = "4">Evaluation results are listed in Table 2.</S>
			<S sid ="166" ssid = "5">With the ideal treebank input, we also assumed ideal input for the factors dependent on previous 3 A reviewer points out that most of the algorithms were proposed for English, where they most likely perform better.</S>
			<S sid ="167" ssid = "6">However, the algorithms also incorporate a theory of saliency, which should be language-independent.</S>
			<S sid ="168" ssid = "7">anaphora resolution results.</S>
			<S sid ="169" ssid = "8">With realistic parsed input, we fed the results of the actual system back into the computation of such factors.</S>
			<S sid ="170" ssid = "9">4.1 Serialization Approaches.</S>
			<S sid ="171" ssid = "10">Algorithmical approaches first apply filters unconditionally; possible exceptions are deemed non- existant or negligible.</S>
			<S sid ="172" ssid = "11">With regard to interaction of preferences, many algorithms (Hobbs, 1978; Strube, 1998; Tetreault, 2001) subscribe to a scheme, which, though completely rigid, performs surprisingly well: The chosen preferences are applied one after the other in a certain predefined order.</S>
			<S sid ="173" ssid = "12">Application of a preference consists in selecting those of the antecedents still available that are ranked highest in the preference order.</S>
			<S sid ="174" ssid = "13">Hobbs (1978)’s algorithm essentially is a concatenation of the preferences Sentence Recency (without cataphora), Common Path, Depth of Embedding, and left-to-right Surface Order.</S>
			<S sid ="175" ssid = "14">It also implements the binding constraints by disallowing sibling to the anaphor in a clause or NP as antecedents.</S>
			<S sid ="176" ssid = "15">Like Lappin and Leass (1994), we replaced this implementation by our own mechanism to check binding constraints, which raised the success rate.</S>
			<S sid ="177" ssid = "16">The Left-Right Centering algorithm of Tetreault (1999) is similar to Hobbs’s algorithm, and is composed of the preferences Sentence Recency (without cataphora), Depth of Embedding, and left-to-right Surface Order.</S>
			<S sid ="178" ssid = "17">Since it is a centering approach, it only inspects the current and last sentence.</S>
			<S sid ="179" ssid = "18">Strube (1998)’s S-list algorithm is also restricted to the current and last sentence.</S>
			<S sid ="180" ssid = "19">Predicative complements and NPs in direct speech are excluded as antecedents.</S>
			<S sid ="181" ssid = "20">The primary ordering criterion is Information Status, followed by Sentence Recency (without cataphora) and left-to-right Surface Order.</S>
			<S sid ="182" ssid = "21">Since serialization provides a quite rigid frame, we conducted an experiment to find the best performing combination of pronoun resolution factors on the treebank and the best combination on the parsed input.</S>
			<S sid ="183" ssid = "22">For this purpose, we checked all permutations of preferences and subtracted preferences from the best-performing combinations until performance degraded (greedy descent).</S>
			<S sid ="184" ssid = "23">Greedy descent outperformed hill-climbing.</S>
			<S sid ="185" ssid = "24">The completely annotated 6.7% of the corpus were used as development set, the rest as test set.</S>
			<S sid ="186" ssid = "25">4.2 Weighting Approaches.</S>
			<S sid ="187" ssid = "26">Compared with the serialization approaches, the algorithm of Lappin and Leass (1994) is more sophisticated: It uses a system of hand-selected weights to control interaction among preferences, so that in principle the order of preference application can Al go rit h m D e f i n i t i o n F-Scores – treebankF Sc or e P ar se r tot al PP ER PP OS AT P D S (H ob bs, 19 78 ) ( T e t r e a u l t , 1 9 9 9 ) ( S t r u b e , 1 9 9 8 ) op ti m al al go r.</S>
			<S sid ="188" ssid = "27">(tr ee ba nk ) op ti m al al go r.</S>
			<S sid ="189" ssid = "28">(p ars ed ) S R C P D E L R S R 2 D E L R I S S R 2 L R S R C P I S D E M C R P G R R L S R C P G R I S D E L R 59 .9 57 .0 57 .9 70 .4 67 .7 6 5.</S>
			<S sid ="190" ssid = "29">1 6 4.</S>
			<S sid ="191" ssid = "30">1 6 5.</S>
			<S sid ="192" ssid = "31">9 7 5.</S>
			<S sid ="193" ssid = "32">6 7 4.</S>
			<S sid ="194" ssid = "33">3 7 0 . 5 6 1 . 9 6 3 . 7 8 2 . 0 8 2 . 0 17 .4 17 .2 12 .0 22 .7 10 .6 4 5 . 4 4 3 . 3 3 9 . 1 4 3 . 7 5 0 . 6 (L ap pi n an d Le as s, 19 94 ) E Q S R R L 65 .4 7 1.</S>
			<S sid ="195" ssid = "34">0 7 8 . 0 16 .6 5 0 . 8 (G e et al. , 19 98 ) (S oo n et al. , 20 01 ) op ti m al al go r.</S>
			<S sid ="196" ssid = "35">(C 4.</S>
			<S sid ="197" ssid = "36">5) H ob bs +M C (S R+ N P) R L (S R/ R L+ G R+ N F/ I S) R L 43 .4 24 .8 71 .1 4 5.</S>
			<S sid ="198" ssid = "37">7 3 0.</S>
			<S sid ="199" ssid = "38">8 7 8.</S>
			<S sid ="200" ssid = "39">2 5 3 . 6 2 3 . 6 7 9 . 0 1 2.</S>
			<S sid ="201" ssid = "40">1 0 . 0 9 . 8 3 6 . 3 2 6 . 8 5 1 . 7 Table 2: Performance of Algorithms switch under different input data.</S>
			<S sid ="202" ssid = "41">In the actual realization, however, the weights of factors lie so much apart that in the majority of cases interaction boils down to serialization.</S>
			<S sid ="203" ssid = "42">The weighting scheme includes Sentence Recency, Grammatical Roles, Role Parallelism, on the basis of the equivalence class approach described in section 3.2.</S>
			<S sid ="204" ssid = "43">Final choice of antecedents is relegated to right-to-left Surface Order.</S>
			<S sid ="205" ssid = "44">Interestingly, the Lappin&amp;Leass algorithm outperforms even the best serialization algorithm on parsed input.</S>
			<S sid ="206" ssid = "45">4.3 Machine Learning Approaches.</S>
			<S sid ="207" ssid = "46">Machine Learning approaches (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002) do not distinguish between filters and preferences.</S>
			<S sid ="208" ssid = "47">They submit all factors as features to the learner.</S>
			<S sid ="209" ssid = "48">For every combination of feature values the learner has the freedom to choose different factors and to assign different strength to them.</S>
			<S sid ="210" ssid = "49">Thus the main problem is not choice and interaction of factors, but rather the formulation of anaphora resolution as a classification problem.</S>
			<S sid ="211" ssid = "50">Two proposals emerge from the literature.</S>
			<S sid ="212" ssid = "51">(1) Given an anaphor and an antecedent, decide if the antecedent is the correct one (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002).</S>
			<S sid ="213" ssid = "52">(2) Given an anaphor and two antecedents, decide which antecedent is more likely to be the correct one (Yang et al., 2003).</S>
			<S sid ="214" ssid = "53">In case (1), the lopsidedness of the distribution is problematic: There are much more negative than positive training examples.</S>
			<S sid ="215" ssid = "54">Machine Learning tools have to surpass a very high baseline: The strategy of never proposing an antecedent typically already yields an f-score of over 90%.</S>
			<S sid ="216" ssid = "55">In case (2), many more correct decisions have to be made before a correct antecedent is found.</S>
			<S sid ="217" ssid = "56">Thus it is important in this scenario, that the set of antecedents is subjected to a strict filtering process in advance so that the system only has to choose among the best candidates and errors are less dangerous.</S>
			<S sid ="218" ssid = "57">Ge et al.</S>
			<S sid ="219" ssid = "58">(1998)’s probabilistic approach combines three factors (aside from the agreement filter): the result of the Hobbs algorithm, Mention Count dependent on the position of the sentence in the article, and the probability of the antecedent occurring in the local context of the pronoun.</S>
			<S sid ="220" ssid = "59">In our re-implementation, we neglected the last factor (see section 3.1).</S>
			<S sid ="221" ssid = "60">Evaluation was performed using 10- fold cross validation.</S>
			<S sid ="222" ssid = "61">Other Machine Learning approaches (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003) make use of decision tree learning4 ; we used C4.5 (Quinlan, 1993).</S>
			<S sid ="223" ssid = "62">To construct the training set, Soon et al.</S>
			<S sid ="224" ssid = "63">(2001) take the nearest correct antecedent in the previous context as a positive example, while all possible antecedents between this antecedent and the pronoun serve as negative examples.</S>
			<S sid ="225" ssid = "64">For testing, potential antecedents are presented to the classifier in Right-to-Left order; the first one classified positive is chosen.</S>
			<S sid ="226" ssid = "65">Apart from agreement, only two of Soon et al.</S>
			<S sid ="227" ssid = "66">(2001)’s features apply to pronominal anaphora: Sentence Recency, and NP Form (with personal pronouns only).</S>
			<S sid ="228" ssid = "67">We used every 10th sentence in Negra for testing, all other sentences for training.</S>
			<S sid ="229" ssid = "68">On parsed input, a very simple decision tree is generated: For every personal and possessive pronoun, the nearest agreeing pronoun is chosen as antecedent; demonstrative pronouns never get an antecedent.</S>
			<S sid ="230" ssid = "69">This tree performs better than the more complicated tree generated from treebank input, where also non-pronouns in previous sentences can serve as antecedents to a personal pronoun.</S>
			<S sid ="231" ssid = "70">Soon et al.</S>
			<S sid ="232" ssid = "71">(2001)’s algorithm performs below its potential.</S>
			<S sid ="233" ssid = "72">We modified it somewhat to get better results.</S>
			<S sid ="234" ssid = "73">For one, we used every possible antecedent 4 On our data, Maximum Entropy (Kehler et al., 2004) had problems with the high baseline, i.e. proposed no antecedents.</S>
			<S sid ="235" ssid = "74">in the training set, which improved performance on the treebank set (by 1.8%) but degraded performance on the parsed data (by 2%).</S>
			<S sid ="236" ssid = "75">Furthermore, we used additional features, viz.</S>
			<S sid ="237" ssid = "76">the grammatical role of antecedent and pronoun, the NP form of the antecedent, and its information status.</S>
			<S sid ="238" ssid = "77">The latter two features were combined to a single feature with very many values, so that they were always chosen first in the decision tree.</S>
			<S sid ="239" ssid = "78">We also used fractional numbers to express intrasentential word distance in addition to Soon et al.</S>
			<S sid ="240" ssid = "79">(2001)’s sentential distance.</S>
			<S sid ="241" ssid = "80">Role Parallelism (Ng and Cardie, 2002) degraded performance (by 0.3% F-value).</S>
			<S sid ="242" ssid = "81">Introducing agreement as a feature had no effect, since the learner always determined that mismatches in agreement preclude coreference.</S>
			<S sid ="243" ssid = "82">Mention Count, Depth of Embedding, and Common Path did not affect performance either.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="244" ssid = "1">The paper has presented a survey of pronoun resolution factors and algorithms.</S>
			<S sid ="245" ssid = "2">Two questions were investigated: Which factors should be chosen, and how should they interact?</S>
			<S sid ="246" ssid = "3">Two types of factors, ‘filters’ and ‘preferences’, were discussed in detail.</S>
			<S sid ="247" ssid = "4">In particular, their restrictive potential and effect on success rate were assessed on the evaluation corpus.</S>
			<S sid ="248" ssid = "5">To address the second question, several well-known algorithms were grouped into three classes according to their solution to factor interaction: Serialization, Weighting, and Machine Learning.</S>
			<S sid ="249" ssid = "6">Six algorithms were evaluated against a common evaluation set so as to facilitate direct comparison.</S>
			<S sid ="250" ssid = "7">Different algorithms have different strengths, in particular as regards their robustness to parsing errors.</S>
			<S sid ="251" ssid = "8">Two of the interaction strategies (Serialization and Machine Learning) allow data-driven optimization.</S>
			<S sid ="252" ssid = "9">Optimal algorithms could be proposed for these strategies.</S>
	</SECTION>
</PAPER>
