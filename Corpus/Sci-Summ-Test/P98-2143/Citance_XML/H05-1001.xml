<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver, and using Singular Value Decomposition (S V D) to identify the main terms.</S>
		<S sid ="2" ssid = "2">We demonstrate that adding anaphoric information results in significant performance improvements over a previously developed system, in which only lexical terms are used as the input to S V D. However, we also show that how anaphoric information is used is crucial: whereas using this information to add new terms does result in improved performance, simple substitution makes the performance worse.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="3" ssid = "3">Many approaches to summarization can be very broadly characterized as T E R M -BA S E D: they attempt to identify the main ‘topics,’ which generally are T E R M S, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997).</S>
			<S sid ="4" ssid = "4">These approaches can be divided again very broadly in ‘lex Kennedy, 1999; Azzam et al., 1999; Bergler et al., 2003; Stuckardt, 2003) identify these terms by running a coreference- or anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms.</S>
			<S sid ="5" ssid = "5">In addition, to our knowledge no authors have convincingly demonstrated that feeding anaphoric information to a summarizer significantly improves the performance of a summarizer using a standard evaluation procedure (a reference corpus and baseline, and widely accepted evaluation measures).</S>
			<S sid ="6" ssid = "6">In this paper we compare two sentence extraction- based summarizers.</S>
			<S sid ="7" ssid = "7">Both use Latent Semantic Analysis (L S A) (Landauer, 1997) to identify the main terms of a text for summarization; however, the first system (Steinberger and Jezek, 2004), discussed in Section 2, only uses lexical information to identify the main topics, whereas the second system exploits both lexical and anaphoric information.</S>
			<S sid ="8" ssid = "8">This second system uses an existing anaphora resolution system to resolve anaphoric expressions, G U I - TA R (Poesio and Kabadjov, 2004); but, crucially, two different ways of using this information for summarization were tested.</S>
			<S sid ="9" ssid = "9">(Section 3.)</S>
			<S sid ="10" ssid = "10">Both sum- marizers were tested over the C A S T corpus (Orasan et al., 2003), as discussed in Section 4, and sig ical’ approaches, among which we would include L S Abased approaches, and ‘coreference-based’ approaches . Lexical approaches to term-based sum- marization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference- (or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and 1 The terms ’anaphora resolution’ and ’coreference resolu-.</S>
			<S sid ="11" ssid = "11">tion’ have been variously defined (Stuckardt, 2003), but the latter term is generally used to refer to the coreference task as defined in M U C and AC E. We use the term ’anaphora resolution’ to refer to the task of identifying successive mentions of the same discourse entity, realized via any type of noun phrase (proper noun, definite description, or pronoun), and whether such discourse entities ’refer’ to objects in the world or not.</S>
			<S sid ="12" ssid = "12">1 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP), pages 1–8, Vancouver, October 2005.</S>
			<S sid ="13" ssid = "13">Qc 2005 Association for Computational Linguistics nificant improvements were observed over both the baseline C A S T system and our previous L S Abased summarizer.</S>
	</SECTION>
	<SECTION title="An LSA-based Summarizer Using. " number = "2">
			<S sid ="14" ssid = "1">Lexical Information Only L S A (Landauer, 1997) is a technique for extracting the ‘hidden’ dimensions of the semantic representation of terms, sentences, or documents, on the basis of their contextual use.</S>
			<S sid ="15" ssid = "2">It is a very powerful technique already used for N L P applications such as information retrieval (Berry et al., 1995) and text segmentation (Choi et al., 2001) and, more recently, multi- and single-document summarization.</S>
			<S sid ="16" ssid = "3">The approach to using L S A in text summarization we followed in this paper was proposed in (Gong and Liu, 2002).</S>
			<S sid ="17" ssid = "4">Gong and Liu propose to start by creating a term by sentences matrix A = tences.</S>
			<S sid ="18" ssid = "5">Furthermore, as demonstrated in (Berry et al., 1995), if a word combination pattern is salient and recurring in document, this pattern will be captured and represented by one of the singular vectors.</S>
			<S sid ="19" ssid = "6">The magnitude of the corresponding singular value indicates the importance degree of this pattern within the document.</S>
			<S sid ="20" ssid = "7">Any sentences containing this word combination pattern will be projected along this singular vector, and the sentence that best represents this pattern will have the largest index value with this vector.</S>
			<S sid ="21" ssid = "8">As each particular word combination pattern describes a certain topic in the document, each singular vector can be viewed as representing a salient topic of the document, and the magnitude of its corresponding singular value represents the degree of importance of the salient topic.</S>
			<S sid ="22" ssid = "9">The summarization method proposed by Gong and Liu (2002) should now be easy to understand.</S>
			<S sid ="23" ssid = "10">T[A1, A2, . . .</S>
			<S sid ="24" ssid = "11">, An], where each column vector Ai rep The matrix V describes the importance degree of resents the weighted term-frequency vector of sentence i in the document under consideration.</S>
			<S sid ="25" ssid = "12">If there are a total of m terms and n sentences in the document, then we will have an m × n matrix A for the document.</S>
			<S sid ="26" ssid = "13">The next step is to apply Singular Value Decomposition (S V D) to matrix A. Given an m × n matrix A, the S V D of A is defined as: (1) A = U ΣV T where U = [uij ] is an m × n column-orthonormal matrix whose columns are called left singular vectors, Σ = diag(σ1, σ2, . . .</S>
			<S sid ="27" ssid = "14">, σn) is an n × n diagonal matrix, whose diagonal elements are non- negative singular values sorted in descending order, and V = [vij ] is an n×n orthonormal matrix, whose columns are called right singular vectors.</S>
			<S sid ="28" ssid = "15">From a mathematical point of view, applying S V D to a matrix derives a mapping between the m- dimensional space spawned by the weighted term- frequency vectors and the r-dimensional singular vector space.</S>
			<S sid ="29" ssid = "16">From a N L P perspective, what the S V Deach ’implicit topic’ in each sentence: the summa rization process simply chooses the most informative sentence for each term.</S>
			<S sid ="30" ssid = "17">In other words, the kth sentence chosen is the one with the largest index value in the kth right singular vector in matrix V T . The summarization method proposed by Gong and Liu has some disadvantages as well, the main of which is that it is necessary to use the same number of dimensions as is the number of sentences we want to choose for a summary.</S>
			<S sid ="31" ssid = "18">However, the higher the number of dimensions of reduced space is, the less significant topic we take into a summary.</S>
			<S sid ="32" ssid = "19">In order to remedy this problem, we (Steinberger and Jezek, 2004) proposed the following modifications to Gong and Liu’s summarization method.</S>
			<S sid ="33" ssid = "20">After computing the S V D of a term by sentences matrix, we compute the length of each sentence vector in matrix V . This is to favour the index values in the matrix V that correspond to the highest singular values (the most significant topics).</S>
			<S sid ="34" ssid = "21">Formally: &apos;Lr does is to derive the latent semantic structure of the (2) sk = i=1 v2 2 document represented by matrix A: a breakdown of the original document into r linearly-independent base vectors (‘topics’).</S>
			<S sid ="35" ssid = "22">Each term and sentence from the document is jointly indexed by these ‘topics’.</S>
			<S sid ="36" ssid = "23">A unique SVD feature is that it is capable of capturing and modelling interrelationships among terms so that it can semantically cluster terms and sen k,i · σi , where sk is the length of the vector of k’th sentence in the modified latent vector space, and its significance score for summarization too.</S>
			<S sid ="37" ssid = "24">The level of dimensionality reduction (r) is essentially learned from the data.</S>
			<S sid ="38" ssid = "25">Finally, we put into the summary the sentences with the highest values in vector s. We showed in previous work (Steinberger and Jezek, 2004) that this modification results in a significant improvement over Gong and Liu’s method.</S>
	</SECTION>
	<SECTION title="Using Anaphora Resolution for. " number = "3">
			<S sid ="39" ssid = "1">Summarization 3.1 The case for anaphora resolution.</S>
			<S sid ="40" ssid = "2">Words are the most basic type of ’term’ that can be used to characterize the content of a document.</S>
			<S sid ="41" ssid = "3">However, being able to identify the most important objects mentioned in the document clearly would lead to an improved analysis of what is important in a text, as shown by the following news article cited by Boguraev and Kennedy (1999): (3) P R I E S T I S C H A R G E D W I T H P O P E AT TAC K A Spanish priest was charged here today with attempting to murder the Pope.</S>
			<S sid ="42" ssid = "4">Juan Fernandez Krohn, aged 32, was arrested after a man armed with a bayonet approached the Pope while he was saying prayers at Fatima on Wednesday night.</S>
			<S sid ="43" ssid = "5">According to the police, Fernandez told the investigators today that he trained for the past six months for the assault.</S>
			<S sid ="44" ssid = "6">If found guilty, the Spaniard faces a prison sentence of 1520 years.</S>
			<S sid ="45" ssid = "7">As Boguraev and Kennedy point out, the title of the article is an excellent summary of the content: an entity (the priest) did something to another entity (the pope).</S>
			<S sid ="46" ssid = "8">Intuitively, understanding that Fernandez and the pope are the central characters is crucial to provide a good summary of texts like these.2 Among the clues that help us to identify such ‘main characters’, the fact that an entity is repeatedly mentioned is clearly important.</S>
			<S sid ="47" ssid = "9">Purely lexical methods, including the L S Abased methods discussed in the previous section, can only capture part of the information about which entities are frequently repeated in the text.</S>
			<S sid ="48" ssid = "10">As example (3) shows, stylistic conventions forbid verbatim repetition, hence the six mentions of Fernandez in the text above contain only one lexical repetition, ’Fernandez’.</S>
			<S sid ="49" ssid = "11">The main problem are pronouns, that tend to share the least lexical similarity with the form used to express the antecedent (and anyway are usually removed by stopword lists, therefore do not 2 It should be noted that for many newspaper articles, indeed many non-educational texts, only a ‘entity-centered’ structure can be clearly identified, as opposed to a ‘relation-centered’ structure of the type hypothesized in Rhetorical Structures Theory (Knott et al., 2001; Poesio et al., 2004).</S>
			<S sid ="50" ssid = "12">get included in the S V D matrix).</S>
			<S sid ="51" ssid = "13">The form of definite descriptions (the Spaniard) doesn’t always overlap with that of their antecedent, either, especially when the antecedent was expressed with a proper name.</S>
			<S sid ="52" ssid = "14">The form of mention which more often overlaps to a degree with previous mentions is proper nouns, and even then at least some way of dealing with acronyms is necessary (cfr.</S>
			<S sid ="53" ssid = "15">European Union / E.U.).</S>
			<S sid ="54" ssid = "16">The motivation for anaphora resolution is that it should tell us which entities are repeatedly mentioned.</S>
			<S sid ="55" ssid = "17">In this work, we tested a mixed approach to integrate anaphoric and word information: using the output of the anaphoric resolver G U I TA R to modify the S V D matrix used to determine the sentences to extract.</S>
			<S sid ="56" ssid = "18">In the rest of this section we first briefly introduce G U I TA R, then discuss the two methods we tested to use its output to help summarization.</S>
			<S sid ="57" ssid = "19">3.2 GUITAR: A General-Purpose Anaphoric.</S>
			<S sid ="58" ssid = "20">Resolver The system we used in these experiments, G U I TA R (Poesio and Kabadjov, 2004), is an anaphora resolution system designed to be high precision, modular, and usable as an off-the-shelf component of a NL processing pipeline.</S>
			<S sid ="59" ssid = "21">The current version of the system includes an implementation of the MARS pronoun resolution algorithm (Mitkov, 1998) and a partial implementation of the algorithm for resolving definite descriptions proposed by Vieira and Poesio (2000).</S>
			<S sid ="60" ssid = "22">The current version of G U I TA R does not include methods for resolving proper nouns.</S>
			<S sid ="61" ssid = "23">3.2.1 Pronoun Resolution Mitkov (1998) developed a robust approach to pronoun resolution which only requires input text to be part-of-speech tagged and noun phrases to be identified.</S>
			<S sid ="62" ssid = "24">Mitkov’s algorithm operates on the basis of antecedent-tracking preferences (referred to hereafter as ”antecedent indicators”).</S>
			<S sid ="63" ssid = "25">The approach works as follows: the system identifies the noun phrases which precede the anaphor within a distance of 2 sentences, checks them for gender and number agreement with the anaphor, and then applies genre- specific antecedent indicators to the remaining candidates (Mitkov, 1998).</S>
			<S sid ="64" ssid = "26">The noun phrase with the highest aggregate score is proposed as antecedent.</S>
			<S sid ="65" ssid = "27">3.2.2 Definite Description Resolution The Vieira / Poesio algorithm (Vieira and Poesio, 2000) attempts to classify each definite description as either direct anaphora, discourse-new, or bridging description.</S>
			<S sid ="66" ssid = "28">The first class includes definite descriptions whose head is identical to that of their antecedent, as in a house . . .</S>
			<S sid ="67" ssid = "29">the house.</S>
			<S sid ="68" ssid = "30">Discourse- new descriptions are definite descriptions that refer to objects not already mentioned in the text and not related to any such object.</S>
			<S sid ="69" ssid = "31">Bridging descriptions are all definite descriptions whose resolution depends on knowledge of relations between objects, such as definite descriptions that refer to an object related to an entity already introduced in the discourse by a relation other than identity, as in the flat . . .</S>
			<S sid ="70" ssid = "32">the living room.</S>
			<S sid ="71" ssid = "33">The Vieira / Poesio algorithm also attempts to identify the antecedents of anaphoric descriptions and the anchors of bridging ones.</S>
			<S sid ="72" ssid = "34">The current version of G U I TA R incorporates an algorithm for resolving direct anaphora derived quite directly from Vieira / Poesio, as well as a statistical version of the methods for detecting discourse new descriptions (Poesio et al., 2005).</S>
			<S sid ="73" ssid = "35">3.3 SVD over Lexical and Anaphoric Terms.</S>
			<S sid ="74" ssid = "36">S V D can be used to identify the ‘implicit topics’ or main terms of a document not only when on the basis of words, but also of coreference chains, or a mixture of both.</S>
			<S sid ="75" ssid = "37">We tested two ways of combining these two types of information.</S>
			<S sid ="76" ssid = "38">3.3.1 The Substitution Method The simplest way of integrating anaphoric information with the methods used in our earlier work is to use anaphora resolution simply as a pre- processing stage of the SVD input matrix creation.</S>
			<S sid ="77" ssid = "39">Firstly, all anaphoric relations are identified by the anaphoric resolver, and anaphoric chains are identified.</S>
			<S sid ="78" ssid = "40">Then a second document is produced, in which all anaphoric nominal expressions are replaced by the first element of their anaphoric chain.</S>
			<S sid ="79" ssid = "41">For example, suppose we have the text in (4).</S>
			<S sid ="80" ssid = "42">(4) S1: Australia’s new conservative government on Wednesday began selling its tough deficit-slashing budget, which sparked violent protests by Aborigines, unions, students and welfare groups even before it was announced.</S>
			<S sid ="81" ssid = "43">S2: Two days of anti-budget street protests preceded spending cuts officially unveiled by Treasurer Peter Costello.</S>
			<S sid ="82" ssid = "44">S3: ”If we don’t do it now, Australia is going to be in deficit and debt into the next century.” S4: As the protesters had feared, Costello revealed a cut to the government’s Aboriginal welfare commission among the hundreds of measures implemented to claw back the deficit.</S>
			<S sid ="83" ssid = "45">An ideal resolver would find 8 anaphoric chains: Chain 1 Australia - we - Australia Chain 2 its new conservative government (Australia’s new conservative government) - the government Chain 3 its tough deficit-slashing budget (Australia’s tough deficit-slashing budget) - it Chain 4 violent protests by Aborigines, unions, students and welfare groups - anti-budget street protests Chain 5 Aborigines, unions, students and welfare groups - the protesters Chain 6 spending cuts - it - the hundreds of measures implemented to claw back the deficit Chain 7 Treasurer Peter CostelloCostello Chain 8 deficit - the deficit By replacing each element of the 8 chains above in the text in (4) with the first element of the chain, we get the text in (5).</S>
			<S sid ="84" ssid = "46">(5) S1: Australia’s new conservative government on Wednesday began selling Australia’s tough deficit- slashing budget, which sparked violent protests by Aborigines, unions, students and welfare groups even before Australia’s tough deficit-slashing budget was announced.</S>
			<S sid ="85" ssid = "47">S2: Two days of violent protests by Aborigines, unions, students and welfare groups preceded spending cuts officially unveiled by Treasurer Peter Costello.</S>
			<S sid ="86" ssid = "48">S3: ”If Australia doesn’t do spending cuts now, Australia is going to be in deficit and debt into the next century.” S4: As Aborigines, unions, students and welfare groups had feared, Treasurer Peter Costello revealed a cut to Australia’s new conservative government’s Aboriginal welfare commission among the spending cuts.</S>
			<S sid ="87" ssid = "49">This text is then used to create the S V D input matrix, as done in the first system.</S>
			<S sid ="88" ssid = "50">3.3.2 The Addition Method An alternative approach is to use S V D to identify ‘topics’ on the basis of two types of ’terms’: terms in the lexical sense (i.e., words) and terms in the sense of objects, which can be represented by anaphoric chains.</S>
			<S sid ="89" ssid = "51">In other words, our representation of sentences would specify not only if they contain a certain word, but also if they contain a mention of a discourse entity (See Figure 1.)</S>
			<S sid ="90" ssid = "52">This matrix would then be used as input to S V D. Figure 1: Addition method.</S>
			<S sid ="91" ssid = "53">The chain ‘terms’ tie together sentences that contain the same anaphoric chain.</S>
			<S sid ="92" ssid = "54">If the terms are lexically the same (direct anaphors - like deficit and the deficit) the basic summarizer works sufficiently.</S>
			<S sid ="93" ssid = "55">However, Gong and Liu showed that the best weighting scheme is boolean (i.e., all terms have the same weight); our own previous results confirmed this.</S>
			<S sid ="94" ssid = "56">The advantage of the addition method is the opportunity to give higher weights to anaphors.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="95" ssid = "1">4.1 The CAST Corpus.</S>
			<S sid ="96" ssid = "2">To evaluate our system, we used the corpus of manually produced summaries created by the C A S T project3 (Orasan et al., 2003).</S>
			<S sid ="97" ssid = "3">The C A S T corpus contains news articles taken from the Reuters Corpus and a few popular science texts from the British National Corpus.</S>
			<S sid ="98" ssid = "4">It contains information about the importance of the sentences (Hasler et al., 2003).</S>
			<S sid ="99" ssid = "5">Sentences are marked as essential or important.</S>
			<S sid ="100" ssid = "6">The corpus also contains annotations for 3 The goal of this project was to investigate to what extent Computer-Aided Summarization can help humans to produce high quality summaries with less effort.</S>
			<S sid ="101" ssid = "7">linked sentences, which are not significant enough to be marked as important/essential, but which have to be considered as they contain information essential for the understanding of the content of other sentences marked as essential/important.</S>
			<S sid ="102" ssid = "8">Four annotators were used for the annotation, three graduate students and one postgraduate.</S>
			<S sid ="103" ssid = "9">Three of the annotators were native English speakers, and the fourth had advanced knowledge of English.</S>
			<S sid ="104" ssid = "10">Unfortunately, not all of the documents were annotated by all of the annotators.</S>
			<S sid ="105" ssid = "11">To maximize the reliability of the summaries used for evaluation, we chose the documents annotated by the greatest number of the annotators; in total, our evaluation corpus contained 37 documents.</S>
			<S sid ="106" ssid = "12">For acquiring manual summaries at specified lengths and getting the sentence scores (for relative utility evaluation) we assigned a score 3 to the sentences marked as essential, a score 2 to important sentences and a score 1 to linked sentences.</S>
			<S sid ="107" ssid = "13">The sentences with highest scores are then selected for ideal summary (at specified lenght).</S>
			<S sid ="108" ssid = "14">4.2 Evaluation Measures.</S>
			<S sid ="109" ssid = "15">Evaluating summarization is a notoriously hard problem, for which standard measures like Precision and Recall are not very appropriate.</S>
			<S sid ="110" ssid = "16">The main problem with P&amp;R is that human judges often disagree what are the top n% most important sentences in a document.</S>
			<S sid ="111" ssid = "17">Using P&amp;R creates the possibility that two equally good extracts are judged very differently.</S>
			<S sid ="112" ssid = "18">Suppose that a manual summary contains sentences [1 2] from a document.</S>
			<S sid ="113" ssid = "19">Suppose also that two systems, A and B, produce summaries consisting of sentences [1 2] and [1 3], respectively.</S>
			<S sid ="114" ssid = "20">Using P&amp;R, system A will be ranked much higher than system B. It is quite possible that sentences 2 and 3 are equally important, in which case the two systems should get the same score.</S>
			<S sid ="115" ssid = "21">To address the problem with precision and recall we used a combination of evaluation measures.</S>
			<S sid ="116" ssid = "22">The first of these, relative utility (RU) (Radev et al., 2000) allows model summaries to consist of sentences with variable ranking.</S>
			<S sid ="117" ssid = "23">With RU, the model summary represents all sentences of the input document with confidence values for their inclusion in the summary.</S>
			<S sid ="118" ssid = "24">For example, a document with five sentences [1 2 3 4 5] is represented as [1/5 2/4 3/4 E va lu at io n M et ho d Le xic al LS A M a n u a l Su bs tit uti on M a n u al Ad dit itio n R el at iv e Ut ili ty 0 . 5 9 5 0 . 5 7 3 0 . 6 6 2F sc or e 0 . 4 2 0 0 . 4 1 0 0 . 4 8 9 C os in e Si m ila rit y 0 . 7 7 4 0 . 8 0 6 0 . 8 2 3 M ai n T op ic Si m ila rit y 0 . 6 8 6 0 . 6 8 2 0 . 7 4 7 Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.</S>
			<S sid ="119" ssid = "25">E va lu at io n M et ho d Le xic al LS A M a n u a l Su bs tit uti on M an ua l Ad dit ion R el at iv e Ut ili ty 0 . 6 4 5 0 . 6 6 2 0 . 6 8 8F sc or e 0 . 5 5 7 0 . 5 4 9 0 . 5 8 3 C os in e Si m ila rit y 0 . 8 6 3 0 . 8 7 8 0 . 8 8 6 M ai n T op ic Si m ila rit y 0 . 8 3 6 0 . 8 2 9 0 . 8 6 6 Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.</S>
			<S sid ="120" ssid = "26">4/1 5/2].</S>
			<S sid ="121" ssid = "27">The second number in each pair indicates the degree to which the given sentence should be part of the summary according to a human judge.</S>
			<S sid ="122" ssid = "28">This number is called the utility of the sentence.</S>
			<S sid ="123" ssid = "29">Utility depends on the input document, the summary length, and the judge.</S>
			<S sid ="124" ssid = "30">In the example, the system that selects sentences [1 2] will not get a higher score than a system that chooses sentences [1 3] given that both summaries [1 2] and [1 3] carry the same number of utility points (5+4).</S>
			<S sid ="125" ssid = "31">Given that no other combination of two sentences carries a higher utility, both systems [1 2] and [1 3] produce optimal extracts.</S>
			<S sid ="126" ssid = "32">To compute relative utility, a number of judges, (N ≥ 1) are asked to assign utility scores to all n sentences in a document.</S>
			<S sid ="127" ssid = "33">The top e sentences according to utility score4 are then called a sentence extract of size e. We can then define the following system performance metric: where X and Y are representations of a system summary and its reference summary based on the vector space model.</S>
			<S sid ="128" ssid = "34">The third measure is Main Topic Similarity.</S>
			<S sid ="129" ssid = "35">This is a content-based evaluation method based on measuring the cosine of the angle between first left singular vectors of a system summary’s and its reference summary’s SVDs.</S>
			<S sid ="130" ssid = "36">(For details see (Steinberger and Jezek, 2004).)</S>
			<S sid ="131" ssid = "37">Finally, we measured RO U G E scores, with the same settings as in the Document Understanding Conference (D U C) 2004.</S>
			<S sid ="132" ssid = "38">4.3 How Much May Anaphora Resolution.</S>
			<S sid ="133" ssid = "39">Help?</S>
			<S sid ="134" ssid = "40">An Upper Bound We annotated all the anaphoric relations in the 37 documents in our evaluation corpus by hand using the annotation tool M M A X (Mueller and Strube, 2003).5 Apart from measuring the performance of G U I TA R over the corpus, this allowed us to establish (6) RU = &apos;Ln j=1 &apos;Ln &apos;LN i=1 &apos;LN uij , the upper bound on the performance improvements j=1 ǫj i=1 uij that could be obtained by adding an anaphoric re where uij is a utility score of sentence j from annotator i, ǫj is 1 for the top e sentences according to the sum of utility scores from all judges and δj is equal to 1 for the top e sentences extracted by the system.</S>
			<S sid ="135" ssid = "41">For details see (Radev et al., 2000).</S>
			<S sid ="136" ssid = "42">The second measure we used is Cosine Similarity, according to the standard formula: &apos;L solver to our summarizer.</S>
			<S sid ="137" ssid = "43">We tested both methods of adding the anaphoric knowledge to the summarizer discussed above.</S>
			<S sid ="138" ssid = "44">Results for the 15% and 30% ratios6 are presented in Tables 1 and 2.</S>
			<S sid ="139" ssid = "45">The baseline is our own previously developed L S Abased sum- marizer without anaphoric knowledge.</S>
			<S sid ="140" ssid = "46">The result is that the substitution method did not lead to significant improvement, but the addition method did: x y (7) cos(X, Y ) = i , &apos;L i (xi )2 · i (yi )2</S>
	</SECTION>
	<SECTION title="We annotated personal. " number = "5">
			<S sid ="141" ssid = "1">pronouns, possessive pronouns, def 4 In the case of ties, some arbitrary but consistent mechanism is used to decide which sentences should be included in inite descriptions and also proper nouns, who will be handled by a future G U I TA R version.</S>
			<S sid ="142" ssid = "2">E va lu at io n M et ho d Le xic al LS A C A S T G U I T A R Su bs tit uti on G U I TA R Ad dit ion R el at iv e Ut ili ty 0 . 5 9 5 0.5 27 0 . 5 3 0 0 . 6 4 0F sc or e 0 . 4 2 0 0.3 48 0 . 3 4 7 0 . 4 4 1 C os in e Si m ila rit y 0 . 7 7 4 0.7 26 0 . 8 0 4 0 . 8 0 5 M ai n T op ic Si m ila rit y 0 . 6 8 6 0.6 30 0 . 6 4 3 0 . 6 9 9 Table 3: Evaluation of the G U I TA R improvement - summarization ratio: 15%.</S>
			<S sid ="143" ssid = "3">E va lu at io n M et ho d Le xic al LS A C A S T G U I T A R Su bs tit uti on G U I T A R Ad dit tio n R el at iv e Ut ili ty 0 . 6 4 5 0.6 18 0 . 6 2 6 0 . 6 7 8F sc or e 0 . 5 5 7 0.5 22 0 . 5 2 4 0 . 5 7 3 C os in e Si m ila rit y 0 . 8 6 3 0.8 55 0 . 8 7 3 0 . 8 7 9 M ai n T op ic Si m ila rit y 0 . 8 3 6 0.8 10 0 . 8 1 8 0 . 8 6 8 Table 4: Evaluation of the G U I TA R improvement - summarization ratio: 30%.</S>
			<S sid ="144" ssid = "4">addition could lead to an improvement in Relative Utility score from .595 to .662 for the 15% ratio, and from .645 to .688 for the 30% ratio.</S>
			<S sid ="145" ssid = "5">Both of these improvements were significant by t-test at 95% confidence.</S>
			<S sid ="146" ssid = "6">4.4 Results with GUITAR.</S>
			<S sid ="147" ssid = "7">To use G U I TA R, we first parsed the texts using Char- niak’s parser (Charniak, 2000).</S>
			<S sid ="148" ssid = "8">The output of the parser was then converted into the M A S -X M L format expected by G U I TA R by one of the preprocessors that come with the system.</S>
			<S sid ="149" ssid = "9">(This step includes heuristic methods for guessing agreement features.)</S>
			<S sid ="150" ssid = "10">Finally, G U I TA R was ran to add anaphoric information to the files.</S>
			<S sid ="151" ssid = "11">The resulting files were then processed by the summarizer.</S>
			<S sid ="152" ssid = "12">G U I TA R achieved a precision of 56% and a recall of 51% over the 37 documents.</S>
			<S sid ="153" ssid = "13">For definite description resolution, we found a precision of 69% and a recall of 53%; for possessive pronoun resolution, the precision was 53%, recall was 53%; for personal pronouns, the precision was 44%, recall was 46%.</S>
			<S sid ="154" ssid = "14">The results with the summarizer are presented in Tables 3 and 4 (relative utility, f-score, cosine, and main topic).</S>
			<S sid ="155" ssid = "15">The contribution of the different anaphora resolution components is addressed in (Kabadjov et al., 2005).</S>
			<S sid ="156" ssid = "16">All versions of our summarizer (the baseline version without anaphora resolution and those using substitution and addition) outperformed the C A S T summarizer, but we have to emphasize that C A S T did not aim at producing a high- performance generic summarizer; only a system that could be easily used for didactical purposes.</S>
			<S sid ="157" ssid = "17">However, our tables also show that using G U I TA R and the addition method lead to significant improvements over our baseline L S A summarizer.</S>
			<S sid ="158" ssid = "18">The improvement in Relative Utility measure was significant by t-test at 95% confidence.</S>
			<S sid ="159" ssid = "19">Using the ROUGE measure we obtained improvement (but not significant).</S>
			<S sid ="160" ssid = "20">On the other hand, the substitution method did not lead to significant improvements, as was to be expected given that no improvement was obtained with ’perfect’ anaphora resolution (see previous section).</S>
			<S sid ="161" ssid = "21">5 Conclusion and Further Research.</S>
			<S sid ="162" ssid = "22">Our main result in this paper is to show that using anaphora resolution in summarization can lead to significant improvements, not only when ’perfect’ anaphora information is available, but also when an automatic resolver is used, provided that the anaphoric resolver has reasonable performance.</S>
			<S sid ="163" ssid = "23">As far as we are aware, this is the first time that such a result has been obtained using standard evaluation measures over a reference corpus.</S>
			<S sid ="164" ssid = "24">We also showed however that the way in which anaphoric information is used matters: with our set of documents at least, substitution would not result in significant improvements even with perfect anaphoric knowledge.</S>
			<S sid ="165" ssid = "25">Further work will include, in addition to extending the set of documents and testing the system with other collections, evaluating the improvement to be achieved by adding a proper noun resolution algorithm to G U I TA R.</S>
	</SECTION>
</PAPER>
