<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Most previous research on verb clustering has focussed on acquiring flat classifications from corpus data, although many manually built classifications are taxonomic in nature.</S>
		<S sid ="2" ssid = "2">Also Natural Language Processing (NLP) applications benefit from taxonomic classifications because they vary in terms of the granularity they require from a classification.</S>
		<S sid ="3" ssid = "3">We introduce a new clustering method called Hierarchical Graph Factorization Clustering (HGFC) and extend it so that it is optimal for the task.</S>
		<S sid ="4" ssid = "4">Our results show that HGFC outperforms the frequently used agglomerative clustering on a hierarchical test set extracted from VerbNet, and that it yields state-of-the-art performance also on a flat test set.</S>
		<S sid ="5" ssid = "5">We demonstrate how the method can be used to acquire novel classifications as well as to extend existing ones on the basis of some prior knowledge about the classification.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">A variety of verb classifications have been built to support NLP tasks.</S>
			<S sid ="7" ssid = "7">These include syntactic and semantic classifications, as well as ones which integrate aspects of both (Grishman et al., 1994; Miller, 1995; Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Hovy et al., 2006).</S>
			<S sid ="8" ssid = "8">Classifications which integrate a wide range of linguistic properties can be particularly useful for tasks suffering from data sparseness.</S>
			<S sid ="9" ssid = "9">One such classification is the taxonomy of English verbs proposed by Levin (1993) which is based on shared (morpho-)syntactic 1023 and semantic properties of verbs.</S>
			<S sid ="10" ssid = "10">Levin’s taxonomy or its extended version in VerbNet (Kipper, 2005) has proved helpful for various NLP application tasks, including e.g. parsing, word sense disambiguation, semantic role labeling, information extraction, question-answering, and machine translation (Swier and Stevenson, 2004; Dang, 2004; Shi and Mihalcea, 2005; Zapirain et al., 2008).</S>
			<S sid ="11" ssid = "11">Because verbs change their meaning and be- haviour across domains, it is important to be able to tune existing classifications as well to build novel ones in a cost-effective manner, when required.</S>
			<S sid ="12" ssid = "12">In recent years, a variety of approaches have been proposed for automatic induction of Levin style classes from corpus data which could be used for this purpose (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ Se´aghdha and Copestake, 2008; Vlachos et al., 2009).</S>
			<S sid ="13" ssid = "13">The best of such approaches have yielded promising results.</S>
			<S sid ="14" ssid = "14">However, they have mostly focussed on acquiring and evaluating flat classifications.</S>
			<S sid ="15" ssid = "15">Levin’s classification is not flat, but taxonomic in nature, which is practical for NLP purposes since applications differ in terms of the granularity they require from a classification.</S>
			<S sid ="16" ssid = "16">In this paper, we experiment with hierarchical Levin-style clustering.</S>
			<S sid ="17" ssid = "17">We adopt as our baseline method a well-known hierarchical method – agglomerative clustering (AGG) – which has been previously used to acquire flat Levin-style classifications (Stevenson and Joanis, 2003) as well as hierarchical verb classifications not based on Levin (Fer- rer, 2004; Schulte im Walde, 2008).</S>
			<S sid ="18" ssid = "18">The method has also been popular in the related task of noun clus Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1023–1033, Edinburgh, Scotland, UK, July 27–31, 2011.</S>
			<S sid ="19" ssid = "19">Qc 2011 Association for Computational Linguistics tering (Ushioda, 1996; Matsuo et al., 2006; Bassiou and Kotropoulos, 2011).</S>
			<S sid ="20" ssid = "20">We introduce then a new method called Hierarchical Graph Factorization Clustering (HGFC) (Yu et al., 2006).</S>
			<S sid ="21" ssid = "21">This graph-based, probabilistic clustering algorithm has some clear advantages over AGG (e.g. it delays the decision on a verb’s cluster membership at any level until a full graph is available, minimising the problem of error propagation) and it has been shown to perform better than several other hierarchical clustering methods in recent comparisons (Yu et al., 2006).</S>
			<S sid ="22" ssid = "22">The method has been applied to the identification of social network communities (Lin et al., 2008), but has not been used (to the best of our knowledge) in NLP before.</S>
			<S sid ="23" ssid = "23">We modify HGFC with a new tree extraction algorithm which ensures a more consistent result, and we propose two novel extensions to it.</S>
			<S sid ="24" ssid = "24">The first is a method for automatically determining the tree structure (i.e. number of clusters to be produced for each level of the hierarchy).</S>
			<S sid ="25" ssid = "25">This avoids the need to predetermine the number of clusters manually.</S>
			<S sid ="26" ssid = "26">The second is addition of soft constraints to guide the clustering performance (Vlachos et al., 2009).</S>
			<S sid ="27" ssid = "27">This is useful for situations where a partial (e.g. a flat) verb classification is available and the goal is to extend it.</S>
			<S sid ="28" ssid = "28">Adopting a set of lexical and syntactic features which have performed well in previous works, we compare the performance of the two methods on test sets extracted from Levin and VerbNet.</S>
			<S sid ="29" ssid = "29">When evaluated on a flat clustering task, HGFC outperforms AGG and performs very similarly with the best flat clustering method reported on the same test set (Sun and Korhonen, 2009).</S>
			<S sid ="30" ssid = "30">When evaluated on a hierarchical task, HGFC performs considerably better than AGG at all levels of gold standard classification.</S>
			<S sid ="31" ssid = "31">The constrained version of HGFC performs the best, as expected, demonstrating the usefulness of soft constraints for extending partial classifications.</S>
			<S sid ="32" ssid = "32">Our qualitative analysis shows that HGFC is capable of detecting novel information not included in our gold standards.</S>
			<S sid ="33" ssid = "33">The unconstrained version can be used to acquire novel classifications from scratch while the constrained version can be used to extend existing ones with additional class members, classes and levels of hierarchy.</S>
	</SECTION>
	<SECTION title="Target classification and test sets. " number = "2">
			<S sid ="34" ssid = "1">The taxonomy of Levin (1993) groups English verbs (e.g. break, fracture, rip) into classes (e.g. 45.1 Break verbs) on the basis of their shared meaning components and (morpho-)syntactic behaviour, defined in terms of diathesis alternations (e.g. the causative/inchoative alternation, where an NP frame alternates with an intransitive frame: Tony broke the window ↔ The window broke).</S>
			<S sid ="35" ssid = "2">It classifies over 3000 verbs in 57 top level classes, some of which divide further into subclasses.</S>
			<S sid ="36" ssid = "3">The extended version of the taxonomy in VerbNet (Kipper, 2005) classifies 5757 verbs.</S>
			<S sid ="37" ssid = "4">Its 5 level taxonomy includes 101 top level and 369 subclasses.</S>
			<S sid ="38" ssid = "5">We used three gold standards (and corresponding test sets) extracted from these resources in our experiments: T1: The first gold standard is a flat gold standard which includes 13 classes appearing in Levin’s original taxonomy (Stevenson and Joanis, 2003).</S>
			<S sid ="39" ssid = "6">We included this small gold standard in our experiments so that we could compare the flat version of our method against previously published methods.</S>
			<S sid ="40" ssid = "7">Following Stevenson and Joanis (2003), we selected 20 verbs from each class which occur at least 100 times in our corpus.</S>
			<S sid ="41" ssid = "8">This gave us 260 verbs in total.</S>
			<S sid ="42" ssid = "9">T2: The second gold standard is a large, hierarchical gold standard which we extracted from VerbNet as follows: 1) We removed all the verbs that have less than 1000 occurrences in our corpus.</S>
			<S sid ="43" ssid = "10">2) In order to minimise the problem of pol- ysemy, we assigned each verb to the class which, according to VerbNet, corresponds to its predominant sense in WordNet (Miller, 1995).</S>
			<S sid ="44" ssid = "11">3) In order to minimise the sparse data problem with very fine- grained classes, we converted the resulting classification into a 3-level representation so that the classes at the 4th and 5th level were combined.</S>
			<S sid ="45" ssid = "12">For example, the sub-classes of Declare verbs (numbered as 29.4.1.1.{1,2,3}) were combined into 29.4.1.</S>
			<S sid ="46" ssid = "13">4) Theclasses that have fewer than 5 members were dis carded.</S>
			<S sid ="47" ssid = "14">The total number of verb senses in the resulting gold standard is 1750, which is 33.2% of the verbs in VerbNet.</S>
			<S sid ="48" ssid = "15">T2 has 51 top level, 117 second level, and 133 third level classes.</S>
			<S sid ="49" ssid = "16">T3: The third gold standard is a subset of T2 where singular classes (top level classes which do not divide into subclasses) are removed.</S>
			<S sid ="50" ssid = "17">This gold standard was constructed to enable proper evaluation of the constrained version of HGFC (introduced in the following section) where we want to compare the impact of constraints across several levels of classification.</S>
			<S sid ="51" ssid = "18">T3 provides classification of 357 verbs into 11 top level, 14 second level, and 32 third level classes.</S>
			<S sid ="52" ssid = "19">For each verb appearing in T1T3, we extracted all the occurrences (up to 10,000) from the British National Corpus (Leech, 1992) and North American News Text Corpus (Graff, 1995).</S>
	</SECTION>
	<SECTION title="Method. " number = "3">
			<S sid ="53" ssid = "1">3.1 Features and feature extraction.</S>
			<S sid ="54" ssid = "2">Previous works on Levin style verb classification have investigated optimal features for this task (Stevenson and Joanis, 2003; Li and Brew, 2008; Sun and Korhonen, 2009)).</S>
			<S sid ="55" ssid = "3">We adopt for our experiments a set of features which have performed well in recent verb clustering works: A: Subcategorization frames (SCFs) and their relative frequencies with individual verbs.</S>
			<S sid ="56" ssid = "4">B: A with SCFs parameterized for prepositions.</S>
			<S sid ="57" ssid = "5">C: B with SCFs parameterized for subjects appearing in grammatical relations associated with the verb in parsed data.</S>
			<S sid ="58" ssid = "6">D: B with SCFs parameterized for objects appearing in grammatical relations associated with the verb in parsed data.</S>
			<S sid ="59" ssid = "7">These features are purely syntactic.</S>
			<S sid ="60" ssid = "8">Although semantic features – verb selectional preferences – proved the best (when used in combination with syntactic features) in the recent work of Sun and Korhonen (2009), we left such features for future work because we noticed that different levels of classification are likely to require semantic features at different granularities.</S>
			<S sid ="61" ssid = "9">We extracted the syntactic features using the system of Preiss et al.</S>
			<S sid ="62" ssid = "10">(2007).</S>
			<S sid ="63" ssid = "11">The system tags, lemma- tizes and parses corpus data using the RASP (Robust Accurate Statistical Parsing toolkit (Briscoe et al., 2006)), and on the basis of the resulting grammatical relations, assigns each occurrence of a verb as a member of one of the 168 verbal SCFs.</S>
			<S sid ="64" ssid = "12">We pa- rameterized the SCFs as described above using the information provided by the system.</S>
			<S sid ="65" ssid = "13">3.2 Clustering.</S>
			<S sid ="66" ssid = "14">We introduce the agglomerative clustering (AGG) and Hierarchical Graph Factorization Clustering (HGFC) methods in the following two subsections, respectively.</S>
			<S sid ="67" ssid = "15">The subsequent two subsections present our extensions to HGFC: (i) automatically determining the cluster structure and (ii) adding soft constraints to guide clustering performance.</S>
			<S sid ="68" ssid = "16">3.2.1 Agglomerative clustering AGG is a method which treats each verb as a singleton cluster and then successively merges two closest clusters until all the clusters have been merged into one.</S>
			<S sid ="69" ssid = "17">We used the SciPy’s implementation (Oliphant, 2007) of the algorithm.</S>
			<S sid ="70" ssid = "18">The cluster distance is measured using linkage criteria.</S>
			<S sid ="71" ssid = "19">We experimented with four commonly used linkage criteria: Single, Average, Complete and Ward’s (Ward Jr., 1963).</S>
			<S sid ="72" ssid = "20">Ward’s criterion performed the best and was used in all the experiments in this paper.</S>
			<S sid ="73" ssid = "21">It measures the increase in variance after two clusters are merged.</S>
			<S sid ="74" ssid = "22">The output of AGG tends to have excessive number of levels.</S>
			<S sid ="75" ssid = "23">Cut-based methods (Wu and Leahy, 1993; Shi and Malik, 2000) are frequently applied to extract a simplified view.</S>
			<S sid ="76" ssid = "24">We followed previous verb clustering works and cut the AGG hierarchy manually.</S>
			<S sid ="77" ssid = "25">AGG suffers from two problems.</S>
			<S sid ="78" ssid = "26">The first is error propagation.</S>
			<S sid ="79" ssid = "27">When a verb is misclassified at a lower level, the error propagates to all the upper levels.</S>
			<S sid ="80" ssid = "28">The second is local pairwise merging, i.e. the fact that only two clusters can be combined at any level.</S>
			<S sid ="81" ssid = "29">For example, in order to group clusters representing Levin classes 9.1, 9.2 and 9.3 into a single cluster representing class 9, the method has to produce intermediate clusters, e.g. 9.{1,2} and 9.3.Such clusters do not always have a semantic inter pretation.</S>
			<S sid ="82" ssid = "30">Although they can be removed using a cut-based method, this requires a predefined cutoff value which is difficult to set (Stevenson and Joanis, 2003).</S>
			<S sid ="83" ssid = "31">In addition, a significant amount of information is lost in pairwise clustering.</S>
			<S sid ="84" ssid = "32">In the above example, only the clusters 9.{1,2} and 9.3 are consid ered, while alternative clusters 9.{1,3} and 9.2 are ignored.</S>
			<S sid ="85" ssid = "33">Ideally, information about all the possible intermediate clusters should be aggregated, but this is intractable in practice.</S>
			<S sid ="86" ssid = "34">3.2.2 Hierarchical Graph Factorization We use the divergence distance: ζ (X, Y ) = xij Clustering ij (xij log yij −xij + yij ).</S>
			<S sid ="87" ssid = "35">Yu et al.</S>
			<S sid ="88" ssid = "36">(2006) showed Our new method HGFC derives a probabilistic bipartite graph from the similarity matrix (Yu et al., 2006).</S>
			<S sid ="89" ssid = "37">The local and global clustering structures are that this cost function is non-increasing under the update rule: wi j learned via the random walk properties of the graph.The method does not suffer from the above prob h˜ip ∝ hip j (H ΛH T )ij λp hjp s.t. h˜ip = 1 (2) i lems with AGG.</S>
			<S sid ="90" ssid = "38">Firstly, there is no error propagation λ˜p ∝ λp wij (H ΛH T )ij hip hjp s.t. λ˜p = wij (3) because the decision on a verb’s membership at any j p ij level is delayed until the full bipartite graph is available and until a tree structure can be extracted from it by aggregating probabilistic information from all the levels.</S>
			<S sid ="91" ssid = "39">Secondly, the bipartite graph enables the wij can be interpreted as the probability of the direct transition between vi and vj : wij = p(vi, vj ), when L;ij wij = 1.</S>
			<S sid ="92" ssid = "40">bip can be interpreted as: construction of a hierarchical structure without any intermediate classes.</S>
			<S sid ="93" ssid = "41">For example, we can group classes 9.{1,2,3} directly into class 9.</S>
			<S sid ="94" ssid = "42">We use HGFC with the distributional similarity bip biq di pq m (4) measure JensenShannon Divergence (djs(v, v )).</S>
			<S sid ="95" ssid = "43">D = diag(d1 , ..., dn ) where di = bip Given a set of verbs, V = {vn}N , we p=0 compute a similarity matrix W where Wij =exp(−djs(v1, v2)).</S>
			<S sid ="96" ssid = "44">W can be encoded by a undi rected graph G (Figure 1(a)), where the verbs are mapped to vertices and the Wij is the edge weight between vertices i and j. The graph G and the cluster structure can be represented by a bipartite graph K (V, U ).</S>
			<S sid ="97" ssid = "45">V are the p(up, uq ) is the similarity between the clusters.</S>
			<S sid ="98" ssid = "46">It takes into account of a weighted average of contributions from all the data.</S>
			<S sid ="99" ssid = "47">This is different from the linkage method where only the data from two clusters are considered.Given the cluster similarity p(up, uq ), we can con vertices on G. U = {up}m represent the hidden m struct a new graph G1 (Figure 1(d)) with the clusters U as vertices.</S>
			<S sid ="100" ssid = "48">The cluster algorithm can be applied clusters.</S>
			<S sid ="101" ssid = "49">For example, looking at Figure 1(b), V on G can be grouped into three clusters u1, u2 and u3.</S>
			<S sid ="102" ssid = "50">The matrix B denotes the n × m adjacency matrix, with bip being the connection weight between the vertex vi and the cluster up.</S>
			<S sid ="103" ssid = "51">Thus, B represents the connections between clusters at an upper and lower level of clustering.</S>
			<S sid ="104" ssid = "52">A flat clustering algorithm can be induced by computing B. The bipartite graph K also induces a similarityagain (Figure 1(e)).</S>
			<S sid ="105" ssid = "53">This process can go on itera tively, leading to a hierarchical graph.</S>
			<S sid ="106" ssid = "54">Algorithm 1 HGFC algorithm (Yu et al., 2006) Require: N verbs V , number of clusters ml for L levels Compute the similarity matrix W0 from V Build the graph G0 from W0 , and m0 ← n for l = 1, 2 to L do Factorize Gl−1 to obtain bipartite graph Kl with the (W ) between vi and vj : w L;m p=1 bip bjp λp adjacency matrix Bl (eq. 1, 2 and 3) Build a graph Gl with similarity matrix Wl =(BΛ−1BT )ij where Λ = diag(λ1, ..., λm).</S>
			<S sid ="107" ssid = "55">There fore, B can be found by approximating the similarity matrix W of G using W derived from K . Given a distance function ζ between two similarity matrices, B approximates W by minimizing the cost function ζ (W, BΛ−1BT ).</S>
			<S sid ="108" ssid = "56">The coupling between B and Λ is removed by setting H = BΛ−1: n min ζ (W, H ΛH T ), s.t. hip = 1 (1) H,Λ i=1 BT Dl Bl according to equation 4 − l end for return BL , BL−1 ...B1 Additional steps need to be performed in order to extract a tree from the hierarchical graph.</S>
			<S sid ="109" ssid = "57">Yu et al.</S>
			<S sid ="110" ssid = "58">(2006) performs the extraction via a propagation of probabilities from the bottom level clusters.</S>
			<S sid ="111" ssid = "59">For a verb vi, the probability of assigning it to cluster v(l) at level l is given by: v 1 u 1 2 v v v v 1 2 1 2 1 v v v v v 3 7 3 7 4 v v u v 8 v 8 5 2 9 9 v 1 v 2 u 1 v u u 3 q 1 2 v 1 4 u 5 2 v 2 6 6 6 v 3 6 v v 5 v 5 7 4 7 4 u 3 u 2 v 3 v 3 8 8 v v 9 9 (a) (b) (c) (d) (e) Figure 1: (a) An undirected graph G representing the similarity matrix; (b) The bipartite graph showing three clusters on G; (c) The induced clusters U ; (d) The new graph G1 over clusters U ; (e) The new bipartite graph over G1 p |vi ) = Vl−1 ...</S>
			<S sid ="112" ssid = "60">V1 p(v(l) |v (l−1) )...p(v (1) |vi ) 3.2.3 Aut oma tical ly dete rmin ing the num ber of clust ers for HG FC HGFC needs the number of levels and clusters at = (D(−1) −1 −1 2 B2 D3 B3 ...Dl Bl )ip (5) This method might not extract a consistent tree structure, because the cluster membership at the lower level does not constrain the upper level membership.</S>
			<S sid ="113" ssid = "61">This prevented us from extracting a Levin style hierarchical classification in our initial experiments.</S>
			<S sid ="114" ssid = "62">For example, where two verbs were grouped together at a lower level, they could belong to separate clusters at an upper level.</S>
			<S sid ="115" ssid = "63">We therefore propose a new tree extraction algorithm (Algorithm 2).</S>
			<S sid ="116" ssid = "64">The new algorithm starts from the top level bipartite graph, and generates consistent labels for each level by taking into account of the tree constraints set at upper levels.</S>
			<S sid ="117" ssid = "65">each level as input.</S>
			<S sid ="118" ssid = "66">However, this information is not always available (e.g. when the goal is to actually learn this information automatically).</S>
			<S sid ="119" ssid = "67">We therefore propose a method for inferring the cluster structure from data.</S>
			<S sid ="120" ssid = "68">As shown in figure 1, a similarity matrix W models one-hop transitions that follow the links from vertices to neighbors.</S>
			<S sid ="121" ssid = "69">A walker can also go to other vertices via multi-hop transitions.</S>
			<S sid ="122" ssid = "70">According to the chain rule of the Markov process, the multi-hop transitions indicate a decaying similarity function on the graph (Yu et al., 2006).</S>
			<S sid ="123" ssid = "71">After t transitions, the similarity matrix (Wt) becomes: Wt = Wt−1 D0 W0Yu et al.</S>
			<S sid ="124" ssid = "72">(2006) proved the correspondence be tween the HGFC levels (l) and the random walk time: A lgorithm 2 Tree extraction algorithm for HGFC Require: Given N , (Bl , ml ) on each level for L levels On the top level L, collect the labels T L (eq. 5) Define C to be a (mL−1 × mL ) zero matrix, Cij ← 1, where i, j = arg maxi,j {BL } t = 2l−1.</S>
			<S sid ="125" ssid = "73">So the vertices at level l induce a similarity matrix of verbs after t-hop transitions.</S>
			<S sid ="126" ssid = "74">The decaying similarity function captures the different scales of clustering structure in the data (Azran and Ghahramani, 2006b).</S>
			<S sid ="127" ssid = "75">The upper levels would have for l = L − 1 to 1 do i = 1 to N do a smalle r numb er of cluster s which repres ent a more for Compute p(vl |vi ) for each cluster p (eq. 5) global structure.</S>
			<S sid ="128" ssid = "76">After several levels, all the verbs l l are expected to be grouped into one cluster.</S>
			<S sid ="129" ssid = "77">The ti = argmaxp {p(vp |vi )|p = 1...ml , Cptl+1 = 0} end for Redefine C to be a (ml−1 × ml ) zero matrix, Cij ← 1, where i, j = arg maxi,j {Bl } number of levels and clusters at each level can thus be learned automatically.We therefore propose a method that uses the de end for return Tree consistent labels T L , T L−1 ...T 1 caying similarity function to learn the hierarchical clustering structure.</S>
			<S sid ="130" ssid = "78">One simple modification to algorithm 1 is to set the number of clusters at level l (ml ) to be ml−1 − 1.</S>
			<S sid ="131" ssid = "79">m is denoted as the number of clusters that have at least one member according to eq. 5.</S>
			<S sid ="132" ssid = "80">We start by treating each verb as a cluster at the bottom level.</S>
			<S sid ="133" ssid = "81">The algorithm stops when all the data points are merged into one cluster.</S>
			<S sid ="134" ssid = "82">The increasingly decaying similarity causes many clusters to have 0 members especially at lower levels, which are pruned in the tree extraction.</S>
			<S sid ="135" ssid = "83">3.2.4 Adding constraints to HGFCThe basic version of HGFC makes no prior as where nij is the size of the intersection between class i and cluster j. We used normalized mutual information (NMI) and F-Score (F) to evaluate hierarchical clustering results on T2 and T3.</S>
			<S sid ="136" ssid = "84">NMI measures the amount of statistical information shared by two random variables representing the clustering result and the gold- standard labels.</S>
			<S sid ="137" ssid = "85">Given random variables A and B: NMI(A, B) = I (A; B) [H (A) + H (B)]/2 sumptions about the classification.</S>
			<S sid ="138" ssid = "86">It is useful I(A, B) = |(vk ∩ cj | log N |vk ∩ cj | for learning novel verb classifications from scratch.</S>
			<S sid ="139" ssid = "87">However, when wishing to extend an existing classification (e.g. VerbNet) it may be desirable to guide the clustering performance on the basis of information that is already known.</S>
			<S sid ="140" ssid = "88">We propose a constrained version of HGFC which makes uses of labels at the bottom level to learn upper level classifications.</S>
			<S sid ="141" ssid = "89">We do this by adding soft constraints to clustering, following Vlachos et al.</S>
			<S sid ="142" ssid = "90">(2009).</S>
			<S sid ="143" ssid = "91">We modify the similarity matrix W as follows: If two verbs have different labels (li = lj ), the similarity between them is decreased by a factor a, and a &lt; 1.</S>
			<S sid ="144" ssid = "92">We set a to 0.5 in the experiments.</S>
			<S sid ="145" ssid = "93">The re k j N |vk ||cj | where |vk ∩ cj | is the number of shared membership between cluster vk and gold-standard class cj . The normalized variant of mutual information (MI) enables the comparison of clustering with different cluster numbers (Manning et al., 2008).</S>
			<S sid ="146" ssid = "94">F is the harmonic mean of precision (P) and recall (R).</S>
			<S sid ="147" ssid = "95">P is calculated using modified purity – a global measure which evaluates the mean precision of clusters.</S>
			<S sid ="148" ssid = "96">Each cluster is associated with its prevalent class.</S>
			<S sid ="149" ssid = "97">The number of verbs in a cluster K that take this class is denoted by nprevalent(K).</S>
			<S sid ="150" ssid = "98">sulting tree is generally consistent with the original L; nprevalent(ki )&gt;2 nprevalent(ki ) classification.</S>
			<S sid ="151" ssid = "99">The influence of the underlying data mPUR = number of verbs (domain or features) is reduced according to a.</S>
	</SECTION>
	<SECTION title="Experimental evaluation. " number = "4">
			<S sid ="152" ssid = "1">We applied the clustering methods introduced in section 3 to the test sets described in section 2 and evaluated them both quantitatively and qualitatively, as described in the subsequent sections.</S>
			<S sid ="153" ssid = "2">4.1 Evaluation methods.</S>
			<S sid ="154" ssid = "3">We used class based accuracy (ACC) and adjusted rand index (Radj ) to evaluate the results on the flat test set T1 (see section 2 for details of T1T3).</S>
			<S sid ="155" ssid = "4">ACC is the proportion of members of dominant clusters DOMCLUSTi within all classes ci.</S>
			<S sid ="156" ssid = "5">ACC = i=1 verbs in DOMCLUSTi number of verbs The formula of Radj is (Hubert and Arabie, 1985): R is calculated using ACC.</S>
			<S sid ="157" ssid = "6">F = 2 · mPUR · ACC mPUR + ACC F is not suitable for comparing results with different cluster numbers (Rosenberg and Hirschberg, 2007).</S>
			<S sid ="158" ssid = "7">Therefore, we only report NMI when the number of classes in clustering and gold-standard is substantially different.</S>
			<S sid ="159" ssid = "8">Finally, we supplemented quantitative evaluation with qualitative evaluation of clusters produced by different methods.</S>
			<S sid ="160" ssid = "9">4.2 Quantitative evaluation.</S>
			<S sid ="161" ssid = "10">We first evaluated AGG and the basic (unconstrained) HGFC on the small flat test set T1.</S>
			<S sid ="162" ssid = "11">The main purpose of this evaluation was to compare the results of our methods against previously published results on the same test set.</S>
			<S sid ="163" ssid = "12">The number of clus L; (nij − L; (ni· L; (n·j /(n ters (K ) and levels (L) were inferred automatically Radj = i,j 2 i 2 j 2 2 2 [L;i ( 2 + L;j ( 2 ] − L;i ( 2 L;j ( 2 /(2 for HGFC as described in section 3.2.3.</S>
			<S sid ="164" ssid = "13">However, to 1 ni· n·j ni· n·j n make the results comparable with previously published ones, we cut the resulting hierarchy at the level of closest match (12 clusters) to the K (13) in the gold-standard.</S>
			<S sid ="165" ssid = "14">For AGG, we cut the hierarchy at 13 clusters.</S>
			<S sid ="166" ssid = "15">Nc Nl HGFC uncons trained A G G N MI F N M I F 13 0 13 3 57 .3 1 36 .6 5 54 .2 2 32 .6 2 11 4 11 7 54 .6 7 37 .9 6 51 .3 5 32 .4 4 50 51 37 .7 5 40 .0 0 32 .6 1 32 .7 8 Table 2: Performance on T2 using a predefined tree structure.</S>
			<S sid ="167" ssid = "16">Table 1: Comparison against Stevenson and Joanis (2003)’s result on T1 (using similar features).</S>
			<S sid ="168" ssid = "17">Table 1 shows our results and the results of Stevenson and Joanis (2003) on T1 when employing AGG using Ward as the linkage criterion.</S>
			<S sid ="169" ssid = "18">In this experiment, we used the same feature set as Stevenson and Joanis (2003) (set B, see section 3.1) and were therefore able to reproduce their AGG result with a difference smaller than 2%.</S>
			<S sid ="170" ssid = "19">When using this simple feature set, HGFC outperforms the best performing AGG clearly: 8.5% in ACC and 7.3% in Radj . We also compared HGFC against the best reported clustering method on T1 to date – that of spectral clustering by Sun and Korhonen (2009).</S>
			<S sid ="171" ssid = "20">We used the feature sets C and D which are similar to the features (SCF parameterized by lexical prefences) in their experiments.</S>
			<S sid ="172" ssid = "21">HGFC obtains F of 49.93% on T1 which is 5% lower than the result of Sun and Korhonen (2009).</S>
			<S sid ="173" ssid = "22">The difference comes from the tree consistency requirement.</S>
			<S sid ="174" ssid = "23">When the HGFC is forced to produce a flat clustering (a one level tree only), it achieves the F of 52.55% which is very close to the performance of spectral clustering.</S>
			<S sid ="175" ssid = "24">We then evaluated our methods on the hierarchical test sets T2 and T3.</S>
			<S sid ="176" ssid = "25">In the first set of experiments, we predefined the tree structure for HGFC by setting L to 3 and K at each level to be the K in the hierarchical gold standard.</S>
			<S sid ="177" ssid = "26">The hierarchy produced by AGG was cut into 3 levels according to K s in the gold standard.</S>
			<S sid ="178" ssid = "27">This enabled direct evaluation of the results against the 3 level gold standards using both NMI and F. The results are reported in tables 2 and 3.</S>
			<S sid ="179" ssid = "28">In these tables, Nc is the number of clusters in HGFC clustering while Nl is the number of classes in the gold standard (the two do not always correspond perfectly because a few clusters have zero members).</S>
			<S sid ="180" ssid = "29">Table 3: Performance on T3 using a predefined tree structure.</S>
			<S sid ="181" ssid = "30">Table 2 compares the results of the unconstrained version of HGFC against those of AGG on our largest test set T2.</S>
			<S sid ="182" ssid = "31">As with T1, HGFC outperforms AGG clearly.</S>
			<S sid ="183" ssid = "32">The benefit can now be seen at 3 different levels of hierarchy.</S>
			<S sid ="184" ssid = "33">On average, the HGFC outperforms AGG 3.5% in NMI and 4.8% in F. The difference between the methods becomes clearer when moving towards the upper levels of the hierarchy.</S>
			<S sid ="185" ssid = "34">Table 3 shows the results of both unconstrained and constrained versions of HGFC and those of AGG on the test set T3 (where singular classes are removed to enable proper evaluation of the constrained method).</S>
			<S sid ="186" ssid = "35">The results are generally generally better on this test set than on T2 – which is to be expected since T3 is a refined subset of T21.</S>
			<S sid ="187" ssid = "36">Recall that the constrained version of HGFC learns the upper levels of classification on the basis of soft constraints set at the bottom level, as described earlier in section 3.2.4.</S>
			<S sid ="188" ssid = "37">As a consequence, NMI and F are both greater than 90% at the bottom level and the results at the top level are notably lower because the impact of the constraints degrades the further away one moves from the bottom level.</S>
			<S sid ="189" ssid = "38">Yet, the relatively high result across all levels shows that the constrained version of HGFC can be employed a useful method to extend the hierarchical structure of known classifications.</S>
			<S sid ="190" ssid = "39">1 NMI is higher on T2, however, because NMI has a higher baseline for larger number of clusters (Vinh et al., 2009).</S>
			<S sid ="191" ssid = "40">NMI is not ideal for comparing the results of T2 and T3.</S>
			<S sid ="192" ssid = "41">T 2 T 3 Nc Nl H GF C Nc Nl H GF C 14 8 13 3 53 .2 6 64 32 54 .9 1 97 11 7 49 .8 5 35 32 50 .8 3 46 51 33 .5 5 20 14 44 .0 2 19 51 25 .8 0 10 14 34 .4 1 9 51 19 .1 7 6 11 32 .2 7 3 51 13 .0 6 Table 4: NMI of unconstrained HGFC when trees for T2 and T3 are inferred automatically.</S>
			<S sid ="193" ssid = "42">Finally, Table 4 shows the results for the unconstrained HGFC on T2 and and T3 when the tree structure is not predefined but inferred automatically as described in section 3.2.3.</S>
			<S sid ="194" ssid = "43">6 levels are learned for T2 and 5 for T3.</S>
			<S sid ="195" ssid = "44">The number of clusters produced ranges from 3 to 148 for T2 and from 6 to 64 for T3.</S>
			<S sid ="196" ssid = "45">We can see that the automatically detected cluster numbers distribute evenly across different levels.</S>
			<S sid ="197" ssid = "46">The scale of the clustering structure is more complete here than in the gold standards.</S>
			<S sid ="198" ssid = "47">In the table, Nc indicates the number of clusters in the inferred tree, while Nl indicates the closest match to the number of classes in the gold standard.</S>
			<S sid ="199" ssid = "48">This evaluation is not fully reliable because the match between the gold standard and the clustering is poor at some levels of hierarchy.</S>
			<S sid ="200" ssid = "49">However, it is encouraging to see that the results do not drop dramatically until the match between the two is really poor.</S>
			<S sid ="201" ssid = "50">4.3 Qualitative evaluation.</S>
			<S sid ="202" ssid = "51">To gain a better insight into the performance of HGFC, we conducted further qualitative analysis of the clusters the two versions of this method produced for T3.</S>
			<S sid ="203" ssid = "52">We focussed on the top level of 11 clusters (in the evaluation against the hierarchical gold standard, see table 3) as the impact of soft constraints is the weakest for the constrained method at this level.</S>
			<S sid ="204" ssid = "53">As expected, the constrained HGFC kept many individual verbs belonging to same Verbnet subclass together (e.g. verbs enjoy, hate, disdain, regret, love, despise, detest, dislike, fear for the class 31.2.1) so that most clusters simply group lower level classes and their members together.</S>
			<S sid ="205" ssid = "54">Three nearly clean clusters were produced which only include sub-classes of the same class (e.g. 31.2.0 and 31.2.1 which both belong to 31.2 Admire verbs).</S>
			<S sid ="206" ssid = "55">However, the remaining 8 clusters group together sub-classes (and their members) belonging to unrelated parent classes.</S>
			<S sid ="207" ssid = "56">Interestingly, 6 of these make both syntactic and semantic sense.</S>
			<S sid ="208" ssid = "57">For example, several such 37.7 Say verbs and 29.5 Conjencture verbs are found together which share the meaning of communication and which take similar sentential complements.</S>
			<S sid ="209" ssid = "58">In contrast, none of the clusters produced by the unconstrained HGFC represent a single VerbNet class.</S>
			<S sid ="210" ssid = "59">The majority represent a high number of classes and fewer members per class.</S>
			<S sid ="211" ssid = "60">Yet many of the clusters make syntactic and semantic sense.</S>
			<S sid ="212" ssid = "61">A good example is a cluster which includes member verbs from 9.7 Spray/Load verbs, 21.2 Carve verbs, 51.3.1 Roll verbs, and 10.4 Wipe verbs.</S>
			<S sid ="213" ssid = "62">The verbs included in this cluster share the meaning of specific type of motion and show similar syntactic behaviour.</S>
			<S sid ="214" ssid = "63">Thorough Levin style investigation of especially the unconstrained method would require looking at shared diathesis alternations between cluster members.</S>
			<S sid ="215" ssid = "64">We left this for future work.</S>
			<S sid ="216" ssid = "65">However, the analysis we conducted confirmed that the constrained method could indeed be used for extending known classifications, while the unconstrained method is more suitable for acquiring novel classifications from scratch.</S>
			<S sid ="217" ssid = "66">The errors in clusters produced by both methods were mostly due to syntactic idiosyncracy and the lack of semantic information in clustering.</S>
			<S sid ="218" ssid = "67">We plan to address the latter problem in our future work.</S>
	</SECTION>
	<SECTION title="Discussion and conclusion. " number = "5">
			<S sid ="219" ssid = "1">We have introduced a new graph-based method – HGFC – to hierarchical verb clustering which avoids some of the problems (e.g. error propagation, pairwise cluster merging) reported with the frequently used AGG method.</S>
			<S sid ="220" ssid = "2">We modified HGFC so that it can be used to automatically determine the tree structure for clustering, and proposed two extensions to it which make it even more suitable for our task.</S>
			<S sid ="221" ssid = "3">The first involves automatically determining the number of clusters to be produced, which is useful when this is not known in advance.</S>
			<S sid ="222" ssid = "4">The second involves adding soft constraints to guide the clustering performance, which is useful when aiming to extend existing classification.</S>
			<S sid ="223" ssid = "5">The results reported in the previous section are promising.</S>
			<S sid ="224" ssid = "6">On a flat test set (T1), the unconstrained version of HGFC outperforms AGG and performs very similarly with the best current flat clustering method (spectral clustering) evaluated on the same dataset.</S>
			<S sid ="225" ssid = "7">On the hierarchical test sets (T2 and T3), the unconstrained and constrained versions of HGFC outperform AGG clearly at all levels of classification.</S>
			<S sid ="226" ssid = "8">The constrained version of HGFC detects the missing hierarchy from the existing gold standards with high accuracy.</S>
			<S sid ="227" ssid = "9">When the number of clusters and levels is learned automatically, the unconstrained method produces a multi-level hierarchy.</S>
			<S sid ="228" ssid = "10">Our evaluation against a 3-level gold standard shows that such a hierarchy is fairly accurate.</S>
			<S sid ="229" ssid = "11">Finally, the results from our qualitative evaluation show that both constrained and unconstrained versions of HGFC are capable of learning valuable novel information not included in the gold standards.</S>
			<S sid ="230" ssid = "12">The previous work on Levin style verb classification has mostly focussed on flat classifications using methods suitable for flat clustering (Schulte im Walde, 2006; Joanis et al., 2008; Sun et al., 2008; Li and Brew, 2008; Korhonen et al., 2008; O´ Se´aghdha and Copestake, 2008; Vlachos et al., 2009).</S>
			<S sid ="231" ssid = "13">However, some works have employed hierarchical clustering as a method to infer flat clustering.</S>
			<S sid ="232" ssid = "14">For example, Schulte im Walde and Brew (2002) employed AGG to initialize the KMeans clustering for German verbs.</S>
			<S sid ="233" ssid = "15">This gave better results than random initialization.</S>
			<S sid ="234" ssid = "16">Stevenson and Joanis (2003) used AGG for flat clustering on T1.</S>
			<S sid ="235" ssid = "17">They cut the hierarchy at the number of classes in the gold standard and found that it is difficult to automatically determine a good cutoff.</S>
			<S sid ="236" ssid = "18">Our evaluation in the previous section shows that HGFC outperforms their implementation of AGG.</S>
			<S sid ="237" ssid = "19">AGG was also used by Ferrer (2004) who performed hierarchical clustering of 514 Spanish verbs.</S>
			<S sid ="238" ssid = "20">The results were evaluated against a hierarchical gold standard resembling that of Levin’s classification in English (Va´zquez et al., 2000).</S>
			<S sid ="239" ssid = "21">Radj of 0.07 was reported for a 15-way classification which is comparable to the result of Stevenson and Joanis (2003).</S>
			<S sid ="240" ssid = "22">Hierarchical clustering has also been performed for the related task of semantic verb classification.</S>
			<S sid ="241" ssid = "23">For example, Basili et al.</S>
			<S sid ="242" ssid = "24">(1993) identified the prob lems of AGG, and applied a conceptual clustering algorithm (Fisher, 1987) to Italian verbs.</S>
			<S sid ="243" ssid = "25">They used semi-automatically acquired semantic roles and the concept types as features.</S>
			<S sid ="244" ssid = "26">No quantitative results were reported.</S>
			<S sid ="245" ssid = "27">The qualitative evaluation shows that the resulting clusters are very fine-grained.</S>
			<S sid ="246" ssid = "28">Schulte im Walde (2008) performed hierarchical clustering of German verbs using human verb association as features and AGG as a method.</S>
			<S sid ="247" ssid = "29">They focussed on two small collections of 56 and 104 verbs and evaluated the result against flat gold standard extracted from GermaNet (Kunze and Lemnitzer, 2002) and German FrameNet (Erk et al., 2003), respectively.</S>
			<S sid ="248" ssid = "30">They reported F of 62.69% for the 56 verbs, and F of 34.68% for the 104 verbs.</S>
			<S sid ="249" ssid = "31">In the future, we plan to extend this research line in several directions.</S>
			<S sid ="250" ssid = "32">First, we will try to determine optimal features for different levels of clustering.</S>
			<S sid ="251" ssid = "33">For example, the general syntactic features (e.g. SCF) may perform the best at top levels of a hierarchy while more specific or refined features (e.g. SCF+pp) may be optimal at lower levels.</S>
			<S sid ="252" ssid = "34">We also plan to investigate incorporating semantic features, like verb selectional preferences, in our feature set.</S>
			<S sid ="253" ssid = "35">It is likely that different levels of clustering require more or less specific selectional preferences.</S>
			<S sid ="254" ssid = "36">One way to obtain the latter is hierarchical clustering of relevant noun data.</S>
			<S sid ="255" ssid = "37">In addition, we plan to apply the unconstrained HGFC to specific domains to investigate its capability to learn novel, previously unknown classifications.</S>
			<S sid ="256" ssid = "38">As for the constrained version of HGFC, we will conduct a larger scale experiment on the Verb- Net data to investigate what kind of upper level hierarchy it can propose for this resource (which currently has over 100 top level classes).</S>
			<S sid ="257" ssid = "39">Finally, we plan to compare HGFC to other hierarchical clustering methods that are relatively new to NLP but have proved promising in other fields, including Bayesian Hierarchical Clustering (Heller and Ghahramani, 2005; Teh et al., 2008) and the method of Azran and Ghahramani (2006a) based on spectral clustering.</S>
	</SECTION>
	<SECTION title="Acknowledgement. " number = "6">
			<S sid ="258" ssid = "1">Our work was funded by the Royal Society University Research Fellowship (AK), the Dorothy Hodgkin Postgraduate Award (LS), the EPSRC grants EP/F030061/1 and EP/G051070/1 (UK) and the EU FP7 project ’PANACEA’.</S>
	</SECTION>
</PAPER>
