Clustering Polysemic Subcategorization Frame Distributions Semantically 
Anna Korhonen.  Yuval Krymolowski  Zvika Marx  Computer Laboratory  Division of Informatics  Interdisciplinary Center  University of Cambridge  University of Edinburgh  for Neural Computation,  15 JJ Thomson Avenue  2 Buccleuch Place  The Hebrew University  Cambridge CB3 0FD, UK  Edinburgh EH8 9LW  Jerusalem, Israel  alk23@cl.cam.ac.uk  Scotland, UK  zvim@cs.huji.ac.il  ykrymolo@inf.ed.ac.uk  Abstract Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated cor.pus data. We describe a new approach which involves clustering subcategoriza.tion frame (S C F) distributions using the Information Bottleneck and nearest neigh.bour methods. In contrast to previous work, we particularly focus on cluster.ing polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, of.fering us a good insight into the potential and limitations of semantically classifying undisambiguated S C F data. 1 Introduction Classiﬁcations which aim to capture the close rela.tion between the syntax and semantics of verbs have attracted a considerable research interest in both lin.guistics and computational linguistics (e.g. (Jack.endoff, 1990; Levin, 1993; Pinker, 1989; Dang et al., 1998; Dorr, 1997; Merlo and Stevenson, 2001)). While such classiﬁcations may not provide a means for full semantic inferencing, they can capture gen.eralizations over a range of linguistic properties, and can therefore be used as a means of reducing redun.dancy in the lexicon and for ﬁlling gaps in lexical knowledge. .This work was partly supported by UK EPSRC project GR/N36462/93: ‘Robust Accurate Statistical Parsing (RASP)’. Verb classiﬁcations have, in fact, been used to support many natural language processing (N L P) tasks, such as language generation, machine transla.tion (Dorr, 1997), document classiﬁcation (Klavans and Kan, 1998), word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002). One attractive property of these classiﬁcations is that they make it possible, to a certain extent, to in.fer the semantics of a verb on the basis of its syn.tactic behaviour. In recent years several attempts have been made to automatically induce semantic verb classes from (mainly) syntactic information in corpus data (Joanis, 2002; Merlo et al., 2002; Schulte im Walde and Brew, 2002). In this paper, we focus on the particular task of classifying subcategorization frame (S C F) distri.butions in a semantically motivated manner. Pre.vious research has demonstrated that clustering can be useful in inferring Levin-style semantic classes (Levin, 1993) from both English and Ger.man verb subcategorization information (Brew and Schulte im Walde, 2002; Schulte im Walde, 2000; Schulte im Walde and Brew, 2002). We propose a novel approach, which involves: (i) obtaining S C F frequency information from a lexi.con extracted automatically using the comprehen.sive system of Briscoe and Carroll (1997) and (ii) applying a clustering mechanism to this informa.tion. We use clustering methods that process raw distributional data directly, avoiding complex pre.processing steps required by many advanced meth.ods (e.g. Brew and Schulte im Walde (2002)). In contrast to earlier work, we give special empha.sis to polysemy. Earlier work has largely ignored this issue by assuming a single gold standard class for each verb (whether polysemic or not). The rel.atively good clustering results obtained suggest that many polysemic verbs do have some predominating sense in corpus data. However, this sense can vary across corpora (Roland et al., 2000), and assuming a single sense is inadequate for an important group of medium and high frequency verbs whose distribu.tion of senses in balanced corpus data is ﬂat rather than zipﬁan (Preiss and Korhonen, 2002). To allow for sense variation, we introduce a new evaluation scheme against a polysemic gold stan.dard. This helps to explain the results and offers a better insight into the potential and limitations of clustering undisambiguated S C F data semantically. We discuss our gold standards and the choice of test verbs in section 2. Section 3 describes the method for subcategorization acquisition and sec.tion 4 presents the approach to clustering. Details of the experimental evaluation are supplied in sec.tion 5. Section 6 concludes with directions for future work. 2 Semantic Verb Classes and Test Verbs Levin’s taxonomy of verbs and their classes (Levin, 1993) is the largest syntactic-semantic verb classiﬁ.cation in English, employed widely in evaluation of automatic classiﬁcations. It provides a classiﬁcation of 3,024 verbs (4,186 senses) into 48 broad / 192 ﬁne grained classes. Although it is quite extensive, it is not exhaustive. As it primarily concentrates on verbs taking N P and P P complements and does not provide a comprehensive set of senses for verbs, it is not suitable for evaluation of polysemic classiﬁ.cations. We employed as a gold standard a substan.tially extended version of Levin’s classiﬁcation constructed by Korhonen (2003). This incorpo.rates Levin’s classes, 26 additional classes by Dorr (1997)1, and 57 new classes for verb types not covered comprehensively by Levin or Dorr. 110 test verbs were chosen from this gold stan.dard, 78 polysemic and 32 monosemous ones. Some low frequency verbs were included to investigate the 1These classes are incorporated in the ’L C S database’ (http://www.umiacs.umd.edu/.bonnie/verbs-English.lcs). effect of sparse data on clustering performance. To ensure that our gold standard covers all (or most) senses of these verbs, we looked into WordNet (Miller, 1990) and assigned all the WordNet senses of the verbs to gold standard classes.2 Two versions of the gold standard were created: monosemous and polysemic. The monosemous one lists only a single sense for each test verb, that cor.responding to its predominant (most frequent) sense in WordNet. The polysemic one provides a compre.hensive list of senses for each verb. The test verbs and their classes are shown in table 1. The classes are indicated by number codes from the classiﬁca.tions of Levin, Dorr (the classes starting with 0) and Korhonen (the classes starting with A).3 The pre.dominant sense is indicated by bold font. 3 Subcategorization Information We obtain our S C F data using the subcategorization acquisition system of Briscoe and Carroll (1997). We expect the use of this system to be beneﬁ.cial: it employs a robust statistical parser (Briscoe and Carroll, 2002) which yields complete though shallow parses, and a comprehensive S C F classiﬁer, which incorporates 163 S C F distinctions, a super.set of those found in the A N LT (Boguraev et al., 1987) and C O M L E X (Grishman et al., 1994) dictio.naries. The S C Fs abstract over speciﬁc lexically.governed particles and prepositions and speciﬁc predicate selectional preferences but include some derived semi-predictable bounded dependency con.structions, such as particle and dative movement. 78 of these ‘coarse-grained’ S C Fs appeared in our data. In addition, a set of 160 ﬁne grained frames were employed. These were obtained by parameter.izing two high frequency S C Fs for prepositions: the simple P P and N P + P P frames. The scope was re.stricted to these two frames to prevent sparse data problems in clustering. A S C F lexicon was acquired using this system from the British National Corpus (Leech, 1992, B N C) so that the maximum of 7000 citations were 2As WordNet incorporates particularly ﬁne grained sense distinctions, some senses were found which did not appear in our gold standard. As many of them appeared marginal and/or low in frequency, we did not consider these additional senses in our experiment. 3The gold standard assumes Levin’s broad classes (e.g. class 10) instead of possible ﬁne-grained ones (e.g. class 10.1). TEST VERB  GOLD STANDARD CLASSES  TEST VERB  GOLD STANDARD CLASSES  TEST VERB  GOLD STANDARD CLASSES  TEST VERB  GOLD STANDARD CLASSES  place  9  dye  24, 21, 41  focus  31, 45  stare  30  lay  9  build  26, 45  force  002, 11  glow  43  drop  9, 45, 004, 47, 51, A54, A30  bake  26, 45  persuade  002  sparkle  43  pour  9, 43, 26, 57, 13, 31  invent  26, 27  urge  002, 37  dry  45  load  9  publish  26, 25  want  002, 005, 29, 32  shut  45  settle  9, 46, A16, 36, 55  cause  27, 002  need  002, 005, 29, 32  hang  47, 9, 42, 40  ﬁll  9, 45, 47  generate  27, 13, 26  grasp  30, 15  sit  47, 9  remove  10, 11, 42  induce  27, 002, 26  understand  30  disappear  48  withdraw  10, A30  acknowledge  29, A25, A35  conceive  30, 29, A56  vanish  48  wipe  10, 9  proclaim  29, 37, A25  consider  30, 29  march  51  brush  10, 9, 41, 18  remember  29, 30  perceive  30  walk  51  ﬁlter  10  imagine  29, 30  analyse  34, 35  travel  51  send  11, A55  specify  29  evaluate  34, 35  hurry  53, 51  ship  11, A58  establish  29, A56  explore  35, 34  rush  53, 51  transport  11, 31  suppose  29, 37  investigate  35, 34  begin  55  carry  11, 54  assume  29, A35, A57  agree  36, 22, A42  continue  55, 47, 51  drag  11, 35, 51, 002  think  29, 005  communicate  36, 11  snow  57, 002  push  11, 12, 23, 9, 002  conﬁrm  29  shout  37  rain  57  pull  11, 12, 13, 23, 40, 016  believe  29, 31, 33  whisper  37  sin  003  give  13  admit  29, 024, 045, 37  talk  37  rebel  003  lend  13  allow  29, 024, 13, 002  speak  37  risk  008, A7  study  14, 30, 34, 35  act  29  say  37, 002  gamble  008, 009  hit  18, 17, 47, A56, 31, 42  behave  29  mention  37  beg  015, 32  bang  18, 43, 9, 47, 36  feel  30, 31, 35, 29  eat  39  pray  015, 32  carve  21, 25, 26  see  30, 29  drink  39  seem  020  add  22, 37, A56  hear  30, A32  laugh  40, 37  appear  020, 48, 29  mix  22, 26, 36  notice  30, A32  smile  40, 37  colour  24, 31, 45  concentrate  31, 45  look  30, 35  Table 1: Test verbs and their monosemous/polysemic gold standard senses used per test verb. The lexicon was evaluated against manually analysed corpus data after an empirically deﬁned threshold of 0.025 was set on relative fre.quencies of S C Fs to remove noisy S C Fs. The method yielded 71.8% precision and 34.5% recall. When we removed the ﬁltering threshold, and evaluated the noisy distribution, F-measure4 dropped from 44.9 to 38.51.5 4 Clustering Method Data clustering is a process which aims to partition a given set into subsets (clusters) of elements that are similar to one another, while ensuring that elements that are not similar are assigned to different clusters. We use clustering for partitioning a set of verbs. Our hypothesis is that information about S C Fs and their associated frequencies is relevant for identifying se.mantically related verbs. Hence, we use S C Fs as rel.evance features to guide the clustering process.6 2·precision·recall 4F = precision+recall 5These ﬁgures are not particularly impressive because our evaluation is exceptionally hard. We use 1) highly polysemic test verbs, 2) a high number of S C Fs and 3) evaluate against manually analysed data rather than dictionaries (the latter have high precision but low recall). 6The relevance of the features to the task is evident when comparing the probability of a randomly chosen pair of verbs verbi and verbj to share the same predominant sense (4.5%) with the probability obtained when verbj is the JS-divergence We chose two clustering methods which do not in.volve task-oriented tuning (such as pre-ﬁxed thresh.olds or restricted cluster sizes) and which approach data straightforwardly, in its distributional form: (i) a simple hard method that collects the nearest neigh.bours (N N) of each verb (ﬁgure 1), and (ii) the In.formation Bottleneck (I B), an iterative soft method (Tishby et al., 1999) based on information-theoretic grounds. The N N method is very simple, but it has some disadvantages. It outputs only one clustering conﬁg.uration, and therefore does not allow examination of different cluster granularities. It is also highly sensitive to noise. Few exceptional neighbourhood relations contradicting the typical trends in the data are enough to cause the formation of a single cluster which encompasses all elements. Therefore we employed the more sophisticated I B method as well. The I B quantiﬁes the rele.vance information of a S C F distribution with re.spect to output clusters, through their mutual infor.mation I(C lusters; S C Fs). The relevance informa.tion is maximized, while the compression informa.tion I(C lusters; V erbs) is minimized. This en.sures optimal compression of data through clusters. The tradeoff between the two constraints is realized nearest neighbour of verbi (36%). NN Clustering: 1. For each verb v: 2. 	Calculate the J S divergence between the S C F distributions of v and all other verbs: f        J S(p, q) = 1 Dp  p+q + Dq  p+q 2223. Connect v with the most similar verb; 4. Find all the connected components Figure 1: Connected components nearest neighbour (N N) clustering. D is the Kullback-Leibler distance. through minimizing the cost term: L = I(C lusters; V erbs) . .I (C lusters; S C Fs) , where . is a parameter that balances the constraints. The I B iterative algorithm ﬁnds a local minimum of the above cost term. It takes three inputs: (i) S C F.verb distributions, (ii) the desired number of clusters K, and (iii) the value of .. Starting from a random conﬁguration, the algo.rithm repeatedly calculates, for each cluster K, verb V and S C F S, the following probabilities: (i) the marginal proportion of the cluster p(K); (ii) the probability p(S|K) for a S C F to occur with mem.bers of the cluster; and (iii) the probability p(K|V ) for a verb to be assigned to the cluster. These prob.abilities are used, each in its turn, for calculating the other probabilities (ﬁgure 2). The collection of all p(S|K)’s for a ﬁxed cluster K can be regarded as a probabilistic center (centroid) of that cluster in the S C F space. The I B method gives an indication of the most informative values of K. 7 Intensifying the weight . attached to the relevance information I(C lusters; S C Fs) allows us to increase the num.ber K of distinct clusters being produced (while too small . would cause some of the output clusters to be identical to one another). Hence, the relevance in.formation grows with K. Accordingly, we consider as the most informative output conﬁgurations those for which the relevance information increases more sharply between K . 1 and K clusters than between K and K + 1. 7Most works on clustering ignore this issue and refer to an arbitrarily chosen number of clusters, or to the number of gold standard classes, which cannot be assumed in realistic applica.tions. IB Clustering (ﬁxed .): 
Perform till convergence, for each time step 
t = 1, 2, . . . : 
..D[p(S|V )Ipt.1(S|K)]1. 	zt(K, V ) = pt.1(K) e(When t = 1, initialize zt(K, V ) arbitrarily) zt(K,V )2. 	pt(K|V ) =  K' zt(K',V )v 3. pt(K) = V p(V )pt(K|V )v 4. pt(S|K) = V p(S|V )pt(V |K) Figure 2: Information Bottleneck (IB) iterative clustering. D is the Kullback-Leibler distance. When the weight of relevance grows, the assign.ment to clusters is more constrained and p(K|V ) be.comes more similar to hard clustering. Let K(V ) = argmax p(K|V ) K denote the most probable cluster of a verb V . For K ≥ 30, more than 85% of the verbs have p(K(V )|V ) > 90% which makes the output cluster.ing approximately hard. For this reason, we decided to use only K(V ) as output and defer a further ex.ploration of the soft output to future work. 5 	Experimental Evaluation 5.1 Data The input data to clustering was obtained from the automatically acquired S C F lexicon for our 110 test verbs (section 2). The counts were extracted from unﬁltered (noisy) S C F distributions in this lexicon.8 The N N algorithm produced 24 clusters on this in.put. From the I B algorithm, we requested K = 2 to 60 clusters. The upper limit was chosen so as to slightly exceed the case when the average clus.ter size 110/K = 2. We chose for evaluation the I B results for K = 25, 35 and 42. For these val.ues, the S C F relevance satisﬁes our criterion for a notable improvement in cluster quality (section 4). The value K = 35 is very close to the actual number (34) of predominant senses in the gold standard. In this way, the I B yields structural information beyond clustering. 8This yielded better results, which might indicate that the unﬁltered “noisy” S CFs contain information which is valuable for the task. 5.2 Method A number of different strategies have been proposed for evaluation of clustering. We concentrate here on those which deliver a numerical value which is easy to interpret, and do not introduce biases towards spe.ciﬁc numbers of classes or class sizes. As we cur.rently assign a single sense to each polysemic verb (sec. 5.4) the measures we use are also applicable for evaluation against a polysemous gold standard. Our ﬁrst measure, the adjusted pairwise preci.sion (A P P), evaluates clusters in terms of verb pairs (Schulte im Walde and Brew, 2002) 9: Kv1 num. of correct pairs in ki |ki|.1A P P = · K num. of pairs in ki |ki|+1 . i=1 A P P is the average proportion of all within-cluster pairs that are correctly co-assigned. It is multiplied by a factor that increases with cluster size. This fac.tor compensates for a bias towards small clusters. Our second measure is derived from purity, a global measure which evaluates the mean precision of the clusters, weighted according to the cluster size (Stevenson and Joanis, 2003). We associate with each cluster its most prevalent semantic class, and denote the number of verbs in a cluster K that take its prevalent class by nprevalent (K). Verbs that do not take this class are considered as errors. Given our task, we are only interested in classes which con.tain two or more verbs. We therefore disregard those clusters where nprevalent (K) = 1. This leads us to deﬁne modiﬁed purity: nprevalent (ki) nprevalent (ki)≥2 mP U R = .number of verbs The modiﬁcation we introduce to purity removes the bias towards the trivial conﬁguration comprised of only singletons. 5.3 Evaluation Against the Predominant Sense We ﬁrst evaluated the clusters against the predom.inant sense, i.e. using the monosemous gold stan.dard. The results, shown in Table 2, demonstrate that both clustering methods perform signiﬁcantly 9Our deﬁnition differs by a factor of 2 from that of Schulte im Walde and Brew (2002). Alg.  K  +PP  –PP  +PP  –PP  A P P:  mPUR:  NN  (24)  21%  19%  48%  45%  IB  25 35 42  12% 14% 15%  9% 9% 9%  39% 48% 50%  32% 38% 39%  R A N D  25  3%  15%  Table 2: Clustering performance on the predominant senses, with and without prepositions. The last entry presents the per.formance of random clustering with K = 25, which yielded the best results among the three values K=25, 35 and 42. better on the task than our random clustering base.line. Both methods show clearly better performance with ﬁne-grained S C Fs (with prepositions, + P P) than with coarse-grained ones (-P P). Surprisingly, the simple N N method performs very similarly to the more sophisticated I B. Being based on pairwise similarities, it shows better per.formance than I B on the pairwise measure. The I B is, however, slightly better according to the global measure (2% with K = 42). The fact that the N N method performs better than the I B with similar K values (N N K = 24 vs. I B K = 25) seems to suggest that the J S divergence provides a better model for the predominant class than the compression model of the I B. However, it is likely that the I B perfor.mance suffered due to our choice of test data. As the method is global, it performs better when the target classes are represented by a high number of verbs. In our experiment, many semantic classes were rep.resented by two verbs only (section 2). Nevertheless, the I B method has the clear advan.tage that it allows for more clusters to be produced. At best it classiﬁed half of the verbs correctly ac.cording to their predominant sense (mP U R = 50%). Although this leaves room for improvement, the re.sult compares favourably to previously published re.sults10. We argue, however, that evaluation against a monosemous gold standard reveals only part of the picture. 10Due to differences in task deﬁnition and experimental setup, a direct comparison with earlier results is impossible. For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure. K  Pred. sense  Multiple senses  Pred. sense  Multiple senses  APP:  mP UR:  NN: (24)  21%  29% (23% + 5.)  48%  60% (46%+ 2.)  IB: 25 35 42  12% 14% 15%  18% (14% + 5.) 20% (16% + 6.) 19% (16% + 3.)  39% 47% 50%  48% (43%+ 3.) 59% (50%+ 4.) 59% (54%+ 2.)  Table 3: Evaluation against the monosemous (Pred.) and pol.ysemous (Multiple) gold standards. The ﬁgures in parentheses are results of evaluation on randomly polysemous data + sig.niﬁcance of the actual ﬁgure. Results were obtained with ﬁne.grained S C Fs (including prepositions). 5.4 Evaluation Against Multiple Senses In evaluation against the polysemic gold standard, we assume that a verb which is polysemous in our corpus data may appear in a cluster with verbs that share any of its senses. In order to evaluate the clus.ters against polysemous data, we assigned each pol.ysemic verb V a single sense: the one it shares with the highest number of verbs in the cluster K(V ). Table 3 shows the results against polysemic and monosemous gold standards. The former are notice.ably better than the latter (e.g. I B with K = 42 is 9% better). Clearly, allowing for multiple gold standard classes makes it easier to obtain better results with evaluation. In order to show that polysemy makes a non.trivial contribution in shaping the clusters, we mea.sured the improvement that can be due to pure chance by creating randomly polysemous gold stan.dards. We constructed 100 sets of random gold stan.dards. In each iteration, the verbs kept their original predominant senses, but the set of additional senses was taken entirely from another verb -chosen at ran.dom. By doing so, we preserved the dominant sense of each verb, the total frequency of all senses and the correlations between the additional senses. The results included in table 3 indicate, with 99.5% conﬁdence (3. and above), that the improve.ment obtained with the polysemous gold standard is not artiﬁcial (except in two cases with 95% conﬁ.dence). 5.5 Qualitative Analysis of Polysemy We performed qualitative analysis to further inves.tigate the effect of polysemy on clustering perfor-Different  Pairs  Fraction  Senses  in cluster  0  39  51%  1  85  10%  2  625  7%  3  1284  3%  4  1437  3%  Table 4: The fraction of verb pairs clustered together, as a function of the number of different senses between pair mem.bers (results of the NN algorithm) Common  one irregular  no irregular  Senses  Pairs  in cluster  Pairs  in cluster  0 1 2  2180 388 44  3% 9% 20%  3018 331 31  3% 12% 35%  Table 5: The fraction of verb pairs clustered together, as a function of the number of shared senses (results of the N N algo.rithm) mance. The results in table 4 demonstrate that the more two verbs differ in their senses, the lower their chance of ending up in the same cluster. From the ﬁgures in table 5 we see that the probability of two verbs to appear in the same cluster increases with the number of senses they share. Interestingly, it is not only the degree of polysemy which inﬂuences the results, but also the type. For verb pairs where at least one of the members displays ‘irregular’ poly.semy (i.e. it does not share its full set of senses with any other verb), the probability of co-occurrence in the same cluster is far lower than for verbs which are polysemic in a ‘regular’ manner (Table 5). Manual cluster analysis against the polysemic gold standard revealed a yet more comprehensive picture. Consider the following clusters (the I B out.put with K = 42): A1: talk (37), speak (37) 
A2: look (30, 35), stare (30) 
A3: focus (31, 45), concentrate (31, 45) 
A4: add (22, 37, A56) 
We identiﬁed a close relation between the clus.tering performance and the following patterns of se.mantic behaviour: 1) Monosemy: We had 32 monosemous test verbs. 10 gold standard classes included 2 or more or these. 7 classes were correctly acquired us.ing clustering (e.g. A1), indicating that clustering monosemous verbs is fairly ‘easy’. 2) Predominant sense: 10 clusters were exam.ined by hand whose members got correctly classi.ﬁed together, despite one of them being polysemous (e.g. A2). In 8 cases there was a clear indication in the data (when examining S C Fs and the selectional preferences on argument heads) that the polysemous verb indeed had its predominant sense in the rele.vant class and that the co-occurrence was not due to noise. 3) Regular Polysemy: Several clusters were pro.duced which represent linguistically plausible inter.sective classes (e.g. A3) (Dang et al., 1998) rather than single classes. 4) Irregular Polysemy: Verbs with irregular pol.ysemy11 were frequently assigned to singleton clus.ters. For example, add (A4) has a ‘combining and attaching’ sense in class 22 which involves N P and P P S C Fs and another ‘communication’ sense in 37 which takes sentential S C Fs. Irregular polysemy was not a marginal phenomenon: it explains 5 of the 10 singletons in our data. These observations conﬁrm that evaluation against a polysemic gold standard is necessary in order to fully explain the results from clustering. 5.6 Qualitative Analysis of Errors Finally, to provide feedback for further development of our verb classiﬁcation approach, we performed a qualitative analysis of errors not resulting from poly.semy. Consider the following clusters (the I B output for K = 42): B1: place (9), build (26, 45), publish (26, 25), carve (21, 25, 26) B2: sin (003), rain (57), snow (57, 002) B3: agree (36, 22, A42), appear (020, 48, 29), begin (55), continue (55, 47, 51) B4: beg (015, 32) Three main error types were identiﬁed: 1) Syntactic idiosyncracy: This was the most fre.quent error type, exempliﬁed in B1, where place is incorrectly clustered with build, publish and carve merely because it takes similar prepositions to these verbs (e.g. in, on, into). 2) Sparse data: Many of the low frequency verbs (we had 12 with frequency less than 300) performed 11Recall our deﬁnition of irregular polysemy, section 5.4. poorly. In B2, sin (which had 53 occurrences) is classiﬁed with rain and snow because it does not occur in our data with the preposition against .the ‘hallmark’ of its gold standard class (’Conspire Verbs’). 3) Problems in S C F acquisition: These were not numerous but occurred e.g. when the system could not distinguish between different control (e.g. sub.ject/object equi/raising) constructions (B3). 6 Discussion and Conclusions This paper has presented a novel approach to auto.matic semantic classiﬁcation of verbs. This involved applying the N N and I B methods to cluster polysemic S C F distributions extracted from corpus data using Briscoe and Carroll’s (1997) system. A principled evaluation scheme was introduced which enabled us to investigate the effect of polysemy on the resulting classiﬁcation. Our investigation revealed that polysemy has a considerable impact on the clusters formed: pol.ysemic verbs with a clear predominant sense and those with similar regular polysemy are frequently classiﬁed together. Homonymic verbs or verbs with strong irregular polysemy tend to resist any classiﬁ.cation. While it is clear that evaluation should account for these cases rather than ignore them, the issue of polysemy is related to another, bigger issue: the po.tential and limitations of clustering in inducing se.mantic information from polysemic S C F data. Our results show that it is unrealistic to expect that the ‘important’ (high frequency) verbs in language fall into classes corresponding to single senses. How.ever, they also suggest that clustering can be used for novel, previously unexplored purposes: to de.tect from corpus data general patterns of seman.tic behaviour (monosemy, predominant sense, reg.ular/irregular polysemy). In the future, we plan to investigate the use of soft clustering (without hardening the output) and de.velop methods for evaluating the soft output against polysemous gold standards. We also plan to work on improving the accuracy of subcategorization ac.quisition, investigating the role of noise (irregular / regular) in clustering, examining whether different syntactic/semantic verb types require different ap.proaches in clustering, developing our gold standard classiﬁcation further, and extending our experiments to a larger number of verbs and verb classes. References B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter, and C. Grover. 1987. The derivation of a grammatically.indexed lexicon from the longman dictionary of con.temporary english. In Proc. of the 25th ACL, pages 193–200, Stanford, CA. C. Brew and S. Schulte im Walde. 2002. Spectral clus.tering for german verbs. In Conference on Empirical Methods in Natural Language Processing, Philadel.phia, USA. E. J. Briscoe and J. Carroll. 1997. Automatic extraction of subcategorization from corpora. In 5th ACL Confer.ence on Applied Natural Language Processing, pages 356–363, Washington DC. E. J. Briscoe and J. Carroll. 2002. Robust accurate sta.tistical annotation of general text. In 3rd International Conference on Language Resources and Evaluation, pages 1499–1504, Las Palmas, Gran Canaria. H. T. Dang, K. Kipper, M. Palmer, and J. Rosenzweig. 1998. Investigating regular sense extensions based on intersective Levin classes. In Proc. of COLING/ACL, pages 293–299, Montreal, Canada. B. Dorr and D. Jones. 1996. Role of word sense disam.biguation in lexical acquisition: predicting semantics from syntactic cues. In 16th International Conference on Computational Linguistics, pages 322–333, Copen.hagen, Denmark. B. Dorr. 	1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation. Machine Translation, 12(4):271–325. R. Grishman, C. Macleod, and A. Meyers. 1994. Com.lex syntax: building a computational lexicon. In In.ternational Conference on Computational Linguistics, pages 268–272, Kyoto, Japan. R. Jackendoff. 	1990. Semantic Structures. MIT Press, Cambridge, Massachusetts. E. Joanis. 	2002. Automatic verb classiﬁcation using a general feature space. Master’s thesis, University of Toronto. J. L. Klavans and M. Kan. 1998. Role of verbs in docu.ment analysis. In Proc. of COLING/ACL, pages 680– 686, Montreal, Canada. A. Korhonen. 	2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge, UK. A. Korhonen. 	2003. Extending Levin’s Classiﬁcation with New Verb Classes. Unpublished manuscript, Uni.versity of Cambridge Computer Laboratory. G. Leech. 1992. 100 million words of english: the british national corpus. Language Research, 28(1):1–13. B. Levin. 1993. 	English Verb Classes and Alternations. Chicago University Press, Chicago. P. Merlo and S. Stevenson. 	2001. Automatic verb clas.siﬁcation based on statistical distributions of argument structure. Computational Linguistics, 27(3):373–408. P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002. A multilingual paradigm for automatic verb classiﬁca.tion. In Proc. of the 40th ACL, Pennsylvania, USA. G. A. Miller. 	1990. WordNet: An on-line lexi.cal database. International Journal of Lexicography, 3(4):235–312. S. Pinker. 1989. Learnability and Cognition: The Acqui.sition of Argument Structure. MIT Press, Cambridge, Massachusetts. J. Preiss and A. Korhonen. 	2002. Improving subcate.gorization acquisition with WSD. In ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, Philadelphia, USA. D. Roland, D. Jurafsky, L. Menn, S. Gahl, E. Elder, and C. Riddoch. 2000. Verb subcatecorization frequency differences between business-news and balanced cor.pora. In ACL Workshop on Comparing Corpora, pages 28–34. S. Schulte im Walde and C. Brew. 	2002. Inducing ger.man semantic verb classes from purely syntactic sub.categorisation information. In Proc. of the 40th ACL, Philadephia, USA. S. Schulte im Walde. 	2000. Clustering verbs seman.tically according to their alternation behaviour. In Proc. of COLING-2000, pages 747–753, Saarbr ¨ucken, Germany. S. Stevenson and E. Joanis. 	2003. Semi-supervised verb-class discovery using noisy features. In Proc. of CoNLL-2003, Edmonton, Canada. N. Tishby, F. C. Pereira, and W. Bialek. 1999. The infor.mation bottleneck method. In Proc. of the 37th Annual Allerton Conference on Communication, Control and Computing, pages 368–377. 