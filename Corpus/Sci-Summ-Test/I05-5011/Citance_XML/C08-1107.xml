<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Most work on unsupervised entailment rule acquisition focused on rules between templates with two variables, ignoring unary rules - entailment rules between templates with a single variable.</S>
		<S sid ="2" ssid = "2">In this paper we investigate two approaches for unsupervised learning of such rules and compare the proposed methods with a binary rule learning method.</S>
		<S sid ="3" ssid = "3">The results show that the learned unary rule-sets outperform the binary rule-set.</S>
		<S sid ="4" ssid = "4">In addition, a novel directional similarity measure for learning entailment, termed Balanced-Inclusion, is the best performing measure.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">In many NLP applications, such as Question Answering (QA) and Information Extraction (IE), it is crucial to recognize whether a specific target meaning is inferred from a text.</S>
			<S sid ="6" ssid = "6">For example, a QA system has to deduce that “SCO sued IBM” is inferred from “SCO won a lawsuit against IBM” to answer “Whom did SCO sue?”.</S>
			<S sid ="7" ssid = "7">This type of reasoning has been identified as a core semantic inference paradigm by the generic Textual Entail- ment framework (Giampiccolo et al., 2007).</S>
			<S sid ="8" ssid = "8">An important type of knowledge needed for such inference is entailment rules.</S>
			<S sid ="9" ssid = "9">An entailment rule specifies a directional inference relation between two templates, text patterns with variables, such as ‘X win lawsuit against Y → X sue Y ’.</S>
			<S sid ="10" ssid = "10">Applying this rule by matching ‘X win lawsuit against Y ’ in the above text allows a QA system to Qc 2008.</S>
			<S sid ="11" ssid = "11">Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid ="12" ssid = "12">Some rights reserved.</S>
			<S sid ="13" ssid = "13">infer ‘X sue Y ’ and identify “IBM”, Y ’s instantiation, as the answer for the above question.</S>
			<S sid ="14" ssid = "14">Entail- ment rules capture linguistic and world-knowledge inferences and are used as an important building block within different applications, e.g.</S>
			<S sid ="15" ssid = "15">(Romano et al., 2006).</S>
			<S sid ="16" ssid = "16">One reason for the limited performance of generic semantic inference systems is the lack of broad-scale knowledge-bases of entailment rules (in analog to lexical resources such as WordNet).</S>
			<S sid ="17" ssid = "17">Supervised learning of broad coverage rule-sets is an arduous task.</S>
			<S sid ="18" ssid = "18">This sparked intensive research on unsupervised acquisition of entailment rules (and similarly paraphrases) e.g.</S>
			<S sid ="19" ssid = "19">(Lin and Pantel, 2001; Szpektor et al., 2004; Sekine, 2005).</S>
			<S sid ="20" ssid = "20">Most unsupervised entailment rule acquisition methods learn binary rules, rules between templates with two variables, ignoring unary rules, rules between unary templates (templates with only one variable).</S>
			<S sid ="21" ssid = "21">However, a predicate quite often appears in the text with just a single variable (e.g. intransitive verbs or passives), where infer ence requires unary rules, e.g. ‘X take a nap → X sleep’ (further motivations in Section 3.1).</S>
			<S sid ="22" ssid = "22">In this paper we focus on unsupervised learning of unary entailment rules.</S>
			<S sid ="23" ssid = "23">Two learning approaches are proposed.</S>
			<S sid ="24" ssid = "24">In our main approach, rules are learned by measuring how similar the variable instantiations of two templates in a corpus are.</S>
			<S sid ="25" ssid = "25">In addition to adapting state-of-the-art similarity measures for unary rule learning, we propose a new measure, termed Balanced-Inclusion, which balances the notion of directionality in entailment with the common notion of symmetric semantic similarity.</S>
			<S sid ="26" ssid = "26">In a second approach, unary rules are derived from binary rules learned by state-of-the- art binary rule learning methods.</S>
			<S sid ="27" ssid = "27">We tested the various unsupervised unary rule 849 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849–856 Manchester, August 2008 learning methods, as well as a binary rule learning method, on a test set derived from a standard IE benchmark.</S>
			<S sid ="28" ssid = "28">This provides the first comparison between the performance of unary and binary rule- sets.</S>
			<S sid ="29" ssid = "29">Several results rise from our evaluation: (a) while most work on unsupervised learning ignored unary rules, all tested unary methods outperformed the binary method; (b) it is better to learn unary rules directly than to derive them from a binary rule-base; (c) our proposed Balanced-Inclusion measure outperformed all other tested methods in terms of F1 measure.</S>
			<S sid ="30" ssid = "30">Moreover, only Balanced- Inclusion improved F1 score over a baseline inference that does not use entailment rules at all .</S>
	</SECTION>
	<SECTION title="Background. " number = "2">
			<S sid ="31" ssid = "1">This section reviews relevant distributional similarity measures, both symmetric and directional, which were applied for either lexical similarity or unsupervised entailment rule learning.</S>
			<S sid ="32" ssid = "2">Distributional similarity measures follow the Distributional Hypothesis, which states that words that occur in the same contexts tend to have similar meanings (Harris, 1954).</S>
			<S sid ="33" ssid = "3">Various measures were proposed in the literature for assessing such similarity between two words, u and v. Given a word q, its set of features Fq and feature weights wq (f ) for f ∈ Fq , a common symmetric similarity measure is Lin similarity (Lin, 1998a): Binary rule learning algorithms adopted such lexical similarity approaches for learning rules between templates, where the features of each template are its variable instantiations in a corpus, such as {X =‘SCO’, Y =‘IBM’} for the example in Section 1.</S>
			<S sid ="34" ssid = "4">Some works focused on learning rules from comparable corpora, containing comparable documents such as different news articles from the same date on the same topic (Barzilay and Lee, 2003; Ibrahim et al., 2003).</S>
			<S sid ="35" ssid = "5">Such corpora are highly informative for identifying variations of the same meaning, since, typically, when variable instantiations are shared across comparable documents the same predicates are described.</S>
			<S sid ="36" ssid = "6">However, it is hard to collect broad-scale comparable corpora, as the majority of texts are non-comparable.</S>
			<S sid ="37" ssid = "7">A complementary approach is learning from the abundant regular, non-comparable, corpora.</S>
			<S sid ="38" ssid = "8">Yet, in such corpora it is harder to recognize variations of the same predicate.</S>
			<S sid ="39" ssid = "9">The DIRT algorithm (Lin and Pantel, 2001) learns non directional binary rules for templates that are paths in a dependency parse-tree between two noun variables X and Y . The similarity between two templates t and tl is the geometric average: Linx(t, tl) · Liny (t, tl) where Linx is the Lin similarity between X ’s in- stantiations of t and X ’s instantiations of tl in a corpus (equivalently for Liny ).</S>
			<S sid ="40" ssid = "10">Some works Lin(u, v) = f ∈Fu ∩Fv [wu(f ) + wv (f )] take the combination of the two variable instantia f ∈Fu wu(f ) + f ∈Fv wv (f ) tions in each template occurrence as a single complex feature, e.g. {X -Y =‘SCOIBM’}, and com where the weight of each feature is the pointwise mutual information (pmi) between the word and the feature: wq (f ) = log[ P r(f |q) ].</S>
			<S sid ="41" ssid = "11">Weeds and Weir (2003) proposed to measure the symmetric similarity between two words by averaging two directional (asymmetric) scores: the coverage of each word’s features by the other.</S>
			<S sid ="42" ssid = "12">The coverage of u by v is measured by: pare between these complex features of t and tl (Ravichandran and Hovy, 2002; Szpektor et al., 2004; Sekine, 2005).</S>
			<S sid ="43" ssid = "13">Directional Measures Most rule learning methods apply a symmetric similarity measure between two templates, viewing them as paraphrasing each other.</S>
			<S sid ="44" ssid = "14">However, entailment is in general a direc tional relation.</S>
			<S sid ="45" ssid = "15">For example, ‘X acquire Y → X own Y ’ and ‘countersuit against X → lawsuit C over(u, v) = f ∈Fu ∩Fv wu(f ) against X ’.</S>
			<S sid ="46" ssid = "16">f ∈Fu wu(f ) The average can be arithmetic or harmonic: W eedsA(u, v) = 1 [C over(u, v) + C over(v, u)] 2 · C over(u, v) · C over(v, u) (Weeds and Weir, 2003) propose a directional measure for learning hyponymy between twowords, ‘l → r’, by giving more weight to the cov erage of the features of l by r (with α &gt; 1 ): W eedsD(l, r) = αC over(l, r)+(1−α)C over(r, l) W eedsH (u, v) = C over(u, v) + C over(v, u) When α=1, this measure degenerates into Weeds et al. also used pmi for feature weights.</S>
			<S sid ="47" ssid = "17">C over(l, r), termed P recision(l, r).</S>
			<S sid ="48" ssid = "18">With P recision(l, r) we obtain a “soft” version of the inclusion hypothesis presented in (Geffet and Dagan, 2005), which expects l to entail r if the “important” features of l appear also in r. Similarly, the LEDIR algorithm (Bhagat et al., 2007) identifies the entailment direction between two binary templates, l and r, which participate in a relation learned by (the symmetric) DIRT, by measuring the proportion of instantiations of l that are covered by the instantiations of r. As far as we know, only (Shinyama et al., 2002) and (Pekar, 2006) learn rules between unary templates.</S>
			<S sid ="49" ssid = "19">However, (Shinyama et al., 2002) relies on comparable corpora for identifying paraphrases and simply takes any two templates from comparable sentences that share a named entity instantiation to be paraphrases.</S>
			<S sid ="50" ssid = "20">Such approach is not feasible for non-comparable corpora where statistical measurement is required.</S>
			<S sid ="51" ssid = "21">(Pekar, 2006) learns rules only between templates related by local discourse (information from different documents is ignored).</S>
			<S sid ="52" ssid = "22">In addition, their template structure is limited to only verbs and their direct syntactic arguments, which may yield incorrect rules, e.g. for light verbs (see Section 5.2).</S>
			<S sid ="53" ssid = "23">To overcome this limitation, we use a more expressive template structure.</S>
	</SECTION>
	<SECTION title="Learning Unary Entailment Rules. " number = "3">
			<S sid ="54" ssid = "1">3.1 Motivations.</S>
			<S sid ="55" ssid = "2">Most unsupervised rule learning algorithms focused on learning binary entailment rules.</S>
			<S sid ="56" ssid = "3">However, using binary rules for inference is not enough.</S>
			<S sid ="57" ssid = "4">First, a predicate that can have multiple arguments may still occur with only one of its arguments.</S>
			<S sid ="58" ssid = "5">For example, in “The acquisition of TCA was successful”, ‘TCA’ is the only argument of ‘acquisition’.</S>
			<S sid ="59" ssid = "6">Second, some predicate expressions are unary by nature.</S>
			<S sid ="60" ssid = "7">For example, modifiers, such as ‘the elected X ’, or intransitive verbs.</S>
			<S sid ="61" ssid = "8">In addition, it appears more tractable to learn all variations for each argument of a predicate separately than to learn them for combinations of argument pairs.</S>
			<S sid ="62" ssid = "9">For these reasons, it seems that unary rule learning should be addressed in addition to binary rule learning.</S>
			<S sid ="63" ssid = "10">We are further motivated by the fact that some (mostly supervised) works in IE found learning unary templates useful for recognizing relevant named entities (Riloff, 1996; Sudo et al., 2003; Shinyama and Sekine, 2006), though they did not attempt to learn generic knowledge bases of entail ment rules.</S>
			<S sid ="64" ssid = "11">This paper investigates acquisition of unary entailment rules from regular non-comparable corpora.</S>
			<S sid ="65" ssid = "12">We first describe the structure of unary templates and then explore two conceivable approaches for learning unary rules.</S>
			<S sid ="66" ssid = "13">The first approach directly assesses the relation between two given templates based on the similarity of their in- stantiations in the corpus.</S>
			<S sid ="67" ssid = "14">The second approach, which was also mentioned in (Iftene and BalahurDobrescu, 2007), derives unary rules from learned binary rules.</S>
			<S sid ="68" ssid = "15">3.2 Unary Template Structure.</S>
			<S sid ="69" ssid = "16">To learn unary rules we first need to define their structure.</S>
			<S sid ="70" ssid = "17">In this paper we work at the syntactic representation level.</S>
			<S sid ="71" ssid = "18">Texts are represented by dependency parse trees (using the Minipar parser (Lin, 1998b)) and templates by parse sub-trees.</S>
			<S sid ="72" ssid = "19">Given a dependency parse tree, any sub-tree can be a candidate template, setting some of its nodes as variables (Sudo et al., 2003).</S>
			<S sid ="73" ssid = "20">However, the number of possible templates is exponential in the size of the sentence.</S>
			<S sid ="74" ssid = "21">In the binary rule learning literature, the main solution for exhaustively learning all rules between any pair of templates in a given corpus is to restrict the structure of templates.</S>
			<S sid ="75" ssid = "22">Typically, a template is restricted to be a path in a parse tree between two variable nodes (Lin and Pantel, 2001; Ibrahim et al., 2003).</S>
			<S sid ="76" ssid = "23">Following this approach, we chose the structure of unary templates to be paths as well, where one end of the path is the template’s variable.</S>
			<S sid ="77" ssid = "24">However, paths with one variable have more expressive power than paths between two variables, since the combination of two unary paths may generate a binary template that is not a path.</S>
			<S sid ="78" ssid = "25">For example, the combination of ‘X call indictable’ and ‘call Y indictable’ is the template ‘X call Y indictable’, which is not a path between X and Y . For every noun node v in a parsed sentence, we generate templates with v as a variable as follows: 1.</S>
			<S sid ="79" ssid = "26">Traverse the path from v towards the root of.</S>
			<S sid ="80" ssid = "27">the parse tree.</S>
			<S sid ="81" ssid = "28">Whenever a candidate predicate is encountered (any noun, adjective or verb) the path from that node to v is taken as a template.</S>
			<S sid ="82" ssid = "29">We stop when the first verb or clause boundary (e.g. a relative clause) is encountered, which typically represent the syntactic boundary of a specific predicate.</S>
			<S sid ="83" ssid = "30">2.</S>
			<S sid ="84" ssid = "31">To enable templates with control verbs and.</S>
			<S sid ="85" ssid = "32">light verbs, e.g. ‘X help preventing’, ‘X make noise’, whenever a verb is encountered we generate templates that are paths between v and the verb’s modifiers, either objects, prepositional complements or infinite or gerund verb forms (paths ending at stop words, e.g. pronouns, are not generated).</S>
			<S sid ="86" ssid = "33">3.</S>
			<S sid ="87" ssid = "34">To capture noun modifiers that act as predi-.</S>
			<S sid ="88" ssid = "35">cates, e.g. ‘the losing X ’, we extract template paths between v and each of its modifiers, nouns or adjectives, that are derived from a verb.</S>
			<S sid ="89" ssid = "36">We use the Catvar database to identify verb derivations (Habash and Dorr, 2003).</S>
			<S sid ="90" ssid = "37">As an example for the procedure, the templates extracted from the sentence “The losing party played it safe” with ‘party’ as the variable are: ‘losing X ’, ‘X play’ and ‘X play safe’.</S>
			<S sid ="91" ssid = "38">3.3 Direct Learning of Unary Rules.</S>
			<S sid ="92" ssid = "39">We applied the lexical similarity measures presented in Section 2 for unary rule learning.</S>
			<S sid ="93" ssid = "40">Each argument instantiation of template t in the corpus is taken as a feature f , and the pmi between t and f is used for the feature’s weight.</S>
			<S sid ="94" ssid = "41">We first adapted DIRT for unary templates (unary-DIRT, applying Lin-similarity to the single feature vector), as well as its output filtering by LEDIR.</S>
			<S sid ="95" ssid = "42">The various Weeds measures were also applied1: symmetric arithmetic average, symmetric harmonic average, weighted arithmetic average and Precision.</S>
			<S sid ="96" ssid = "43">After initial analysis, we found that given a right hand side template r, symmetric measures such as Lin (in DIRT) generally tend to prefer (score higher) relations (l, r) in which l and r are related but do not necessarily participate in an entailment or equivalence relation, e.g. the wrong rule ‘kill X ↔ injure X ’.</S>
			<S sid ="97" ssid = "44">On the other hand, directional measures such as Weeds Precision tend to prefer directional rules in which the entailing template is infrequent.</S>
			<S sid ="98" ssid = "45">If an infrequent template has common instantiations with another template, the coverage of its features is typically high, whether or not an entailment relation exists between the two templates.</S>
			<S sid ="99" ssid = "46">This behavior generates high-score incorrect rules.</S>
			<S sid ="100" ssid = "47">Based on this analysis, we propose a new measure that balances the two behaviors, termed Balanced-Inclusion (BInc).</S>
			<S sid ="101" ssid = "48">BInc identifies entailing templates based on a directional measure but penalizes infrequent templates using a symmetric measure: BI nc(l, r) = Lin(l, r) · P recision(l, r) 3.4 Deriving Unary Rules From Binary Rules.</S>
			<S sid ="102" ssid = "49">An alternative way to learn unary rules is to first learn binary entailment rules and then derive unary rules from them.</S>
			<S sid ="103" ssid = "50">We derive unary rules from a given binary rule-base in two steps.</S>
			<S sid ="104" ssid = "51">First, for each binary rule, we generate all possible unary rules that are part of that rule (each unary template is extracted following the same procedure described in Section 3.2).</S>
			<S sid ="105" ssid = "52">For example, from ‘X find solu tion to Y → X solve Y ’ we generate the unary rules ‘X find → X solve’, ‘X find solution → Xsolve’, ‘solution to Y → solve Y ’ and ‘find solu tion to Y → solve Y ’.</S>
			<S sid ="106" ssid = "53">The score of each generated rule is set to be the score of the original binary rule.</S>
			<S sid ="107" ssid = "54">The same unary rule can be derived from different binary rules.</S>
			<S sid ="108" ssid = "55">For example, ‘hire Y → employ Y ’ is derived both from ‘X hire Y → X employ Y ’ and ‘hire Y for Z → employ Y for Z ’, having a different score from each original binary rule.</S>
			<S sid ="109" ssid = "56">The second step of the algorithm aggregates the different scores yielded for each derived rule to produce the final rule score.</S>
			<S sid ="110" ssid = "57">Three aggregation functions were tested: sum (Derived-Sum), average (Derived-Avg) and maximum (Derived-Max).</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "4">
			<S sid ="111" ssid = "1">We want to evaluate learned unary and binary rule bases by their utility for NLP applications through assessing the validity of inferences that are performed in practice using the rule base.</S>
			<S sid ="112" ssid = "2">To perform such experiments, we need a test- set of seed templates, which correspond to a set of target predicates, and a corpus annotated with all argument mentions of each predicate.</S>
			<S sid ="113" ssid = "3">The evaluation assesses the correctness of all argument extractions, which are obtained by matching in the corpus either the seed templates or templates that entail them according to the rule-base (the latter corresponds to rule-application).</S>
			<S sid ="114" ssid = "4">Following (Szpektor et al., 2008), we found the ACE 2005 event training set2 useful for this purpose.</S>
			<S sid ="115" ssid = "5">This standard IE dataset includes 33 types of event predicates such as Injure, Sue and Divorce.</S>
			<S sid ="116" ssid = "6">1 We applied the best performing parameter values pre-.</S>
			<S sid ="117" ssid = "7">sented in (Bhagat et al., 2007) and (Weeds and Weir, 2003).</S>
			<S sid ="118" ssid = "8">2 http://projects.ldc.upenn.edu/ace/ All event mentions are annotated in the corpus, including the instantiated arguments of the predicate.</S>
			<S sid ="119" ssid = "9">ACE guidelines specify for each event its possible arguments, each associated with a semantic role.</S>
			<S sid ="120" ssid = "10">For instance, some of the Injure event arguments are Agent, Victim and Time.</S>
			<S sid ="121" ssid = "11">To utilize the ACE dataset for evaluating entail- ment rule applications, we manually represented each ACE event predicate by unary seed templates.</S>
			<S sid ="122" ssid = "12">For example, the seed templates for Injure are ‘A injure’, ‘injure V ’ and ‘injure in T ’.</S>
			<S sid ="123" ssid = "13">We mapped each event role annotation to the corresponding seed template variable, e.g. ‘Agent’ to A and ‘Victim’ to V in the above example.</S>
			<S sid ="124" ssid = "14">Templates are matched using a syntactic matcher that handles simple morpho-syntactic phenomena, as in (Szpektor and Dagan, 2007).</S>
			<S sid ="125" ssid = "15">A rule application is considered correct if the matched argument is annotated by the corresponding ACE role.</S>
			<S sid ="126" ssid = "16">For testing binary rule-bases, we automatically generated binary seed templates from any two unary seeds that share the same predicate.</S>
			<S sid ="127" ssid = "17">For example, for Injure the binary seeds ‘A injure V ’, ‘A injure in T ’ and ‘injure V in T ’ were automatically generated from the above unary seeds.</S>
			<S sid ="128" ssid = "18">We performed two adaptations to the ACE dataset to fit it better to our evaluation needs.</S>
			<S sid ="129" ssid = "19">First, our evaluation aims at assessing the correctness of inferring a specific target semantic meaning, which is denoted by a specific predicate, using rules.</S>
			<S sid ="130" ssid = "20">Thus, four events that correspond ambiguously to multiple distinct predicates were ignored.</S>
			<S sid ="131" ssid = "21">For instance, the Transfer-Money event refers to both donating and lending money, and thus annotations of this event cannot be mapped to a specific seed template.</S>
			<S sid ="132" ssid = "22">We also omitted 3 events with less than 10 mentions, and were left with 26 events (6380 argument mentions).</S>
			<S sid ="133" ssid = "23">Additionally, we regard all entailing mentions under the textual entailment definition as correct.</S>
			<S sid ="134" ssid = "24">However, event mentions are annotated as correct in ACE only if they explicitly describe the target event.</S>
			<S sid ="135" ssid = "25">For instance, a Divorce mention does entail a preceding marriage event but it does not explicitly describe it, and thus it is not annotated as a Marry event.</S>
			<S sid ="136" ssid = "26">To better utilize the ACE dataset, we considered for a target event the annotations of other events that entail it as being correct as well.</S>
			<S sid ="137" ssid = "27">We note that each argument was considered sep person, but did not consider the place and time of the divorce act to be those of the marriage .</S>
	</SECTION>
	<SECTION title="Results and Analysis. " number = "5">
			<S sid ="138" ssid = "1">We implemented the unary rule learning algorithms described in Section 3 and the binary DIRT algorithm (Lin and Pantel, 2001).</S>
			<S sid ="139" ssid = "2">We executed each method over the Reuters RCV1 corpus3, learning for each template r in the corpus the top 100 rules in which r is entailed by another template l, ‘l → r’.</S>
			<S sid ="140" ssid = "3">All rules were learned in canonical form (Szpektor and Dagan, 2007).</S>
			<S sid ="141" ssid = "4">The rule-base learned by binary DIRT was taken as the input for deriving unary rules from binary rules.</S>
			<S sid ="142" ssid = "5">The performance of each acquired rule-base was measured for each ACE event.</S>
			<S sid ="143" ssid = "6">We measured the percentage of correct argument mentions extracted out of all correct argument mentions annotated for the event (recall) and out of all argument mentions extracted for the event (precision).</S>
			<S sid ="144" ssid = "7">We also measured F1, their harmonic average, and report macro average Recall, Precision and F1 over the 26 event types.</S>
			<S sid ="145" ssid = "8">No threshold setting mechanism is suggested in the literature for the scores of the different algorithms, especially since rules for different right hand side templates have different score ranges.</S>
			<S sid ="146" ssid = "9">Thus, we follow common evaluation practice (Lin and Pantel, 2001; Geffet and Dagan, 2005) and test each learned rule-set by taking the top K rules for each seed template, where K ranges from 0 to 100.</S>
			<S sid ="147" ssid = "10">When K =0, no rules are used and mentions are extracted only by direct matching of seed templates.</S>
			<S sid ="148" ssid = "11">Our rule application setting provides a rather simplistic IE system (for example, no named entity recognition or approximate template matching).</S>
			<S sid ="149" ssid = "12">It is thus useful for comparing different rule-bases, though the absolute extraction figures do not reflect the full potential of the rules.</S>
			<S sid ="150" ssid = "13">In Secion 5.2 we analyze the full-system’s errors to isolate the rules’ contribution to overall system performance.</S>
			<S sid ="151" ssid = "14">5.1 Results.</S>
			<S sid ="152" ssid = "15">In this section we focus on the best performing variations of each algorithm type: binary DIRT, unary DIRT, unary Weeds Harmonic, BInc and Derived-Avg.</S>
			<S sid ="153" ssid = "16">We omitted the results of methods that were clearly inferior to others: (a) W eedsA, W eedsD and W eeds-P recision did not increase arately.</S>
			<S sid ="154" ssid = "17">For example, we marked a mention of a divorced person as entailing the marriage of that 3 http://about.reuters.com/researchandstandards/corpus/ Recall over not using rules because rules with infrequent templates scored highest and arithmetic averaging could not balance well these high scores; (b) out of the methods for deriving unary rules from binary rule-bases, Derived-Avg performed best; (c) filtering with (the directional) LEDIR did not improve the performance of unary DIRT.</S>
			<S sid ="155" ssid = "18">Figure 1 presents Recall, Precision and F1 of the methods for different cutoff points.</S>
			<S sid ="156" ssid = "19">First, we observe that even when matching only the seed templates (K =0), unary seeds outperform the binary seeds in terms of both Precision and Recall.</S>
			<S sid ="157" ssid = "20">This surprising behavior is consistent through all rule cutoff points: all unary learning algorithms perform better than binary DIRT in all parameters.</S>
			<S sid ="158" ssid = "21">The inferior behavior of binary DIRT is analyzed in Section 5.2.</S>
			<S sid ="159" ssid = "22">The graphs show that symmetric unary approaches substantially increase recall, but dramatically decrease precision already at the top 10 rules.</S>
			<S sid ="160" ssid = "23">As a result, F1 only decreases for these methods.</S>
			<S sid ="161" ssid = "24">Lin similarity (DIRT) and Weeds-Harmonic show similar behaviors.</S>
			<S sid ="162" ssid = "25">They consistently outperform Derived-Avg.</S>
			<S sid ="163" ssid = "26">One reason for this is that incorrect unary rules may be derived even from correct binary rules.</S>
			<S sid ="164" ssid = "27">For example, from ‘X gain seat on Y → elect X to Y ’ the incorrect unary rule ‘X gain → elect X ’ is also generated.</S>
			<S sid ="165" ssid = "28">This problem is less frequent when unary rules are directly scored based on their corpus statistics.</S>
			<S sid ="166" ssid = "29">The directional measure of BInc yields a more accurate rule-base, as can be seen by the much slower precision reduction rate compared to the other algorithms.</S>
			<S sid ="167" ssid = "30">As a result, it is the only algorithm that improves over the F1 baseline of K =0, with the best cutoff point at K =20.</S>
			<S sid ="168" ssid = "31">BInc’s recall increases moderately compared to other unary learning approaches, but it is still substantially better than not using rules (a relative recall increase of 50% already at K =10).</S>
			<S sid ="169" ssid = "32">We found that many of the correct mentions missed by BInc but identified by other methods are due to occasional extractions of incorrect frequent rules, such as partial templates (see Section 5.2).</S>
			<S sid ="170" ssid = "33">This is reflected in the very low precision of the other methods.</S>
			<S sid ="171" ssid = "34">On the other hand, some correct rules were only learned by BInc, e.g. ‘countersuit against X → X sue’ and ‘X take wife → X marry’.</S>
			<S sid ="172" ssid = "35">When only one argument is annotated for a specific event mention (28% of ACE predicate mentions, which account for 15% of all annotated ar Figure 1: Average Precision, Recall and F1 at different top K rule cutoff points.</S>
			<S sid ="173" ssid = "36">guments), binary rules either miss that mention, or extract both the correct argument and another incorrect one.</S>
			<S sid ="174" ssid = "37">To neutralize this bias, we also tested the various methods only on event mentions annotated with two or more arguments and obtained similar results to those presented for all mentions.</S>
			<S sid ="175" ssid = "38">This further emphasizes the general advantage of using unary rules over binary rules.</S>
			<S sid ="176" ssid = "39">5.2 Analysis.</S>
			<S sid ="177" ssid = "40">Binary-DIRT We analyzed incorrect rules both for binary-DIRT and BInc by randomly sampling, for each algorithm, 200 rules that extracted incorrect mentions.</S>
			<S sid ="178" ssid = "41">We manually classified each rule ‘l → r’ as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in correct mentions; (b) Partial Template - l is only a part of a correct template that entails r. For exam ple, learning ‘X decide → X meet’ instead of ‘X decide to meet → X meet’; (e) Incorrect - other incorrect rules, e.g. ‘charge X → convict X ’.Table 1 summarizes the analysis and demon strates two problems of binary-DIRT.</S>
			<S sid ="179" ssid = "42">First, relative to BInc, it tends to learn incorrect rules for high frequency templates, and therefore extracted many more incorrect mentions for the same number of incorrect rules.</S>
			<S sid ="180" ssid = "43">Second, a large percentage of incorrect mentions extracted are due to partial templates at the rule left-hand-side.</S>
			<S sid ="181" ssid = "44">Such rules are leaned because many binary templates have a more complex structure than paths between arguments.</S>
			<S sid ="182" ssid = "45">As explained in Section 3.2 the unary template structure we use is more expressive, enabling to learn the correct rules.</S>
			<S sid ="183" ssid = "46">For example, BInc learned‘take Y into custody → arrest Y ’ while binary DIRT learned ‘X take Y → X arrest Y ’.</S>
			<S sid ="184" ssid = "47">System Level Analysis We manually analyzed the reasons for false positives (incorrect extractions) and false negatives (missed extractions) of BInc, at its best performing cutoff point (K =20), by sampling 200 extractions of each type.</S>
			<S sid ="185" ssid = "48">From the false positives analysis (Table 2) we see that 39% of the errors are due to incorrect rules.</S>
			<S sid ="186" ssid = "49">The main reasons for learning such rules are those discussed in Section 3.3: (a) related templates that are not entailing; (b) infrequent templates.</S>
			<S sid ="187" ssid = "50">All learning methods suffer from these issues.</S>
			<S sid ="188" ssid = "51">As was shown by our results, BInc provides a first step towards reducing these problems.</S>
			<S sid ="189" ssid = "52">Yet, these issues require further research.</S>
			<S sid ="190" ssid = "53">Apart from incorrectly learned rules, incorrect template matching (e.g. due to parse errors) and context mismatch contribute together 46% of the errors.</S>
			<S sid ="191" ssid = "54">Context mismatches occur when the entailing template is matched in inappropriate contexts.</S>
			<S sid ="192" ssid = "55">For example, ‘slam X → attack X ’ should not be applied when X is a ball, only when it is a person.</S>
			<S sid ="193" ssid = "56">The rule-set net effect on system precision is better estimated by removing these errors and fixing the annotation errors, which yields 72% precision.</S>
			<S sid ="194" ssid = "57">Bi na ry DI RT Bal anc ed Inc lus ion Co rre ct 1 6 (70) 3 8 (91) Par tial Te mp lat e 2 7 (2665) 6 (81) Inc orr ect 15 7 (2584) 15 6 (787) Tot al 20 0 (5319) 20 0 (959) Table 1: Rule type distribution of a sample of 200 rules that extracted incorrect mentions.</S>
			<S sid ="195" ssid = "58">The corresponding numbers of incorrect mentions extracted by the sampled rules is shown in parentheses.</S>
			<S sid ="196" ssid = "59">R e a s o n % me nti on s In co rr ec t R ul e le ar n e d C o nt ex t m is m at c h M at c h er ro r A n n ot at io n pr o bl e m 3 9 . 0 2 7 . 0 1 9 . 0 1 5 . 0 Table 2: Distribution of reasons for false positives (incorrect argument extractions) by BInc at K =20.</S>
			<S sid ="197" ssid = "60">R e a s o n % me nti on s Rul e not lea rn ed Ma tch err or Dis cou rse an aly sis ne ed ed Arg um ent is pre dic ativ e 6 1 . 5 2 5 . 0 1 2 . 0 1.5 Table 3: Distribution of reasons for false negatives (missed argument mentions) by BInc at K =20.</S>
			<S sid ="198" ssid = "61">Table 3 presents the analysis of false negatives.</S>
			<S sid ="199" ssid = "62">First, we note that 12% of the arguments cannot be extracted by rules alone, due to necessary discourse analysis.</S>
			<S sid ="200" ssid = "63">Thus, a recall upper bound for entailment rules is 88%.</S>
			<S sid ="201" ssid = "64">Many missed extractions are due to rules that were not learned (61.5%).</S>
			<S sid ="202" ssid = "65">However, 25% of the mentions were missed because of incorrect syntactic matching of correctly learned rules.</S>
			<S sid ="203" ssid = "66">By assuming correct matches in these cases we isolate the recall of the rule-set (along with the seeds), which yields 39% recall.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="204" ssid = "1">We presented two approaches for unsupervised acquisition of unary entailment rules from regular (non-comparable) corpora.</S>
			<S sid ="205" ssid = "2">In the first approach, rules are directly learned based on distributional similarity measures.</S>
			<S sid ="206" ssid = "3">The second approach derives unary rules from a given rule-base of binary rules.</S>
			<S sid ="207" ssid = "4">Under the first approach we proposed a novel directional measure for scoring entailment rules, termed Balanced-Inclusion.</S>
			<S sid ="208" ssid = "5">We tested the different approaches utilizing a standard IE test-set and compared them to binary rule learning.</S>
			<S sid ="209" ssid = "6">Our results suggest the advantage of learning unary rules: (a) unary rule-bases perform better than binary rules; (b) it is better to directly learn unary rules than to derive them from binary rule-bases.</S>
			<S sid ="210" ssid = "7">In addition, the Balanced-Inclusion measure outperformed all other tested methods.</S>
			<S sid ="211" ssid = "8">In future work, we plan to explore additional unary template structures and similarity scores, and to improve rule application utilizing context matching methods such as (Szpektor et al., 2008).</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="212" ssid = "9">This work was partially supported by ISF grant 1095/05, the IST Programme of the European Community under the PASCAL Network of Excellence IST2002-506778 and the NEGEV project (www.negev-initiative.org).</S>
	</SECTION>
</PAPER>
