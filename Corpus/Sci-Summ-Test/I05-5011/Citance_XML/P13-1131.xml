<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Automatic acquisition of inference rules for predicates has been commonly addressed by computing distributional similarity between vectors of argument words, operating at the word space level.</S>
		<S sid ="2" ssid = "2">A recent line of work, which addresses context sensitivity of rules, represented contexts in a latent topic space and computed similarity over topic vectors.</S>
		<S sid ="3" ssid = "3">We propose a novel two-level model, which computes similarities between word-level vectors that are biased by topic-level context representations.</S>
		<S sid ="4" ssid = "4">Evaluations on a naturally- distributed dataset show that our model significantly outperforms prior word-level and topic-level models.</S>
		<S sid ="5" ssid = "5">We also release a first context-sensitive inference rule set.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Inference rules for predicates have been identified as an important component in semantic applications, such as Question Answering (QA) (Ravichandran and Hovy, 2002) and Information Extraction (IE) (Shinyama and Sekine, 2006).</S>
			<S sid ="7" ssid = "7">For example, the inference rule ‘X treat Y → X relieve Y ’ can be useful to extract pairs of drugs and the illnesses which they relieve, or to answer a question like “Which drugs relieve headache?”.</S>
			<S sid ="8" ssid = "8">Along this vein, such inference rules constitute a crucial component in generic modeling of textual inference, under the Textual Entailment paradigm (Dagan et al., 2006; Dinu and Wang, 2009).</S>
			<S sid ="9" ssid = "9">Motivated by these needs, substantial research was devoted to automatic learning of inference rules from corpora, mostly in an unsupervised dis- tributional setting.</S>
			<S sid ="10" ssid = "10">This research line was mainly initiated by the highly-cited DIRT algorithm (Lin and Pantel, 2001), which learns inference for binary predicates with two argument slots (like the rule in the example above).</S>
			<S sid ="11" ssid = "11">DIRT represents a predicate by two vectors, one for each of the argument slots, where the vector entries correspond to the argument words that occurred with the predicate in the corpus.</S>
			<S sid ="12" ssid = "12">Inference rules between pairs of predicates are then identified by measuring the similarity between their corresponding argument vectors.</S>
			<S sid ="13" ssid = "13">This general scheme was further enhanced in several directions, e.g. directional similarity (Bhagat et al., 2007; Szpektor and Dagan, 2008) and meta-classification over similarity values (Berant et al., 2011).</S>
			<S sid ="14" ssid = "14">Consequently, several knowledge resources of inference rules were released, containing the top scoring rules for each predicate (Schoenmackers et al., 2010; Berant et al., 2011; Nakashole et al., 2012).</S>
			<S sid ="15" ssid = "15">The above mentioned methods provide a single confidence score for each rule, which is based on the obtained degree of argument-vector similarities.</S>
			<S sid ="16" ssid = "16">Thus, a system that applies an inference rule to a text may estimate the validity of the rule application based on the pre-specified rule score.</S>
			<S sid ="17" ssid = "17">However, the validity of an inference rule may depend on the context in which it is applied, such as the context specified by the given predicate’s arguments.</S>
			<S sid ="18" ssid = "18">For example, ‘AT&amp;T acquire T Mobile → AT&amp;T purchase T-Mobile’, is a validapplication of the rule ‘X acquire Y → X purchase Y ’, while ‘Children acquire skills → Chil dren purchase skills’ is not.</S>
			<S sid ="19" ssid = "19">To address this issue, a line of works emerged which computes a context- sensitive reliability score for each rule application, based on the given context.</S>
			<S sid ="20" ssid = "20">The major trend in context-sensitive inference models utilizes latent or class-based methods for context modeling (Pantel et al., 2007; Szpektor et al., 2008; Ritter et al., 2010; Dinu and Lapata, 2010b).</S>
			<S sid ="21" ssid = "21">In particular, the more recent methods (Ritter et al., 2010; Dinu and Lapata, 2010b) modeled predicates in context as a probability distribution over topics learned by a Latent Dirichlet Allo 1331 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1331–1340, Sofia, Bulgaria, August 49 2013.</S>
			<S sid ="22" ssid = "22">Qc 2013 Association for Computational Linguistics cation (LDA) model.</S>
			<S sid ="23" ssid = "23">Then, similarity is measured between the two topic distribution vectors corresponding to the two sides of the rule in the given context, yielding a context-sensitive score for each particular rule application.</S>
			<S sid ="24" ssid = "24">We notice at this point that while context- insensitive methods represent predicates by argument vectors in the original fine-grained word space, context-sensitive methods represent them as vectors at the level of latent topics.</S>
			<S sid ="25" ssid = "25">This raises the question of whether such coarse-grained topic vectors might be less informative in determining the semantic similarity between the two predicates.</S>
			<S sid ="26" ssid = "26">To address this hypothesized caveat of prior context-sensitive rule scoring methods, we propose a novel generic scheme that integrates word- level and topic-level representations.</S>
			<S sid ="27" ssid = "27">Our scheme can be applied on top of any context-insensitive “base” similarity measure for rule learning, which operates at the word level, such as Cosine or Lin (Lin, 1998).</S>
			<S sid ="28" ssid = "28">Rather than computing a single context-insensitive rule score, we compute a distinct word-level similarity score for each topic in an LDA model.</S>
			<S sid ="29" ssid = "29">Then, when applying a rule in a given context, these different scores are weighed together based on the specific topic distribution under the given context.</S>
			<S sid ="30" ssid = "30">This way, we calculate learning, based on distributional similarity at the word level, and then context-sensitive scoring for rule applications, based on topic-level similarity.</S>
			<S sid ="31" ssid = "31">Some further discussion of related work appears in Section 6.</S>
			<SUBSECTION>2.1 Context-insensitive Rule Learning.</SUBSECTION>
			<S sid ="32" ssid = "32">A predicate inference rule ‘LHS → RHS’, such as ‘X acquire Y → X purchase Y’, specifies a directional inference relation between two predicates.</S>
			<S sid ="33" ssid = "33">Each rule side consists of a lexical predicate and (two) variable slots for its arguments.1 Different representations have been used to specify predicates and their argument slots, such as word lemma sequences, regular expressions and dependency parse fragments.</S>
			<S sid ="34" ssid = "34">A rule can be applied when its LHS matches a predicate with a pair of arguments in a text, allowing us to infer its RHS, with the corresponding instantiations for the argument variables.</S>
			<S sid ="35" ssid = "35">For example, given the text “AT&amp;T acquires T-Mobile”, the above rule infers “AT&amp;T purchases T-Mobile”.</S>
			<S sid ="36" ssid = "36">The DIRT algorithm (Lin and Pantel, 2001) follows the distributional similarity paradigm to learn predicate inference rules.</S>
			<S sid ="37" ssid = "37">For each predicate, DIRT represents each of its argument slots by an argument vector.</S>
			<S sid ="38" ssid = "38">We denote the two vectors of the X and Y slots of a predicate pred by vx ed similarity over vectors in the original word space, while biasing them towards the given context via a topic model.</S>
			<S sid ="39" ssid = "39">In order to promote replicability and equal-term comparison with our results, we based our experiments on publicly available datasets, both for unsupervised learning of the evaluated models and for testing them over a random sample of rule applications.</S>
			<S sid ="40" ssid = "40">We apply our two-level scheme over three state-of-the-art context-insensitive similarity measures.</S>
			<S sid ="41" ssid = "41">The evaluation compares performances both with the original context-insensitive measures and with recent LDA-based context- sensitive methods, showing consistent and robust advantages of our scheme.</S>
			<S sid ="42" ssid = "42">Finally, we release a context-sensitive rule resource comprising over and vy , respectively.</S>
			<S sid ="43" ssid = "43">Each entry of a vector v corresponds to a particular word (or term) w that instantiated the argument slot in a learning corpus, with a value v(w) = P M I (pred, w) (with PMI standing for point-wise mutual information).</S>
			<S sid ="44" ssid = "44">To learn inference rules, DIRT considers (in principle) each pair of binary predicates that occurred in the corpus for a candidate rule,‘LHS → RHS’.</S>
			<S sid ="45" ssid = "45">Then, DIRT computes a reliabil ity score for the rule by combining the measured similarities between the corresponding argument vectors of the two rule sides.</S>
			<S sid ="46" ssid = "46">Concretely, denoting by l and r the predicates appearing in the two rule sides, DIRT’s reliability score is defined as follows: 2,000 frequent verbs and one million rules.</S>
			<S sid ="47" ssid = "47">scoreDIRT(LHS → RHS) = r (1) sim(vx, vx) · sim(vy , vy )</S>
	</SECTION>
	<SECTION title="Background and Model Setting. " number = "2">
			<S sid ="48" ssid = "1">This section presents components of prior work which are included in our model and experiments, setting the technical preliminaries for the rest of the paper.</S>
			<S sid ="49" ssid = "2">We first present context-insensitive rule l r l where sim(v, v1) is a vector similarity measure.</S>
			<S sid ="50" ssid = "3">Specifically, DIRT employs the Lin similarity 1 We follow most of the inference-rule learning literature, which focused on binary predicates.</S>
			<S sid ="51" ssid = "4">However, our context- sensitive scheme can be applied to any arity.</S>
			<S sid ="52" ssid = "5">measure from (Lin, 1998), defined as follows: 1 w∈v∩v (2) own model and in comparative model evaluations.</S>
			<S sid ="53" ssid = "6">We note that a similar LDA model construction was employed also in (Se´aghdha, 2010), for esti w∈v∪v [v(w) + v1(w)] We note that the general DIRT scheme may be used while employing other “base” vector similarity measures.</S>
			<S sid ="54" ssid = "7">For example, the Lin measure is symmetric, and thus using it would yield the same reliability score when swapping the two sides of a rule.</S>
			<S sid ="55" ssid = "8">This issue has been addressed in a separate line of research which introduced directional similarity measures suitable for inference relations (Bhagat et al., 2007; Szpektor and Dagan, 2008; Kotlerman et al., 2010).</S>
			<S sid ="56" ssid = "9">In our experiments we apply our proposed context-sensitive similarity scheme over three different base similarity mea sures.</S>
			<S sid ="57" ssid = "10">mating predicate-argument likelihood.</S>
			<S sid ="58" ssid = "11">First, an LDA model is constructed, as follows.</S>
			<S sid ="59" ssid = "12">Similar to the construction of argument vectors in the distributional model (described above in subsection 2.1), all arguments instantiating each predicate slot are extracted from a large learning corpus.</S>
			<S sid ="60" ssid = "13">Then, for each slot of each predicate, a pseudo-document is constructed containing the set of all argument words that instantiated this slot in the corpus.</S>
			<S sid ="61" ssid = "14">We denote the two documents constructed for the X and Y slots of a predicate pred by dx and dy , respectively.</S>
			<S sid ="62" ssid = "15">In comparison to pred predthe distributional model, these two documents cor respond to the analogous argument vectors vx and vy , both containing exactly the same set of DIRT and similar context-insensitive inference methods provide a single reliability score for a learned inference rule, which aims to predict the validity of the rule’s applications.</S>
			<S sid ="63" ssid = "16">However, as exemplified in the Introduction, an inference rule may be valid in some contexts but invalid in others (e.g. acquiring entails purchasing for goods, but not for skills).</S>
			<S sid ="64" ssid = "17">Since vector similarity in DIRT is computed over the single aggregate argument vector, the obtained reliability score tends to be biased towards the dominant contexts of the involved predicates.</S>
			<S sid ="65" ssid = "18">For example, we may expect a higher score for ‘acquire → purchase’ than for ‘acquire → learn’, since the former matches a more frequent sense of acquire in a typical corpus.</S>
			<S sid ="66" ssid = "19">Following this observation, it is desired to obtain a context-sensitive reliability score for each rule application in a given context, as described next.</S>
			<S sid ="67" ssid = "20">2.2 Context-sensitive Rule Applications.</S>
			<S sid ="68" ssid = "21">To assess the reliability of applying an inference rule in a given context we need some model for context representation, that should affect the rule reliability score.</S>
			<S sid ="69" ssid = "22">A major trend in past work is to represent contexts in a reduced-dimensionality latent or class-based model.</S>
			<S sid ="70" ssid = "23">A couple of earlier works utilized a cluster-based model (Pantel et al., 2007) and an LSA-based model (Szpektor et al., 2008), in a selectional-preferences style approach.</S>
			<S sid ="71" ssid = "24">Several more recent works utilize a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) framework.</S>
			<S sid ="72" ssid = "25">We now present an underlying unified view of the topic-level models in (Ritter et al., 2010; Dinu and Lapata, 2010b), which we follow in our pred words.</S>
			<S sid ="73" ssid = "26">Next, an LDA model is learned from the set of all pseudo-documents, extracted for all predicates.2 The learning process results in the construction of K latent topics, where each topic t specifies a distribution over all words, denoted by p(w|t), and a topic distribution for each pseudo- document d, denoted by p(t|d).</S>
			<S sid ="74" ssid = "27">Within the LDA model we can derive the a-posteriori topic distribution conditioned on a particular word within a document, denoted by p(t|d, w) ∝ p(w|t) · p(t|d).</S>
			<S sid ="75" ssid = "28">In the topic-level model, d corresponds to a predicate slot and w to a particular argument word instantiating this slot.</S>
			<S sid ="76" ssid = "29">Hence, p(t|d, w) is viewed as specifying the rele vance (or likelihood) of the topic t for the predicate slot in the context of the given argument in- stantiation.</S>
			<S sid ="77" ssid = "30">For example, for the predicate slot ‘acquire Y ’ in the context of the argument ‘IBM’, we expect high relevance for a topic about companies, while in the context of the argument ‘knowledge’ we expect high relevance for a topic about abstract concepts.</S>
			<S sid ="78" ssid = "31">Accordingly, the distribution p(t|d, w)over all topics provides a topic-level representa tion for a predicate slot in the context of a particular argument w. This representation is used by the topic-level model to compute a context-sensitive score for inference rule applications, as follows.</S>
			<S sid ="79" ssid = "32">2 We note that there are variants in the type of LDA model and the way the pseudo-documents are constructed in the referenced prior work.</S>
			<S sid ="80" ssid = "33">In order to focus on the inference methods rather than on the underlying LDA model, we use the LDA framework described in this paper for all compared methods.</S>
			<S sid ="81" ssid = "34">Consider the application of an inference rule ‘LHS → RHS’ in the context of a particular pair of arguments for the X and Y slots, denoted by wx and wy , respectively.</S>
			<S sid ="82" ssid = "35">Denoting by l and r the predicates appearing in the two rule sides, the reliability score of the topic-level model is defined as follows (we present a geometric mean formulation for consistency with DIRT): scoreTopic(LHS → RHS, wx, wy ) (3) slots in the original word-level space while biasing the similarity measure through topic-level context models.</S>
	</SECTION>
	<SECTION title="Two-level Context-sensitive Inference. " number = "3">
			<S sid ="83" ssid = "1">Our model follows the general DIRT scheme while extending it to handle context-sensitive scoring of rule applications, addressing the scenario dealt by the context-sensitive topic models.</S>
			<S sid ="84" ssid = "2">In particular, we define the context-sensitive score = sim(dx, dx, wx) · sim(dy , dy , w ) l r l r y scoreWT, where W T stands for the combination of the Word/Topic levels:where sim(d, d1, w) is a topic-distribution similar ity measure conditioned on a given context word.</S>
			<S sid ="85" ssid = "3">Specifically, Ritter et al.</S>
			<S sid ="86" ssid = "4">(2010) utilized the dot product form for their similarity measure: scoreWT(LHS → RHS, wx, wy ) (6) = sim(vx, vx, wx) · sim(vy , vy , w ) l r l r y simDC(d, d1, w) = Σt[p(t|d, w) · p(t|d1, w)] (4) Thus, our model computes similarity over word (the subscript DC stands for double-conditioning, as both distributions are conditioned on the argument word, unlike the measure below).</S>
			<S sid ="87" ssid = "5">Dinu and Lapata (2010b) presented a slightly different similarity measure for topic distributions that performed better in their setting as well as in a related later paper on context-sensitive scoring of lexical similarity (Dinu and Lapata, 2010a).</S>
			<S sid ="88" ssid = "6">In this measure, the topic distribution for the right hand side of the rule is not conditioned on w: level (rather than topic-level) argument vectors, while biasing it according to the specific argument words in the given rule application context.</S>
			<S sid ="89" ssid = "7">The core of our contribution is thus defining the context-sensitive word-level vector similaritymeasure sim(v, v1, w), as described in the remain der of this section.</S>
			<S sid ="90" ssid = "8">Following the methods in Section 2, for each predicate pred we construct, from the learning corpus, its argument vectors vx y pred and vpred as simSC(d, d1, w) = Σt[p(t|d, w) · p(t|d1)] (5) well as its argument pseudo-documents dx dy and (the subscript SC stands for single-conditioning, as only the left distribution is conditioned on the argument word).</S>
			<S sid ="91" ssid = "9">They also experimented with a few variants for the structure of the similarity measure and assessed that best results are obtained with the dot product form.</S>
			<S sid ="92" ssid = "10">In our experiments, we employ these two similarity measures for topic distributions as baselines representing topic-level models.</S>
			<S sid ="93" ssid = "11">Comparing the context-insensitive and context- sensitive models, we see that both of them measure similarity between vector representations of corresponding predicate slots.</S>
			<S sid ="94" ssid = "12">However, while DIRT computes sim(v, v1) over vectors in the original word-level space, topic-level models compute sim(d, d1, w) by measuring similarity of vec tors in a reduced-dimensionality latent space.</S>
			<S sid ="95" ssid = "13">As conjectured in the introduction, such coarse-grain representation might lead to loss of information.</S>
			<S sid ="96" ssid = "14">Hence, in the next section we propose a combined two-level model, which represents predicatepred.</S>
			<S sid ="97" ssid = "15">For convenience, when referring to an argument vector v, we will denote the correspond ing pseudo-document by dv . Based on all pseudo- documents we learn an LDA model and obtain its associated probability distributions.</S>
			<S sid ="98" ssid = "16">The calculation of sim(v, v1, w) is composed of two steps.</S>
			<S sid ="99" ssid = "17">At learning time, we compute for each candidate rule a separate, topic-biased, similarity score per each of the topics in the LDA model.</S>
			<S sid ="100" ssid = "18">Then, at rule application time, we compute an overall reliability score for the rule by combining the per-topic similarity scores, while biasing the score combination according to the given context of w. These two steps are described in the following two subsections.</S>
			<S sid ="101" ssid = "19">3.1 Topic-biased Word-vector Similarities.</S>
			<S sid ="102" ssid = "20">Given a pair of word vectors v and v1, and any desired “base” vector similarity measure sim (e.g. simLin), we compute a topic-biased similarity score for each LDA topic t, denoted by simt(v, v1).</S>
			<S sid ="103" ssid = "21">simt(v, v1) is computed by applying the original similarity measure over topic-biased versions of v and v1, denoted by vt and v1 : simt(v, v1) = sim(vt, v1 ) where vt(w) = v(w) · p(t|dv , w) That is, each value in the biased vector, vt(w), is obtained by weighing the original value v(w) by the relevance of the topic t to the argument word w within dv . This way, rather than replacing altogether the word-level values v(w) by thetopic probabilities p(t|dv , w), as done in the topiclevel models, we use the latter to only bias the for mer while preserving fine-grained word-level representations.</S>
			<S sid ="104" ssid = "22">The notation Lint denotes the simt measure when applied using Lin as the base similarity measure sim.</S>
			<S sid ="105" ssid = "23">This learning process results in K different topic-biased similarity scores for each candidate rule, where K is the number of LDA topics.</S>
			<S sid ="106" ssid = "24">Table 1 illustrates topic-biased similarities for the Y slot of two rules involving the predicate ‘acquire’.</S>
			<S sid ="107" ssid = "25">As can be seen, the topic-biased score Lint for ‘ac quire → learn’ for t2 is higher than the Lin score, since this topic is characterized by arguments that commonly appear with both predicates of the rule.</S>
			<S sid ="108" ssid = "26">Consequently, the two predicates are found to be distributionally similar when biased for this topic.</S>
			<S sid ="109" ssid = "27">On the other hand, the topic-biased similarity for t1 is substantially lower, since prominent words in this topic are likely to occur with ‘acquire’ but not with ‘learn’, yielding low distributional similarity.</S>
			<S sid ="110" ssid = "28">Opposite behavior is exhibited for the rule ‘acquire → purchase’.</S>
			<S sid ="111" ssid = "29">3.2 Context-sensitive Similarity.</S>
			<S sid ="112" ssid = "30">When applying an inference rule, we compute for each slot its context-sensitive similarity scoresimWT(v, v1, w), where v and v1 are the slot’s ar gument vectors for the two rule sides and w is the word instantiating the slot in the given rule application.</S>
			<S sid ="113" ssid = "31">This score is computed as a weighted average of the rule’s K topic-biased similarity scores simt.</S>
			<S sid ="114" ssid = "32">In this average, each topic is weighed by its “relevance” for the context in which the rule is applied, which consists of the left-hand-side predicate v and the argument w. This relevance is cap Table 1: Two characteristic topics for the Y slot of ‘acquire’, along with their topic-biased Lin similarities scores Lint, compared with the original Lin similarity, for two rules.</S>
			<S sid ="115" ssid = "33">The relevance of each topic to different arguments of ‘acquire’ is illustrated by showing the top 5 words in the argument vector vy for which the illustrated topic is the most likely one.</S>
			<S sid ="116" ssid = "34">tured by p(t|dv , w): simWT(v, v1, w) = )[p(t|dv , w) · simt(v, v1)] t (7) This way, a rule application would obtain a high score only if the current context fits those topics for which the rule is indeed likely to be valid, as captured by a high topic-biased similarity.</S>
			<S sid ="117" ssid = "35">The notation LinW T denotes the simWT measure, when using Lint as the topic-biased similarity measure.Table 2 illustrates the calculation of context sensitive similarity scores in four rule applications, involving the Y slot of the predicate ‘acquire’.</S>
			<S sid ="118" ssid = "36">We observe that relative to the fixed context-insensitive Lin score, the score of ‘ac quire → learn’ is substantially promoted for the argument ‘skill’ while being demoted for ‘Skype’.</S>
			<S sid ="119" ssid = "37">The opposite behavior is observed for ‘acquire → purchase’, altogether demonstrating how our model successfully biases the similarity score according to rule validity in context.</S>
	</SECTION>
	<SECTION title="Experimental  Settings. " number = "4">
			<S sid ="120" ssid = "1">To evaluate our model, we compare it both to context-insensitive similarity measures as well as to prior context-sensitive methods.</S>
			<S sid ="121" ssid = "2">Furthermore, to better understand its applicability in typical NLP tasks, we focus on an evaluation setting that corresponds to a natural distribution of examples from a large corpus.</S>
			<S sid ="122" ssid = "3">To pic t1 t2 T o p 5 w o r d s cal bio ch em c o r e l n e t w o r k s v i a c o m fi n a n c i a ll y rig hts s y n dr o m e m aj or it y k n o wl e d g e s ki ll ‘acqu ire Skype → learn Skype ’ p(t |dv , w) 0.9 74 0.0 00 Li nt (v, v ) 0.0 40 0.3 34 Li n WT (v, v , w) 0 . 0 3 9 Lin (v, v ) 0 . 1 6 5 ‘acquir e Skype → purchas e Skype’ p(t |dv , w) 0.9 74 0.0 00 Li nt (v, v ) 0.4 27 0.2 41 Li n WT (v, v , w) 0 . 4 1 7 Lin (v, v ) 0 . 2 6 7 ‘ac quir e skill → lear n skill ’ p(t |dv , w) 0.0 00 0.3 80 Li nt (v, v ) 0.0 40 0.3 34 Li n WT (v, v , w) 0 . 2 5 1 Lin (v, v ) 0 . 1 6 5 ‘acqu ire skill → purch ase skill’ p(t |dv , w) 0.0 00 0.3 80 Li nt (v, v ) 0.4 27 0.2 41 Li n WT (v, v , w) 0 . 1 8 1 Lin (v, v ) 0 . 2 6 7 Table 2: Context-sensitive similarity scores (in bold) for the Y slots of four rule applications.</S>
			<S sid ="123" ssid = "4">The components of the score calculation are shown for the topics of Table 1.</S>
			<S sid ="124" ssid = "5">For each rule application, the table shows a couple of the topic-biased scores Lint of the rule (as in Table 1), along with the topic relevance for the given context p(t|dv , w), whichweighs the topic-biased scores in the LinW T cal culation.</S>
			<S sid ="125" ssid = "6">The context-insensitive Lin score is shown for comparison.</S>
			<S sid ="126" ssid = "7">4.1 Evaluated Rule Application Methods.</S>
			<S sid ="127" ssid = "8">We evaluated the following rule application methods: the original context-insensitive word model, following DIRT (Lin and Pantel, 2001), as described in Equation 1, denoted by CI; our own topic-word context-sensitive model, as described in Equation 6, denoted by WT.</S>
			<S sid ="128" ssid = "9">In addition, we evaluated two variants of the topic-level context- sensitive model, denoted DC and SC.</S>
			<S sid ="129" ssid = "10">DC follows the double conditioned contextualized similarity measure according to Equation 4, as implemented by (Ritter et al., 2010), while SC follows the single conditioned one at Equation 5, as implemented by (Dinu and Lapata, 2010b; Dinu and Lapata, 2010a).</S>
			<S sid ="130" ssid = "11">Since our model can contextualize various dis- tributional similarity measures, we evaluated the performance of all the above methods on several base similarity measures and their learned rule sets, namely Lin (Lin, 1998), BInc (Szpektor and Dagan, 2008) and vector Cosine similarity.</S>
			<S sid ="131" ssid = "12">The Lin similarity measure is described in Equation 2.</S>
			<S sid ="132" ssid = "13">Binc (Szpektor and Dagan, 2008) is a directional similarity measure between word vectors, which outperformed Lin for predicate inference (Szpek- tor and Dagan, 2008).</S>
			<S sid ="133" ssid = "14">To build the rule-sets and models for the tested approaches we utilized the ReVerb corpus (Fader et al., 2011), a large scale publicly available web- based open extractions data set, containing about 15 million unique template extractions.3 ReVerb template extractions/instantiations are in the form of a tuple (x, pred, y), containing pred, a verb predicate, x, the argument instantiation of the template’s slot X , and y, the instantiation of the template’s slot Y . ReVerb includes over 600,000 different templates that comprise a verb but may also include other words, for example ‘X can accommodate up to Y ’.</S>
			<S sid ="134" ssid = "15">Yet, many of these templates share a similar meaning, e.g. ‘X accommodate up to Y ’, ‘X can accommodate up to Y ’, ‘X will accommodate up to Y ’, etc. Following Sekine (2005), we clustered templates that share their main verb predicate in order to scale down the number of different predicates in the corpus and collect richer word co- occurrence statistics per predicate.</S>
			<S sid ="135" ssid = "16">Next, we applied some cleanup preprocessing to the ReVerb extractions.</S>
			<S sid ="136" ssid = "17">This includes discarding stop words, rare words and non-alphabetical words instantiating either the X or the Y arguments.</S>
			<S sid ="137" ssid = "18">In addition, we discarded all predicates that co-occur with less than 100 unique argument words in each slot.</S>
			<S sid ="138" ssid = "19">The remaining corpus consists of 7 million unique extractions and 2,155 verb predicates.</S>
			<S sid ="139" ssid = "20">Finally, we trained an LDA model, as described in Section 2, using Mallet (McCallum, 2002).</S>
			<S sid ="140" ssid = "21">Then, for each original context-insensitive similarity measure, we learned from ReVerb a rule-set comprised of the top 500 rules for every identified predicate.</S>
			<S sid ="141" ssid = "22">To complete the learning, we calculated the topic-biased similarity score for each learned rule under each LDA topic, as specified in our context-sensitive model.</S>
			<S sid ="142" ssid = "23">We release a rule set comprising the top 500 context-sensitive rules that we learned for each of the verb predicates in our learning corpus, along with our trained LDA 3 ReVerb is available at http://reverb.cs.</S>
			<S sid ="143" ssid = "24">washington.edu/ Me tho d Li n BI nc Co sin e V al id 26 6 25 4 2 7 2 In va lid 54 5 52 3 5 3 9 T ot al 81 1 77 7 8 1 1 Table 3: Sizes of rule application test set for each learned rule-set.</S>
			<S sid ="144" ssid = "25">model.4 4.2 Evaluation Task.</S>
			<S sid ="145" ssid = "26">To evaluate the performance of the different methods we chose the dataset constructed by Zeichner et al.</S>
			<S sid ="146" ssid = "27">(2012).</S>
			<S sid ="147" ssid = "28">5 This publicly available dataset contains about 6,500 manually annotated predicate template rule applications, each one labeled as correct or incorrect.</S>
			<S sid ="148" ssid = "29">For example, ‘Jack agree with Jill --- Jack feel sorry for Jill’ is a rule application in this dataset, labeled as incorrect, and‘Registration open this month → Registration be gin this month’ is another rule application, labeled as correct.</S>
			<S sid ="149" ssid = "30">Rule applications were generated by randomly sampling extractions from ReVerb, such as (‘Jack’,‘agree with’,‘Jill’) and then sampling possible rules for each, such as ‘agree with → feel sorry for’.</S>
			<S sid ="150" ssid = "31">Hence, this dataset provides naturally distributed rule inferences with respect to ReVerb.</S>
			<S sid ="151" ssid = "32">Whenever we evaluated a distributional similarity measure (namely Lin, BInc, or Cosine), we discarded instances from Zeichner et al.’s dataset in which the assessed rule is not in the context- insensitive rule-set learned for this measure or the argument instantiation of the rule is not in the LDA lexicon.</S>
			<S sid ="152" ssid = "33">We refer to the remaining instances as the test set per measure, e.g. Lin’s test set.</S>
			<S sid ="153" ssid = "34">Table 3 details the size of each such test set in our experiment.</S>
			<S sid ="154" ssid = "35">Finally, the task under which we assessed the tested models is to rank all rule applications in each test set, aiming to rank the valid rule applications above the invalid ones.</S>
	</SECTION>
	<SECTION title="Results. " number = "5">
			<S sid ="155" ssid = "1">We evaluated the performance of each tested method by measuring Mean Average Precision (MAP) (Manning et al., 2008) of the rule application ranking computed by this method.</S>
			<S sid ="156" ssid = "2">In order 4 Our resource is available at: http://www.cs.biu.</S>
			<S sid ="157" ssid = "3">ac.il/˜nlp/downloads/wt-rules.html 5 The dataset is available at: http://.</S>
			<S sid ="158" ssid = "4">www.cs.biu.ac.il/˜nlp/downloads/ annotation-rule-application.htm Table 4: MAP values on corresponding test set obtained by each method.</S>
			<S sid ="159" ssid = "5">Figures in parentheses indicate optimal number of LDA topics.</S>
			<S sid ="160" ssid = "6">to compute MAP values and corresponding statistical significance, we randomly split each test set into 30 subsets.</S>
			<S sid ="161" ssid = "7">For each method we computed Average Precision on every subset and then took the average over all subsets as the MAP value.</S>
			<S sid ="162" ssid = "8">Since all tested context-sensitive approaches are based on LDA topics, we varied for each method the number of LDA topics K that optimizes its performance, ranging from 25 to 1600 topics.</S>
			<S sid ="163" ssid = "9">We used LDA hyperparameters β = 0.01 and α = 0.1 for K &lt; 600 and α = 50 for K &gt;= 600.</S>
			<S sid ="164" ssid = "10">Table 4 presents the optimal MAP performance of each tested measure.</S>
			<S sid ="165" ssid = "11">Our main result is that our model outperforms all other methods, both context-insensitive and context-sensitive, by a relative increase of more than 10% for all three similarity measures that we tested.</S>
			<S sid ="166" ssid = "12">This improvement is statistically significant at p &lt; 0.01 for BInc and Lin, and p &lt; 0.015 for Cosine, using paired t- test.</S>
			<S sid ="167" ssid = "13">This shows that our model indeed successfully leverages contextual information beyond the basic context-agnostic rule scores and is robust across measures.</S>
			<S sid ="168" ssid = "14">Surprisingly, both baseline topic-level context- sensitive methods, namely DC and SC, underper- formed compared to their context-insensitive baselines.</S>
			<S sid ="169" ssid = "15">While Dinu and Lapata (Dinu and Lap- ata, 2010b) did show improvement over context- insensitive DIRT, this result was obtained on the verbs of the Lexical Substitution Task in SemEval (McCarthy and Navigli, 2007), which was manually created with a bias for context-sensitive substitutions.</S>
			<S sid ="170" ssid = "16">However, our result suggests that topic- level models might not be robust enough when applied to a random sample of inferences.</S>
			<S sid ="171" ssid = "17">An interesting indication of the differences between our word-topic model, WT, and topic-only models, DC and SC, lies in the optimal number of LDA topics required for each method.</S>
			<S sid ="172" ssid = "18">The number of topics in the range 25100 performed almost equally well under the WT model for all base measures, with a moderate decline for higher numbers.</S>
			<S sid ="173" ssid = "19">The need for this rather small number of topics is due to the nature of utilization of topics in WT.</S>
			<S sid ="174" ssid = "20">Specifically, topics are leveraged for high-level domain disambiguation, while fine grained word- level distributional similarity is computed for each rule under each such domain.</S>
			<S sid ="175" ssid = "21">This works best for a relatively low number of topics.</S>
			<S sid ="176" ssid = "22">However, in higher numbers, topics relate to narrower domains and then topic biased word level similarity may become less effective due to potential sparseness.</S>
			<S sid ="177" ssid = "23">On the other hand, DC and SC rely on topics as a surrogate to predicate-argument co-occurrence features, and thus require a relatively large number of them to be effective.</S>
			<S sid ="178" ssid = "24">Delving deeper into our test-set, Zeichner et al. provided a more detailed annotation for each invalid rule application.</S>
			<S sid ="179" ssid = "25">Specifically, they annotated whether the context under which the rule is applied is valid.</S>
			<S sid ="180" ssid = "26">For example, in ‘John bought my car --- John sold my car’ the inference is invalid due to an inherently incorrect rule, but the context is valid.</S>
			<S sid ="181" ssid = "27">On the other hand in ‘my boss raised my salary --- my boss constructed my salary’ the context {‘my boss’, ‘my salary’} for applying ‘raise → construct’ is invalid.</S>
			<S sid ="182" ssid = "28">Following, we splitthe test-set for the base Lin measure into two test sets: (a) test-setvc, which includes all correct rule applications and incorrect ones only under valid contexts, and (b) test-setivc, which includes again all correct rule applications but incorrect ones only under invalid contexts.</S>
			<S sid ="183" ssid = "29">Table 5 presents the performance of each compared method on the two test sets.</S>
			<S sid ="184" ssid = "30">On test- setivc, where context mismatches are abundant, our model outperformed all other baselines (statistically significant at p &lt; 0.01).</S>
			<S sid ="185" ssid = "31">In addition, this time DC slightly outperformed CI.</S>
			<S sid ="186" ssid = "32">This result more explicitly shows the advantages of integrating word-level and context-sensitive topic- level similarities for differentiating valid and invalid contexts for rule applications.</S>
			<S sid ="187" ssid = "33">Yet, many invalid rule applications occur under valid contexts due to inherently incorrect rules, and we want to make sure that also in this scenario our model does not fall behind the context-insensitive measure.</S>
			<S sid ="188" ssid = "34">Indeed, on test-setvc, in which context mismatches are rare, our algorithm is still better than the original measure, indicating that WT can be safely applied to distributional similarity measures without concerns of reduced performance in different context scenarios.</S>
			<S sid ="189" ssid = "35">test seti vc test set vc S i z e (v ali d:i nv ali d) 4 3 2 (2 66 :1 66 ) 6 4 5 (2 66 :3 79 ) CI 0.7 80 0.5 87 D C 0.7 96 0.4 98 SC 0.7 79 0.5 12 W T 0.8 54 0.6 21 Table 5: MAP results for the two split Lin test- sets.</S>
	</SECTION>
	<SECTION title="Discussion and Future  Work. " number = "6">
			<S sid ="190" ssid = "1">This paper addressed the problem of computing context-sensitive reliability scores for predicate inference rules.</S>
			<S sid ="191" ssid = "2">In particular, we proposed a novel scheme that applies over any base distributional similarity measure which operates at the word level, and computes a single context-insensitive score for a rule.</S>
			<S sid ="192" ssid = "3">Based on such a measure, our scheme constructs a context-sensitive similarity measure that computes a reliability score for predicate inference rules applications in the context of given arguments.</S>
			<S sid ="193" ssid = "4">The contextualization of the base similarity score was obtained using a topic-level LDA model, which was used in a novel way.</S>
			<S sid ="194" ssid = "5">First, it provides a topic bias for learning separate per- topic word-level similarity scores between predicates.</S>
			<S sid ="195" ssid = "6">Then, given a specific candidate rule application, the LDA model is used to infer the topic distribution relevant to the context specified by the given arguments.</S>
			<S sid ="196" ssid = "7">Finally, the context- sensitive rule application score is computed as a weighted average of the per-topic word-level similarity scores, which are weighed according to the inferred topic distribution.</S>
			<S sid ="197" ssid = "8">While most works on context-insensitive predicate inference rules, such as DIRT (Lin and Pan- tel, 2001), are based on word-level similarity measures, almost all prior models addressing context- sensitive predicate inference rules are based on topic models (except for (Pantel et al., 2007), which was outperformed by later models).</S>
			<S sid ="198" ssid = "9">We therefore focused on comparing the performance of our two-level scheme with state-of-the-art prior topic-level and word-level models of distributional similarity, over a random sample of inference rule applications.</S>
			<S sid ="199" ssid = "10">Under this natural setting, the two- level scheme consistently outperformed both types of models when tested with three different base similarity measures.</S>
			<S sid ="200" ssid = "11">Notably, our model shows stable performance over a large subset of the data where context sensitivity is rare, while topic-level models tend to underperform in such cases compared to the base context-insensitive methods.</S>
			<S sid ="201" ssid = "12">Our work is closely related to another research line that addresses lexical similarity and substitution scenarios in context.</S>
			<S sid ="202" ssid = "13">While we focus on lexical-syntactic predicate templates and instantiations of their argument slots as context, lexical similarity methods consider various lexical units that are not necessarily predicates, with their context typically being the collection of words in a window around them.</S>
			<S sid ="203" ssid = "14">Various approaches have been proposed to address lexical similarity.</S>
			<S sid ="204" ssid = "15">A number of works are based on a compositional semantics approach, where a prior representation of a target lexical unit is composed with the representations of words in its given context (Mitchell and Lapata, 2008; Erk and Pado´ , 2008; Thater et al., 2010).</S>
			<S sid ="205" ssid = "16">Other works (Erk and Pado´ , 2010; Reisinger and Mooney, 2010) use a rather large word window around target words and compute similarities between clusters comprising instances of word windows.</S>
			<S sid ="206" ssid = "17">In addition, (Dinu and Lapata, 2010a) adapted the predicate inference topic model from (Dinu and Lap- ata, 2010b) to compute lexical similarity in context.</S>
			<S sid ="207" ssid = "18">A natural extension of our work would be to extend our two level model to accommodate context- sensitive lexical similarity.</S>
			<S sid ="208" ssid = "19">For this purpose we will need to redefine the scope of context in our model, and adapt our method to compute context- biased lexical similarities accordingly.</S>
			<S sid ="209" ssid = "20">Then we will also be able to evaluate our model on the Lexical Substitution Task (McCarthy and Navigli, 2007), which has been commonly used in recent years as a benchmark for context-sensitive lexical similarity models.</S>
			<S sid ="210" ssid = "21">In a different NLP task, Eidelman et al.</S>
			<S sid ="211" ssid = "22">(2012) utilize a similar approach to ours for improving the performance of statistical machine translation (SMT).</S>
			<S sid ="212" ssid = "23">They learn an LDA model on the source language side of the training corpus with the purpose of identifying implicit sub-domains.</S>
			<S sid ="213" ssid = "24">Then they utilize the distribution over topics inferred for each document in their corpus to compute separate per-topic translation probability tables.</S>
			<S sid ="214" ssid = "25">Finally, they train a classifier to translate a given target word based on these tables and the inferred topic distribution of the given document in which the target word appears.</S>
			<S sid ="215" ssid = "26">A notable difference be tween our approach and theirs is that we use predicate pseudo-documents consisting of argument in- stantiations to learn our LDA model, while Eidel- man et al. use the real documents in a corpus.</S>
			<S sid ="216" ssid = "27">We believe that combining these two approaches may improve performance for both textual inference and SMT and plan to experiment with this direction in future work.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="217" ssid = "28">This work was partially supported by the Israeli Ministry of Science and Technology grant 38705, the Israel Science Foundation grant 880/12, and the European Community’s Seventh Framework Programme (FP7/20072013) under grant agreement no. 287923 (EXCITEMENT).</S>
	</SECTION>
</PAPER>
