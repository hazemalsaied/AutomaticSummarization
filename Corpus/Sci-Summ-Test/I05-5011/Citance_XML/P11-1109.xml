<PAPER>
	<ABSTRACT>
	<SECTION title="Introduction. " number = "1">
			<S sid ="1" ssid = "1">Natural language allows us to express the same information in many ways, which makes natural language processing (NLP) a challenging area.</S>
			<S sid ="2" ssid = "2">Accordingly, many researchers have recognized that automatic paraphrasing is an indispensable component of intelligent NLP systems (Iordanskaja et al., 1991; McKeown et al., 2002; Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Kauchak and Barzi- lay, 2006; CallisonBurch et al., 2006) and have tried to acquire a large amount of paraphrase knowledge, which is a key to achieving robust automatic paraphrasing, from corpora (Lin and Pantel, 2001; Barzi- lay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003).</S>
			<S sid ="3" ssid = "3">We propose a method to extract phrasal paraphrases from pairs of sentences that define the same 1087 mass and 02 increases the risk of bone fracture.</S>
			<S sid ="4" ssid = "4">We define paraphrase as a pair of expressions between which entailment relations of both directions hold.</S>
			<S sid ="5" ssid = "5">(Androutsopoulos and Malakasiotis, 2010).</S>
			<S sid ="6" ssid = "6">Our objective is to extract phrasal paraphrases from pairs of sentences that define the same concept.</S>
			<S sid ="7" ssid = "7">We propose a supervised method that exploits various kinds of lexical similarity features and contextual features.</S>
			<S sid ="8" ssid = "8">Sentences defining certain concepts are acquired automatically on a large scale from the Web by applying a quite simple supervised method.</S>
			<S sid ="9" ssid = "9">Previous methods most relevant to our work used parallel corpora such as multiple translations of the same source text (Barzilay and McKeown, 2001) or automatically acquired parallel news texts (Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).</S>
			<S sid ="10" ssid = "10">The former requires a large amount of manual labor to translate the same texts Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, Portland, Oregon, June 1924, 2011.</S>
			<S sid ="11" ssid = "11">Qc 2011 Association for Computational Linguistics in several ways.</S>
			<S sid ="12" ssid = "12">The latter would suffer from the fact that it is not easy to automatically retrieve large bodies of parallel news text with high accuracy.</S>
			<S sid ="13" ssid = "13">On the contrary, recognizing definition sentences for the same concept is quite an easy task at least for Japanese, as we will show, and we were able to find a huge amount of definition sentence pairs from normal Web texts.</S>
			<S sid ="14" ssid = "14">In our experiments, about 30 million definition sentence pairs were extracted from 6×108Web documents, and the estimated number of para phrases recognized in the definition sentences using our method was about 300,000, for a precision rate of about 94%.</S>
			<S sid ="15" ssid = "15">Also, our experimental results show that our method is superior to well-known competing methods (Barzilay and McKeown, 2001; Koehn et al., 2007) for extracting paraphrases from definition sentence pairs.</S>
			<S sid ="16" ssid = "16">Our evaluation is based on bidirectional checking of entailment relations between paraphrases that considers the context dependence of a paraphrase.</S>
			<S sid ="17" ssid = "17">Note that using definition sentences is only the beginning of our research on paraphrase extraction.</S>
			<S sid ="18" ssid = "18">We have a more general hypothesis that sentences fulfilling the same pragmatic function (e.g. definition) for the same topic (e.g. osteoporosis) convey mostly the same information using different expressions.</S>
			<S sid ="19" ssid = "19">Such functions other than definition may include the usage of the same Linux command, the recipe for the same cuisine, or the description of related work on the same research issue.</S>
			<S sid ="20" ssid = "20">Section 2 describes related works.</S>
			<S sid ="21" ssid = "21">Section 3 presents our proposed method.</S>
			<S sid ="22" ssid = "22">Section 4 reports on evaluation results.</S>
			<S sid ="23" ssid = "23">Section 5 concludes the paper.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="24" ssid = "1">The existing work for paraphrase extraction is categorized into two groups.</S>
			<S sid ="25" ssid = "2">The first involves a dis- tributional similarity approach pioneered by Lin and Pantel (2001).</S>
			<S sid ="26" ssid = "3">Basically, this approach assumes that two expressions that have a large distributional similarity are paraphrases.</S>
			<S sid ="27" ssid = "4">There are also variants of this approach that address entailment acquisition (Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor and Dagan, 2008; Hashimoto et al., 2009).</S>
			<S sid ="28" ssid = "5">These methods can be applied to a normal monolingual corpus, and it has been shown that a large number of paraphrases or entailment rules could be extracted.</S>
			<S sid ="29" ssid = "6">How ever, the precision of these methods has been relatively low.</S>
			<S sid ="30" ssid = "7">This is due to the fact that the evidence, i.e., distributional similarity, is just indirect evidence of paraphrase/entailment.</S>
			<S sid ="31" ssid = "8">Accordingly, these methods occasionally mistake antonymous pairs for paraphrases/entailment pairs, since an expression and its antonymous counterpart are also likely to have a large distributional similarity.</S>
			<S sid ="32" ssid = "9">Another limitation of these methods is that they can find only paraphrases consisting of frequently observed expressions since they must have reliable distributional similarity values for expressions that constitute paraphrases.</S>
			<S sid ="33" ssid = "10">The second category is a parallel corpus approach (Barzilay and McKeown, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Dolan et al., 2004).</S>
			<S sid ="34" ssid = "11">Our method belongs to this category.</S>
			<S sid ="35" ssid = "12">This approach aligns expressions between two sentences in parallel corpora, based on, for example, the overlap of words/contexts.</S>
			<S sid ="36" ssid = "13">The aligned expressions are assumed to be paraphrases.</S>
			<S sid ="37" ssid = "14">In this approach, the expressions do not need to appear frequently in the corpora.</S>
			<S sid ="38" ssid = "15">Furthermore, the approach rarely mistakes antonymous pairs for paraphrases/entailment pairs.</S>
			<S sid ="39" ssid = "16">However, its limitation is the difficulty in preparing a large amount of parallel corpora, as noted before.</S>
			<S sid ="40" ssid = "17">We avoid this by using definition sentences, which can be easily acquired on a large scale from the Web, as parallel corpora.</S>
			<S sid ="41" ssid = "18">Murata et al.</S>
			<S sid ="42" ssid = "19">(2004) used definition sentences in two manually compiled dictionaries, which are considerably fewer in the number of definition sentences than those on the Web.</S>
			<S sid ="43" ssid = "20">Thus, the coverage of their method should be quite limited.</S>
			<S sid ="44" ssid = "21">Furthermore, the precision of their method is much poorer than ours as we report in Section 4.</S>
			<S sid ="45" ssid = "22">For a more extensive survey on paraphrasing methods, see Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010).</S>
	</SECTION>
	<SECTION title="Proposed method. " number = "3">
			<S sid ="46" ssid = "1">Our method, targeting the Japanese language, consists of two steps: definition sentence acquisition and paraphrase extraction.</S>
			<S sid ="47" ssid = "2">We describe them below.</S>
			<S sid ="48" ssid = "3">3.1 Definition sentence acquisition.</S>
			<S sid ="49" ssid = "4">We acquire sentences that define a concept (definition sentences) as in Example (2), which defines “骨 粗鬆症” (osteoporosis), from the 6 × 108 Web pages (Akamine et al., 2010) and the Japanese Wikipedia.</S>
			<S sid ="50" ssid = "5">(2) 骨粗鬆症とは、骨がもろくなってしまう病気だ。 (Osteoporosis is a disease that makes bones fragile.)</S>
			<S sid ="51" ssid = "6">Fujii and Ishikawa (2002) developed an unsupervised method to find definition sentences from the Web using 18 sentential templates and a language model constructed from an encyclopedia.</S>
			<S sid ="52" ssid = "7">On the other hand, we developed a supervised method to achieve a higher precision.</S>
			<S sid ="53" ssid = "8">We use one sentential template and an SVM classifier.</S>
			<S sid ="54" ssid = "9">Specifically, we first collect definition sentence candidates by a template “ˆNP とは.*”, where ˆ is the beginning of sentence and NP is the noun phrase expressing the concept to be defined followed by a particle sequence, “と” (comitative) and “は” (topic) (and optionally followed by comma), as exemplified in (2).</S>
			<S sid ="55" ssid = "10">As a result, we collected 3,027,101 sentences.</S>
			<S sid ="56" ssid = "11">Although the particle sequence tends to mark the topic of the definition sentence, it can also appear in interrogative sentences and normal assertive sentences in which a topic is strongly emphasized.</S>
			<S sid ="57" ssid = "12">To remove such non-definition sentences, we classify the candidate sentences using an SVM classifier with a polynominal kernel (d = 2).1 Since Japanese is a head-final language and we can judge whether a sentence is interrogative or not from the last words in the sentence, we included morpheme N -grams and bag-of-words (with the window size of N ) at the end of sentences in the feature set.</S>
			<S sid ="58" ssid = "13">The 92.2, and 91.4, respectively.</S>
			<S sid ="59" ssid = "14">Using the classifier, we acquired 1,925,052 positive sentences from all of the collected sentences.</S>
			<S sid ="60" ssid = "15">After adding definition sentences from Wikipedia articles, which are typically the first sentence of the body of each article (Kazama and Torisawa, 2007), we obtained a total of 2,141,878 definition sentence candidates, which covered 867,321 concepts ranging from weapons to rules of baseball.</S>
			<S sid ="61" ssid = "16">Then, we coupled two definition sentences whose defined concepts were the same and obtained 29,661,812 definition sentence pairs.</S>
			<S sid ="62" ssid = "17">Obviously, our method is tailored to Japanese.</S>
			<S sid ="63" ssid = "18">For a language-independent method of definition acquisition, see Navigli and Velardi (2010) as an example.</S>
			<S sid ="64" ssid = "19">3.2 Paraphrase extraction.</S>
			<S sid ="65" ssid = "20">Paraphrase extraction proceeds as follows.</S>
			<S sid ="66" ssid = "21">First, each sentence in a pair is parsed by the dependency parser KNP2 and dependency tree fragments that constitute linguistically well-formed constituents are extracted.</S>
			<S sid ="67" ssid = "22">The extracted dependency tree fragments are called candidate phrases hereafter.</S>
			<S sid ="68" ssid = "23">We restricted candidate phrases to predicate phrases that consist of at least one dependency relation, do not contain demonstratives, and in which all the leaf nodes are nominal and all of the constituents are consecutive in the sentence.</S>
			<S sid ="69" ssid = "24">KNP indicates whether each candidate phrase is a predicate based on the POS of the head morpheme.</S>
			<S sid ="70" ssid = "25">Then, we check all the pairs of candidate phrases between two definition sentences to find paraphrase pairs.3 In (1), repeated in (3), candidate phrase pairs to be features are also useful for confirming that the head checked include (01 decreases the quantity of bone,verb is in the present tense, which definition sen 01 reduces bone mass), (01 decreases the quantity tences should be.</S>
			<S sid ="71" ssid = "26">Also, we added the morpheme of bone, 02 increases the risk of bone fracture), (02 N -grams and bag-of-words right after the particle sequence in the feature set since we observe that non-definition sentences tend to have interrogative related words like “何” (what) or “一体” ((what) on makes bones fragile, 01 makes bones fragile, 02 fracture).</S>
			<S sid ="72" ssid = "27">reduces bone mass), and (02 increases the risk of bone earth) right after the particle sequence.</S>
			<S sid ="73" ssid = "28">We chose 5 (3) a. Osteoporosis is a disease that 01 decreases the quantity of bone and 02 makes bones fragile.</S>
			<S sid ="74" ssid = "29">as N from our preliminary experiments.</S>
			<S sid ="75" ssid = "30">b. Osteoporosis is a disease that 01 reduces boneOur training data was constructed from 2,911 sen tences randomly sampled from all of the collected sentences.</S>
			<S sid ="76" ssid = "31">61.1% of them were labeled as positive.</S>
			<S sid ="77" ssid = "32">In the 10-fold cross validation, the classifier’s accuracy, precision, recall, and F1 were 89.4, 90.7, 1 We use SVMlight available at http://svmlight.</S>
			<S sid ="78" ssid = "33">joachims.org/.</S>
			<S sid ="79" ssid = "34">mass and 02 increases the risk of bone fracture.</S>
			<S sid ="80" ssid = "35">2 http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/knp.html.</S>
			<S sid ="81" ssid = "36">3 Our method discards candidate phrase pairs in which one subsumes the other in terms of their character string, or the difference is only one proper noun like “toner cartridges that Apple Inc. made” and “toner cartridges that Xerox made.” Proper nouns are recognized by KNP.</S>
			<S sid ="82" ssid = "37">f1 The ratio of the num ber of mor phe mes shar ed betw een two cand idate phra ses to the num ber of all of the mor phe mes in the two phra ses.</S>
			<S sid ="83" ssid = "38">f2 The ratio of the num ber of a cand idate phra se’s mor phe mes, for whic h ther e is a mor phe me with smal l edit dista nce (1 in our expe rime nt) in ano ther can did ate phr ase, to the nu mb er of all of the mo rph em es in the two phr ase s. Not e that Jap ane se has ma ny ort hog rap hic al vari atio ns and edit dist anc e is use ful for ide ntif yin g the m. f3 The ratio of the num ber of a cand idate phra se’s mor phe mes, for whic h ther e is a mor phe me with the sam e pron unci atio n in anot her cand idate phr ase, to the nu mb er of all of the mo rph em es in the two phr ase s. Pro nun ciat ion is als o use ful for ide ntif yin g ort hog rap hic vari atio ns.</S>
			<S sid ="84" ssid = "39">Pro nun ciat ion is giv en by KN P. f4 The ratio of the num ber of mor phe mes of a shor ter cand idate phra se to that of a long er one.</S>
			<S sid ="85" ssid = "40">f5 The iden tity of the infle cted form of the head mor phe me betw een two cand idate phra ses: 1 if they are iden tical, 0 othe rwis e. f6 The iden tity of the POS of the head mor phe me betw een two cand idate phra ses: 1 or 0.</S>
			<S sid ="86" ssid = "41">f7 The iden tity of the infle ctio n (con juga tion) of the head mor phe me betw een two cand idate phra ses: 1 or 0.</S>
			<S sid ="87" ssid = "42">f8 The ratio of the num ber of mor phe mes that appe ar in a cand idate phra se seg men t of a defi nitio n sent ence s1 and in a seg men t that is NO T a part of the can did ate phr ase of ano ther defi niti on sen ten ce s2 to the nu mb er of all of the mo rph em es of s1 ’s can did ate phr ase, i.e. ho w ma ny extr a mo rph em es are inc orp orat ed into s1 ’s can did ate phr ase.</S>
			<S sid ="88" ssid = "43">f9 The reve rsed (s1 ↔ s2 ) versi on of f8.</S>
			<S sid ="89" ssid = "44">f10 The ratio of the num ber of pare nt depe nden cy tree frag men ts that are shar ed by two cand idate phra ses to the num ber of all of the pare nt de pend ency tree frag men ts of the two phra ses.</S>
			<S sid ="90" ssid = "45">Dep ende ncy tree frag men ts are repr esen ted by the pron unci atio n of their com pone nt mor phe mes.</S>
			<S sid ="91" ssid = "46">f11 A varia tion of f10; tree frag men ts are repr esen ted by the base form of their com pone nt mor phe mes.</S>
			<S sid ="92" ssid = "47">f12 A varia tion of f10; tree frag men ts are repr esen ted by the POS of their com pone nt mor phe mes.</S>
			<S sid ="93" ssid = "48">f13 The ratio of the num ber of unig rams (mor phe mes) that appe ar in the chil d cont ext of both cand idate phra ses to the num ber of all of the chil d cont ext mor phe mes of both cand idate phra ses.</S>
			<S sid ="94" ssid = "49">Uni gra ms are repr esen ted by the pron unci atio n of the mor phe me. f14 A varia tion of f13; unig rams are repr esen ted by the base form of the mor phe me. f15 A varia tion of f14; the num erat or is the num ber of chil d cont ext unig rams that are adja cent to both cand idate phra ses.</S>
			<S sid ="95" ssid = "50">f16 The ratio of the num ber of trigr ams that appe ar in the chil d cont ext of both cand idate phra ses to the num ber of all of the chil d cont ext mor phe mes of both cand idate phra ses.</S>
			<S sid ="96" ssid = "51">Trig rams are repr esen ted by the pron unci atio n of the mor phe me. f17 Cosi ne simi larit y betw een two defi nitio n sent ence s from whic h a cand idate phra se pair is extr acte d. Table 1: Features used by paraphrase classifier.</S>
			<S sid ="97" ssid = "52">The paraphrase checking of candidate phrase pairs is performed by an SVM classifier with a linear kernel that classifies each pair of candidate phrases into a paraphrase or a non-paraphrase.4 Candidate phrase pairs are ranked by their distance from the SVM’s hyperplane.</S>
			<S sid ="98" ssid = "53">Features for the classifier are based on our observation that two candidate phrases tend to be paraphrases if the candidate phrases themselves are sufficiently similar and/or their surrounding contexts are sufficiently similar.</S>
			<S sid ="99" ssid = "54">Table 1 lists the features used by the classifier.5 Basically, they represent either the similarity of candidate phrases (f1 9) or that of their contexts (f1017).</S>
			<S sid ="100" ssid = "55">We think that they have various degrees of discriminative power, and thus we use the SVM to adjust their weights.</S>
			<S sid ="101" ssid = "56">Figure 1 illustrates features f812, for which you may need supplemental remarks.</S>
			<S sid ="102" ssid = "57">English is used for ease of explanation.</S>
			<S sid ="103" ssid = "58">In the figure, f8 has a positive value since the candidate phrase of s1 contains morphemes “of bone”, which do not appear in the can 4 We use SVMperf available at http://svmlight.</S>
			<S sid ="104" ssid = "59">joachims.org/svm perf.html.</S>
			<S sid ="105" ssid = "60">Figure 1: Illustration of features f812.</S>
			<S sid ="106" ssid = "61">didate phrase of s2 but do appear in the other part of s2 , i.e. they are extra morphemes for s1’s candidate phrase.</S>
			<S sid ="107" ssid = "62">On the other hand, f9 is zero since there is no such extra morpheme in s2’s candidate phrase.</S>
			<S sid ="108" ssid = "63">Also, features f1012 have positive values since the two candidate phrases share two parent dependency tree fragments, (that increases) and (of fracture).</S>
			<S sid ="109" ssid = "64">We have also tried the following features, which we do not detail due to space limitation: the similarity of candidate phrases based on semantically similar nouns (Kazama and Torisawa, 2008), entailing/entailed verbs (Hashimoto et al., 2009), and the identity of the pronunciation and base form of the 5 In the table, the parent context of a candidate phrase con-.</S>
			<S sid ="110" ssid = "65">sists of expressions that appear in ancestor nodes of the candi head morpheme; N -grams (N =1,2,3) of child and date phrase in terms of the dependency structure of the sentence.</S>
			<S sid ="111" ssid = "66">Child contexts are defined similarly.</S>
			<S sid ="112" ssid = "67">parent contexts represented by either the inflectedform, base form, pronunciation, or POS of mor Original definition sentence pair (s1 , s2 ) Paraphrased definition sentence pair (sf , sf ) s1 : Osteoporosis is a disease that reduces bone mass and makes bones fragile.</S>
			<S sid ="113" ssid = "68">s2 : Osteoporosis is a disease that decreases the quantity of bone and increases the risk of bone fracture.</S>
			<S sid ="114" ssid = "69">sf : Osteoporosis is a disease that decreases the quantity of bone and makes bones fragile.</S>
			<S sid ="115" ssid = "70">sf : Osteoporosis is a disease that reduces bone mass and increases the risk of bone fracture.</S>
			<S sid ="116" ssid = "71">Figure 2: Bidirectional checking of entailment relation (→) of p1 → p2 and p2 → p1 . p1 is “reduces bone mass” in s1 and p2 is “decreases the quantity of bone” in s2 . p1 and p2 are exchanged between s1 and s2 to generate corresponding paraphrased sentences sf and sf . p1 → p2 (p2 → p1 ) is verified if s1 → sf (s2 → sf ) holds.</S>
			<S sid ="117" ssid = "72">In this 1 2 1 2 case, both of them hold.</S>
			<S sid ="118" ssid = "73">English is used for ease of explanation.</S>
			<S sid ="119" ssid = "74">pheme; parent/child dependency tree fragments represented by either the inflected form, base form, pronunciation, or POS; adjacent versions (cf.</S>
			<S sid ="120" ssid = "75">f15) of N -gram features and parent/child dependency tree texts.</S>
			<S sid ="121" ssid = "76">The labeling process is as follows.</S>
			<S sid ="122" ssid = "77">First, from each candidate phrase pair (p1, p2) and its source definition sentence pair (s1, s2), we create two para phrase sentence pairs (sf , sf ) by exchanging p1 and 1 2features.</S>
			<S sid ="123" ssid = "78">These amount to 78 features, but we even p2 between s1 and s2.</S>
			<S sid ="124" ssid = "79">Then, annotators check if s1 tually settled on the 17 features in Table 1 through entails sf and s2 entails sfso that entailment rela ablation tests to evaluate the discriminative power of each feature.</S>
			<S sid ="125" ssid = "80">The ablation tests were conducted using training data that we prepared.</S>
			<S sid ="126" ssid = "81">In preparing the training data, we faced the problem that the completely random sampling of candidate paraphrase pairs provided us with only a small number of positive examples.</S>
			<S sid ="127" ssid = "82">Thus, we automatically collected candidate paraphrase pairs that were expected to have a high likelihood of being positive as examples to be labeled.</S>
			<S sid ="128" ssid = "83">The likelihood was calculated by simply summing all of the 78 feature values that we have tried, since they indicate the likelihood of a given candidate paraphrase pair’s being a paraphrase.</S>
			<S sid ="129" ssid = "84">Note that val ues of the features f8 and f9 are weighted with −1, since they indicate the unlikelihood.</S>
			<S sid ="130" ssid = "85">Specifically, we first randomly sampled 30,000 definition sentence pairs from the 29,661,812 pairs, and collected 3,000 candidate phrase pairs that had the highest likelihood from them.</S>
			<S sid ="131" ssid = "86">The manual labeling of each candidate phrase pair (p1 , p2) was based on bidirec tional checking of entailment relation, p1 → p2 and p2 → p1, with p1 and p2 embedded in contexts.</S>
			<S sid ="132" ssid = "87">tions of both directions p1 → p2 and p2 → p1 are checked.</S>
			<S sid ="133" ssid = "88">Figure 2 shows an example of bidirectional checking.</S>
			<S sid ="134" ssid = "89">In this example, both entailment relations, s1 → s1 and s2 → s2, hold, and thus the candidate f f ( , p ) is judged as positive.</S>
			<S sid ="135" ssid = "90">We used phrase pair p1 2(p1, p2), for which entailment relations of both di rections held, as positive examples (1,092 pairs) and the others as negative ones (1,872 pairs).6 We built the paraphrase classifier from the training data.</S>
			<S sid ="136" ssid = "91">As mentioned, candidate phrase pairs were ranked by the distance from the SVM’s hyperplane.</S>
	</SECTION>
	<SECTION title="Experiment. " number = "4">
			<S sid ="137" ssid = "1">In this paper, our claims are twofold.</S>
			<S sid ="138" ssid = "2">I. Definition sentences on the Web are a treasure trove of paraphrase knowledge (Section 4.2).</S>
			<S sid ="139" ssid = "3">II.</S>
			<S sid ="140" ssid = "4">Our method of paraphrase acquisition from definition sentences is more accurate than well- known competing methods (Section 4.1).</S>
			<S sid ="141" ssid = "5">We first verify claim II by comparing our method with that of Barzilay and McKeown (2001) (BM 7 This scheme is similar to the one proposed by method), Moses (Koehn et al., 2007) (SMT Szpektor et al.</S>
			<S sid ="142" ssid = "6">(2007).</S>
			<S sid ="143" ssid = "7">We adopt this scheme since paraphrase judgment might be unstable between an- notators unless they are given a particular context method), and that of Murata et al.</S>
			<S sid ="144" ssid = "8">(2004) (Mrt method).</S>
			<S sid ="145" ssid = "9">The first two methods are well known for accurately extracting semantically equivalent phrase 8based on which they make a judgment.</S>
			<S sid ="146" ssid = "10">As de pairs from parallel corpora.</S>
			<S sid ="147" ssid = "11">Then, we verify claim scribed below, we use definition sentences as contexts.</S>
			<S sid ="148" ssid = "12">We admit that annotators might be biased by this in some unexpected way, but we believe that this is a more stable method than that without con 6 The remaining 36 pairs were discarded as they contained.</S>
			<S sid ="149" ssid = "13">garbled characters of Japanese.</S>
			<S sid ="150" ssid = "14">7 http://www.statmt.org/moses/ 8 As anonymous reviewers pointed out, they are unsupervised methods and thus unable to be adapted to definition sen I by comparing definition sentence pairs with sentence pairs that are acquired from the Web using Yahoo!JAPAN API9 as a paraphrase knowledge source.</S>
			<S sid ="151" ssid = "15">In the latter data set, two sentences of each pair are expected to be semantically similar regardless of whether they are definition sentences.</S>
			<S sid ="152" ssid = "16">Both sets contain 100,000 pairs.</S>
			<S sid ="153" ssid = "17">Three annotators (not the authors) checked evaluation samples.</S>
			<S sid ="154" ssid = "18">Fleiss’ kappa (Fleiss, 1971) was 0.69 (substantial agreement (Landis and Koch, 1977)).</S>
			<S sid ="155" ssid = "19">4.1 Our method vs. competing methods.</S>
			<S sid ="156" ssid = "20">In this experiment, paraphrase pairs are extracted from 100,000 definition sentence pairs that are randomly sampled from the 29,661,812 pairs.</S>
			<S sid ="157" ssid = "21">Before reporting the experimental results, we briefly describe the BM, SMT, and Mrt methods.</S>
			<S sid ="158" ssid = "22">BM method Given parallel sentences like multiple translations of the same source text, the BM method works iteratively as follows.</S>
			<S sid ="159" ssid = "23">First, it collects from the parallel sentences identical word pairs and their contexts (POS N -grams with indices indicating corresponding words between paired contexts) as positive examples and those of different word pairs as negative ones.</S>
			<S sid ="160" ssid = "24">Then, each context is ranked based on the frequency with which it appears in positive (negative) examples.</S>
			<S sid ="161" ssid = "25">The most likely K positive (negative) contexts are used to extract positive (negative) paraphrases from the parallel sentences.</S>
			<S sid ="162" ssid = "26">Extracted positive (negative) paraphrases and their morpho-syntactic patterns are used to collect additional positive (negative) contexts.</S>
			<S sid ="163" ssid = "27">All the positive (negative) contexts are ranked, and additional paraphrases and their morpho-syntactic patterns are extracted again.</S>
			<S sid ="164" ssid = "28">This iterative process finishes if no further paraphrase is extracted or the number of iterations reaches a predefined threshold T . In this experiment, following Barzilay and McKeown (2001), K is 10 and N is 1 to 3.</S>
			<S sid ="165" ssid = "29">The value of T is not given in their paper.</S>
			<S sid ="166" ssid = "30">We chose 3 as its value based on our preliminary experiments.</S>
			<S sid ="167" ssid = "31">Note that paraphrases extracted by this method are not ranked.</S>
			<S sid ="168" ssid = "32">tences.</S>
			<S sid ="169" ssid = "33">Nevertheless, we believe that comparing these methods with ours is very informative, since they are known to be accurate and have been influential.</S>
			<S sid ="170" ssid = "34">9 http://developer.yahoo.co.jp/webapi/ SMT method Our SMT method uses Moses (Koehn et al., 2007) and extracts a phrase table, a set of two phrases that are translations of each other, given a set of two sentences that are translations of each other.</S>
			<S sid ="171" ssid = "35">If you give Moses monolingual parallel sentence pairs, it should extract a set of two phrases that are paraphrases of each other.</S>
			<S sid ="172" ssid = "36">In this experiment, default values were used for all parameters.</S>
			<S sid ="173" ssid = "37">To rank extracted phrase pairs, we assigned each of them the product of two phrase translation probabilities of both directions that were given by Moses.</S>
			<S sid ="174" ssid = "38">For other SMT methods, see Quirk et al.</S>
			<S sid ="175" ssid = "39">(2004) and Bannard and CallisonBurch (2005) among others.</S>
			<S sid ="176" ssid = "40">Mrt method Murata et al.</S>
			<S sid ="177" ssid = "41">(2004) proposed a method to extract paraphrases from two manually compiled dictionaries.</S>
			<S sid ="178" ssid = "42">It simply regards a difference between two definition sentences of the same word as a paraphrase candidate.</S>
			<S sid ="179" ssid = "43">Paraphrase candidates are ranked according to an unsupervised scoring scheme that implements their assumption.</S>
			<S sid ="180" ssid = "44">They assume that a paraphrase candidate tends to be a valid paraphrase if it is surrounded by infrequent strings and/or if it appears multiple times in the data.</S>
			<S sid ="181" ssid = "45">In this experiment, we evaluated the unsupervised version of our method in addition to the supervised one described in Section 3.2, in order to compare it fairly with the other methods.</S>
			<S sid ="182" ssid = "46">The unsupervised method works in the same way as the supervised one, except that it ranks candidate phrase pairs by the sum of all 17 feature values, instead of the distance from the SVM’s hyperplane.</S>
			<S sid ="183" ssid = "47">In other words, no supervised learning is used.</S>
			<S sid ="184" ssid = "48">All the feature values are weighted with 1, except for f8 and f9, whichare weighted with −1 since they indicate the unlike lihood of a candidate phrase pair being paraphrases.</S>
			<S sid ="185" ssid = "49">BM, SMT, Mrt, and the two versions of our method were used to extract paraphrase pairs from the same 100,000 definition sentence pairs.</S>
			<S sid ="186" ssid = "50">Evaluation scheme Evaluation of each paraphrase pair (p1, p2) was based on bidirectional checking of entailment relations p1 → p2 and p2 → p1 in a way similar to the labeling of the training data.</S>
			<S sid ="187" ssid = "51">The difference is that contexts for evaluation are two sentences that are retrieved from the Web and contain p1 and p2 , instead of definition sentences from which p1 and p2 are extracted.</S>
			<S sid ="188" ssid = "52">This is intended to check whether extracted paraphrases are also valid for contexts other than those from which they are extracted.</S>
			<S sid ="189" ssid = "53">The evaluation proceeds as follows.</S>
			<S sid ="190" ssid = "54">For the top m paraphrase pairs of each method (in the case of the BM method, randomly sampled m pairs were used, since the method does not rank paraphrase pairs), we retrieved a sentence pair (s1, s2 ) for each paraphrase pair (p1, p2) from the Web, such that s1 contains p1 and s2 contains p2.</S>
			<S sid ="191" ssid = "55">In doing so, we make sure that neither s1 nor s2 are the definition sentences from which p1 and p2 are extracted.</S>
			<S sid ="192" ssid = "56">For each method, we randomly sample n samples from all of the paraphrase pairs (p1, p2) for which both s1 and s2 are retrieved.</S>
			<S sid ="193" ssid = "57">Then, from each (p1, p2) and (s1, s2), we create two paraphrasesentence pairs (sf , sf ) by exchanging p1 and p2 be Defi nitio n sent ence pair s Sup Uns B M SM T Mr t with trivi al 1,381,42 4 24,0 49 9,56 2 18,1 84 with out trivi al 1,377,57 3 23,4 90 7,25 6 18,1 39 Web sent ence pair s Sup Uns B M SM T Mr t with trivi al 277,17 2 5,1 01 4,58 6 4,9 78 with out trivi al 274,72 0 4,3 99 2,34 2 4,9 58 Table 2: Number of extracted paraphrases.</S>
			<S sid ="194" ssid = "58">(p1, p2) is regarded as trivial if the pronunciation is the same between p1 and p2 ,10 or all of the content words contained in p1 are the same as those of p2.</S>
			<S sid ="195" ssid = "59">Graph (b) gives a precision curve for each method.</S>
			<S sid ="196" ssid = "60">Again, Sup outperforms the others too, and maintains a precision rate of about 90% until the top 1,000.</S>
			<S sid ="197" ssid = "61">These results support our claim II.</S>
			<S sid ="198" ssid = "62">The upper half of Table 2 shows the number of 1 2 tween s1 and s2.</S>
			<S sid ="199" ssid = "63">All samples, each consisting of (p1, p2), (s1 , s2), and (sf , sf ), are checked by three extracted paraphrases with/without trivial pairs for each method.11 Sup and Uns extracted many more 1 2 human annotators to determine whether s1 entails s1 and s2 entails s2 so that entailment relations of f f both directions are verified.</S>
			<S sid ="200" ssid = "64">In advance of evaluation annotation, all the evaluation samples are shuffled so that the annotators cannot find out which sample is given by which method for fairness.</S>
			<S sid ="201" ssid = "65">We regard each paraphrase pair as correct if at least two annota- tors judge that entailment relations of both directions hold for it.</S>
			<S sid ="202" ssid = "66">You may wonder whether only one pair of sentences (s1, s2 ) is enough for evaluation since a correct (wrong) paraphrase pair might be judged as wrong (correct) accidentally.</S>
			<S sid ="203" ssid = "67">Nevertheless, we suppose that the final evaluation results are reliable if the number of evaluation samples is sufficient.</S>
			<S sid ="204" ssid = "68">In this experiment, m is 5,000 and n is 200.</S>
			<S sid ="205" ssid = "69">We use Yahoo!JAPAN API to retrieve sentences.</S>
			<S sid ="206" ssid = "70">Graph (a) in Figure 3 shows a precision curve for each method.</S>
			<S sid ="207" ssid = "71">Sup and Uns respectively indicate the supervised and unsupervised versions of our method.</S>
			<S sid ="208" ssid = "72">The figure indicates that Sup outperforms all the others and shows a high precision rate of about 94% at the top 1,000.</S>
			<S sid ="209" ssid = "73">Remember that this is the result of using 100,000 definition sentence pairs.</S>
			<S sid ="210" ssid = "74">Thus, we estimate that Sup can extract about 300,000 paraphrase pairs with a precision rate of about 94%, if we use all 29,661,812 definition sentence pairs that we acquired.</S>
			<S sid ="211" ssid = "75">Furthermore, we measured precision after trivial paraphrase pairs were discarded from the evaluation samples of each method.</S>
			<S sid ="212" ssid = "76">A candidate phrase pair paraphrases.</S>
			<S sid ="213" ssid = "77">It is noteworthy that Sup performed the best in terms of both precision rate and the number of extracted paraphrases.</S>
			<S sid ="214" ssid = "78">Table 3 shows examples of correct and incorrect outputs of Sup.</S>
			<S sid ="215" ssid = "79">As the examples indicate, many of the extracted paraphrases are not specific to definition sentences and seem very reusable.</S>
			<S sid ="216" ssid = "80">However, there are few paraphrases involving metaphors or idioms in the outputs due to the nature of definition sentences.</S>
			<S sid ="217" ssid = "81">In this regard, we do not claim that our method is almighty.</S>
			<S sid ="218" ssid = "82">We agree with Sekine (2005) who claims that several different methods are required to discover a wider variety of paraphrases.</S>
			<S sid ="219" ssid = "83">In graphs (a) and (b), the precision of the SMT method goes up as rank goes down.</S>
			<S sid ="220" ssid = "84">This strange behavior is due to the scoring by Moses that worked poorly for the data; it gave 1.0 to 82.5% of all the samples, 38.8% of which were incorrect.</S>
			<S sid ="221" ssid = "85">We suspect SMT methods are poor at monolingual alignment for paraphrasing or entailment tasks since, in the tasks, data is much noisier than that used for SMT.</S>
			<S sid ="222" ssid = "86">See MacCartney et al.</S>
			<S sid ="223" ssid = "87">(2008) for similar discussion.</S>
			<S sid ="224" ssid = "88">4.2 Definition pairs vs. Web sentence pairs.</S>
			<S sid ="225" ssid = "89">To collect Web sentence pairs, first, we randomly sampled 1.8 million sentences from the Web corpus.</S>
			<S sid ="226" ssid = "90">10 There are many kinds of orthographic variants in Japanese, which can be identified by their pronunciation.</S>
			<S sid ="227" ssid = "91">11 We set no threshold for candidate phrase pairs of each.</S>
			<S sid ="228" ssid = "92">method, and counted all the candidate phrase pairs in Table 2.</S>
			<S sid ="229" ssid = "93">1 0.8 ’Sup_def’ ’Uns_def’ ’SMT_def’ ’BM_def’ ’Mrt_def’ 1 0.8 ’Sup_def_n’ ’Uns_def_n’ ’SMT_def_n’ ’BM_def_n’ ’Mrt_def_n’ 0.6 0.6 0.4 0.4 0.2 0.2 0 0 1000 2000 3000 4000 5000 Top-N 0 0 1000 2000 3000 4000 5000 Top-N (a) Definition sentence pairs with trivial paraphrases (b) Definition sentence pairs without trivial paraphrases 1 0.8 ’Sup_www’ ’Uns_www’ ’SMT_www’ ’BM_www’ ’Mrt_www’ 1 0.8 ’Sup_www_n’ ’Uns_www_n’ ’SMT_www_n’ ’BM_www_n’ ’Mrt_www_n’ 0.6 0.6 0.4 0.4 0.2 0.2 0 0 1000 2000 3000 4000 5000 Top-N 0 0 1000 2000 3000 4000 5000 Top-N (c) Web sentence pairs with trivial paraphrases (d) Web sentence pairs without trivial paraphrases Figure 3: Precision curves of paraphrase extraction.</S>
			<S sid ="230" ssid = "94">Ra nk P a r a p h r a s e p a i r Corr ect 1 3 1 9 7 0 1 1 2 6 5 6 8 4 1 9 2 9 1, 5 5 3 2, 2 4 3 2, 8 5 5 2, 9 3 1 3, 6 6 7 4, 8 7 0 5, 5 0 1 10, 67 5 112, 819 193, 553 メー ルア ドレ スに メー ルを 送る (sen d a mes sage to the e mail addr ess) ⇔ メー ルア ドレ スに 電子 メー ルを 送る (sen d an e mail mes sage to the e mail addr ess) お 客 様 の 依 頼 に よ る (req uest ed by a cust ome r) ⇔ お 客 様 の 委 託 に よ る (co mmi ssio ned by a cust ome r) 企 業 の 財 政 状 況 を 表 す (des crib e the fisca l cond ition of com pany ) ⇔ 企 業 の 財 政 状 態 を 示 す (indi cate the fisca l state of com pan y) イン フォ メー ショ ンを 得る (get infor mati on) ⇔ ニ ュ ー ス を 得 る (get new s) き ま り の こ と で す (it is a conv enti on) ⇔ ル ー ル の こ と で す (it is a rule) 地震 のエ ネル ギー 規模 をあ らわ す (repr esen t the ener gy scal e of eart hqua ke) ⇔ 地震 の規 模を 表す (repr esen t the scal e of eart hqu ake) 細 胞 を 酸 化 さ せ る (cau se the oxid atio n of cells ) ⇔ 細 胞 を 老 化 さ せ る (cau se cellu lar agin g) 角 質 を 取 り 除 く (rem ove dead skin cells ) ⇔ 角 質 を は が す (pee l off dead skin cells ) 胎児 の発 育に 必要 だ (req uire d for the deve lop men t of fetus ) ⇔ 胎児 の発 育成 長に 必要 不可 欠だ (indi spen sabl e for the gro wth and deve lop men t of fetu s) 視 力 を 矯 正 す る (corr ect eyes ight) ⇔ 視 力 矯 正 を 行 う (perf orm eyes ight corr ecti on) チ ャ ラ に し て も ら う (call it even ) ⇔ 帳 消 し に し て も ら う (call it quit s) ハー ドデ ィス ク上 に蓄 積さ れる (acc umu lated on a hard disk ) ⇔ ハー ドデ ィス クド ライ ブに 保存 され る (stor ed on a hard disk driv e) 有 害 物 質 を 排 泄 す る (exc rete har mful subs tanc e) ⇔ 有 害 毒 素 を 排 出 す る (disc harg e har mful toxi n) １つ のＣ ＰＵ の内 部に ２つ のプ ロセ ッサ コア を搭 載す る (mo unt two proc esso r core s on one CPU ) ⇔ １つ のパ ッケ ー ジに ２つ のプ ロセ ッサ コア を集 積す る (buil d two proc esso r core s into one pack age) 外 貨 を 売 買 す る (trad e forei gn curr enci es) ⇔ 通 貨 を 交 換 す る (exc hang e one curr ency for anot her) 派 遣 先 企 業 の 社 員 に な る (bec ome a regu lar staff me mbe r of the com pany whe re (s)h e has wor ked as a tem p) ⇔ 派 遣 先 に 直 接 雇 用 さ れ る (em ploy ed by the com pany whe re (s)h e has wor ked as a tem p) Ｗｅ ｂサ イト にア クセ スす る (acc ess Web sites ) ⇔ ＷＷ Ｗサ イト を訪 れる (visi t WW W sites ) Inco rrect 9 0 3 2, 5 3 0 3, 0 0 8 ブラ ウザ に送 信さ れる (sen d to a Web bro wser ) ⇔ パソ コン に送 信さ れる (sen d to a PC) 調 和 を は か る (inte nd to bala nce) ⇔ リ フ レ ッ シ ュ を 図 る (inte nd to refre sh) 消化 酵素 では 消化 でき ない (una ble to dige st with dige stive enzy mes) ⇔ 消化 酵素 で消 化さ れ難 い (har d to dige st with dige stive enzy mes ) Table 3: Examples of correct and incorrect paraphrases extracted by our supervised method with their rank.</S>
			<S sid ="231" ssid = "95">We call them sampled sentences.</S>
			<S sid ="232" ssid = "96">Then, using Yahoo!JAPAN API, we retrieved up to 20 snippets relevant to each sampled sentence using all of the nouns in each sentence as a query.</S>
			<S sid ="233" ssid = "97">After that, each snippet was split into sentences, which we call snippet sentences.</S>
			<S sid ="234" ssid = "98">We paired a sampled sentence and a snippet sentence that was the most similar to the sampled sentence.</S>
			<S sid ="235" ssid = "99">Similarity is the number of nouns shared by the two sentences.</S>
			<S sid ="236" ssid = "100">Finally, we randomly sampled 100,000 pairs from all the pairs.</S>
			<S sid ="237" ssid = "101">Paraphrase pairs were extracted from the Web sentence pairs by using BM, SMT, Mrt and the supervised and unsupervised versions of our method.</S>
			<S sid ="238" ssid = "102">The features used with our methods were selected from all of the 78 features mentioned in Section 3.2 so that they performed well for Web sentence pairs.</S>
			<S sid ="239" ssid = "103">Specifically, the features were selected by ablation tests using training data that was tailored to Web sentence pairs.</S>
			<S sid ="240" ssid = "104">The training data consisted of 2,741 sentence pairs that were collected in the same way as the Web sentence pairs and was labeled in the same way as described in Section 3.2.</S>
			<S sid ="241" ssid = "105">Graph (c) of Figure 3 shows precision curves.</S>
			<S sid ="242" ssid = "106">We also measured precision without trivial pairs in the same way as the previous experiment.</S>
			<S sid ="243" ssid = "107">Graph (d) shows the results.</S>
			<S sid ="244" ssid = "108">The lower half of Table 2 shows the number of extracted paraphrases with/without trivial pairs for each method.</S>
			<S sid ="245" ssid = "109">Note that precision figures of our methods in graphs (c) and (d) are lower than those of our methods in graphs (a) and (b).</S>
			<S sid ="246" ssid = "110">Additionally, none of the methods achieved a precision rate of 90% using Web sentence pairs.12 We think that a precision rate of at least 90% would be necessary if you apply automatically extracted paraphrases to NLP tasks without manual annotation.</S>
			<S sid ="247" ssid = "111">Only the combination of Sup and definition sentence pairs achieved that precision.</S>
			<S sid ="248" ssid = "112">Also note that, for all of the methods, the numbers of extracted paraphrases from Web sentence pairs are fewer than those from definition sentence pairs.</S>
			<S sid ="249" ssid = "113">From all of these results, we conclude that our claim I is verified.</S>
			<S sid ="250" ssid = "114">12 Precision of SMT is unexpectedly good.</S>
			<S sid ="251" ssid = "115">We found some.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="252" ssid = "1">We proposed a method of extracting paraphrases from definition sentences on the Web.</S>
			<S sid ="253" ssid = "2">From the experimental results, we conclude that the following two claims of this paper are verified.</S>
			<S sid ="254" ssid = "3">1.</S>
			<S sid ="255" ssid = "4">Definition sentences on the Web are a treasure.</S>
			<S sid ="256" ssid = "5">trove of paraphrase knowledge.</S>
			<S sid ="257" ssid = "6">2.</S>
			<S sid ="258" ssid = "7">Our method extracts many paraphrases from.</S>
			<S sid ="259" ssid = "8">the definition sentences on the Web accurately; it can extract about 300,000 paraphrases from 6 × 108 Web documents with a precision rate of about 94%.</S>
			<S sid ="260" ssid = "9">Our future work is threefold.</S>
			<S sid ="261" ssid = "10">First, we will release extracted paraphrases from all of the 29,661,812 definition sentence pairs that we acquired, after human annotators check their validity.</S>
			<S sid ="262" ssid = "11">The result will be available through the ALAGIN forum.13 Second, we plan to induce paraphrase rules from paraphrase instances.</S>
			<S sid ="263" ssid = "12">Though our method can extract a variety of paraphrase instances on a large scale, their coverage might be insufficient for real NLP applications since some paraphrase phenomena are highly productive.</S>
			<S sid ="264" ssid = "13">Therefore, we need paraphrase rules in addition to paraphrase instances.</S>
			<S sid ="265" ssid = "14">Barzilay and McKeown (2001) induced simple POS-based paraphrase rules from paraphrase instances, which can be a good starting point.</S>
			<S sid ="266" ssid = "15">Finally, as mentioned in Section 1, the work in this paper is only the beginning of our research on paraphrase extraction.</S>
			<S sid ="267" ssid = "16">We are trying to extract far more paraphrases from a set of sentences fulfilling the same pragmatic function (e.g. definition) for the same topic (e.g. osteoporosis) on the Web.</S>
			<S sid ="268" ssid = "17">Such functions other than definition may include the usage of the same Linux command, the recipe for the same cuisine, or the description of related work on the same research issue.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="269" ssid = "18">We would like to thank Atsushi Fujita, Francis Bond, and all of the members of the Information Analysis Laboratory, Universal Communication Research Institute at NICT.</S>
			<S sid ="270" ssid = "19">Web sentence pairs consisting of two mostly identical sentences on rare occasions.</S>
			<S sid ="271" ssid = "20">The method worked relatively well for them.</S>
			<S sid ="272" ssid = "21">13 http://alagin.jp/</S>
	</SECTION>
</PAPER>
