<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We propose a new specifically designed method for paraphrase generation based on MonteCarlo sampling and show how this algorithm is suitable for its task.</S>
		<S sid ="2" ssid = "2">Moreover, the basic algorithm presented here leaves a lot of opportunities for future improvement.</S>
		<S sid ="3" ssid = "3">In particular, our algorithm does not constraint the scoring function in opposite to Viterbi based decoders.</S>
		<S sid ="4" ssid = "4">It is now possible to use some global features in paraphrase scoring functions.</S>
		<S sid ="5" ssid = "5">This algorithm opens new outlooks for paraphrase generation and other natural language processing applications like statistical machine translation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">A paraphrase generation system is a program which, given a source sentence, produces a different sentence with almost the same meaning.</S>
			<S sid ="7" ssid = "7">Paraphrase generation is useful in applications to choose between different forms to keep the most appropriate one.</S>
			<S sid ="8" ssid = "8">For instance, automatic summary can be seen as a particular paraphrasing task (Barzilay and Lee, 2003) with the aim of selecting the shortest paraphrase.</S>
			<S sid ="9" ssid = "9">Paraphrases can also be used to improve natural language processing (NLP) systems.</S>
			<S sid ="10" ssid = "10">(CallisonBurch et al., 2006) improved machine translations by augmenting the coverage of patterns that can be translated.</S>
			<S sid ="11" ssid = "11">Similarly, (Sekine, 2005) improved information retrieval based on pattern recognition by introducing paraphrase generation.</S>
			<S sid ="12" ssid = "12">In order to produce paraphrases, a promising approach is to see the paraphrase generation problem as a translation problem, where the target language is the same as the source language (Quirk et al., 2004; Bannard and CallisonBurch, 2005).</S>
			<S sid ="13" ssid = "13">A problem that has drawn less attention is the generation step which corresponds to the decoding step in SMT.</S>
			<S sid ="14" ssid = "14">Most paraphrase generation tools use some standard SMT decoding algorithms (Quirk et al., 2004) or some off-the-shelf decoding tools like MOSES (Koehn et al., 2007).</S>
			<S sid ="15" ssid = "15">The goal of a decoder is to find the best path in the lattice produced from a paraphrase table.</S>
			<S sid ="16" ssid = "16">This is basically achieved by using dynamic programming and especially the Viterbi algorithm associated with beam searching.</S>
			<S sid ="17" ssid = "17">However decoding algorithms were designed for translation, not for paraphrase generation.</S>
			<S sid ="18" ssid = "18">Although left-to-right decoding is justified for translation, it may not be necessary for paraphrase generation.</S>
			<S sid ="19" ssid = "19">A paraphrase generation tool usually starts with a sentence which may be very similar to some potential solution.</S>
			<S sid ="20" ssid = "20">In other words, there is no need to &quot;translate&quot; all of the sentences.</S>
			<S sid ="21" ssid = "21">Moreover, decoding may not be suitable for non-contiguous transformation rules.</S>
			<S sid ="22" ssid = "22">In addition, dynamic programming imposes an incremental scoring function to evaluate the quality of each hypothesis.</S>
			<S sid ="23" ssid = "23">For instance, it cannot capture some scattered syntactical dependencies.</S>
			<S sid ="24" ssid = "24">Improving on this major issue is a key point to improve paraphrase generation systems.</S>
			<S sid ="25" ssid = "25">This paper first presents an alternative to decoding that is based on transformation rule application in section 2.</S>
			<S sid ="26" ssid = "26">In section 3 we propose a paraphrase generation method for this paradigm based on an algorithm used in two-player games.</S>
			<S sid ="27" ssid = "27">Section 4 briefly explain experimental context and its associated protocol for evaluation of the proposed system.</S>
			<S sid ="28" ssid = "28">We compare the proposed algorithm with a baseline system in section 5.</S>
			<S sid ="29" ssid = "29">Finally, in section 6, we point to future research tracks to improve paraphrase generation tools.</S>
	</SECTION>
	<SECTION title="Statistical paraphrase generation using. " number = "2">
			<S sid ="30" ssid = "1">transformation rules The paraphrase generation problem can be seen as an exploration problem.</S>
			<S sid ="31" ssid = "2">We seek the best paraphrase according to a scoring function in a space 249 Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 249–252, Suntec, Singapore, 4 August 2009.</S>
			<S sid ="32" ssid = "3">Qc 2009 ACL and AFNLP to search by applying successive transformations.</S>
			<S sid ="33" ssid = "4">This space is composed of states connected by actions.</S>
			<S sid ="34" ssid = "5">An action is a transformation rule with a place where it applies in the sentence.</S>
			<S sid ="35" ssid = "6">States are a sentence with a set of possible actions.</S>
			<S sid ="36" ssid = "7">Applying an action in a given state consists in transforming the sentence of the state and removing all rules that are no more applicable.</S>
			<S sid ="37" ssid = "8">In our framework, each state, except the root, can be a final state.</S>
			<S sid ="38" ssid = "9">This is modelised by adding a stop rule as a particular action.</S>
			<S sid ="39" ssid = "10">We impose the constraint that any transformed part of the source sentence cannot be transformed anymore.</S>
			<S sid ="40" ssid = "11">This paradigm is more approriate for paraphrase generation than the standard SMT approach in respect to several points: there is no need for left- to-right decoding because a transformation can be applied anywhere without order; there is no need to transform the whole of a sentence because each state is a final state; there is no need to keep the identity transformation for each phrase in the paraphrase table; the only domain knowledge needed is a generative model and a scoring function for final states; it is possible to mix different generative models because a statistical paraphrase table, an analogical solver and a paraphrase memory for instance; there is no constraint on the scoring function because it only scores final states.</S>
			<S sid ="41" ssid = "12">Note that the branching factor with a paraphrase table can be around thousand actions per states which makes the generation problem a difficult computational problem.</S>
			<S sid ="42" ssid = "13">Hence we need an efficient generation algorithm.</S>
	</SECTION>
	<SECTION title="Monte-Carlo based Paraphrase. " number = "3">
			<S sid ="43" ssid = "1">Generation UCT (Kocsis and Szepesvári, 2006) (Upper Confidence bound applied to Tree) is a MonteCarlo planning algorithm that have some interesting properties: it grows the search tree non-uniformly and favours the most promising sequences, without pruning branch; it can deal with high branching factor; it is an anytime algorithm and returns best solution found so far when interrupted; it does not require expert domain knowledge to evaluate states.</S>
			<S sid ="44" ssid = "2">These properties make it ideally suited for games with high branching factor and for which there is no strong evaluation function.</S>
			<S sid ="45" ssid = "3">For the same reasons, this algorithm sounds interesting for paraphrase generation.</S>
			<S sid ="46" ssid = "4">In particular, it does not put constraint on the scoring function.</S>
			<S sid ="47" ssid = "5">We propose a variation of the UCT algorithm for paraphrase generation named MCPG for MonteCarlo based Paraphrase Generation.</S>
			<S sid ="48" ssid = "6">The main part of the algorithm is the sampling step.</S>
			<S sid ="49" ssid = "7">An episode of this step is a sequence of states and actions, s1, a1, s2, a2, . . .</S>
			<S sid ="50" ssid = "8">, sT , from the root state to a final state.</S>
			<S sid ="51" ssid = "9">During an episode construction, there are two ways to select the action ai to perfom from a state si.</S>
			<S sid ="52" ssid = "10">If the current state was already explored in a previous episode, the action is selected according to a compromise between exploration and exploitation.</S>
			<S sid ="53" ssid = "11">This compromise is computed using the UCBTunned formula (Auer et al., 2001) associated with the RAVE heuristic (Gelly and Silver, 2007).</S>
			<S sid ="54" ssid = "12">If the current state is explored for the first time, its score is estimated using MonteCarlo sampling.</S>
			<S sid ="55" ssid = "13">In other word, to complete the episode, the actions ai, ai+1, . . .</S>
			<S sid ="56" ssid = "14">, aT −1, aT are selected randomly until a stop rule is drawn.</S>
			<S sid ="57" ssid = "15">At the end of each episode, a reward is computed for the final state sT using a scoring function and the value of each (state, action) pair of the episode is updated.</S>
			<S sid ="58" ssid = "16">Then, the algorithm computes an other episode with the new values.</S>
			<S sid ="59" ssid = "17">Periodically, the sampling step is stopped and the best action at the root state is selected.</S>
			<S sid ="60" ssid = "18">This action is then definitely applied and a sampling is restarted from the new root state.</S>
			<S sid ="61" ssid = "19">The action sequence is built incrementally and selected after being enough sampled.</S>
			<S sid ="62" ssid = "20">For our experiments, we have chosen to stop sampling regularly after a fixed amount η of episodes.</S>
			<S sid ="63" ssid = "21">Our main adaptation of the original algorithm is in the (state, action) value updating procedure.</S>
			<S sid ="64" ssid = "22">Since the goal of the algorithm is to maximise a scoring function, we use the maximum reachable score from a state as value instead of the score expectation.</S>
			<S sid ="65" ssid = "23">This algorithm suits the paradigm proposed for paraphrase generation.</S>
	</SECTION>
	<SECTION title="Experimental context. " number = "4">
			<S sid ="66" ssid = "1">This section describes the experimental context and the methodology followed to evaluate our statistical paraphrase generation tool.</S>
			<S sid ="67" ssid = "2">4.1 Data.</S>
			<S sid ="68" ssid = "3">For the experiment reported in section 5, we use one of the largest, multilingual, freely available aligned corpus, Europarl (Koehn, 2005).</S>
			<S sid ="69" ssid = "4">It consists of European parliament debates.</S>
			<S sid ="70" ssid = "5">We choose French as the language for paraphrases and English as the pivot language.</S>
			<S sid ="71" ssid = "6">For this pair of languages, the corpus consists of 1, 487, 459 French sentences aligned with 1, 461, 429 English sentences.</S>
			<S sid ="72" ssid = "7">Note that the sentences in this corpus are long, with an average length of 30 words per French sentence and 27.1 for English.</S>
			<S sid ="73" ssid = "8">We randomly extracted 100 French sentences as a test corpus.</S>
			<S sid ="74" ssid = "9">4.2 Language model and paraphrase table.</S>
			<S sid ="75" ssid = "10">Paraphrase generation tools based on SMT methods need a language model and a paraphrase table.</S>
			<S sid ="76" ssid = "11">Both are computed on a training corpus.</S>
			<S sid ="77" ssid = "12">The language models we use are n-gram language models with back-off.</S>
			<S sid ="78" ssid = "13">We use SRILM (Stolcke, 2002) with its default parameters for this pur SMT decoder.</S>
			<S sid ="79" ssid = "14">We use the MOSES decoder (Koehn et al., 2007) as a baseline.</S>
			<S sid ="80" ssid = "15">The MOSES scoring function is set by four weighting factors αΦ, αLM , αD , αW . Conventionally, these four weights are adjusted during a tuning step on a training corpus.</S>
			<S sid ="81" ssid = "16">The tuning step is inappropriate for paraphrase because there is no such tuning corpus available.</S>
			<S sid ="82" ssid = "17">We empirically set αΦ = 1, αLM = 1, αD = 10 and αW = 0.</S>
			<S sid ="83" ssid = "18">Hence, the scoring function (or reward function for MCPG) is equivalent to: R(f |f, I ) = p(f ) × Φ(f |f , I ) where f and f are the source and target sentences, I a segmentation in phrases of f , p(f ) the language model score and Φ(f |f , I ) = pose.</S>
			<S sid ="84" ssid = "19">The length of the n-grams is five.</S>
			<S sid ="85" ssid = "20">i∈I p(f |f i) the paraphrase table score.</S>
			<S sid ="86" ssid = "21">To build a paraphrase table, we use the construction method via a pivot language proposed in (Bannard and CallisonBurch, 2005).</S>
			<S sid ="87" ssid = "22">Three heuristics are used to prune the paraphrase table.</S>
			<S sid ="88" ssid = "23">The first heuristic prunes any entry in the paraphrase table composed of tokens with a probability lower than a threshold E. The second, called pruning pivot heuristic, consists in deleting all pivot clusters larger than a threshold τ . The last heuristic keeps only the κ most probable paraphrases for each source phrase in the final paraphrase table.</S>
			<S sid ="89" ssid = "24">For this study, we empirically fix E = 10−5, τ = 200 and κ = 10.</S>
			<S sid ="90" ssid = "25">4.3 Evaluation Protocol.</S>
			<S sid ="91" ssid = "26">The MCPG algorithm needs two parameters.</S>
			<S sid ="92" ssid = "27">One is the number of episodes η done before selecting the best action at root state.</S>
			<S sid ="93" ssid = "28">The other is k, an equivalence parameter which balances the exploration/exploitation compromise (Auer et al., 2001).</S>
			<S sid ="94" ssid = "29">We empirically set η = 1, 000, 000 and k = 1, 000.</S>
			<S sid ="95" ssid = "30">For our algorithm, note that identity paraphrase probabilities are biased: for each phrase it is equal to the probability of the most probable paraphrase.</S>
			<S sid ="96" ssid = "31">Moreover, as the source sentence is the best meaning preserved &quot;paraphrase&quot;, a sentence cannot have a better score.</S>
			<S sid ="97" ssid = "32">Hence, we use a slightly different scoring function: We developed a dedicated website to allow the human judges with some flexibility in workplaces  R(f |f, I ) = min  p (f ) n p(f i|f i)  , 1  i i  and evaluation periods.</S>
			<S sid ="98" ssid = "33">We retain the principle of the two-step evaluation, common in the machine  p(f ) i∈I f i I=f ti p(f |f )  translation domain and already used for paraphrase evaluation (Bannard and CallisonBurch, 2005).</S>
			<S sid ="99" ssid = "34">The question asked to the human evaluator for the syntactic task is: Is the following sentence in good French?</S>
			<S sid ="100" ssid = "35">The question asked to the human evaluator for the semantic task is: Do the following two sentences express the same thing?</S>
			<S sid ="101" ssid = "36">In our experiments, each paraphrase was evaluated by two native French evaluators.</S>
	</SECTION>
	<SECTION title="Comparison with a SMT decoder. " number = "5">
			<S sid ="102" ssid = "1">In order to validate our algorithm for paraphrase generation, we compare it with an off-the-shelf Note that for this model, there is no need to know the identity transformations probability for unchanged part of the sentence.</S>
			<S sid ="103" ssid = "2">Results are presented in Table 1.</S>
			<S sid ="104" ssid = "3">The Kappa statistics associated with the results are 0.84, 0.64 and 0.59 which are usually considered as a &quot;perfect&quot;, &quot;substantial&quot; and &quot;moderate&quot; agreement.</S>
			<S sid ="105" ssid = "4">Results are close to evaluations from the baseline system.</S>
			<S sid ="106" ssid = "5">The main differences are from Kappa statistics which are lower for the MOSES system evaluation.</S>
			<S sid ="107" ssid = "6">Judges changed between the two experiments.</S>
			<S sid ="108" ssid = "7">We may wonder whether an evaluation with only two judges is reliable.</S>
			<S sid ="109" ssid = "8">This points to the ambiguity of any paraphrase definition.</S>
			<S sid ="110" ssid = "9">Sy st e m M O S E S M C P G W ell for m ed (K ap pa ) 64 % (0.</S>
			<S sid ="111" ssid = "10">57 ) 63 % (0.</S>
			<S sid ="112" ssid = "11">84 ) M ea ni ng pr es er ve d (K ap pa ) 58 % (0.</S>
			<S sid ="113" ssid = "12">48 ) 55 % (0.</S>
			<S sid ="114" ssid = "13">64 ) W ell for m ed an d m ea ni ng pr es er ve d (K ap pa ) 50 % (0.</S>
			<S sid ="115" ssid = "14">54 ) 49 % (0.</S>
			<S sid ="116" ssid = "15">59 ) Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot language.</S>
			<S sid ="117" ssid = "16">Comparison between the baseline system MOSES and our algorithm MCPG.</S>
			<S sid ="118" ssid = "17">By doing this experiment, we have shown that our algorithm with a biased paraphrase table is state-of-the-art to generate paraphrases.</S>
	</SECTION>
	<SECTION title="Conclusions and further  research. " number = "6">
			<S sid ="119" ssid = "1">In this paper, we have proposed a different paradigm and a new algorithm in NLP field adapted for statistical paraphrases generation.</S>
			<S sid ="120" ssid = "2">This method, based on large graph exploration by MonteCarlo sampling, produces results comparable with state-of-the-art paraphrase generation tools based on SMT decoders.</S>
			<S sid ="121" ssid = "3">The algorithm structure is flexible and generic enough to easily work with discontinous patterns.</S>
			<S sid ="122" ssid = "4">It is also possible to mix various transformation methods to increase paraphrase variability.</S>
			<S sid ="123" ssid = "5">The rate of ill-formed paraphrase is high at 37%.</S>
			<S sid ="124" ssid = "6">The result analysis suggests an involvement of the non-preservation of the original meaning when a paraphrase is evaluated ill-formed.</S>
			<S sid ="125" ssid = "7">Although the mesure is not statistically significant because the test corpus is too small, the same trend is also observed in other experiments.</S>
			<S sid ="126" ssid = "8">Improving on the language model issue is a key point to improve paraphrase generation systems.</S>
			<S sid ="127" ssid = "9">Our algorithm can work with unconstraint scoring functions, in particular, there is no need for the scoring function to be incremental as for Viterbi based decoders.</S>
			<S sid ="128" ssid = "10">We are working to add, in the scoring function, a linguistic knowledge based analyzer to solve this problem.</S>
			<S sid ="129" ssid = "11">Because MCPG is based on a different paradigm, its output scores cannot be directly compared to MOSES scores.</S>
			<S sid ="130" ssid = "12">In order to prove the optimisation qualities of MCPG versus state-of-the-art decoders, we are transforming our paraphrase generation tool into a translation tool.</S>
	</SECTION>
</PAPER>
