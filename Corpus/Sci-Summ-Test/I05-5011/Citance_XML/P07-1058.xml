<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Obtaining large volumes of inference knowledge, such as entailment rules, has become a major factor in achieving robust semantic processing.</S>
		<S sid ="2" ssid = "2">While there has been substantial research on learning algorithms for such knowledge, their evaluation methodology has been problematic, hindering further research.</S>
		<S sid ="3" ssid = "3">We propose a novel evaluation methodology for entailment rules which explicitly addresses their semantic properties and yields satisfactory human agreement levels.</S>
		<S sid ="4" ssid = "4">The methodology is used to compare two state of the art learning algorithms, exposing critical issues for future progress.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">In many NLP applications, such as Question Answering (QA) and Information Extraction (IE), it is crucial to recognize that a particular target meaning can be inferred from different text variants.</S>
			<S sid ="6" ssid = "6">For example, a QA system needs to identify that “Aspirin lowers the risk of heart attacks” can be inferred from “Aspirin prevents heart attacks” in order to answer the question “What lowers the risk of heart attacks?”.</S>
			<S sid ="7" ssid = "7">This type of reasoning has been recognized as a core semantic inference task by the generic textual entailment framework (Dagan et al., 2006).</S>
			<S sid ="8" ssid = "8">A major obstacle for further progress in semantic inference is the lack of broad-scale knowledge- bases for semantic variability patterns (Bar-Haim et al., 2006).</S>
			<S sid ="9" ssid = "9">One prominent type of inference knowledge representation is inference rules such as para 456 phrases and entailment rules.</S>
			<S sid ="10" ssid = "10">We define an entail- ment rule to be a directional relation between two templates, text patterns with variables, e.g. ‘X prevent Y → X lower the risk of Y ’.</S>
			<S sid ="11" ssid = "11">The left-hand- side template is assumed to entail the right-hand- side template in certain contexts, under the same variable instantiation.</S>
			<S sid ="12" ssid = "12">Paraphrases can be viewed as bidirectional entailment rules.</S>
			<S sid ="13" ssid = "13">Such rules capture basic inferences and are used as building blocks for more complex entailment inference.</S>
			<S sid ="14" ssid = "14">For example, given the above rule, the answer “Aspirin” can be identified in the example above.</S>
			<S sid ="15" ssid = "15">The need for large-scale inference knowledge- bases triggered extensive research on automatic acquisition of paraphrase and entailment rules.</S>
			<S sid ="16" ssid = "16">Yet the current precision of acquisition algorithms is typically still mediocre, as illustrated in Table 1 for DIRT (Lin and Pantel, 2001) and TEASE (Szpek- tor et al., 2004), two prominent acquisition algorithms whose outputs are publicly available.</S>
			<S sid ="17" ssid = "17">The current performance level only stresses the obvious need for satisfactory evaluation methodologies that would drive future research.</S>
			<S sid ="18" ssid = "18">The prominent approach in the literature for evaluating rules, termed here the rule-based approach, is to present the rules to human judges asking whether each rule is correct or not.</S>
			<S sid ="19" ssid = "19">However, it is difficult to explicitly define when a learned rule should be considered correct under this methodology, and this was mainly left undefined in previous works.</S>
			<S sid ="20" ssid = "20">As the criterion for evaluating a rule is not well defined, using this approach often caused low agreement between human judges.</S>
			<S sid ="21" ssid = "21">Indeed, the standards for evaluation in this field are lower than other fields: many papers Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 456–463, Prague, Czech Republic, June 2007.</S>
			<S sid ="22" ssid = "22">Qc 2007 Association for Computational Linguistics don’t report on human agreement at all and those that do report rather low agreement levels.</S>
			<S sid ="23" ssid = "23">Yet it is crucial to reliably assess rule correctness in order to measure and compare the performance of different algorithms in a replicable manner.</S>
			<S sid ="24" ssid = "24">Lacking a good evaluation methodology has become a barrier for further advances in the field.</S>
			<S sid ="25" ssid = "25">In order to provide a well-defined evaluation methodology we first explicitly specify when entail- ment rules should be considered correct, following the spirit of their usage in applications.</S>
			<S sid ="26" ssid = "26">We then propose a new instance-based evaluation approach.</S>
			<S sid ="27" ssid = "27">Under this scheme, judges are not presented only with the rule but rather with a sample of sentences that match its left hand side.</S>
			<S sid ="28" ssid = "28">The judges then assess whether the rule holds under each specific example.</S>
			<S sid ="29" ssid = "29">A rule is considered correct only if the percentage of examples assessed as correct is sufficiently high.</S>
			<S sid ="30" ssid = "30">We have experimented with a sample of input verbs for both DIRT and TEASE.</S>
			<S sid ="31" ssid = "31">Our results show significant improvement in human agreement over the rule-based approach.</S>
			<S sid ="32" ssid = "32">It is also the first comparison between such two state-of-the-art algorithms, which showed that they are comparable in precision but largely complementary in their coverage.</S>
			<S sid ="33" ssid = "33">Additionally, the evaluation showed that both algorithms learn mostly one-directional rules rather than (symmetric) paraphrases.</S>
			<S sid ="34" ssid = "34">While most NLP applications need directional inference, previous acquisition works typically expected that the learned rules would be paraphrases.</S>
			<S sid ="35" ssid = "35">Under such an expectation, unidirectional rules were assessed as incorrect, underestimating the true potential of these algorithms.</S>
			<S sid ="36" ssid = "36">In addition, we observed that many learned rules are context sensitive, stressing the need to learn contextual constraints for rule applications.</S>
	</SECTION>
	<SECTION title="Background: Entailment Rules and their. " number = "2">
			<S sid ="37" ssid = "1">Evaluation 2.1 Entailment Rules.</S>
			<S sid ="38" ssid = "2">An entailment rule ‘L → R’ is a directional relation between two templates, L and R. For example, ‘X acquire Y → X own Y ’ or ‘X beat Y → X play against Y ’.</S>
			<S sid ="39" ssid = "3">Templates correspond to text fragments with variables, and are typically either linear phrases or parse sub-trees.The goal of entailment rules is to help applica Input Correct Incorrect X change Y (DIRT) (↔) X modify Y (←) X amend Y (←) X revise Y X adopt Y X create Y X stick to Y X change Y (TEASE) (↔) X alter Y (→) X affect Y (←) X extend Y X maintain Y X follow Y X use Y Table 1: Examples of templates suggested by DIRT and TEASE as having an entailment relation, in some direction, with the input template ‘X change Y ’.</S>
			<S sid ="40" ssid = "4">The entailment direction arrows were judged manually and added for readability.</S>
			<S sid ="41" ssid = "5">tions infer one text variant from another.</S>
			<S sid ="42" ssid = "6">A rule can be applied to a given text only when L can be inferred from it, with appropriate variable instantiation.</S>
			<S sid ="43" ssid = "7">Then, using the rule, the application deduces that R can also be inferred from the text under the same variable instantiation.</S>
			<S sid ="44" ssid = "8">For example, the rule ‘X lose to Y → Y beat X ’ can be used to infer “Liverpool beat Chelsea” from “Chelsea lost to Liverpool in the semifinals”.</S>
			<S sid ="45" ssid = "9">Entailment rules should typically be applied only in specific contexts, which we term relevant contexts.</S>
			<S sid ="46" ssid = "10">For example, the rule ‘X acquire Y → X buy Y ’ can be used in the context of ‘buying’ events.</S>
			<S sid ="47" ssid = "11">However, it shouldn’t be applied for “Students acquired a new language”.</S>
			<S sid ="48" ssid = "12">In the same manner, the rule ‘X acquire Y → X learn Y ’ should be applied only when Y corresponds to some sort of knowledge, as in the latter example.</S>
			<S sid ="49" ssid = "13">Some existing entailment acquisition algorithms can add contextual constraints to the learned rules (Sekine, 2005), but most don’t. However, NLP applications usually implicitly incorporate some contextual constraints when applying a rule.</S>
			<S sid ="50" ssid = "14">For example, when answering the question “Which companies did IBM buy?” a QA system would apply the rule ‘X acquire Y → X buy Y ’ correctly, since the phrase “IBM acquire X ” is likely to be found mostly in relevant economic contexts.</S>
			<S sid ="51" ssid = "15">We thus expect that an evaluation methodology should consider context relevance for entailment rules.</S>
			<S sid ="52" ssid = "16">For example, we would like both ‘X acquire Y → X buy Y ’ and ‘X acquire Y → X learn Y ’ to be assessed as correct (the second rule should not be deemed incorrect just because it is not applicable in frequent economic contexts).</S>
			<S sid ="53" ssid = "17">Finally, we highlight that the common notion of “paraphrase rules” can be viewed as a special case of entailment rules: a paraphrase ‘L ↔ R’ holds if both templates entail each other.</S>
			<S sid ="54" ssid = "18">Following the textual entailment formulation, we observe that many applied inference settings require only directional entailment, and a requirement for symmetric paraphrase is usually unnecessary.</S>
			<S sid ="55" ssid = "19">For example, in order to answer the question “Who owns Overture?” it suffices to use a directional entailment rule whose right hand side is ‘X own Y ’, such as ‘X acquire Y → X own Y ’, which is clearly not a paraphrase.</S>
			<S sid ="56" ssid = "20">2.2 Evaluation of Acquisition Algorithms.</S>
			<S sid ="57" ssid = "21">Many methods for automatic acquisition of rules have been suggested in recent years, ranging from distributional similarity to finding shared contexts (Lin and Pantel, 2001; Ravichandran and Hovy, 2002; Shinyama et al., 2002; Barzilay and Lee, 2003; Szpektor et al., 2004; Sekine, 2005).</S>
			<S sid ="58" ssid = "22">However, there is still no common accepted framework for their evaluation.</S>
			<S sid ="59" ssid = "23">Furthermore, all these methods learn rules as pairs of templates {L, R} in a symmetric manner, without addressing rule directional- ity.</S>
			<S sid ="60" ssid = "24">Accordingly, previous works (except (Szpektor et al., 2004)) evaluated the learned rules under the paraphrase criterion, which underestimates the practical utility of the learned rules (see Section 2.1).</S>
			<S sid ="61" ssid = "25">One approach which was used for evaluating automatically acquired rules is to measure their contribution to the performance of specific systems, such as QA (Ravichandran and Hovy, 2002) or IE (Sudo et al., 2003; Romano et al., 2006).</S>
			<S sid ="62" ssid = "26">While measuring the impact of learned rules on applications is highly important, it cannot serve as the primary approach for evaluating acquisition algorithms for several reasons.</S>
			<S sid ="63" ssid = "27">First, developers of acquisition algorithms often do not have access to the different applications that will later use the learned rules as generic modules.</S>
			<S sid ="64" ssid = "28">Second, the learned rules may affect individual systems differently, thus making observations that are based on different systems incomparable.</S>
			<S sid ="65" ssid = "29">Third, within a complex system it is difficult to assess the exact quality of entailment rules independently of effects of other system components.</S>
			<S sid ="66" ssid = "30">Thus, as in many other NLP learning settings, a direct evaluation is needed.</S>
			<S sid ="67" ssid = "31">Indeed, the prominent approach for evaluating the quality of rule acquisition algorithms is by human judgment of the learned rules (Lin and Pantel, 2001; Shinyama et al., 2002; Barzilay and Lee, 2003; Pang et al., 2003; Szpektor et al., 2004; Sekine, 2005).</S>
			<S sid ="68" ssid = "32">In this evaluation scheme, termed here the rule-based approach, a sample of the learned rules is presented to the judges who evaluate whether each rule is correct or not.</S>
			<S sid ="69" ssid = "33">The criterion for correctness is not explicitly described in most previous works.</S>
			<S sid ="70" ssid = "34">By the common view of context relevance for rules (see Section 2.1), a rule was considered correct if the judge could think of reasonable contexts under which it holds.</S>
			<S sid ="71" ssid = "35">We have replicated the rule-based methodology but did not manage to reach a 0.6 Kappa agreement level between pairs of judges.</S>
			<S sid ="72" ssid = "36">This approach turns out to be problematic because the rule correctness criterion is not sufficiently well defined and is hard to apply.</S>
			<S sid ="73" ssid = "37">While some rules might obviously be judged as correct or incorrect (see Table 1), judgment is often more difficult due to context relevance.</S>
			<S sid ="74" ssid = "38">One judge might come up with a certain context that, to her opinion, justifies the rule, while another judge might not imagine that context or think that it doesn’t sufficiently support rule correctness.</S>
			<S sid ="75" ssid = "39">For example, in our experiments one of the judges did not identify the valid “religious holidays” context for the correct rule ‘X observe Y → X celebrate Y ’.</S>
			<S sid ="76" ssid = "40">Indeed, only few earlier works reported inter-judge agreement level, and those that did reported rather low Kappa values, such as 0.54 (Barzilay and Lee, 2003) and 0.550.63 (Szpektor et al., 2004).</S>
			<S sid ="77" ssid = "41">To conclude, the prominent rule-based methodology for entailment rule evaluation is not sufficiently well defined.</S>
			<S sid ="78" ssid = "42">It results in low inter-judge agreement which prevents reliable and consistent assessments of different algorithms.</S>
	</SECTION>
	<SECTION title="Instance-based Evaluation Methodology. " number = "3">
			<S sid ="79" ssid = "1">As discussed in Section 2.1, an evaluation methodology for entailment rules should reflect the expected validity of their application within NLP systems.</S>
			<S sid ="80" ssid = "2">Following that line, an entailment rule ‘L → R’ should be regarded as correct if in all (or at least most) relevant contexts in which the instantiated template L is inferred from the given text, the instan R u l e S e n t e n c e J u d g m e n t 1 X se ek Y → X dis cl os e Y If he is arr est ed , he ca n im m ed iat el y se ek ba il.</S>
			<S sid ="81" ssid = "3">Le ft no t en tai le d 2 X cla rif y Y → X pr ep ar e Y H e di dn ’t cla rif y hi s po sit io n on th e su bj ec t. Le ft no t en tai le d 3 X hit Y → X ap pr oa ch Y Ot he r ea rt hq ua ke s ha ve hit Le ba no n sin ce ’8 2.</S>
			<S sid ="82" ssid = "4">Irr ele va nt co nt ex t 4 X los e Y → X su rre nd er Y Br ea d ha s re ce ntl y los t its su bs id y. Irr ele va nt co nt ex t 5 X re gu lat e Y → X ref or m Y T he S R A re gu lat es th e sal e of su ga r. N o en tai lm en t 6 X res ig n Y → X sh ar e Y Lo pe z res ig ne d hi s po st at V W las t we ek . N o en tai lm en t 7 X set Y → X all o w Y T he co m mi tte e set th e fol lo wi ng re fu nd s. En tai lm en t ho lds 8 X str es s Y → X sta te Y Be n Ya hi a als o str es se d th e ne ed fo r ac tio n. En tai lm en t ho lds Table 2: Rule evaluation examples and their judgment.</S>
			<S sid ="83" ssid = "5">tiated template R is also inferred from the text.</S>
			<S sid ="84" ssid = "6">This reasoning corresponds to the common definition of entailment in semantics, which specifies that a text L entails another text R if R is true in every circumstance (possible world) in which L is true (Chierchia and McConnellGinet, 2000).</S>
			<S sid ="85" ssid = "7">It follows that in order to assess if a rule is correct we should judge whether R is typically entailed from those sentences that entail L (within relevant contexts for the rule).</S>
			<S sid ="86" ssid = "8">We thus present a new evaluation scheme for entailment rules, termed the instance-based approach.</S>
			<S sid ="87" ssid = "9">At the heart of this approach, human judges are presented not only with a rule but rather with a sample of examples of the rule’s usage.</S>
			<S sid ="88" ssid = "10">Instead of thinking up valid contexts for the rule the judges need to assess the rule’s validity under the given context in each example.</S>
			<S sid ="89" ssid = "11">The essence of our proposal is a (apparently nontrivial) protocol of a sequence of questions, which determines rule validity in a given sentence.</S>
			<S sid ="90" ssid = "12">We shall next describe how we collect a sample of examples for evaluation and the evaluation process.</S>
			<S sid ="91" ssid = "13">3.1 Sampling Examples.</S>
			<S sid ="92" ssid = "14">Given a rule ‘L → R’, our goal is to generate evaluation examples by finding a sample of sentences from which L is entailed.</S>
			<S sid ="93" ssid = "15">We do that by automatically retrieving, from a given corpus, sentences that match L and are thus likely to entail it, as explained below.</S>
			<S sid ="94" ssid = "16">For each example sentence, we automatically extract the arguments that instantiate L and generate two phrases, termed left phrase and right phrase, which are constructed by instantiating the left template L and the right template R with the extracted arguments.</S>
			<S sid ="95" ssid = "17">For example, the left and right phrases generated for example 1 in Table 2 are “he seek bail” and “he disclose bail”, respectively.</S>
			<S sid ="96" ssid = "18">Finding sentences that match L can be performed at different levels.</S>
			<S sid ="97" ssid = "19">In this paper we match lexical- syntactic templates by finding a sub-tree of the sentence parse that is identical to the template structure.</S>
			<S sid ="98" ssid = "20">Of course, this matching method is not perfect and will sometimes retrieve sentences that do not entail the left phrase for various reasons, such as incorrect sentence analysis or semantic aspects like negation, modality and conditionals.</S>
			<S sid ="99" ssid = "21">See examples 12 in Table 2 for sentences that syntactically match L but do not entail the instantiated left phrase.</S>
			<S sid ="100" ssid = "22">Since we should assess R’s entailment only from sentences that entail L, such sentences should be ignored by the evaluation process.</S>
			<S sid ="101" ssid = "23">3.2 Judgment Questions.</S>
			<S sid ="102" ssid = "24">For each example generated for a rule, the judges are presented with the given sentence and the left and right phrases.</S>
			<S sid ="103" ssid = "25">They primarily answer two questions that assess whether entailment holds in this example, following the semantics of entailment rule application as discussed above: Qle: Is the left phrase entailed from the sentence?</S>
			<S sid ="104" ssid = "26">A positive/negative answer corresponds to a ‘Left entailed/not entailed’ judgment.</S>
			<S sid ="105" ssid = "27">Qre: Is the right phrase entailed from the sentence?</S>
			<S sid ="106" ssid = "28">A positive/negative answer corresponds to an ‘Entailment holds/No entailment’ judgment.</S>
			<S sid ="107" ssid = "29">The first question identifies sentences that do not entail the left phrase, and thus should be ignored when evaluating the rule’s correctness.</S>
			<S sid ="108" ssid = "30">While inappropriate matches of the rule left-hand-side may happen and harm an overall system precision, such errors should be accounted for a system’s rule matching module rather than for the rules’ precision.</S>
			<S sid ="109" ssid = "31">The second question assesses whether the rule application is valid or not for the current example.</S>
			<S sid ="110" ssid = "32">See examples 58 in Table 2 for cases where entailment does or doesn’t hold.</S>
			<S sid ="111" ssid = "33">Thus, the judges focus only on the given sentence in each example, so the task is actually to evaluate whether textual entailment holds between the sentence (text) and each of the left and right phrases (hypotheses).</S>
			<S sid ="112" ssid = "34">Following past experience in textual entailment evaluation (Dagan et al., 2006) we expect a reasonable agreement level between judges.</S>
			<S sid ="113" ssid = "35">As discussed in Section 2.1, we may want to ignore examples whose context is irrelevant for the rule.</S>
			<S sid ="114" ssid = "36">To optionally capture this distinction, the judges are asked another question: Qrc: Is the right phrase a likely phrase in English?</S>
			<S sid ="115" ssid = "37">A positive/negative answer corresponds to a ‘Relevant/Irrelevant context’ evaluation.</S>
			<S sid ="116" ssid = "38">If the right phrase is not likely in English then the given context is probably irrelevant for the rule, because it seems inherently incorrect to infer an implausible phrase.</S>
			<S sid ="117" ssid = "39">Examples 34 in Table 2 demonstrate cases of irrelevant contexts, which we may choose to ignore when assessing rule correctness.</S>
			<S sid ="118" ssid = "40">3.3 Evaluation Process.</S>
			<S sid ="119" ssid = "41">For each example, the judges are presented with the three questions above in the following order: (1) Qle (2) Qrc (3) Qre.</S>
			<S sid ="120" ssid = "42">If the answer to a certain question is negative then we do not need to present the next questions to the judge: if the left phrase is not entailed then we ignore the sentence altogether; and if the context is irrelevant then the right phrase cannot be entailed from the sentence and so the answer to Qre is already known as negative.</S>
			<S sid ="121" ssid = "43">The above entailment judgments assume that we can actually ask whether the left or right phrases are correct given the sentence, that is, we assume that a truth value can be assigned to both phrases.</S>
			<S sid ="122" ssid = "44">This is the case when the left and right templates correspond, as expected, to semantic relations.</S>
			<S sid ="123" ssid = "45">Yet sometimes learned templates are (erroneously) not relational, e.g. ‘X , Y , IBM’ (representing a list).</S>
			<S sid ="124" ssid = "46">We therefore let the judges initially mark rules that include such templates as non-relational, in which case their examples are not evaluated at all.</S>
			<S sid ="125" ssid = "47">3.4 Rule Precision.</S>
			<S sid ="126" ssid = "48">We compute the precision of a rule by the percentage of examples for which entailment holds out of all “relevant” examples.</S>
			<S sid ="127" ssid = "49">We can calculate the precision in two ways, as defined below, depending on whether we ignore irrelevant contexts or not (obtaining lower precision if we don’t).</S>
			<S sid ="128" ssid = "50">When systems answer an information need, such as a query or question, irrelevant contexts are sometimes not encountered thanks to additional context which is present in the given input (see Section 2.1).</S>
			<S sid ="129" ssid = "51">Thus, the following two measures can be viewed as upper and lower bounds for the expected precision of the rule applications in actual systems: upper bound precision: #Entailment holds #Relevant context lower bound precision: #Entailment holds #Left entailed where # denotes the number of examples with the corresponding judgment.</S>
			<S sid ="130" ssid = "52">Finally, we consider a rule to be correct only if its precision is at least 80%, which seems sensible for typical applied settings.</S>
			<S sid ="131" ssid = "53">This yields two alternative sets of correct rules, corresponding to the upper bound and lower bound precision measures.</S>
			<S sid ="132" ssid = "54">Even though judges may disagree on specific examples for a rule, their judgments may still agree overall on the rule’s correctness.</S>
			<S sid ="133" ssid = "55">We therefore expect the agreement level on rule correctness to be higher than the agreement on individual examples.</S>
	</SECTION>
	<SECTION title="Experimental Settings. " number = "4">
			<S sid ="134" ssid = "1">We applied the instance-based methodology to evaluate two state-of-the-art unsupervised acquisition algorithms, DIRT (Lin and Pantel, 2001) and TEASE (Szpektor et al., 2004), whose output is publicly available.</S>
			<S sid ="135" ssid = "2">DIRT identifies semantically related templates in a local corpus using distributional similarity over the templates’ variable instantiations.</S>
			<S sid ="136" ssid = "3">TEASE acquires entailment relations from the Web for a given input template I by identifying characteristic variable instantiations shared by I and other templates.</S>
			<S sid ="137" ssid = "4">For the experiment we used the published DIRT and TEASE knowledge-bases1.</S>
			<S sid ="138" ssid = "5">For every given input template I , each knowledge-base provides a list of learned output templates {Oj }nI , where nI is the number of output templates learned for I . Each output template is suggested as holding an entailment relation with the input template I , but the algorithms do not specify the entailment direction(s).</S>
			<S sid ="139" ssid = "6">Thus, each pair {I , Oj } induces two candidate directional entailment rules: ‘I → Oj ’ and ‘Oj → I ’.</S>
			<S sid ="140" ssid = "7">4.1 Test Set Construction.</S>
			<S sid ="141" ssid = "8">The test set construction consists of three sampling steps: selecting a set of input templates for the two algorithms, selecting a sample of output rules to be evaluated, and selecting a sample of sentences to be judged for each rule.</S>
			<S sid ="142" ssid = "9">First, we randomly selected 30 transitive verbs out of the 1000 most frequent verbs in the Reuters RCV1 corpus2.</S>
			<S sid ="143" ssid = "10">For each verb we manually constructed a lexical-syntactic input template by adding subject and object variables.</S>
			<S sid ="144" ssid = "11">For example, for the verb ‘seek’ we constructed the template explained in Section 3.1).</S>
			<S sid ="145" ssid = "12">A sample of 15 matching sentences was randomly selected, or all matching sentences if less than 15 were found.</S>
			<S sid ="146" ssid = "13">Finally, an example for judgment was generated from each sampled sentence and its left and right phrases (see Section 3.1).</S>
			<S sid ="147" ssid = "14">We did not find sentences for 108 rules, and thus we ended up with 646 unique rules that could be evaluated (with 8945 examples to be judged).</S>
			<S sid ="148" ssid = "15">4.2 Evaluating the Test-Set.</S>
			<S sid ="149" ssid = "16">Two human judges evaluated the examples.</S>
			<S sid ="150" ssid = "17">We randomly split the examples between the judges.</S>
			<S sid ="151" ssid = "18">100 rules (1287 examples) were cross annotated for agreement measurement.</S>
			<S sid ="152" ssid = "19">The judges followed the procedure in Section 3.3 and the correctness of each rule was assessed based on both its upper and lower bound precision values (Section 3.4).</S>
	</SECTION>
	<SECTION title="Methodology Evaluation Results. " number = "5">
			<S sid ="153" ssid = "1">We assessed the instance-based methodology by measuring the agreement level between judges.</S>
			<S sid ="154" ssid = "2">The ‘X subj obj judges agreed on 75% of the 1287 shared exam ←− seek −→ Y ’.</S>
			<S sid ="155" ssid = "3">Next, for each input template I we considered the learned templates {Oj }nI from each knowledge- base.</S>
			<S sid ="156" ssid = "4">Since DIRT has a long tail of templates with a low score and very low precision, DIRT templates whose score is below a threshold of 0.1 were filtered out3.</S>
			<S sid ="157" ssid = "5">We then sampled 10% of the templates in each output list, limiting the sample size to be between 520 templates for each list (thus balancing between sufficient evaluation data and judgment load).</S>
			<S sid ="158" ssid = "6">For each sampled template O we evaluated both directional rules, ‘I → O’ and ‘O → I ’.</S>
			<S sid ="159" ssid = "7">In total, we sampled 380 templates, inducing 760 directional rules out of which 754 rules were unique.</S>
			<S sid ="160" ssid = "8">Last, we randomly extracted a sample of example sentences for each rule ‘L → R’ by utilizing a search engine over the first CD of Reuters RCV1.</S>
			<S sid ="161" ssid = "9">First, we retrieved all sentences containing all lexical terms within L. The retrieved sentences were parsed using the Minipar dependency parser (Lin, 1998), keeping only sentences that syntactically match L (as 1 Available at http://aclweb.org/aclwiki/index.php?title=Textual Entailment Resource Pool 2 http://about.reuters.com/researchandstandards/corpus/ 3 Following advice by Patrick Pantel, DIRT’s coauthor..</S>
			<S sid ="162" ssid = "10">ples, corresponding to a reasonable Kappa value of 0.64.</S>
			<S sid ="163" ssid = "11">A similar kappa value of 0.65 was obtained for the examples that were judged as either entail- ment holds/no entailment by both judges.</S>
			<S sid ="164" ssid = "12">Yet, our evaluation target is to assess rules, and the Kappa values for the final correctness judgments of the shared rules were 0.74 and 0.68 for the lower and upper bound evaluations.</S>
			<S sid ="165" ssid = "13">These Kappa scores are regarded as ‘substantial agreement’ and are substantially higher than published agreement scores and those we managed to obtain using the standard rule- based approach.</S>
			<S sid ="166" ssid = "14">As expected, the agreement on rules is higher than on examples, since judges may disagree on a certain example but their judgements would still yield the same rule assessment.</S>
			<S sid ="167" ssid = "15">Table 3 illustrates some disagreements that were still exhibited within the instance-based evaluation.</S>
			<S sid ="168" ssid = "16">The primary reason for disagreements was the difficulty to decide whether a context is relevant for a rule or not, resulting in some confusion between ‘Irrelevant context’ and ‘No entailment’.</S>
			<S sid ="169" ssid = "17">This may explain the lower agreement for the upper bound precision, for which examples judged as ’Irrelevant context’ are ignored, while for the lower bound both R u l e S e n t e n c e J u d g e 1 J u d g e 2 X sig n Y → X set Y Ir aq an d Tu rk ey sig n ag re e m en t to in cr ea se tra de co op er ati on En tai lm en t ho lds Irr ele va nt co nt ex t X w or se n Y → X slo w Y Ne ws of th e str ik e w or se ne d th e sit ua tio n Irr ele va nt co nt ex t N o en tai lm en t X ge t Y → X wa nt Y H e wi ll ge t hi s pa ra de on Tu es da y En tai lm en t ho lds N o en tai lm en t Table 3: Examples for disagreement between the two judges.</S>
			<S sid ="170" ssid = "18">judgments are conflated and represent no entailment.</S>
			<S sid ="171" ssid = "19">Our findings suggest that better ways for distinguishing relevant contexts may be sought in future research for further refinement of the instance-based evaluation methodology.</S>
			<S sid ="172" ssid = "20">About 43% of all examples were judged as ’Left not entailed’.</S>
			<S sid ="173" ssid = "21">The relatively low matching precision (57%) made us collect more examples than needed, since ’Left not entailed’ examples are ignored.</S>
			<S sid ="174" ssid = "22">Better matching capabilities will allow collecting and judging fewer examples, thus improving the efficiency of the evaluation process.</S>
	</SECTION>
	<SECTION title="DIRT and TEASE Evaluation Results. " number = "6">
			<S sid ="175" ssid = "1">DIRT TEASE P Y P Y Rules: Templates: U pp er Bo un d 4 4 % 22 .6 3 8 % 26 .9 Lo we r Bo un d 27 .3 % 14 .1 23 .6 % 16 .8 Table 4: Average Precision (P) and Yield (Y) at the rule and template levels.</S>
			<S sid ="176" ssid = "2">We evaluated the quality of the entailment rules produced by each algorithm using two scores: (1) micro average Precision, the percentage of correct rules out of all learned rules, and (2) average Yield, the average number of correct rules learned for each input template I , as extrapolated based on the sample4.</S>
			<S sid ="177" ssid = "3">Since DIRT and TEASE do not identify rule directionality, we also measured these scores at the 4 Since the rules are matched against the full corpus (as in IR.</S>
			<S sid ="178" ssid = "4">evaluations), it is difficult to evaluate their true recall.</S>
			<S sid ="179" ssid = "5">template level, where an output template O is considered correct if at least one of the rules ‘I → O’ or ‘O → I ’ is correct.</S>
			<S sid ="180" ssid = "6">The results are presented in Table 4.</S>
			<S sid ="181" ssid = "7">The major finding is that the overall quality of DIRT and TEASE is very similar.</S>
			<S sid ="182" ssid = "8">Under the specific DIRT cutoff threshold chosen, DIRT exhibits somewhat higher Precision while TEASE has somewhat higher Yield (recall that there is no particular natural cutoff point for DIRT’s output).</S>
			<S sid ="183" ssid = "9">Since applications typically apply rules in a specific direction, the Precision for rules reflects their expected performance better than the Precision for templates.</S>
			<S sid ="184" ssid = "10">Obviously, future improvement in precision is needed for rule learning algorithms.</S>
			<S sid ="185" ssid = "11">Meanwhile, manual filtering of the learned rules can prove effective within limited domains, where our evaluation approach can be utilized for reliable filtering as well.</S>
			<S sid ="186" ssid = "12">The substantial yield obtained by these algorithms suggest that they are indeed likely to be valuable for recall increase in semantic applications.</S>
			<S sid ="187" ssid = "13">In addition, we found that only about 15% of the correct templates were learned by both algorithms, which implies that the two algorithms largely complement each other in terms of coverage.</S>
			<S sid ="188" ssid = "14">One explanation may be that DIRT is focused on the domain of the local corpus used (news articles for the published DIRT knowledge-base), whereas TEASE learns from the Web, extracting rules from multiple domains.</S>
			<S sid ="189" ssid = "15">Since Precision is comparable it may be best to use both algorithms in tandem.</S>
			<S sid ="190" ssid = "16">We also measured whether O is a paraphrase of I , i.e. whether both ‘I → O’ and ‘O → I ’ are correct.</S>
			<S sid ="191" ssid = "17">Only 2025% of all correct templates were assessed as paraphrases.</S>
			<S sid ="192" ssid = "18">This stresses the significance of evaluating directional rules rather than only paraphrases.</S>
			<S sid ="193" ssid = "19">Furthermore, it shows that in order to improve precision, acquisition algorithms must identify rule directionality.</S>
			<S sid ="194" ssid = "20">About 28% of all ‘Left entailed’ examples were evaluated as ‘Irrelevant context’, yielding the large difference in precision between the upper and lower precision bounds.</S>
			<S sid ="195" ssid = "21">This result shows that in order to get closer to the upper bound precision, learning algorithms and applications need to identify the relevant contexts in which a rule should be applied.</S>
			<S sid ="196" ssid = "22">Last, we note that the instance-based quality assessment corresponds to the corpus from which the example sentences were taken.</S>
			<S sid ="197" ssid = "23">It is therefore best to evaluate the rules using a corpus of the same domain from which they were learned, or the target application domain for which the rules will be applied.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "7">
			<S sid ="198" ssid = "1">Accurate learning of inference knowledge, such as entailment rules, has become critical for further progress of applied semantic systems.</S>
			<S sid ="199" ssid = "2">However, evaluation of such knowledge has been problematic, hindering further developments.</S>
			<S sid ="200" ssid = "3">The instance-based evaluation approach proposed in this paper obtained acceptable agreement levels, which are substantially higher than those obtained for the common rule- based approach.</S>
			<S sid ="201" ssid = "4">We also conducted the first comparison between two state-of-the-art acquisition algorithms, DIRT and TEASE, using the new methodology.</S>
			<S sid ="202" ssid = "5">We found that their quality is comparable but they effectively complement each other in terms of rule coverage.</S>
			<S sid ="203" ssid = "6">Also, we found that most learned rules are not paraphrases but rather one-directional entailment rules, and that many of the rules are context sensitive.</S>
			<S sid ="204" ssid = "7">These findings suggest interesting directions for future research, in particular learning rule direction- ality and relevant contexts, issues that were hardly explored till now.</S>
			<S sid ="205" ssid = "8">Such developments can be then evaluated by the instance-based methodology, which was designed to capture these two important aspects of entailment rules.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="206" ssid = "9">The authors would like to thank Ephi Sachs and Iddo Greental for their evaluation.</S>
			<S sid ="207" ssid = "10">This work was partially supported by ISF grant 1095/05, the IST Programme of the European Community under the PASCAL Network of Excellence IST2002-506778, and the ITCirst/University of Haifa collaboration.</S>
	</SECTION>
</PAPER>
