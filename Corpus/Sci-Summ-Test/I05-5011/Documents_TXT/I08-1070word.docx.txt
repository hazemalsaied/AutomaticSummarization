Computing Paraphrasability of Syntactic  Variants Using Web Snippets



Atsushi Fujita 	Satoshi Sato
Graduate School of Engineering, Nagoya University
{fujita,ssato}@nuee.nagoya-u.ac.jp








Abstract

In a broad range of natural language pro- 
cessing tasks, large-scale knowledge-base of 
paraphrases is anticipated to improve their 
performance. The key issue in creating such 
a resource is to establish a practical method 
of computing semantic equivalence and syn- 
tactic substitutability, i.e., paraphrasability, 
between given pair of expressions.   This 
paper addresses the issues of computing 
paraphrasability, focusing on syntactic vari- 
ants of predicate phrases.  Our model esti- 
mates paraphrasability based on traditional 
distributional similarity measures, where the 
Web snippets are used to overcome the data 
sparseness problem in handling predicate 
phrases.  Several feature sets are evaluated 
through empirical experiments.

1   Introduction

One of the common characteristics of human lan- 
guages is that the same concept can be expressed by 
various linguistic expressions. Such linguistic vari- 
ations are called paraphrases. Handling paraphrases 
is one of the key issues in a broad range of natu- 
ral language processing (NLP) tasks.  In informa- 
tion retrieval, information extraction, and question 
answering, technology of recognizing if or not the 
given pair of expressions are paraphrases is desired 
to gain a higher coverage. On the other hand, a sys- 
tem which generates paraphrases for given expres- 
sions is useful for text-transcoding tasks, such as 
machine translation and summarization, as well as 
beneficial to human, for instance, in text-to-speech, 
text simplification, and writing assistance.
  

Paraphrase phenomena can roughly be divided 
into two groups according to their compositionality. 
Examples in (1) exhibit a degree of compositional- 
ity, while each example in (2) is composed of totally 
different lexical items.
(1)    a. be in our favor ⇔ be favorable for us
b. show a sharp decrease ⇔ decrease sharply
(Fujita et al., 2007)
(2)	a. burst into tears ⇔ cried 
b. comfort ⇔ console
                (Barzilay and McKeown, 2001) 
A  number  of  studies  have  been  carried  out  on 
both  compositional  (morpho-syntactic)  and  non-
compositional (lexical and idiomatic) paraphrases
(see Section 2). In most research, paraphrases have 
been represented with the similar templates, such as 
shown in (3) and (4).
(3)    a. N1 V N2 ⇔ N1 ’s V -ing of N2
b. N1 V N2 ⇔ N2 be V -en by N1
(Harris, 1957)
(4)    a. X wrote Y ⇔ X is the author of Y
b. X solves Y ⇔ X deals with Y
(Lin and Pantel, 2001)
  The weakness of these templates is that they 
should be applied only in some contexts.  In other 
words, the lack of applicability conditions for slot 
fillers may lead incorrect paraphrases.   One way 
to specify the applicability condition is to enumer- 
ate correct slot fillers.  For example, Pantel et al. 
(2007) have harvested instances for the given para- 
phrase templates based on the co-occurrence statis- 
tics of slot fillers and lexicalized part of templates 
(e.g.  “deal with” in (4b)).  Yet, there is no method 
which assesses semantic equivalence and syntactic 
substitutability of resultant pairs of expressions.


  In this paper, we propose a method of directly 
computing semantic equivalence and syntactic sub- 
stitutability, i.e., paraphrasability, particularly focus- 
ing on automatically generated compositional para- 
phrases (henceforth, syntactic variants) of predicate 
phrases.  While previous studies have mainly tar- 
geted at words or canned phrases, we treat predicate 
phrases having a bit more complex structures.
  This paper addresses two issues in handling 
phrases.   The first is feature engineering.   Gener- 
ally speaking, phrases appear less frequently than 
single words. This implies that we can obtain only a 
small amount of information about phrases. To over- 
come the data sparseness problem, we investigate if 
the Web snippet can be used as a dense corpus for 
given phrases.  The second is the measurement of 
paraphrasability. We assess how well the traditional 
distributional similarity measures approximate the 
paraphrasability of predicate phrases.

2   Related work

2.1   Representation of paraphrases

Several types of compositional paraphrases,  such 
as passivization and nominalization, have been rep- 
resented with some grammar formalisms, such as 
transformational generative grammar (Harris, 1957) 
and  synchronous  tree  adjoining  grammar  (Dras,
1999). These grammars, however, lack the informa- 
tion of applicability conditions.
  Word association within phrases has been an at- 
tractive topic.   Meaning-Text Theory (MTT) is a 
framework which takes into account several types 
of lexical dependencies in handling paraphrases 
(Mel’cˇuk and Polgue`re, 1987).   A bottleneck of 
MTT is that a huge amount of lexical knowledge is 
required to represent various relationships between 
lexical items. Jacquemin (1999) has represented the 
syntagmatic and paradigmatic correspondences be- 
tween paraphrases with context-free transformation 
rules and morphological and/or semantic relations 
between lexical items, targeting at syntactic variants 
of technical terms that are typically noun phrases 
consisting of more than one word.  We have pro- 
posed a framework of generating syntactic variants 
of predicate phrases (Fujita et al., 2007). Following 
the previous work, we have been developing three 
sorts of resources for Japanese.


2.2   Acquiring paraphrase rules

Since the late 1990’s, the task of automatic acqui- 
sition of paraphrase rules has drawn the attention of 
an increasing number of researchers. Although most 
of the proposed methods do not explicitly eliminate 
compositional paraphrases, their output tends to be 
non-compositional paraphrase.
  Previous approaches to this task are two-fold. The 
first group espouses the distributional hypothesis 
(Harris, 1968).  Among a number of models based 
on this hypothesis, two algorithms are referred to 
as the state-of-the-art. DIRT (Lin and Pantel, 2001) 
collects paraphrase rules consisting of a pair of paths 
between two nominal slots based on point-wise mu- 
tual information. TEASE (Szpektor et al., 2004) dis- 
covers binary relation templates from the Web based 
on sets of representative entities for given binary re- 
lation templates. These systems often output direc- 
tional rules such as exemplified in (5).
(5)    a. X is charged by Y
⇒ Y announced the arrest of X
b. X prevent Y  ⇒ X lower the risk of Y
They are actually called inference/entailment rules, 
and paraphrase is defined as bidirectional infer- 
ence/entailment relation1 . While the similarity score 
in DIRT is symmetric for given pair of paths, the al- 
gorithm of TEASE considers the direction.
  The other utilizes a sort of parallel texts, such as 
multiple translation of the same text (Barzilay and 
McKeown, 2001; Pang et al., 2003), corresponding 
articles from multiple news sources (Barzilay and 
Lee, 2003; Dolan et al., 2004), and bilingual corpus 
(Wu and Zhou, 2003; Bannard and Callison-Burch,
2005). This approach is, however, limited by the dif-
ficulty of obtaining parallel/comparable corpora.

2.3   Acquiring paraphrase instances

As reviewed in Section 1, paraphrase rules gener- 
ate incorrect paraphrases, because their applicability 
conditions are not specified. To avoid the drawback, 
several linguistic clues, such as fine-grained classifi- 
cation of named entities and coordinated sentences, 
have been utilized (Sekine, 2005; Torisawa, 2006). 
Although these clues restrict phenomena to those 
appearing in particular domain or those describing 
coordinated events, they have enabled us to collect

1 See http://nlp.cs.nyu.edu/WTEP/


paraphrases accurately. The notion of Inferential Se- 
lectional Preference (ISP) has been introduced by 
Pantel et al. (2007).  ISP can capture more general 
phenomena than above two; however, it lacks abili- 
ties to distinguish antonym relations.

2.4   Computing  semantic equivalence
Semantic equivalence between given pair of expres- 
sions has so far been estimated under the distribu- 
tional hypothesis (Harris, 1968). Geffet and Dagan 
(2005) have extended it to the distributional inclu- 
sion hypothesis for recognizing the direction of lex- 
ical entailment.  Weeds et al. (2005), on the other 
hand, have pointed out the limitations of lexical sim- 
ilarity and syntactic transformation, and have pro- 
posed to directly compute the distributional similar- 
ity of pair of sub-parses based on the distributions 
of their modifiers and parents. We think it is worth 
examining if the Web can be used as the source for 
extracting features of phrases.

3	Computing  paraphrasability between 
predicate phrases  using Web snippets

We define the concept of paraphrasability as follows: 
A grammatical phrase s is paraphrasable
with another phrase t, iff t satisfies the fol-
lowing three:
• t is grammatical
• t holds if s holds
• t is substitutable for s in some context
Most previous studies on acquiring paraphrase rules
have evaluated resultant pairs from only the second 
viewpoint, i.e., semantic equivalence. Additionally, 
we assume that one of a pair (t) of syntactic vari- 
ants is automatically generated from the other (s). 
Thus, grammaticality of t should also be assessed. 
We also take into account the syntactic substitutabil- 
ity, because head-words of syntactic variants some- 
times have different syntactic categories.
  Given a pair of predicate phrases, we compute 
their paraphrasability in the following procedure: 
Step 1. Retrieve Web snippets for each phrase.
Step 2. Extract features for each phrase.
Step 3. Compute their paraphrasability as distribu- 
tional similarity between their features.
  The rest of this section elaborates on each step in 
turn, taking Japanese as the target language.


3.1   Retrieving Web snippets
In general, phrases appear less frequently than sin- 
gle words.  This raises a crucial problem in com- 
puting paraphrasability of phrases, i.e., the sparse- 
ness of features for given phrases. One possible way 
to overcome the problem is to take back-off statis- 
tics assuming the independence between constituent 
words (Torisawa, 2006; Pantel et al., 2007). This ap- 
proach, however, has a risk of involving noises due 
to ambiguity of words.
  We take another approach, which utilizes the Web 
as a source of examples instead of a limited size of 
corpus. For each of the source and target phrases, we 
retrieve snippets via the Yahoo API2. The number of 
snippets is set to 500.

3.2   Extracting features
The second step extracts the features for each phrase 
from Web snippets. We have some options for fea- 
ture set, feature weighting, and snippet collection.

Feature sets
  To assess a given pair of phrases against the defi- 
nition of paraphrasability, the following three sets of 
features are examined.
HITS: A phrase must appear in the Web if it is 
grammatical. The more frequently a phrase ap- 
pears, the more likely it is grammatical.
BOW:  A pair of phrases are likely to be semanti- 
cally similar, if the distributions of words sur- 
rounding the phrases are similar.
MOD: A pair of phrases are likely to be substi- 
tutable with each other, if they share a number 
of instances of modifiers and modifiees.
  To extract BOW features from sentences includ- 
ing the given phrase within Web snippets, a morpho- 
logical analyzer MeCab3 was firstly used; however, 
it resulted wrong POS tags for unknown words, and 
hurt statistics. Thus, finally ChaSen4 is used.
  To collect MOD features, a dependency parser 
CaboCha5  is used.   Figure 1 depicts an example 
of extracting MOD features from a sentence within 
Web snippet.  A feature is generated from a bun- 
setsu, the Japanese base-chunk, which is either mod-
2 http://developer.yahoo.co.jp/search/
3 http://mecab.sourceforge.net/
4 http://chasen.naist.jp/hiki/ChaSen/
5 http://chasen.org/˜taku/software/cabocha/


Sentence within snippet
(dependency tree)



yoteeii-da



Features

Modifiee/D: yotei


the source phrase.  
The “anchored 
version” of Web 
snippets is retrieved 
in the following 
steps:


kenshhoouu--ssuurruu


(plan)


Step 2-1. Determine the 
anchor using Web snip-
pets for the given 
source phrase. We 
regarded


saaiiggeenn--sseeii-o


kuwasshhiikkuu


Modifier/D: kuwashii


a  noun  which  
most  frequently  
modifies the


jjiikkkkeenn--kkeekkkkaa--nnoo

kkaarree--nnoo


Given phrase


(in detail)

Modifier/D: kare_no
(his)


source 
phrase as 
its 
anchor. 
Example
s of 
source 
phrases 
and their 
anchors 
are 
shown in 
(6).
Step 2-2. 
Retrieve Web 
snippets by 
querying the


(I am) planning to verify the reproducibility of his experimental result in detail.

Figure 1:  An example of MOD feature extraction. 
An oval in the dependency tree denotes a bunsetsu.

ifier or modifiee of the given phrase.  Each feature 
is composed of three or more elements:  (i) modi- 
fier or modifiee, (ii) dependency relation types (di- 
rect dependency, appositive, or parallel, c.f., RASP 
and MINIPAR), (iii) base form of the head-word, 
and (iv) case marker following noun, auxiliary verb 
and verbal suffixes if they appear.  The last feature 
is employed to distinguish the subtle difference of 
meaning of predicate phrases, such as voice, tense, 
aspect, and modality. While Lin and Pantel (2001) 
have calculated similarities of paths based on slot 
fillers of subject and object slots, MOD targets at 
sub-trees and utilizes any modifiers and modifiees.


anchor  for  the  source  phrase  AND each of 
source and target phrases, respectively.
Step 2-3. Extract features for HITS, BOW, MOD.
Those sets are referred to as Anc.∗, while the 
normal versions are referred to as Nor.∗.
(6)    a. “emi:o:ukaberu” ··· “manmen”
(be smiling ··· from ear to ear)
b. “doriburu:de:kake:agaru”  ··· “saido”
(overlap by dribbling ··· side)
c. “yoi:sutaato:o:kiru” ··· “saisaki”
(make a good start ··· good sign)

3.3   Computing  paraphrasability
Paraphrasability is finally computed by two conven- 
tional distributional similarity measures. The first is 
the measure proposed in (Lin and Pantel, 2001):
Lf ∈Fs ∩Ft  (w(s, f )+ w(t, f ))


Feature weighting


ParLin (s⇒t)= L



f ∈Fs


w(s, f )+ L



f ∈Ft


w(t, f ) ,


Geffet and Dagan (2004) have reported on that the
better quality of feature vector (weighting function) 
leads better results. So far, several weighting func- 
tions have been proposed, such as point-wise mu- 
tual information (Lin and Pantel, 2001) and Rela- 
tive Feature Focus (Geffet and Dagan, 2004). While 
these functions compute weights using a small cor- 
pus for merely re-ranking samples, we are devel- 
oping a measure that assesses the paraphrasability 
of arbitrary pair of phrases, where a more robust 
weighting function is necessary.  Therefore we di-
rectly use frequencies of features within Web snip-


where Fs and Ft denote feature sets for s and t, re- 
spectively. w(x, f ) stands for the weight (frequency 
in our experiment) of f in Fx.
While ParLin   is symmetric, it has been argued
that it is important to determine the direction of para-
phrase. As an asymmetric measure, we examine α- 
skew divergence defined by the following equation 
(Lee, 1999):
dskew (t, s)   =	D (Ps αPt + (1 − α)Ps ) ,

where Px  denotes a probability distribution esti-
6


pets as weight. Normalization will be done when the


mated


from a feature set Fx . How well Pt  approx-


paraphrasability is computed (Section 3.3).

Source-focused feature extraction
  Independent collection of Web snippets for each 
phrase of a given pair might yield no intersection of 
feature sets even if they have the same meaning. To 
obtain more reliable feature sets, we retrieve Web


imates Ps is calculated based on the KL divergence,
D.  The parameter α is set to 0.99, following tradi- 
tion, because the optimization of α is difficult.  To 
take consistent measurements, we define the para- 
phrasability score Parskew as follows:
Parskew (s⇒t)  = 	exp (−dskew (t, s)) .
6 We estimate them simply using maximum likelihood esti-


snippets by querying the phrase AND the anchor of


mation, i.e., Px (f ) = w(x, f )/ P
∈  x


w(x, f ∗ ).


Table 1: # of sampled source phrases and automatically generated syntactic variants.

Phras
e type
# of 
token
s  # 
of 
types
th 
ty
pe
s 
C
ov
.(
%
)
Out
put	Ave.
N  : 
C : 
V
N1 
: 
N2 
: C 
: V 
N  : 
C : 
V1  : 
V2
N  : 
C : 
Adv  
: V
Adj 
: N  
: C 
: V 
N  : 
C : 
Adj
20,20
0,041 
4,323,
756
3,79
6,35
1 
2,01
3,68
2
32
5,9
64    
21
3,9
23
1,20
9,26
5    
923,
475
37
8,6
17    
23
3,9
52
78
8,0
38    
20
3,8
45
1,000 
1,014	10.7
107 
1,00
5	6.3
15 
1,0
22	12.9
21 
1,0
97	3.9
20 
1,0
49	14.1
86 
1,0
03	31.4
1,5
36   
(48
9)   
3.1
88,0
40   
(966
) 
91.1
75,3
44   
(982
) 
76.7
8,2
81   
(52
3) 
15.
7
1
2
8	(50)   2.6
3,2
12   
(99
2)   
3.2
Total
26,69
8,276 
7,912
,633
6
,
1
9
0
176,5
41 
(4,00
2) 
44.1

Table 2: # of syntactic variants whose paraphrasability scores are computed.
Nor.HITS ⊃ Nor.BOW.∗⊃ Nor.MOD.∗. Anc.HITS ⊃ Anc.BOW.∗⊃ Anc.MOD.∗.
Nor.HITS ⊃ Anc.HITS. Nor.BOW.∗⊃ Anc.BOW.∗. Nor.MOD.∗⊃ Anc.MOD.∗. X denotes the set of syntactic variants whose scores are computed based on X.


Phras
e type
N
o
r
.
H
I
T
S
Outpu
t	Ave.
N
o
r
.
B
O
W
.
∗
Outpu
t	Ave.
N
o
r
.
M
O
D
.
∗
Outpu
t	Ave.
A
n
c
.
H
I
T
S
Outpu
t	Ave.
A
n
c
.
B
O
W
.
∗
Outpu
t	Ave.
A
n
c
.
M
O
D
.
∗
Outpu
t	Ave.
M
a
i
n
i
c
h
i
Outpu
t	Ave.
N  : 
C : 
V
N1 
: N2 
: C 
: V 
N  : 
C : 
V1  : 
V2
N  : 
C : 
Adv  
: V
Adj 
: N  
: C 
: V 
N  : 
C : 
Adj
1,40
5   
(489
)  
2.9
9,54
4   
(964
)  
9.9
3,76
9   
(876
)  
4.3
69
0   
(35
9)  
1.9
4
5	(20)  2.3
1,45
9   
(885
)  
1.6
1,40
2   
(488
)  
2.9
9,24
9   
(922) 
10.0
3,40
6   
(774
)  
4.4
50
6   
(24
7)  
2.0
4
5	(20)  2.3
1,45
9   
(885
)  
1.6
1,39
6   
(488
)  
2.9
8,65
2   
(921
)  
9.4
3,10
9   
(762
)  
4.1
47
5   
(23
3)  
2.0
4
2	(17)  2.5
1,39
9   
(864
)  
1.6
1,36
8   
(488
)  
2.8
7,43
7   
(897
)  
8.3
2,51
7   
(697
)  
3.6
34
2   
(17
4)  
2.0
4
1	(18)  2.3
1,23
5   
(809
)  
1.5
1,36
6   
(487
)  
2.8
7,42
4   
(894
)  
8.3
2,49
7   
(690
)  
3.6
33
9   
(17
3)  
2.0
4
1	(18)  2.3
1,23
5   
(809
)  
1.5
1,36
0   
(487
)  
2.8
6,79
5   
(891
)  
7.6
2,25
8   
(679
)  
3.3
32
2   
(16
8)  
1.9
3
9	(16)  2.4
1,16
1   
(779
)  
1.5
1,10
3   
(457
)  
2.4
3,04
1   
(948
)  
3.2
1,15
6   
(548
)  
2.1
21
5   
(16
7)  
1.3
1
4	(7)  2.0
55
9   
(45
9)  
1.2
Total
16,91
2 
(3,59
3)  
4.7
16,06
7 
(3,33
6)  
4.8
15,07
3 
(3,28
5)  
4.6
12,94
0 
(3,08
3)  
4.2
12,90
2 
(3,07
1)  
4.2
11,93
5 
(3,02
0)  
4.0
6,08
8 
(2,5
86)  
2.4



Now Parx falls within [0, 1], and a larger Parx indi- 
cates a more paraphrasable pair of phrases.

4   Experimental setting

We conduct empirical experiments to evaluate the 
proposed methods. Settings are described below.

4.1   Test collection
First, source phrases were sampled from a 15 years 
of newspaper articles (Mainichi 1991-2005, approx- 
imately 1.5GB). Referring to the dependency struc- 
ture  given  by  CaboCha,  we  extracted  most  fre- 
quent 1,000+ phrases for each of 6 phrase types. 
These phrases were then fed to a system proposed 
in (Fujita et al., 2007) to generate syntactic vari- 
ants.     The  numbers  of  the  source  phrases  and 
their syntactic variants are summarized in Table 1, 
where the numbers in the parentheses indicate that 
of source phrases paraphrased.  At least one can- 
didate was generated for 4,002 (64.7%) phrases. 
Although the system generates numerous syntactic 
variants from a given phrase, most of them are er- 
roneous.  For example, among 159 syntactic vari- 
ants that are automatically generated for the phrase 
“songai:baishou:o:motomeru” (demand compensa- 
tion for damages), only 8 phrases are grammatical, 
and only 5 out of 8 are correct paraphrases.
Paraphrasability of each pair of source phrase and
candidate is then computed by the methods pro- 
posed in Section 3.  Table 2 summarizes the num- 
bers of pairs whose features can be extracted from 
the Web snippets.  While more than 90% of candi- 
dates were discarded due to ’No hits’ in the Web,


at least one candidate survived for 3,020 (48.8%) 
phrases. Mainichi is a baseline which counts HITS 
in the corpus used for sampling source phrases.

4.2   Samples for evaluation

We sampled three sets of pairs for evaluation, where 
Mainichi, ∗.HITS, ∗.BOW, ∗.MOD, the harmonic 
mean of the scores derived from ∗.BOW and ∗.MOD 
(referred to as ∗.HAR), and two distributional simi- 
larity measures for ∗.BOW, ∗.MOD, and ∗.HAR, in
total 15 models, are compared.
Ev.Gen:  This investigates how well a correct can- 
didate is ranked first among candidates for a 
given phrase using the top-ranked pairs for ran- 
domly sampled 200 source phrases for each of
15 models.
Ev.Rec: This assesses how well a method gives 
higher scores to correct candidates using the
200-best pairs for each of 15 models.
Ev.Ling: This compares paraphrasability of each 
phrase type using the 20-best pairs for each of
6 phrase type and 14 Web-based models.

4.3   Criteria of paraphrasability
To assess by human the paraphrasability discussed 
in Section 3, we designed the following four ques- 
tions based on (Szpektor et al., 2007):
Qsc:  Is s a correct phrase in Japanese?
Qtc:  Is t a correct phrase in Japanese?
Qs2t:  Does t hold if s holds and can t substituted for
s in some context?
Qt2s:  Does s hold if t holds and can s substituted 
for t in some context?


5   Experimental results

5.1   Agreement of human judge
Two human assessors separately judged all of the
1,152 syntactic variant pairs (for 962 source phrases) 
within the union of the three sample sets.  They 
agreed on all four questions for 795 (68.4%) pairs. 
For the 963 (83.6%) pairs that passed Qsc  and Qtc 
in both two judges, we obtained reasonable agree- 
ment ratios 86.9% and 85.0% and substantial Kappa 
values 0.697 and 0.655 for assessing Qs2t and Qt2s.

5.2   Ev.Gen
Table 3 shows the results for Ev.Gen,  where the 
strict precision is calculated based on the number 
of two positive judges for Qs2t , while the lenient 
precision is for at least one positive judge for the
same question.  ∗.MOD  and ∗.HAR outperformed
the other models, although there was no statistically
significant difference7 .  Significant differences be- 
tween Mainichi and the other models in lenient pre- 
cisions indicate that the Web enables us to compute 
paraphrasability more accurately than a limited size 
of corpus.
  From a closer look at the distributions of para- 
phrasability scores of ∗.BOW and ∗.MOD shown in
Table 4, we find that if a top-ranked candidate for 
a given phrase is assigned enough high score, it is
very likely to be correct.  The scores of Anc.∗ are
distributed in a wider range than those of Nor.∗, pre-
serving precision. This allows us to easily skim the
most reliable portion by setting a threshold.

5.3   Ev.Rec
The results for Ev.Rec, as summarized in Table 5, 
show the significant differences of performances be-
tween Mainichi or ∗.HITS  and the other models.
The results of ∗.HITS supported the importance of
comparing features of phrases.  On the other hand,
∗.BOW performed as well as ∗.MOD and ∗.HAR.
This sounds nice because BOW features can be ex-
tracted extremely quickly and accurately.
Unfortunately, Anc.∗ led only a small impact on
strict precisions. We speculate that the selection of
the anchor is inadequate. Another possible interpre- 
tation is that source phrases are rarely ambiguous, 
because they contain at least two content words. In
7 p < 0.05 in 2-sample test for equality of proportions.


Table 3: Precision for 200 candidates 
(Ev.Gen).


Mode
l
S
t
r
i
c
t
Leni
ent

Nor
.∗
Anc
.∗
No
r.∗
An
c.∗
Maini
chi
HITS 
BOW
.Lin 
BOW
.skew 
MOD
.Lin 
MOD
.skew 
HAR.
Lin 
HAR.
skew
77 
(39%
)
84 
(42%
)
82 
(41%
)
86 
(43%
)
91 
(46%
)
92 
(46%
)
90 
(45%
)
93 
(47%
)
-	-
83 
(42%
)
85 
(43%
)
87 
(44%
)
91 
(46%
)
90 
(45%
)
90 
(45%
)
90 
(45%
)
101 
(51%
)
120 
(60%
)
123 
(62%
)
125 
(63%
)
130 
(65%
)
132 
(66%
)
129 
(65%
)
134 
(67%
)
-	-
119 
(60%)
124 
(62%)
124 
(62%)
131 
(66%)
130 
(65%)
130 
(65%)
131 
(66%)

Table 4: Distribution of paraphrasability scores 
and lenient precision (Ev.Gen).

Par 
(s⇒
t)
N
o
r
.
B
O
W
A
n
c
.
B
O
W

L
i
n
s
k
e
w
L
i
n
s
k
e
w
0.9-
1.0
0.8-
1.0
0.7-
1.0
0.6-
1.0
0.5-
1.0
0.4-
1.0
0.3-
1.0
0.2-
1.0
0.1-
1.0
0.0-
1.0
11/  
12  
(92
%)
45/  
49  
(92
%)
72/  
88  
(82
%)
94/1
27  
(74
%)
102/1
45  
(70%
)
107/1
58  
(68%
)
113/1
73  
(65%
)
119/1
84  
(65%
)
123/1
98  
(62%
)
123/2
00  
(62%
)
0/   
0	-
1/   
1 
(10
0%
)
7/   
7 
(10
0%
)
11/  
11 
(100
%)
13/  
13 
(100
%)
13/  
14  
(93
%)
25/  
26  
(96
%)
40/  
41  
(98
%)
74/  
86  
(86
%)
125/2
00  
(63%)
17/  
18  
(94
%)
45/  
50  
(90
%)
73/  
92  
(79
%)
83/1
13  
(74
%)
96/1
28  
(75
%)
103/1
45  
(71%
)
114/1
66  
(69%
)
121/1
86  
(65%
)
124/2
00  
(62%
)
124/2
00  
(62%
)
2/   
2 
(10
0%
)
6/   
6 
(10
0%
)
10/  
11  
(91
%)
12/  
13  
(92
%)
14/  
15  
(93
%)
21/  
22  
(96
%)
31/  
32  
(97
%)
49/  
50  
(98
%)
82/  
99  
(83
%)
124/2
00  
(62%)
Varia
nce
0
.
0
5
2
0
.
0
3
1
0
.
0
6
1
0
.
0
4
4
Par 
(s⇒
t)
N
o
r
.
M
O
D
A
n
c
.
M
O
D

L
i
n
s
k
e
w
L
i
n
s
k
e
w
0.9-
1.0
0.8-
1.0
0.7-
1.0
0.6-
1.0
0.5-
1.0
0.4-
1.0
0.3-
1.0
0.2-
1.0
0.1-
1.0
0.0-
1.0
2/   
2 
(10
0%
)
10/  
10 
(100
%)
13/  
14  
(93
%)
20/  
21  
(95
%)
31/  
32  
(97
%)
42/  
44  
(96
%)
61/  
68  
(90
%)
81/  
92  
(88
%)
105/1
33  
(79%)
130/2
00  
(65%)
0/   
0	-
0/   
0	-
0/   
0	-
1/   
1 
(10
0%
)
6/   
6 
(10
0%
)
11/  
11 
(100
%)
12/  
12 
(100
%)
13/  
13 
(100
%)
17/  
18  
(94
%)
132/2
00  
(66%)
7/   
7 
(10
0%
)
12/  
13  
(92
%)
17/  
18  
(94
%)
27/  
28  
(96
%)
36/  
37  
(97
%)
51/  
53  
(96
%)
61/  
68  
(90
%)
82/  
94  
(87
%)
104/1
26  
(83%)
131/2
00  
(66%)
1/   
1 
(10
0%
)
2/   
2 
(10
0%
)
6/   
6 
(10
0%
)
9/   
9 
(10
0%
)
10/  
10 
(100
%)
12/  
12 
(100
%)
13/  
14  
(93
%)
18/  
19  
(95
%)
24/  
25  
(96
%)
130/2
00  
(65%)
Varia
nce
0
.
0
5
7
0
.
0
1
4
0
.
0
7
2
0
.
0
3
0



paraphrase generation, capturing the correct 
bound- ary of phrases is rather vital, because the 
source phrase is usually assumed to be 
grammatical.  Qsc for 55 syntactic variants (for 44 
source phrases) were actually judged incorrect.
  The lenient precisions, which were reaching a 
ceiling, implied the limitation of the proposed 
meth- ods.   Most common errors among the 
proposed methods were generated by a 
transformation pattern
N1  : N2  : C  : V   ⇒  N2  : C  : V .  
Typically,
dropping a nominal element N1 of the given 
nominal
compound N1 : N2 generalizes the meaning that 
the
compound conveys, and thus results correct para- 
phrases.  However, it caused errors in some 
cases; for example, since N1 was the semantic 
head in (7), dropping it was incorrect.

(7)    s. “shukketsu:taryou:de:shibou-
suru”
(die due to heavy blood loss)
t.∗“taryou:de:shibou-suru” (die due to plenty)


Table 5: Precision for 200 candidates (Ev.Rec).


Table 6: Precision for each phrase type (Ev.Ling).



Phras
e type
S
t
r
i
c
t
L
e
ni
e
nt
N  : 
C : 
V
N1 
: N2 
: C 
: V 
N  : 
C : 
V1  : 
V2
N  : 
C : 
Adv  
: V
Adj 
: N  
: C 
: V 
N  : 
C : 
Adj
52/ 
98 
(53
%)
51/ 
72 
(71
%)
42/ 
86 
(49
%)
33/ 
61 
(54
%)
0/ 
25  
(0
%)
18/ 
73 
(25
%)
69/ 
98 
(70
%)
64/ 
72 
(89
%)
60/ 
86 
(70
%)
44/ 
61 
(72
%)
4/ 
25 
(16
%)
38/ 
73 
(52
%)
Total
196/4
15 
(47%
)
279/4
15 
(67%
)





5.4   Ev.Ling
Finally the results for Ev.Ling is shown in Table 6. 
Paraphrasability of syntactic variants for phrases 
containing an adjective was poorly computed.  The 
primal source of errors for Adj  : N  : C : V type 
phrases was the subtle change of nuance by switch- 
ing syntactic heads as illustrated in (8), where un- 
derlines indicate heads.
(8)		s. “yoi:shigoto:o:suru”            (do a good job) 
t1./=“yoku:shigoto-suru”                  (work hard) 
t2./=“shigoto:o:yoku:suru”   (improve the work)
Most errors in paraphrasing N  : C : Adj type 
phrases, on the other hand, were caused due to the 
difference of aspectual property and agentivity be- 
tween adjectives and verbs.  For example, (9s) can 
describe not only things those qualities have been 
improved as inferred by (9t), but also those origi- 
nally having a high quality.  Qs2t  for (9) was thus
judged incorrect.
(9)    s. “shitsu:ga:takai”         (having high quality)
t./=“shitsu:ga:takamaru”	(quality rises)
  Precisions of syntactic variants for the other types 
of phrases were higher, but they tended to include 
trivial paraphrases such as shown in (10) and (11). 
Yet, collecting paraphrase instances statically will 
contribute to paraphrase recognition tasks.
(10)  s. “shounin:o:eru”                                 (clear)
    t. “shounin-sa-re-ru” 	(be approved) 
(11)  s. “eiga:o:mi:owaru” (finish seeing the movie)
t. “eiga:ga:owaru” 	(the movie ends)

6   Discussion

As described in the previous sections, our quite 
naive methods have shown fairly good performances 
in this first trial.   This section describes some re- 
maining issues to be discussed further.


 	Table 7:  # of features. 	
Nor.BOW Nor.MOD Anc.BOW Anc.MOD

# of 
featur
es 
(type)
73
,8
48
471,
720
72,1
09
409,
379
avera
ge 
featur
es 
(type)
1
,
3
2
2
2
1
1
1,2
77
2
0
2
avera
ge 
featur
es 
(toke
n)
4
,
8
8
3
3
9
1
4,7
28
3
8
3


are semantically equivalent and syntactically substi- 
tutable, following the spirit described in (Fujita et
al., 2007).  Through the comparisons of Nor.∗ and
Anc.∗, we have shown a little evidence that the am-
biguity of phrases was not problematic at least for
handling syntactic variants, arguing the necessity of 
detecting the appropriate phrase boundaries.
To overcome the data sparseness problem, Web
snippets are harnessed. Features extracted from the 
snippets outperformed newspaper corpus; however, 
the small numbers of features for phrases shown in 
Table 7 and the lack of sophisticated weighting func- 
tion suggest that the problem might persist. To ex- 
amine the proposed features and measures further, 
we plan to use TSUBAKI8, an indexed Web corpus 
developed for NLP research, because it allow us to 
obtain snippets as much as it archives.
  The use of larger number of snippets increases 
the computation time for assessing paraphrasability. 
For reducing it as well as gaining a higher cover- 
age, the enhancement of the paraphrase generation 
system is necessary. A look at the syntactic variants 
automatically generated by a system, which we pro- 
posed, showed that the system could generate syn- 
tactic variants for only a half portion of the input, 
producing many erroneous ones (Section 4.1).  To 
prune a multitude of incorrect candidates, statisti- cal 
language models such as proposed in (Habash,
2004) will be incorporated.  In parallel, we plan to 
develop a paraphrase generation system which lets us 
to quit from the labor of maintaining patterns such as 
shown in (4). We think a more unrestricted gener- 
ation algorithm will gain a higher coverage, preserv- 
ing the meaning as far as handling syntactic variants
of predicate phrases.


The  aim  of  this  study  is  to  create  a  thesaurus	 	


of phrases to recognize and generate phrases that


8 http://tsubaki.ixnlp.nii.ac.jp/se/index.cgi


7   Conclusion

In this paper, we proposed a method of assessing 
paraphrasability between automatically generated 
syntactic variants of predicate phrases.  Web snip- 
pets were utilized to overcome the data sparseness 
problem, and the conventional distributional similar- 
ity measures were employed to quantify the similar- 
ity of feature sets for the given pair of phrases. Em- 
pirical experiments revealed that features extracted 
from the Web snippets contribute to the task, show- 
ing promising results, while no significant difference 
was observed between two measures.
  In future, we plan to address several issues such as 
those described in Section 6. Particularly, at present, 
the coverage and portability are of our interests.

Acknowledgments

We are deeply grateful to all anonymous reviewers 
for their valuable comments.  This work was sup- 
ported in part by MEXT Grant-in-Aid for Young 
Scientists (B) 18700143, and for Scientific Research 
(A) 16200009, Japan.

References

Colin Bannard and Chris Callison-Burch. 2005.  Paraphrasing 
with bilingual parallel corpora.  In Proceedings of the 43rd 
Annual Meeting of the Association for Computational Lin- 
guistics (ACL), pages 597–604.
Regina Barzilay and Kathleen R. McKeown. 2001. Extracting 
paraphrases from a parallel corpus.  In Proceedings of the
39th Annual Meeting of the Association for Computational
Linguistics (ACL), pages 50–57.
Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: 
an unsupervised approach using multiple-sequence align- 
ment.  In Proceedings of the 2003 Human Language Tech- 
nology Conference and the North American Chapter of the 
Association for Computational Linguistics (HLT-NAACL), 
pages 16–23.
Bill Dolan, Chris Quirk, and Chris Brockett.  2004.  Unsu- 
pervised construction of large paraphrase corpora: exploit- 
ing massively parallel news sources.  In Proceedings of the
20th International Conference on Computational Linguistics
(COLING), pages 350–356.
Mark Dras.  1999.  Tree adjoining grammar and the reluctant 
paraphrasing  of text. Ph.D. thesis, Division of Information 
and Communication Science, Macquarie University.
Atsushi Fujita, Shuhei Kato, Naoki Kato, and Satoshi Sato.
2007.   A compositional approach toward dynamic phrasal 
thesaurus. In Proceedings of the ACL-PASCAL Workshop on 
Textual Entailment and Paraphrasing  (WTEP), pages 151–
158.
Maayan Geffet and Ido Dagan.  2004.  Feature vector quality 
and distributional similarity. In Proceedings of the 20th In- 
ternational Conference on Computational Linguistics (COL- 
ING), pages 247–253.



Maayan Geffet and Ido Dagan.  2005.  The distributional in- 
clusion hypotheses and lexical entailment.  In Proceedings 
of the 43rd Annual Meeting of the Association for Computa- 
tional Linguistics (ACL), pages 107–116.
Nizar Habash. 2004. The use of a structural N-gram language 
model in generation-heavy hybrid machine translation.  In 
Proceedings of the 3rd International Natural Language Gen- 
eration Conference (INLG), pages 61–69.
Zellig Harris. 1957. Co-occurrence and transformation in lin- 
guistic structure. Language, 33(3):283–340.
Zellig Harris.   1968.   Mathematical structures  of language.
John Wiley & Sons.
Christian Jacquemin. 1999. Syntagmatic and paradigmatic rep- 
resentations of term variation.  In Proceedings of the 37th 
Annual Meeting of the Association for Computational Lin- 
guistics (ACL), pages 341–348.
Lillian Lee.  1999.   Measures of distributional similarity.  In 
Proceedings of the 37th Annual Meeting of the Association 
for Computational Linguistics (ACL), pages 25–32.
Dekang Lin and Patrick Pantel.  2001.  Discovery of inference 
rules for question answering.  Natural Language Engineer- 
ing, 7(4):343–360.
Igor Mel’cˇuk and Alain Polgue`re.  1987.  A formal lexicon in 
meaning-text theory (or how to do lexica with words). Com- 
putational Linguistics, 13(3-4):261–275.
Bo Pang, Kevin Knight, and Daniel Marcu.   2003.   Syntax- 
based alignment of multiple translations: extracting para- 
phrases and generating new sentences.   In Proceedings of 
the 2003 Human Language Technology Conference and the 
North American Chapter of the Association for Computa- 
tional Linguistics (HLT-NAACL), pages 102–109.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy 
Chklovski, and Eduard Hovy.  2007.  ISP: Learning infer- 
ential selectional preferences.   In Proceedings of Human 
Language Technologies 2007: The Conference of the North 
American Chapter of the Association for Computational Lin- 
guistics (NAACL-HLT ), pages 564–571.
Satoshi Sekine.  2005.  Automatic paraphrase discovery based 
on context and keywords between NE pairs. In Proceedings 
of the 3rd International Workshop on Paraphrasing  (IWP), 
pages 80–87.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaventura Cop- 
pola.  2004.   Scaling Web-based acquisition of entailment 
relations.  In Proceedings of the 2004 Conference on Em- 
pirical Methods in Natural Language Processing (EMNLP), 
pages 41–48.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007. Instance- 
based evaluation of entailment rule acquisition. In Proceed- 
ings of the 45th Annual Meeting of the Association for Com- 
putational Linguistics (ACL), pages 456–463.
Kentaro Torisawa.  2006.  Acquiring inference rules with tem- 
poral constraints by using Japanese coordinated sentences 
and noun-verb co-occurrences.  In Proceedings of the Hu- 
man Language Technology Conference of the North Ameri- 
can Chapter of the Association for Computational Linguis- 
tics (HLT-NAACL), pages 57–64.
Julie Weeds, David Weir, and Bill Keller. 2005.  The distribu- 
tional similarity of sub-parses.  In Proceedings of the ACL 
Workshop on Empirical Modeling of Semantic Equivalence 
and Entailment, pages 7–12.
Hua Wu and Ming Zhou.  2003.  Synonymous collocation ex- 
traction using translation information. In Proceedings of the
41st Annual Meeting of the Association for Computational
Linguistics (ACL), pages 120–127.

