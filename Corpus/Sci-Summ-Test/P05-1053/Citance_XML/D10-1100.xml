<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we introduce the new task of social event extraction from text.</S>
		<S sid ="2" ssid = "2">We distinguish two broad types of social events depending on whether only one or both parties are aware of the social contact.</S>
		<S sid ="3" ssid = "3">We annotate part of Automatic Content Extraction (ACE) data, and perform experiments using Support Vector Machines with Kernel methods.</S>
		<S sid ="4" ssid = "4">We use a combination of structures derived from phrase structure trees and dependency trees.</S>
		<S sid ="5" ssid = "5">A characteristic of our events (which distinguishes them from ACE events) is that the participating entities can be spread far across the parse trees.</S>
		<S sid ="6" ssid = "6">We use syntactic and semantic insights to devise a new structure derived from dependency trees and show that this plays a role in achieving the best performing system for both social event detection and classification tasks.</S>
		<S sid ="7" ssid = "7">We also use three data sampling approaches to solve the problem of data skewness.</S>
		<S sid ="8" ssid = "8">Sampling methods improve the F1-measure for the task of relation detection by over 20% absolute over the baseline.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">This paper introduces a novel natural language processing (NLP) task, social event extraction.</S>
			<S sid ="10" ssid = "10">We are interested in this task because it contributes to our overall research goal, which is to extract a social network from written text.</S>
			<S sid ="11" ssid = "11">The extracted social network can be used for various applications such as summarization, question-answering, or the detection of main characters in a story.</S>
			<S sid ="12" ssid = "12">For example, we manually extracted the social network of characters in Alice in Wonderland and ran standard social network analysis algorithms on the network.</S>
			<S sid ="13" ssid = "13">The most influential characters in the story were correctly detected.</S>
			<S sid ="14" ssid = "14">Moreover, characters occurring in a scene together were given same social roles and positions.</S>
			<S sid ="15" ssid = "15">Social network extraction has recently been applied to literary theory (Elson et al., 2010) and has the potential to help organize novels that are becoming machine readable.</S>
			<S sid ="16" ssid = "16">We take a “social network” to be a network consisting of individual human beings and groups of human beings who are connected to each other by the virtue of participating in social events.</S>
			<S sid ="17" ssid = "17">We define social events to be events that occur between people where at least one person is aware of the other and of the event taking place.</S>
			<S sid ="18" ssid = "18">For example, in the sentence John talks to Mary, entities John and Mary are aware of each other and the talking event.</S>
			<S sid ="19" ssid = "19">In the sentence John thinks Mary is great, only John is aware of Mary and the event is the thinking event.</S>
			<S sid ="20" ssid = "20">In the sentence Rabbit ran by Alice there is no evidence about the cognitive states of Rabbit and Alice (because the Rabbit could have run by Alice without any one of them noticing each other).</S>
			<S sid ="21" ssid = "21">A text can describe a social network in two ways: explicitly, by stating the type of relationship between two individuals (e.g. husband-wife), or implicitly, by describing an event which creates or perpetuates a social relationship (e.g. John talked to Mary).</S>
			<S sid ="22" ssid = "22">We will call these types of events social events.</S>
			<S sid ="23" ssid = "23">We define two types of social events: interaction, in which both parties are aware of the social event (e.g., a conversation), and observation, in which only one party is aware of the interaction (e.g., thinking about or 1024 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024–1034, MIT, Massachusetts, USA, 911 October 2010.</S>
			<S sid ="24" ssid = "24">Qc 2010 Association for Computational Linguistics spying on someone).</S>
			<S sid ="25" ssid = "25">Note that the notion of cognitive state is crucial to our definition.</S>
			<S sid ="26" ssid = "26">This paper is the first attempt to detect and classify social events present in text.</S>
			<S sid ="27" ssid = "27">Our task is different from related tasks, notably from the Automated Content Extraction (ACE) relation and event extraction tasks because the events are different (they are a class of events defined through the effect on participants’ cognitive state), and the linguistic realization is different.</S>
			<S sid ="28" ssid = "28">Mentions of entities1 engaged in a social event are often quite distant from each other in the sentence (unlike in ACE relations where about 70% of relations are local, in our social event annotation, only 25% of the events are local.</S>
			<S sid ="29" ssid = "29">In fact, the average number of words between entities participating in any social event is 9.)</S>
			<S sid ="30" ssid = "30">We use tree kernel methods (on structures derived from phrase structure trees and dependency trees) in conjunction with Support Vector Machines (SVMs) to solve our tasks.</S>
			<S sid ="31" ssid = "31">For the design of structures and type of kernel, we take motivation from a system proposed by Nguyen et al.</S>
			<S sid ="32" ssid = "32">(2009) which is a state- of-the-art system for relation extraction.</S>
			<S sid ="33" ssid = "33">Data skew- ness turns out to be a big challenge for the task of relation detection since there are many more pairs of entities without a relation as compared to pairs of entities that have a relation.</S>
			<S sid ="34" ssid = "34">In this paper we discuss three data sampling techniques that deal with this skewness and allow us to gain over 20% in F1- measure over our baseline system.</S>
			<S sid ="35" ssid = "35">Moreover, we introduce a new sequence kernel that outperforms previously proposed sequence kernels for the task of social event detection and plays a role to achieve the best performing system for the task of social event detection and classification.</S>
			<S sid ="36" ssid = "36">The paper is structured as follows.</S>
			<S sid ="37" ssid = "37">In Section 2, we compare our work to existing work, notably the ACE extraction literature.</S>
			<S sid ="38" ssid = "38">In Section 3, we present our task in detail, and explain how we annotated our corpus.</S>
			<S sid ="39" ssid = "39">We also show why this is a novel task, and how it is different from the ACE extraction tasks.</S>
			<S sid ="40" ssid = "40">We then discuss kernel methods and the structures we use, and introduce our new structure in Section 4.</S>
			<S sid ="41" ssid = "41">In Section 5, we present the sampling methods used for experiments.</S>
			<S sid ="42" ssid = "42">In Section 6 we present our exper 1 An entity mention is a reference of an entity in text.</S>
			<S sid ="43" ssid = "43">Also, we use entities and people interchangeably since the only entities we are interested in are people or groups of people.</S>
			<S sid ="44" ssid = "44">iments and results for social event detection and social event classification tasks.</S>
			<S sid ="45" ssid = "45">We conclude in Section 7 and mention our future direction of research.</S>
	</SECTION>
	<SECTION title="Literature Survey. " number = "2">
			<S sid ="46" ssid = "1">There has not been much work in developing techniques for ACE event extraction as compared to ACE relation extraction.</S>
			<S sid ="47" ssid = "2">The most salient work for event extraction is Grishman et al.</S>
			<S sid ="48" ssid = "3">(2005) and Ji and Grishman (2008).</S>
			<S sid ="49" ssid = "4">To solve the task for event extraction, Grishman et al.</S>
			<S sid ="50" ssid = "5">(2005) mainly use a combination of pattern matching and statistical modeling techniques.</S>
			<S sid ="51" ssid = "6">They extract two kinds of patterns: 1) the sequence of constituent heads separating anchor and its arguments and 2) a predicate argument sub- graph of the sentence connecting anchor to all the event arguments.</S>
			<S sid ="52" ssid = "7">In conjunction they use a set of Maximum Entropy based classifiers for 1) Trigger labeling, 2) Argument classification and 3) Event classification.</S>
			<S sid ="53" ssid = "8">Ji and Grishman (2008) further exploit a correlation between senses of verbs (that are triggers for events) and topics of documents.</S>
			<S sid ="54" ssid = "9">Our work shares some similarities.</S>
			<S sid ="55" ssid = "10">However, instead of building different classifiers, we use kernel methods with SVMs that “naturally” combine various patterns.</S>
			<S sid ="56" ssid = "11">The structures we use for kernel methods are a super-set of the patterns used by Grishman et al.</S>
			<S sid ="57" ssid = "12">(2005).</S>
			<S sid ="58" ssid = "13">Moreover, in our work, we take gold annotation for entity mentions, and do not deal with the task of named entity detection or resolution.</S>
			<S sid ="59" ssid = "14">Finally, our social events are a broad class of event types, and they involve linguistic expressions for expressing interactions and cognition that do not seem to have a correlation with the topics of documents.</S>
			<S sid ="60" ssid = "15">There has been much work in extracting ACE relations.</S>
			<S sid ="61" ssid = "16">The supervised approaches used for relation extraction can broadly be divided into three main categories: 1) feature-based approaches 2) kernel- based approaches and 3) a combination of feature and kernel based approaches.</S>
			<S sid ="62" ssid = "17">The state-of-the-art feature based approach is that of GuoDong et al.</S>
			<S sid ="63" ssid = "18">(2005).</S>
			<S sid ="64" ssid = "19">They use diverse lexical, syntactic and semantic knowledge for the task.</S>
			<S sid ="65" ssid = "20">The lexical features they use are words between, before, and after target entity mentions, the type of entity (Person, Organization etc.), the type of mention (named, nominal or pronominal) and a feature called overlap that counts the number of other entity mentions and words between the target entities.</S>
			<S sid ="66" ssid = "21">To incorporate syntactic features they use features extracted from base phrase chunking, dependency trees and phrase structure trees.</S>
			<S sid ="67" ssid = "22">To incorporate semantic features, their approach uses resources like a country list and WordNet.</S>
			<S sid ="68" ssid = "23">GuoDong et al.</S>
			<S sid ="69" ssid = "24">(2005) report that 70% of the entities are embedded within each other or separated by just one word.</S>
			<S sid ="70" ssid = "25">This is a major difference to our task because most of our relations span over a long distance in a sentence.</S>
			<S sid ="71" ssid = "26">Collins and Duffy (2002) are among the earliest researchers to propose the use of tree kernels for various NLP tasks.</S>
			<S sid ="72" ssid = "27">Since then kernels have been used for the task of relation extraction (Zelenko et al., 2002; Zhao and Grishman, 2005; Zhang et al., 2006; Moschitti, 2006b; Nguyen et al., 2009).</S>
			<S sid ="73" ssid = "28">For an excellent review of these techniques, see Nguyen et al.</S>
			<S sid ="74" ssid = "29">(2009).</S>
			<S sid ="75" ssid = "30">In addition, there has been some work that combines feature and kernel based methods (Harabagiu et al., 2005; Culotta and Jeffrey, 2004; Zhou et al., 2007).</S>
			<S sid ="76" ssid = "31">Apart from using kernels over dependency trees, Culotta and Jeffrey (2004) incorporate features like words, part of speech (POS) tags, syntactic chunk tag, entity type, entity level, relation argument and WordNet hypernym.</S>
			<S sid ="77" ssid = "32">Harabagiu et al.</S>
			<S sid ="78" ssid = "33">(2005) leverage this approach by adding more semantic feature derived from semantic parsers for FrameNet and PropBank.</S>
			<S sid ="79" ssid = "34">Zhou et al.</S>
			<S sid ="80" ssid = "35">(2007) use a context sensitive kernel in conjunction with features they used in their earlier publication (GuoDong et al., 2005).</S>
			<S sid ="81" ssid = "36">However, we take an approach similar to Nguyen et al.</S>
			<S sid ="82" ssid = "37">(2009).</S>
			<S sid ="83" ssid = "38">This is because it incorporates many of the features suggested in feature-based approaches by using combinations of various structures derived from phrase structure trees and dependency trees.</S>
			<S sid ="84" ssid = "39">In addition we use data sampling techniques to deal with the problem of data skewness.</S>
			<S sid ="85" ssid = "40">We not only try the structures suggested by Nguyen et al.</S>
			<S sid ="86" ssid = "41">(2009) but also introduce a new sequence structure on dependency trees.</S>
			<S sid ="87" ssid = "42">We discuss their structures and kernel method in detail in Section 4.</S>
	</SECTION>
	<SECTION title="Social Event Annotation Data. " number = "3">
			<S sid ="88" ssid = "1">3.1 Social Event Annotation.</S>
			<S sid ="89" ssid = "2">notably the ACE effort (Doddington et al., 2004).</S>
			<S sid ="90" ssid = "3">We leverage this work by annotating social events on the English part of ACE 2005 Multilingual Training Data2 that has already been annotated for entities, relations and events.</S>
			<S sid ="91" ssid = "4">In Agarwal et al.</S>
			<S sid ="92" ssid = "5">(2010), we introduce a comprehensive set of social events which are conceptually different from the event annotation that already exists for ACE.</S>
			<S sid ="93" ssid = "6">Since our annotation task is complex and layered, in Agarwal et al.</S>
			<S sid ="94" ssid = "7">(2010) we present confusion matrices, Cohen’s Kappa, and F-measure values for each of the decision points that the annotators go through in the process of selecting a type and subtype for an event.</S>
			<S sid ="95" ssid = "8">Our annotation scheme is reliable, achieving a moderate kappa for relation detection (0.68) and a high kappa for relation classification (0.86).</S>
			<S sid ="96" ssid = "9">We also achieve a high global agreement of 69.7% using a measure which is inspired by Automated Content Extraction (ACE) inter-annotator agreement measure.</S>
			<S sid ="97" ssid = "10">This compares favorably to the ACE annotation effort.</S>
			<S sid ="98" ssid = "11">Following are the two broad types of social events that were annotated: Interaction event (INR): When both entities participating in an event are aware of each other and of the social event, we say they have an INR relation.</S>
			<S sid ="99" ssid = "12">Consider the following Example (1).</S>
			<S sid ="100" ssid = "13">(1) [Toujan Faisal], 54, {said} [she] was {informed} of the refusal by an [Interior Ministry committee] overseeing election preparations.</S>
			<S sid ="101" ssid = "14">INR As is intuitive, if one person informs the other about something, both have to be cognizant of each other and of the informing event in which they are both participating.</S>
			<S sid ="102" ssid = "15">Observation event (OBS): When only one person (out of the two people that are participating in an event) is aware of the other and of the social event, we say they have an OBS relation.</S>
			<S sid ="103" ssid = "16">Of the type OBS, there are three subtypes: Physical Proximity (PPR), Perception (PCR) and Cognition (COG).</S>
			<S sid ="104" ssid = "17">PPR requires that one entity can observe the other entity in real time not through a broadcast medium, in contrast to the subtype PCR, where one entity observes the other through media (TV, radio, magazines etc.) Any other observation event that is not PPR or PCR There has been much work in the past on annotating entities, relations and events in free text, most 2 Version: 6.0, Catalog number: LDC2005E18.</S>
			<S sid ="105" ssid = "18">is COG.</S>
			<S sid ="106" ssid = "19">Consider the aforementioned Example (1).</S>
			<S sid ="107" ssid = "20">In this sentence, the event said marks a COG relation between Toujan Faisal and the committee.</S>
			<S sid ="108" ssid = "21">This is because, when one person talks about another person, the other person must be present in the first person’s cognitive state without any requirement on physical proximity or external medium.</S>
			<S sid ="109" ssid = "22">As the annotations revealed, PPR and PCR occurred only twice and once, respectively, in the part of ACE corpus we annotated.</S>
			<S sid ="110" ssid = "23">(They occur more frequently in another genre we are investigating such as literary texts.)</S>
			<S sid ="111" ssid = "24">We omit these extremely low- frequency categories from our current study; in this paper we build classifiers to detect and classify only INR and COG events.</S>
			<S sid ="112" ssid = "25">3.2 Comparison Between Social Events and.</S>
			<S sid ="113" ssid = "26">ACE Annotations The ACE effort is about entity, relation and event annotation.</S>
			<S sid ="114" ssid = "27">We use their annotations for entity types PER.Individual and PER.Group and add our social event annotations.</S>
			<S sid ="115" ssid = "28">Our event annotations are different from ACE event annotations because we annotate text that expresses the cognitive states of the people involved, or allows the annotator to infer it.</S>
			<S sid ="116" ssid = "29">Therefore, at the top level of classification we differentiate between events in which only one entity is cognizant of the other (observation) versus events when both entities are cognizant of each other (interaction).</S>
			<S sid ="117" ssid = "30">This distinction is, we believe, novel in event or relation annotation.</S>
			<S sid ="118" ssid = "31">Now we present statistics and examples to make clear how our annotations are different from ACE event annotations.</S>
			<S sid ="119" ssid = "32">The statistics are based on 62 documents from the ACE corpus.</S>
			<S sid ="120" ssid = "33">These files contain a total of 212 social events.</S>
			<S sid ="121" ssid = "34">We found a total of 63 candidate ACE events that had at least two Person entities involved.</S>
			<S sid ="122" ssid = "35">Out of these 63 candidate events, 54 match our annotations.</S>
			<S sid ="123" ssid = "36">The majority of social events that match the ACE events are of type INR.</S>
			<S sid ="124" ssid = "37">On analysis, we found that most of these correspond to the ACE event type CONTACT.</S>
			<S sid ="125" ssid = "38">Specifically, the “meeting” event, which is an ACE CONTACT event and an INR event according to our definition, is the major cause of overlap.</S>
			<S sid ="126" ssid = "39">However, our type INR has a broader definition than ACE type CONTACT.</S>
			<S sid ="127" ssid = "40">For example, in Example 1, we recorded an INR event between Toujan Faisal and committee (event span: informed).</S>
			<S sid ="128" ssid = "41">ACE does not record any event between these two entities because informed does not entail a CONTACT event for ACE event annotations.</S>
			<S sid ="129" ssid = "42">Another example that will clarify the difference is the following: (2) In central Baghdad, [a Reuters cameraman] and [a cameraman for Spain’s Telecinco] died when an American tank fired on the Palestine Hotel ACE has annotated the above example as an event of type CONFLICT in which there are two entities that are of type person: the Reuters cameraman and the cameraman for Spain’s Telecinco, both of which are arguments of type “Victim”.</S>
			<S sid ="130" ssid = "43">Being an event that has two person entities involved makes the above sentence a potential social event.</S>
			<S sid ="131" ssid = "44">However, we do not record any event between these entities since the text does not reveal the cognitive states of the two entities; we do not know whether one was aware of the other.</S>
			<S sid ="132" ssid = "45">ACE defines a class of social relations (PER- SOC) that records named relations like friendship, coworker, long lasting etc. Also, there already exist systems that detect and classify these relations well.</S>
			<S sid ="133" ssid = "46">Therefore, even though these relations are directly relevant to our overall goal of social event extraction, we do not annotate, detect or classify these relations in this paper.</S>
	</SECTION>
	<SECTION title="Tree Kernels, Discrete Structures, and. " number = "4">
			<S sid ="134" ssid = "1">Language In this section, we give details of the structures and kernel we use for our classification tasks.</S>
			<S sid ="135" ssid = "2">We also discuss our motivation behind using these methods.</S>
			<S sid ="136" ssid = "3">Linear learning machines are one of the most popular machines used for classification problems.</S>
			<S sid ="137" ssid = "4">The objective of a typical classification problem is to learn a function that separates the data into different classes.</S>
			<S sid ="138" ssid = "5">The data is usually in the form of features extracted from abstract objects like strings, trees, etc. A drawback of learning by using complex functions is that complex functions do not generalize well and thus tend to over-fit.</S>
			<S sid ="139" ssid = "6">The research community therefore prefers linear classifiers over other complex classifiers.</S>
			<S sid ="140" ssid = "7">But more often than not, the data is not linearly separable.</S>
			<S sid ="141" ssid = "8">It can be made linearly separable by increasing the dimensionality of data but then learning suffers from the curse of dimensionality and classification becomes computationally intractable.</S>
			<S sid ="142" ssid = "9">This is where kernels come to the rescue.</S>
			<S sid ="143" ssid = "10">The well-known kernel trick aids us in finding similarity between feature vectors in a high dimensional space without having to write down the expanded feature space.</S>
			<S sid ="144" ssid = "11">The essence of kernel methods is that they compare two feature vectors in high dimensional space by using a dot product that is a T1-Individual nsubj said ccomp informed auxpass prep prep function of the dot product of feature vectors in the lower dimensional space.</S>
			<S sid ="145" ssid = "12">Moreover, Convolution Kernels (first introduced by Haussler (1999)) can be used to compare abstract objects instead of feature vectors.</S>
			<S sid ="146" ssid = "13">This is because these kernels involve a recursive calculation over the “parts” of a discrete structure.</S>
			<S sid ="147" ssid = "14">This calculation is usually made computationally efficient using Dynamic Programming techniques.</S>
			<S sid ="148" ssid = "15">Therefore, Convolution Kernels alleviate the need of feature extraction (which usually requires domain knowledge, results in extraction of incomplete information and introduces noise in the data).</S>
			<S sid ="149" ssid = "16">Therefore, we use convolution kernels with a linear Toujan_Faisal appos 54 Individual nsubjpass she was of pobj refusal det the by T2-Group pobj committee ... learning machine (Support Vector Machines) for our classification task.</S>
			<S sid ="150" ssid = "17">Now we present the “discrete” structures followed by the kernel we used.</S>
			<S sid ="151" ssid = "18">We use the structures previously used by Nguyen et al.</S>
			<S sid ="152" ssid = "19">(2009), and propose one new structure.</S>
			<S sid ="153" ssid = "20">Although we experimented with all of their structures,3 here we only present the ones that perform best for our classification task.</S>
			<S sid ="154" ssid = "21">All the structures and their combinations are derived from a variation of the underlying structures, Phrase Structure Trees (PST) and Dependency Trees (DT).</S>
			<S sid ="155" ssid = "22">For all trees we first extract their Path Enclosed Tree, which is the smallest common subtree that contains the two target entities (Moschitti, 2004).</S>
			<S sid ="156" ssid = "23">We use the Stanford parser (Klein and Manning, 2003) to get the basic PSTs and DTs.</S>
			<S sid ="157" ssid = "24">Following are the structures that we refer to in our experiments and results section: PET: This refers to the smallest common phrase structure tree that contains the two target entities.</S>
			<S sid ="158" ssid = "25">Dependency Words (DW) tree: This is the smallest common dependency tree that contains the two target entities.</S>
			<S sid ="159" ssid = "26">In Figure 1, since the target entities are at the leftmost and rightmost branch of the depen 3 We omitted SK6, which is the worst performing sequence kernel in (Nguyen et al., 2009).</S>
			<S sid ="160" ssid = "27">Figure 1: Dependency parse tree for the sentence (in the ACE corpus): “[Toujan Faisal], 54, {said} [she] was {informed} of the refusal by an [Interior Ministry committee] overseeing election preparations.” dency tree, this is in fact a DW (ignoring the grammatical relations on the arcs).</S>
			<S sid ="161" ssid = "28">Grammatical Relation (GR) tree: If we replace the words at the nodes by their relation to their corresponding parent in DW, we get a GR tree.</S>
			<S sid ="162" ssid = "29">For example, in Figure 1, replacing Toujan Faisal by nsubj, 54 by appos, she by nsubjpass and so on.</S>
			<S sid ="163" ssid = "30">Grammatical Relation Word (GRW) tree: We get this tree by adding the grammatical relations as separate nodes between a node and its parent.</S>
			<S sid ="164" ssid = "31">For example, in Figure 1, adding nsubj as a node between T1-Individual and Toujan Faisal, appos as a node between 54 and Toujan Faisal, and so on.</S>
			<S sid ="165" ssid = "32">Sequence Kernel of words (SK1): This is the sequence of words between the two entities, including their tags.</S>
			<S sid ="166" ssid = "33">For our example in Figure 1, it would be T1-Individual Toujan Faisal 54 said she was informed of the refusal by an T2 Group Interior Ministry committee.</S>
			<S sid ="167" ssid = "34">Sequence in GRW tree (SqGRW): This is the new structure that we introduce which, to the best of our knowledge, has not been used before for similar tasks.</S>
			<S sid ="168" ssid = "35">It is the sequence of nodes from one target to the other in the GRW tree.</S>
			<S sid ="169" ssid = "36">For example, in Figure 1, this would be Toujan Faisal nsubj T1- Individual said ccomp informed prep by T2-Group pobj committee.</S>
			<S sid ="170" ssid = "37">We also use combinations of these structures (which we refer to as “combined-structures”).</S>
			<S sid ="171" ssid = "38">For example, PET GR SqGRW means we used the three structures (PET, GR and SqGRW) together with a kernel that calculates similarity between forests.</S>
			<S sid ="172" ssid = "39">We use the Partial Tree (PT) kernel, first proposed by Moschitti (2006a), for structures derived from dependency trees and Subset Tree (SST) kernel, proposed by Collins and Duffy (2002), for structures derived from phrase structure trees.</S>
			<S sid ="173" ssid = "40">PT is a relaxed version of the SST; SST measures the similarity between two PSTs by counting all subtrees common to the two PSTs.</S>
			<S sid ="174" ssid = "41">However, there is one constraint: all daughter nodes of a node must be included.</S>
			<S sid ="175" ssid = "42">In PTs this constraint is removed.</S>
			<S sid ="176" ssid = "43">Therefore, in contrast to SSTs, PT kernels compare many more substructures.</S>
			<S sid ="177" ssid = "44">They have been used successfully by (Moschitti, 2004) for the task of semantic role labeling.</S>
			<S sid ="178" ssid = "45">The choices we have made are motivated by the following considerations.</S>
			<S sid ="179" ssid = "46">We are interested in modeling classes of events which are characterized by the cognitive states of participants–who is aware of whom.</S>
			<S sid ="180" ssid = "47">The predicate-argument structure of verbs can encode much of this information very efficiently, and classes of verbs express their predicate-argument structure in similar ways.</S>
			<S sid ="181" ssid = "48">For example, many verbs of communication can express their arguments using the same pattern: John talked/spoke/lectured/ranted/testified to Mary about Percy.</S>
			<S sid ="182" ssid = "49">Independently of the verb, John is in a COG relation with Percy and in an INR relation with Mary.</S>
			<S sid ="183" ssid = "50">All these verbs allow us to drop either or both of the prepositional phrases, without altering the interpretation of the remaining constituents.</S>
			<S sid ="184" ssid = "51">And even more strikingly, any verb that can be put in that position is likely to have this interpretation; for example, we are likely to interpret the neologistic John gazooked to Mary about Percy as a similarly structured social event.</S>
			<S sid ="185" ssid = "52">The regular relation between verb alternations and meaning components has been extensively studied (Levin, 1993; Schuler, 2005).</S>
			<S sid ="186" ssid = "53">This regularity in the syntactic predicate-argument structure allows us to overcome lexical sparseness.</S>
			<S sid ="187" ssid = "54">However, in order to exploit such regularities, we need to have access to a representation which makes the predicate- argument structure clear.</S>
			<S sid ="188" ssid = "55">Dependency representations do this.</S>
			<S sid ="189" ssid = "56">Phrase structure representations also represent predicate-argument structure, but in an indirect way through the structural configurations, and we expect this to increase the burden on the learner.</S>
			<S sid ="190" ssid = "57">(In some phrase structure representations, some arguments and adjuncts are not disambiguated.)</S>
			<S sid ="191" ssid = "58">When using dependency structures, the SST kernel is far less appealing, since it forces us to always consider all daughter nodes of a node.</S>
			<S sid ="192" ssid = "59">However, as we have seen, it is certain daughter nodes, such as the presence of a to PP and a about PP, which are important, while other daughters, such as temporal or locative adjuncts, should be disregarded.</S>
			<S sid ="193" ssid = "60">The PT kernel allows us to do this.</S>
	</SECTION>
	<SECTION title="Sampling Methods. " number = "5">
			<S sid ="194" ssid = "1">In this section we present the data sampling methods we use to deal with data skewness.</S>
			<S sid ="195" ssid = "2">We employ two well-known data sampling methods on the training data before creating a model for test data; random under-sampling and random oversampling (Kotsiantis et al., 2006; Japkowicz, 2000; Weiss and Provost, 2001).</S>
			<S sid ="196" ssid = "3">These techniques are non-heuristic sampling methods that aim at balancing the class proportions by removing examples of the majority class and by duplicating instances of the minority class respectively.</S>
			<S sid ="197" ssid = "4">The reason for using these techniques is that learning is usually optimized to achieve high accuracy.</S>
			<S sid ="198" ssid = "5">Therefore, when presented with skewed training data, a classifier may learn the target concept with a high accuracy by only predicting the majority class.</S>
			<S sid ="199" ssid = "6">But if one looks at the precision, recall, and F-measure, of such a classifier, they will be very low for the minority class.</S>
			<S sid ="200" ssid = "7">Since, like other researchers, we are evaluating the goodness of a model based on its precision, recall and F-measure and not on the accuracy on the test set, either we should change the optimization function of the classifier or employ data sampling techniques.</S>
			<S sid ="201" ssid = "8">We employ the latter because by balancing the class ratio, we are presenting the classifier with a more challenging task of achieving a good accuracy when the majority base class is about 50%.</S>
			<S sid ="202" ssid = "9">The major drawbacks of the two techniques is that under-sampling throws away important information whereas oversampling is prone to over-fitting (due to data duplication).</S>
			<S sid ="203" ssid = "10">As our results show, throwing away information about the majority class is much better than the system that tries to learn in an unbalanced scenario, but it performs worse than an approach using data duplication.</S>
			<S sid ="204" ssid = "11">Since we are using SVMs as a classifier, over-fitting is unlikely as reported by Kolcz et al.</S>
			<S sid ="205" ssid = "12">(2003).</S>
			<S sid ="206" ssid = "13">In order to be sure that we are not over-fitting, we tried another sampling method proposed by Ha and Bunke (1997), which is shown to be good solution to avoid over-fitting by Chawla et al.</S>
			<S sid ="207" ssid = "14">(2002).</S>
			<S sid ="208" ssid = "15">This sampling technique proposes to generate synthetic examples of the minority class by “perturbing” the training data.</S>
			<S sid ="209" ssid = "16">Specifically, Ha and Bunke (1997) produced new synthetic examples for the task of handwritten character recognition by doing operations like rotation and skew on characters.</S>
			<S sid ="210" ssid = "17">The basic idea is to produce synthetic examples that are “close” to the real example from which these synthetic points are generated.</S>
			<S sid ="211" ssid = "18">Analogously, we tried two transformations on our dependency tree structures to produce synthetic examples.</S>
			<S sid ="212" ssid = "19">The first transformation is based on the observation that in control verb constructions, the matrix verb typically does not contribute to the interpretation as a social event or not.</S>
			<S sid ="213" ssid = "20">In this transformation, we lower the subject to an argument verb if it does not have a subject, and repeat this procedure iteratively.</S>
			<S sid ="214" ssid = "21">As it turned out, this transformation only occurred 15 times, and therefore it does not serve the purpose of oversampling.</S>
			<S sid ="215" ssid = "22">We tried a more relaxed transformation on the rightmost target in the tree.</S>
			<S sid ="216" ssid = "23">Here, the observation is that for the COG social events, the second target may be very deeply embedded in the tree.</S>
			<S sid ="217" ssid = "24">For example, in Example 1, Toujan Faisal and the Interior Ministry Committee participate in a COG event (because Faisal is aware of the Committee during the saying event).</S>
			<S sid ="218" ssid = "25">However, the contents of what Faisal said is only relevant to the extent that it pertains to the committee.</S>
			<S sid ="219" ssid = "26">The depth of the embedding of the second target creates issues of data sparseness, as the path-enclosed trees become very node, attaching it on the left, and to recalculate the path-enclosed tree, which is now smaller.</S>
			<S sid ="220" ssid = "27">This is repeated iteratively, so that a sentence with a deeply embedded second target can yield a large number of synthesized structures.</S>
	</SECTION>
	<SECTION title="Experiments And Results. " number = "6">
			<S sid ="221" ssid = "1">In this section we present experiments and results for our two tasks: social event detection and classification.</S>
			<S sid ="222" ssid = "2">For the social event detection task, we wish to validate the following research hypotheses.</S>
			<S sid ="223" ssid = "3">First, we aim to show the importance of using data sampling when evaluating on F-measure; specifically, we expect under-sampling to outperform no sampling, oversampling to outperform under-sampling, and oversampling with transformations to out perform oversampling without transformations.</S>
			<S sid ="224" ssid = "4">In contrast, the social event classification task does not suffer from data skewness because the INR and COG relations; both occur almost the same number of times.</S>
			<S sid ="225" ssid = "5">Therefore, sampling methods may not be applied for this task.</S>
			<S sid ="226" ssid = "6">Second, for both tasks, we expect that a combination of kernels will outperform individual kernels.</S>
			<S sid ="227" ssid = "7">Moreover, we expect that dependency trees will have a crucial role in achieving the best performance.</S>
			<S sid ="228" ssid = "8">6.1 Experimental Setup.</S>
			<S sid ="229" ssid = "9">We use part of ACE data that we annotated for social events.</S>
			<S sid ="230" ssid = "10">In all, we annotated 138 ACE documents.</S>
			<S sid ="231" ssid = "11">We retained the ACE entity annotations.</S>
			<S sid ="232" ssid = "12">We consider all entity mention pairs in a sentence.</S>
			<S sid ="233" ssid = "13">If our annotators recorded a relation between a pair of entity mentions, we say there is a relation between the corresponding entities.</S>
			<S sid ="234" ssid = "14">If there are any other pairs of entity mentions for the same pair of entity, we discard those.</S>
			<S sid ="235" ssid = "15">For all other pairs of entity mentions, we say there is no relation.</S>
			<S sid ="236" ssid = "16">Out of 138 files, four files did not have any positive or negative examples (because there were very few and sparse entity mentions in these four files).</S>
			<S sid ="237" ssid = "17">We found a total of 1291 negative examples, 172 examples belonging to class INR and 174 belonging to class COG.</S>
			<S sid ="238" ssid = "18">We use Jet’s sentence splitter4 and the Stanford Parser (Klein and Manning, 2003) for phrase structure trees and dependency parses.</S>
			<S sid ="239" ssid = "19">For classifica large and very diverse.</S>
			<S sid ="240" ssid = "20">Our transformation, therefore, is to move the second target to its grandmother 4 http://cs.nyu.edu/grishman/jet/jetDownload.html tion, we used Alessandro Moschitti’s SVM-Light- TK package (Moschitti, 2006b) which is built on the SVM-Light implementation of Joakhims (1999).</S>
			<S sid ="241" ssid = "21">For all our experiments, we perform 5-fold cross- validation.</S>
			<S sid ="242" ssid = "22">We randomly divide the whole corpus into 5 equal parts, such that no news story (or document) gets divided among two parts.</S>
			<S sid ="243" ssid = "23">For each fold, we then merge 4 parts to create a training corpus and treat the remaining part as a test corpus.</S>
			<S sid ="244" ssid = "24">By keeping individual news stories intact, we make sure that vocabulary specific to one story does not unrealistically improve the performance.</S>
			<S sid ="245" ssid = "25">6.2 Social Event Detection.</S>
			<S sid ="246" ssid = "26">Social event detection is the task of detecting if any social event exists between a pair of entities in a sentence.</S>
			<S sid ="247" ssid = "27">We formulate the problem as a binary classification task by labeling an example that does not have a social event as class -1 and by labeling an example that either has an INR or COG social event as class 1.</S>
			<S sid ="248" ssid = "28">First we present results for our baseline system.</S>
			<S sid ="249" ssid = "29">Our baseline system uses various structures and their combinations but without any data balancing.</S>
			<S sid ="250" ssid = "30">5 K e r n e l P R F 1 P E T 70 .2 8 21 .4 6 32 .3 8 G R 87 .7 9 15 .2 1 25 .5 5 G R W 76 .4 2 8.</S>
			<S sid ="251" ssid = "31">2 6 1 4.</S>
			<S sid ="252" ssid = "32">8 S q G R W 48 .7 8 6.</S>
			<S sid ="253" ssid = "33">0 8 10 .3 8 P E T G R 70 .2 1 27 .7 6 38 .8 9 P E T G R S q G R W 71 .0 6 26 .7 4 38 .0 2 G R S q G R W 8 2.</S>
			<S sid ="254" ssid = "34">0 24 .4 7 36 .1 2 G R W S q G R W 68 .1 9 17 .0 1 25 .0 6 G R G R W S q G R W 79 .8 1 21 .9 9 32 .5 7 Table 1: Baseline System for the task of social event detection.</S>
			<S sid ="255" ssid = "35">The proportion of positive data in training and test set is 21.1% and 20.6% respectively.</S>
			<S sid ="256" ssid = "36">Table 1 presents results for our baseline system.</S>
			<S sid ="257" ssid = "37">Grammatical relation tree structure (GR), a structure derived from dependency tree by replacing the words by their grammatical relations achieves the best precision.</S>
			<S sid ="258" ssid = "38">This is probably because the clas 5 Although we experimented with many more structures and their combinations, due to space restrictions we mention only the top results.</S>
			<S sid ="259" ssid = "39">sifier learns that if both the arguments of a predicate contain target entities then it is a social event.</S>
			<S sid ="260" ssid = "40">Among kernels for single structures, the path enclosed tree for PSTs (PET) achieves the best recall.</S>
			<S sid ="261" ssid = "41">Furthermore, a combination of structures derived from PSTs and DTs performs best.</S>
			<S sid ="262" ssid = "42">The sequence kernels, perform much worse than SqGRW (F1-measure as low as 0.45).</S>
			<S sid ="263" ssid = "43">Since it is the same case for all subsequent experiments, we omit them from the discussion.</S>
			<S sid ="264" ssid = "44">K e r n e l P R F 1 P E T 28 .8 9 77 .0 6 41 .9 6 G R 35 .6 8 72 .4 7 47 .3 7 G R W 2 9.</S>
			<S sid ="265" ssid = "45">7 8 3.</S>
			<S sid ="266" ssid = "46">6 4 3.</S>
			<S sid ="267" ssid = "47">6 S q G R W 34 .3 1 84 .1 5 48 .6 1 P E T G R 34 .3 8 83 .9 4 48 .5 2 P E T G R S q G R W 34 .3 4 83 .6 6 48 .5 2 G R S q G R W 33 .4 5 81 .7 3 47 .2 7 G R W S q G R W 32 .8 7 84 .4 4 47 .1 1 G R G R W S q G R W 32 .7 3 83 .2 6 46 .8 2 Table 2: Under-sampled system for the task of relation detection.</S>
			<S sid ="268" ssid = "48">The proportion of positive examples in the training and test corpus is 50.0% and 20.6% respectively.</S>
			<S sid ="269" ssid = "49">We now turn to experiments involving sampling.</S>
			<S sid ="270" ssid = "50">Table 2 presents results for under-sampling, i.e. randomly removing examples belonging to the negative class until its size matches the positive class.</S>
			<S sid ="271" ssid = "51">Table 2 shows a large gain in F1-measure of 9.72% absolute over the baseline system (Table 1).</S>
			<S sid ="272" ssid = "52">We found that worst performing kernel with under-sampling is SK1 with an F1-measure of 39.2% which is better than the best performance without under- sampling.</S>
			<S sid ="273" ssid = "53">These results make it clear that doing under-sampling greatly improves the performance of the classifier, despite the fact that we are using less training data (fewer negative examples).</S>
			<S sid ="274" ssid = "54">This is as expected because we are evaluating on F1-measure and the classifier is optimizing for accuracy.</S>
			<S sid ="275" ssid = "55">Table 3 presents results for oversampling i.e. replicating positive examples to achieve an equal number of examples belonging to the positive and negative class.</S>
			<S sid ="276" ssid = "56">Table 3 shows that the gain over the baseline system now is 22.2% absolute.</S>
			<S sid ="277" ssid = "57">Also, the gain over the under-sampled system is 12.5% K e r n e l P R F 1 P E T 5 0.</S>
			<S sid ="278" ssid = "58">9 57 .2 1 53 .6 2 G R 43 .5 7 67 .2 1 52 .5 9 G R W 46 .0 5 64 .1 5 53 .3 1 S q G R W 4 2.</S>
			<S sid ="279" ssid = "59">4 72 .7 5 5 3.</S>
			<S sid ="280" ssid = "60">5 P E T G R 56 .4 2 6 6.</S>
			<S sid ="281" ssid = "61">2 60 .6 3 P E T G R S q G R W 57 .2 8 66 .2 6 61 .1 1 G R S q G R W 44 .3 5 71 .1 7 54 .5 2 G R W S q G R W 44 .7 7 68 .7 9 54 .1 2 G R G R W S q G R W 46 .7 9 71 .5 4 56 .4 5 Table 3: Over-sampled system for the task of relation detection.</S>
			<S sid ="282" ssid = "62">The proportion of positive examples in the training and test corpus is 50.0% and 20.6% respectively.</S>
			<S sid ="283" ssid = "63">absolute.</S>
			<S sid ="284" ssid = "64">As in the baseline system, a combination of structures performs best.</S>
			<S sid ="285" ssid = "65">As in the under- sampled system, when the data is balanced, SqGRW (sequence kernel on dependency tree in which grammatical relations are inserted as intermediate nodes) achieves the best recall.</S>
			<S sid ="286" ssid = "66">Here, the PET and GR kernel perform similar: this is different from the results of (Nguyen et al., 2009) where GR performed much worse than PET for ACE data.</S>
			<S sid ="287" ssid = "67">This exemplifies the difference in the nature of our event annotations from that of ACE relations.</S>
			<S sid ="288" ssid = "68">Since the average distance between target entities in the surface word order is higher for our events, the phrase structure trees are bigger.</S>
			<S sid ="289" ssid = "69">This means that implicit feature space is much sparser and thus not the best representation.</S>
			<S sid ="290" ssid = "70">Table 4 presents results for using the oversampling method with transformation that produces synthetic positive examples by using a transformation on dependency trees such that the new synthetic examples are “close” to the original examples.</S>
			<S sid ="291" ssid = "71">This method achieves a gain 16.78% over the baseline system.</S>
			<S sid ="292" ssid = "72">We expected this system to perform better than the over-sampled system but it does not.</S>
			<S sid ="293" ssid = "73">This suggests that our over-sampled system is not over-fitting; a concern with using oversampling techniques.</S>
			<S sid ="294" ssid = "74">6.3 Social Event Classification.</S>
			<S sid ="295" ssid = "75">For the social event classification task, we only consider pairs of entities that have an event.</S>
			<S sid ="296" ssid = "76">Since these events could only be INR or COG, this is a binary classification problem.</S>
			<S sid ="297" ssid = "77">However, now we are interested in both outcomes of the classification, while earlier we were only interested in knowing how well we were finding relations (and not in how well we were finding “non-relations”).</S>
			<S sid ="298" ssid = "78">Therefore, accuracy is the relevant metric (Table 5).</S>
			<S sid ="299" ssid = "79">Table 5: System for the task of relation classification.</S>
			<S sid ="300" ssid = "80">The two classes are INR and COG, and we evaluate using accuracy (Acc.).</S>
			<S sid ="301" ssid = "81">The proportion of INR relations in training and test set is 49.7% and 49.63% respectively.</S>
			<S sid ="302" ssid = "82">Table 4: Over-sampled System with transformation for relation detection.</S>
			<S sid ="303" ssid = "83">The proportion of positive examples in the training and test corpus is 51.7% and 20.6% respectively.</S>
			<S sid ="304" ssid = "84">Even though the task of reasoning if an event is about one-way or mutual cognition seems hard, our system beats the chance baseline by 28.72%.</S>
			<S sid ="305" ssid = "85">These results show that there are significant clues in the lexical and syntactic structures that help in differentiating between interaction and cognition social events.</S>
			<S sid ="306" ssid = "86">Once again we notice that the combination of kernels works better than single kernels alone, though the difference here is less pronounced.</S>
			<S sid ="307" ssid = "87">Among the combined-structure approaches, combinations with dependency-derived structures continue to outperform those not including dependency (the best all-phrase structure performer is PET SK1 with 75.7% accuracy, not shown in Table 5).</S>
	</SECTION>
	<SECTION title="Conclusion And Future Work. " number = "7">
			<S sid ="308" ssid = "1">In this paper, we have introduced the novel tasks of social event detection and classification.</S>
			<S sid ="309" ssid = "2">We show that data sampling techniques play a crucial role for the task of relation detection.</S>
			<S sid ="310" ssid = "3">Through oversampling we achieve an increase in F1-measure of 22.2% absolute over a baseline system.</S>
			<S sid ="311" ssid = "4">Our experiments show that as a result of how language expresses the relevant information, dependency-based structures are best suited for encoding this information.</S>
			<S sid ="312" ssid = "5">Furthermore, because of the complexity of the task, a combination of phrase based structures and dependency-based structures perform the best.</S>
			<S sid ="313" ssid = "6">This revalidates the observation of Nguyen et al.</S>
			<S sid ="314" ssid = "7">(2009) that phrase structure representations and dependency representations add complimentary value to the learning task.</S>
			<S sid ="315" ssid = "8">We also introduced a new sequence structure (SqGRW) which plays a role in achieving the best accuracy for both, social event detection and social event classification tasks.</S>
			<S sid ="316" ssid = "9">In the future, we will use other parsers (such as semantic parsers) and explore new types of linguistically motivated structures and transformations.</S>
			<S sid ="317" ssid = "10">We will also investigate the relation between classes of social events and their syntactic realization.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="318" ssid = "11">The work was funded by NSF grant IIS0713548.</S>
			<S sid ="319" ssid = "12">We thank Dr. Alessandro Moschitti and TrucVien T. Nguyen for helping us with re-implementing their system.</S>
			<S sid ="320" ssid = "13">We acknowledge Boyi Xie for his assistance in implementing the system.</S>
			<S sid ="321" ssid = "14">We would also like to thank Dr. Claire Monteleoni and Daniel Bauer for useful discussions and feedback.</S>
	</SECTION>
</PAPER>
