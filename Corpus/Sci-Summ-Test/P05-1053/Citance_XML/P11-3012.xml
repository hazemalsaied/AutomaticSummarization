<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Relation extraction in documents allows the detection of how entities being discussed in a document are related to one another (e.g. part- of).</S>
		<S sid ="2" ssid = "2">This paper presents an analysis of a relation extraction system based on prior work but applied to the J.D. Power and Associates Sentiment Corpus to examine how the system works on documents from a range of social media.</S>
		<S sid ="3" ssid = "3">The results are examined on three different subsets of the JDPA Corpus, showing that the system performs much worse on documents from certain sources.</S>
		<S sid ="4" ssid = "4">The proposed explanation is that the features used are more appropriate to text with strong editorial standards than the informal writing style of blogs.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">To summarize accurately, determine the sentiment, or answer questions about a document it is often necessary to be able to determine the relationships between entities being discussed in the document (such as part-of or member-of).</S>
			<S sid ="6" ssid = "6">In the simple sentiment example Example 1.1: I bought a new car yesterday.</S>
			<S sid ="7" ssid = "7">I love the powerful engine.</S>
			<S sid ="8" ssid = "8">determining the sentiment the author is expressing about the car requires knowing that the engine is a part of the car so that the positive sentiment being expressed about the engine can also be attributed to the car.</S>
			<S sid ="9" ssid = "9">In this paper we examine our preliminary results from applying a relation extraction system to the J.D. Power and Associates (JDPA) Sentiment Corpus (Kessler et al., 2010).</S>
			<S sid ="10" ssid = "10">Our system uses lexical features from prior work to classify relations, and we examine how the system works on different subsets from the JDPA Sentiment Corpus, breaking the source documents down into professionally written reviews, blog reviews, and social networking reviews.</S>
			<S sid ="11" ssid = "11">These three document types represent quite different writing styles, and we see significant difference in how the relation extraction system performs on the documents from different sources.</S>
	</SECTION>
	<SECTION title="Relation Corpora. " number = "2">
			<S sid ="12" ssid = "1">2.1 ACE-2004 Corpus.</S>
			<S sid ="13" ssid = "2">The Automatic Content Extraction (ACE) Corpus (Mitchell, et al., 2005) is one of the most common corpora for performing relation extraction.</S>
			<S sid ="14" ssid = "3">In addition to the co-reference annotations, the Corpus is annotated to indicate 23 different relations between real-world entities that are mentioned in the same sentence.</S>
			<S sid ="15" ssid = "4">The documents consist of broadcast news transcripts and newswire articles from a variety of news organizations.</S>
			<S sid ="16" ssid = "5">2.2 JDPA Sentiment Corpus.</S>
			<S sid ="17" ssid = "6">The JDPA Corpus consists of 457 documents containing discussions about cars, and 180 documents discussing cameras (Kessler et al., 2010).</S>
			<S sid ="18" ssid = "7">In this work we only use the automotive documents.</S>
			<S sid ="19" ssid = "8">The documents are drawn from a variety of sources, and we particularly focus on the 24% of the documents from the JDPA Power Steering blog, 18% from Blogspot, and 18% from LiveJournal.</S>
			<S sid ="20" ssid = "9">64 Proceedings of the ACLHLT 2011 Student Session, pages 64–68, Portland, OR, USA 1924 June 2011.</S>
			<S sid ="21" ssid = "10">Qc 2011 Association for Computational Linguistics The annotated mentions in the Corpus are single or multi-word expressions which refer to a particular real world or abstract entity.</S>
			<S sid ="22" ssid = "11">The mentions are annotated to indicate sets of mentions which constitute co-reference groups referring to the same entity.</S>
			<S sid ="23" ssid = "12">Five relationships are annotated between these entities: PartOf, FeatureOf, Produces, InstanceOf, and MemberOf.</S>
			<S sid ="24" ssid = "13">One significant difference between these relation annotations and those in the ACE Corpus is that the former are relations between sets of mentions (the co-reference groups) rather than between individual mentions.</S>
			<S sid ="25" ssid = "14">This means that these relations are not limited to being between mentions in the same sentence.</S>
			<S sid ="26" ssid = "15">So in Example 1.1, “engine” would be marked as a part of “car” in the JDPA Corpus annotations, but there would be no relation annotated in the ACE Corpus.</S>
			<S sid ="27" ssid = "16">For a more direct comparison to the ACE Corpus results, we restrict ourselves only to mentions within the same sentence (we discuss this decision further in section 5.4).</S>
	</SECTION>
	<SECTION title="Relation Extraction System. " number = "3">
			<S sid ="28" ssid = "1">3.1 Overview.</S>
			<S sid ="29" ssid = "2">The system extracts all pairs of mentions in a sentence, and then classifies each pair of mentions as either having a relationship, having an inverse relationship, or having no relationship.</S>
			<S sid ="30" ssid = "3">So for the PartOf relation in the JDPA Sentiment Corpus we consider both the relation “X is part of Y” and “Y is part of X”.</S>
			<S sid ="31" ssid = "4">The classification of each mention pair is performed using a support vector machine implemented using libLinear (Fan et al., 2008).</S>
			<S sid ="32" ssid = "5">To generate the features for each of the mention pairs a proprietary JDPA Tokenizer is used for parsing the document and the Stanford Parser (Klein and Manning, 2003) is used to generate parse trees and part of speech tags for the sentences in the documents.</S>
			<S sid ="33" ssid = "6">3.2 Features.</S>
			<S sid ="34" ssid = "7">We used Zhou et al.’s lexical features (Zhou et al., 2005) as the basis for the features of our system similar to what other researchers have done (Chan and Roth, 2010).</S>
			<S sid ="35" ssid = "8">Additional work has extended these features (Jiang and Zhai, 2007) or incorporated other data sources (e.g. WordNet), but in this paper we focus solely on the initial step of applying these same lexical features to the JDPA Corpus.</S>
			<S sid ="36" ssid = "9">The Mention Level, Overlap, Base Phrase Chunk- ing, Dependency Tree, and Parse Tree features are the same as Zhou et al.</S>
			<S sid ="37" ssid = "10">(except for using the Stanford Parser rather than the Collins Parser).</S>
			<S sid ="38" ssid = "11">The minor changes we have made are summarized below: • Word Features: Identical, except rather than using a heuristic to determine the head word of the phrase it is chosen to be the noun (or any other word if there are no nouns in the mention) that is the least deep in the parse tree.</S>
			<S sid ="39" ssid = "12">This change has minimal impact.</S>
			<S sid ="40" ssid = "13">• Entity Types: Some of the entity types in the JDPA Corpus indicate the type of the relation (e.g. CarFeature, CarPart) and so we replace those entity types with “Unknown”.</S>
			<S sid ="41" ssid = "14">• Token Class: We added an additional feature (TC12+ET12) indicating the Token Class of the head words (e.g. Abbreviation, DollarAm- mount, Honorific) combined with the entity types.</S>
			<S sid ="42" ssid = "15">• Semantic Information: These features are specific to the ACE relations and so are not used.</S>
			<S sid ="43" ssid = "16">In Zhou et al.’s work, this set of features increases the overall F-Measure by 1.5.</S>
	</SECTION>
	<SECTION title="Results. " number = "4">
			<S sid ="44" ssid = "1">4.1 ACE Corpus Results.</S>
			<S sid ="45" ssid = "2">We ran our system on the ACE-2004 Corpus as a baseline to prove that the system worked properly and could approximately duplicate Zhou et al.’s results.</S>
			<S sid ="46" ssid = "3">Using 5-fold cross validation on the newswire and broadcast news documents in the dataset we achieved an average overall F-Measure of 50.6 on the fine-grained relations.</S>
			<S sid ="47" ssid = "4">Although a bit lower than Zhou et al.’s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features.</S>
			<S sid ="48" ssid = "5">4.2 JDPA Sentiment Corpus Results.</S>
			<S sid ="49" ssid = "6">We randomly divided the JDPA Corpus into training (70%), development (10%), and test (20%) datasets.</S>
			<S sid ="50" ssid = "7">Table 1 shows relation extraction results of the system on the test portion of the corpus.</S>
			<S sid ="51" ssid = "8">The results are further broken out by three different source types to highlight the differences caused R el ati o n All Docum ents LiveJo urnal Blog spot J D P A P R F P R F P R F P R F F EA T U R E O F 4 4.</S>
			<S sid ="52" ssid = "9">8 42 .3 43 .5 2 6.</S>
			<S sid ="53" ssid = "10">8 35 .8 30 .6 4 4.</S>
			<S sid ="54" ssid = "11">1 40 .0 42 .0 5 9.</S>
			<S sid ="55" ssid = "12">0 55 .0 56 .9 M E M B E R O F 3 4.</S>
			<S sid ="56" ssid = "13">1 10 .7 16 .3 0 . 0 0.</S>
			<S sid ="57" ssid = "14">0 0.</S>
			<S sid ="58" ssid = "15">0 3 6.</S>
			<S sid ="59" ssid = "16">0 13 .2 19 .4 3 6.</S>
			<S sid ="60" ssid = "17">4 13 .7 19 .9 PA R T O F 4 6.</S>
			<S sid ="61" ssid = "18">5 34 .7 39 .8 4 1.</S>
			<S sid ="62" ssid = "19">4 17 .5 24 .6 4 8.</S>
			<S sid ="63" ssid = "20">1 35 .6 40 .9 4 8.</S>
			<S sid ="64" ssid = "21">8 43 .9 46 .2 P R O D U C E S 5 1.</S>
			<S sid ="65" ssid = "22">7 49 .2 50 .4 0 5.</S>
			<S sid ="66" ssid = "23">0 36 .4 08 .8 4 3.</S>
			<S sid ="67" ssid = "24">7 36 .0 39 .5 6 6.</S>
			<S sid ="68" ssid = "25">5 64 .6 65 .6 IN ST A N C E O F 3 7.</S>
			<S sid ="69" ssid = "26">1 16 .7 23 .0 4 4.</S>
			<S sid ="70" ssid = "27">8 14 .9 22 .4 4 2.</S>
			<S sid ="71" ssid = "28">1 13 .0 19 .9 3 0.</S>
			<S sid ="72" ssid = "29">9 29 .6 30 .2 O ve ra ll 4 6.</S>
			<S sid ="73" ssid = "30">0 36 .2 40 .5 2 7.</S>
			<S sid ="74" ssid = "31">1 22 .6 24 .6 4 5.</S>
			<S sid ="75" ssid = "32">2 33 .3 38 .3 5 3.</S>
			<S sid ="76" ssid = "33">7 46 .5 49 .9 Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.</S>
			<S sid ="77" ssid = "34">Li ve Jo ur na l Bl og sp ot J D P A A C E To ke ns Pe r S en te nc e 1 9 . 2 1 8 . 6 1 6 . 5 1 9 . 7 Re lati on s Pe r S en te nc e 1 . 0 8 1 . 7 1 2 . 5 6 0 . 5 6 Re lati on s N ot In Sa me S en te nc e 3 3 % 3 0 % 2 7 % 0 % Tr ai ni ng M en tio n Pa irs in On e Se nt en ce 5 8 , 4 5 2 5 4 , 4 8 0 95 ,6 30 7 7, 5 7 2 M en tio ns Pe r S en te nc e 4 . 2 6 4 . 3 2 4 . 0 3 3 . 1 6 M en tio ns Pe r E nti ty 1 . 7 3 1 . 6 3 1 . 3 3 2 . 3 6 M en tio ns Wi th O nly O ne To ke n 7 7 . 3 % 7 3 . 2 % 6 1.</S>
			<S sid ="78" ssid = "35">2 % 5 6.</S>
			<S sid ="79" ssid = "36">2 % Table 2: Selected document statistics for three JDPA Corpus document sources.</S>
			<S sid ="80" ssid = "37">by the writing styles from different types of media: LiveJournal (livejournal.com), a social media site where users comment and discuss stories with each other; Blogspot (blospot.com), Google’s blogging platform; and JDPA (jdpower.com’s Power Steering blog), consisting of reviews of cars written by JDPA professional writers/analysts.</S>
			<S sid ="81" ssid = "38">These subsets were selected because they provide the extreme (JDPA and LiveJournal) and average (Blogspot) results for the overall dataset.</S>
	</SECTION>
	<SECTION title="Analysis. " number = "5">
			<S sid ="82" ssid = "1">Overall the system is not performing as well as it does on the ACE-2004 dataset.</S>
			<S sid ="83" ssid = "2">However, there is a 25 point F-Measure difference between the Live- Journal and JDPA authored documents.</S>
			<S sid ="84" ssid = "3">This suggests that the informal style of the LiveJournal documents may be reducing the effectiveness of the features developed by Zhou et al., which were developed on newswire and broadcast news transcript documents.</S>
			<S sid ="85" ssid = "4">In the remainder of this section we look at a statistical analysis of the training portion of the JDPA Corpus, separated by document source, and suggest areas where improved features may be able to aid relation extraction on the JDPA Corpus.</S>
			<S sid ="86" ssid = "5">5.1 Document Statistic Effects on Classifier.</S>
			<S sid ="87" ssid = "6">Table 2 summarizes some important statistical differences between the documents from different sources.</S>
			<S sid ="88" ssid = "7">These differences suggest two reasons why the instances being used to train the classifier could be skewed disproportionately towards the JDPA authored documents.</S>
			<S sid ="89" ssid = "8">First, the JDPA written documents express a much larger number of relations between entities.</S>
			<S sid ="90" ssid = "9">When training the classifier, these differences will cause a large share of the instances that have a relation to be from a JDPA written document, skewing the classifier towards any language clues specific to these documents.</S>
			<S sid ="91" ssid = "10">Second, the number of mention pairs occurring within one sentence is significantly higher in the JDPA authored documents than the other documents.</S>
			<S sid ="92" ssid = "11">This disparity is even true on a per sentence or per document basis.</S>
			<S sid ="93" ssid = "12">This provides the classifier with significantly more negative examples written in a JDPA written style.</S>
			<S sid ="94" ssid = "13">Word Percent of All Tokens in Documents LiveJourna l Bl og sp ot JD P A A C E c a r I i t I t i t s t h e 0 . 8 6 1 . 9 1 1 . 4 2 0 . 3 3 0 . 2 5 4 . 4 3 0 . 7 1 1 . 2 8 0 . 9 7 0 . 2 7 0 . 1 8 4 . 6 0 0.</S>
			<S sid ="95" ssid = "14">2 0 0.</S>
			<S sid ="96" ssid = "15">2 4 0.</S>
			<S sid ="97" ssid = "16">2 3 0.</S>
			<S sid ="98" ssid = "17">3 5 0.</S>
			<S sid ="99" ssid = "18">2 2 3.</S>
			<S sid ="100" ssid = "19">5 4 0.</S>
			<S sid ="101" ssid = "20">0 1 0.</S>
			<S sid ="102" ssid = "21">2 1 0.</S>
			<S sid ="103" ssid = "22">6 3 0.</S>
			<S sid ="104" ssid = "23">0 9 0.</S>
			<S sid ="105" ssid = "24">1 9 4.</S>
			<S sid ="106" ssid = "25">8 1 Table 4: Frequency of some common words per token.</S>
			<S sid ="107" ssid = "26">Table 3: Top 10 phrases in mention pairs whose relation was incorrectly classified, and the total percentage of errors from the top ten.</S>
			<S sid ="108" ssid = "27">5.2 Common Errors.</S>
			<S sid ="109" ssid = "28">Table 3 shows the mention phrases that occur most commonly in the incorrectly classified mention pairs.</S>
			<S sid ="110" ssid = "29">For the LiveJournal and Blogspot data, many more of the errors are due to a few specific phrases being classified incorrectly such as “car”, “Maybach”, and various forms of “it”.</S>
			<S sid ="111" ssid = "30">The top four phrases constitute 17% of the errors for LiveJournal and 14% for Blogspot.</S>
			<S sid ="112" ssid = "31">Whereas the JDPA documents have the errors spread more evenly across mention phrases, with the top 10 phrases constituting 13.6% of the total errors.</S>
			<S sid ="113" ssid = "32">Furthermore, the phrases causing many of the problems for the LiveJournal and Blogspot relation detection are generic nouns and pronouns such as “car” and “it”.</S>
			<S sid ="114" ssid = "33">This suggests that the classifier is having difficulty determining relationships when these less descriptive words are involved.</S>
			<S sid ="115" ssid = "34">5.3 Vocabulary.</S>
			<S sid ="116" ssid = "35">To investigate where these variations in phrase error rates comes from, we performed two analyses of the word frequencies in the documents: Table 4 shows the frequency of some common words in the documents; Table 5 shows the frequency of a select set of parts-of-speech per sentence in the document.</S>
			<S sid ="117" ssid = "36">Table 5: Frequency of select part-of-speech tags.</S>
			<S sid ="118" ssid = "37">We find that despite all the documents discussing cars, the JDPA reviews use the word “car” much less often, and use proper nouns significantly more often.</S>
			<S sid ="119" ssid = "38">Although “car” also appears in the top ten errors on the JDPA documents, the total percentage of the errors is one fifth of the error rate on the LiveJournal documents.</S>
			<S sid ="120" ssid = "39">The JDPA authored documents also tend to have more multi-word mention phrases (Table 2) suggesting that the authors use more descriptive language when referring to an entity.</S>
			<S sid ="121" ssid = "40">77.3% of the mentions in LiveJournal documents use only a single word while 61.2% of mentions JDPA authored documents are a single word.</S>
			<S sid ="122" ssid = "41">Rather than descriptive noun phrases, the Live- Journal and Blogspot documents make more use of pronouns.</S>
			<S sid ="123" ssid = "42">LiveJournal especially uses pronouns often, to the point of averaging one per sentence, while JDPA uses only one every five sentences.</S>
			<S sid ="124" ssid = "43">5.4 Extra-Sentential Relations.</S>
			<S sid ="125" ssid = "44">Many relations in the JDPA Corpus occur between entities which are not mentioned in the same sentence.</S>
			<S sid ="126" ssid = "45">Our system only detects relations between mentions in the same sentence, causing about 29% of entity relations to never be detected (Table 2).</S>
			<S sid ="127" ssid = "46">The LiveJournal documents are more likely to contain relationships between entities that are not mentioned in the same sentence.</S>
			<S sid ="128" ssid = "47">In the semantic role labeling (SRL) domain, extra-sentential arguments have been shown to significantly improve SRL performance (Gerber and Chai, 2010).</S>
			<S sid ="129" ssid = "48">Improvements in entity relation extraction could likely be made by extending Zhou et al.’s features across sentences.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="130" ssid = "1">The above analysis shows that at least some of the reason for the system performing worse on the JDPA Corpus than on the ACE-2004 Corpus is that many of the documents in the JDPA Corpus have a different writing style from the news articles in the ACE Corpus.</S>
			<S sid ="131" ssid = "2">Both the ACE news documents, and the JDPA authored documents are written by professional writers with stronger editorial standards than the other JDPA Corpus documents, and the relation extraction system performs much better on professionally edited documents.</S>
			<S sid ="132" ssid = "3">The heavy use of pronouns and less descriptive mention phrases in the other documents seems to be one cause of the reduction in relation extraction performance.</S>
			<S sid ="133" ssid = "4">There is also some evidence that because of the greater number of relations in the JPDA authored documents that the classifier training data could be skewed more towards those documents.</S>
			<S sid ="134" ssid = "5">Future work needs to explore features that can address the difference in language usage that the different authors use.</S>
			<S sid ="135" ssid = "6">This work also does not address whether the relation extraction task is being negatively impacted by poor tokenization or parsing of the documents rather than the problems being caused by the relation classification itself.</S>
			<S sid ="136" ssid = "7">Further work is also needed to classify extra-sentential relations, as the current methods look only at relations occurring within a single sentence thus ignoring a large percentage of relations between entities.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="137" ssid = "8">This work was partially funded and supported by J. D. Power and Associates.</S>
			<S sid ="138" ssid = "9">I would like to thank Nicholas Nicolov, Jason Kessler, and Will Headden for their help in formulating this work, and my thesis advisers: Jim Martin, Rodney Nielsen, and Mike Mozer.</S>
	</SECTION>
</PAPER>
