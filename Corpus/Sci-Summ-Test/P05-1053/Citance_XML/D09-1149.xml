<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification.</S>
		<S sid ="2" ssid = "2">First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set.</S>
		<S sid ="3" ssid = "3">We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance.</S>
		<S sid ="4" ssid = "4">Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself.</S>
		<S sid ="5" ssid = "5">This suggests that a proper sampling strategy is critical in semi-supervised learning.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">With the dramatic increase in the amount of textual information available in digital archives and the WWW, there has been growing interest in techniques for automatically extracting information from text documents.</S>
			<S sid ="7" ssid = "7">Information Extraction (IE) is such a technology that IE systems are expected to identify relevant information (usually of predefined types) from text documents in a certain domain and put them in a structured format.</S>
			<S sid ="8" ssid = "8">According to the scope of the NIST AutomaticContent Extraction (ACE) program (ACE, 2000 2007), current research in IE has three main objectives: Entity Detection and Tracking (EDT), Relation Detection and Characterization (RDC), and Event Detection and Characterization (EDC).</S>
			<S sid ="9" ssid = "9">This paper focuses on the ACE RDC subtask, where many machine learning methods have been proposed, including supervised methods (Miller et al., 2000; Zelenko et al., 2002; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005; Zhang et al., 2006; Qian et al., 2008), semi-supervised methods (Brin, 1998; Agichtein and Gravano, 2000; Zhang, 2004; Chen et al., 2006; Zhou et al., 2008), and unsupervised methods (Hasegawa et al., 2004; Zhang et al., 2005) . Current work on semantic relation extraction task mainly uses supervised learning methods, since it achieves relatively better performance.</S>
			<S sid ="10" ssid = "10">However this method requires a large amount of manually labeled relation instances, which is both time-consuming and laborious.</S>
			<S sid ="11" ssid = "11">In the contrast, unsupervised methods do not need definitions of relation types and hand-tagged data, but it is difficult to evaluate their performance since there are no criteria for evaluation.</S>
			<S sid ="12" ssid = "12">Therefore, semi-supervised learning has received more and more attention, as it can balance the advantages and disadvantages between supervised and unsupervised methods.</S>
			<S sid ="13" ssid = "13">With the plenitude of unlabeled natural language data at hand, semi-supervised learning can significantly reduce the need for labeled data with only limited sacrifice in performance.</S>
			<S sid ="14" ssid = "14">Specifically, a bootstrapping algorithm chooses the unlabeled instances with the highest probability of being correctly labeled and use them to augment labeled training data iteratively.</S>
			<S sid ="15" ssid = "15">Although previous work (Yarowsky, 1995; Blum and Mitchell, 1998; Abney, 2000; Zhang, 2004) has tackled the bootstrapping approach from both the theoretical and practical point of view, many key problems still remain unresolved, such as the selection of initial seed set.</S>
			<S sid ="16" ssid = "16">Since the size of the initial seed set is usually small (e.g. 1437 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437–1445, Singapore, 67 August 2009.</S>
			<S sid ="17" ssid = "17">Qc 2009 ACL and AFNLP 100 instances), the imbalance of relation types or manifold structure (cluster structure) in it will severely weaken the strength of bootstrapping.</S>
			<S sid ="18" ssid = "18">Therefore, it is critical for a bootstrapping approach to select the most appropriate initial seed set.</S>
			<S sid ="19" ssid = "19">However, current systems (Zhang, 2004; Chen et al., 2006) use a randomly sampling strategy, which fails to explore the affinity nature among the training instances.</S>
			<S sid ="20" ssid = "20">Alternatively, Zhou et al.</S>
			<S sid ="21" ssid = "21">(2008) bootstrap a set of weighted support vectors from both labeled and unlabeled data using SVM.</S>
			<S sid ="22" ssid = "22">Nevertheless, the initial labeled data is still randomly generated only to ensure that there are at least 5 instances for every relation subtype.</S>
			<S sid ="23" ssid = "23">This paper presents a new approach to selecting the initial seed set based on stratified sampling strategy in the bootstrapping procedure for semi-supervised semantic relation classification.</S>
			<S sid ="24" ssid = "24">The motivation behind the stratified sampling is that every relation type should be as much as possible represented in the initial seed set, thus leading to more instances with diverse structures being added to the labeled set.</S>
			<S sid ="25" ssid = "25">In addition, we also explore different strategies to augment reliably classified instances to the labeled data iteratively, and attempt to find a stoppage criterion for the iteration procedure to greatly decrease the training time, other than using up all the unlabeled set.</S>
			<S sid ="26" ssid = "26">The rest of this paper is organized as follows.</S>
			<S sid ="27" ssid = "27">First, Section 2 reviews related work on semi- supervised relation extraction.</S>
			<S sid ="28" ssid = "28">Then we present an underlying supervised learner in Section 3.</S>
			<S sid ="29" ssid = "29">Section 4 details various key aspects of the bootstrapping procedure, including the stratified sampling strategy.</S>
			<S sid ="30" ssid = "30">Experimental results are reported in Section 5.</S>
			<S sid ="31" ssid = "31">Finally we conclude our work in Section 6.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="32" ssid = "1">Within the realm of information extraction, currently there are several representative semi- supervised learning systems for extracting relations between named entities.</S>
			<S sid ="33" ssid = "2">DIPRE (Dual Iterative Pattern Relation Expansion) (Brin, 1998) is a system based on bootstrapping that exploits the duality between patterns and relations to augment the target relation starting from a small sample.</S>
			<S sid ="34" ssid = "3">However, it only extracts simple relations such as (author, title) pairs from the WWW.</S>
			<S sid ="35" ssid = "4">Snowball (Agichtein and Gravano, 2000) is another bootstrapping- based system that extracts relations from unstructured text.</S>
			<S sid ="36" ssid = "5">Snowball shares much in common with DIPRE, including the use of both the bootstrapping framework and the pattern matching approach to extract new unlabeled instances.</S>
			<S sid ="37" ssid = "6">Due to pattern matching techniques, their systems are hard to be adapted to the general problem of relation extraction.</S>
			<S sid ="38" ssid = "7">Zhang (2004) approaches the relation classification problem with bootstrapping on top of SVM.</S>
			<S sid ="39" ssid = "8">He uses various lexical and syntactic features in the BootProject algorithm based on random feature projection to extract top-level relation types in the ACE corpus.</S>
			<S sid ="40" ssid = "9">Evaluation shows that bootstrapping can alleviate the burden of hand annotations for supervised learning methods to a certain extent.</S>
			<S sid ="41" ssid = "10">Chen et al.</S>
			<S sid ="42" ssid = "11">(2006) investigate a semi- supervised learning algorithm based on label propagation for relation extraction, where labeled and unlabeled examples and their distances are represented as the nodes and the weights of edges respectively in a connected graph, then the label information is propagated from any vertex to nearby vertices through weighted edges iteratively, finally the labels of unlabeled examples are inferred after the propagation process converges.</S>
			<S sid ="43" ssid = "12">Zhou et al.</S>
			<S sid ="44" ssid = "13">(2008) integrate the advantages of SVM bootstrapping in learning critical instances and label propagation in capturing the manifold structure in both the labeled and unlabeled data, by first bootstrapping a moderate number of weighted support vectors through a co-training procedure from all the available data, and then applying label propagation algorithm via the bootstrapped support vectors.</S>
			<S sid ="45" ssid = "14">However, in most current systems, the initial seed set is selected randomly such that they may not adequately represent the inherent structure of unseen examples, hence the power of bootstrapping may be severely weakened.</S>
			<S sid ="46" ssid = "15">This paper presents a simple yet effective approach to generate the initial seed set by applying the stratified sampling strategy, originated from statistics theory.</S>
			<S sid ="47" ssid = "16">Furthermore, we try to employ the same stratified strategy to augment the labeled set.</S>
			<S sid ="48" ssid = "17">Finally, we attempt to find a reasonable criterion to terminate the iteration process.</S>
	</SECTION>
	<SECTION title="Underlying Supervised Learning. " number = "3">
			<S sid ="49" ssid = "1">A semi-supervised learning system usually consists of two relevant components: an underlying supervised learner and a bootstrapping algorithm on top of it.</S>
			<S sid ="50" ssid = "2">In this section we discuss the former, while the latter will be described in the following section.</S>
			<S sid ="51" ssid = "3">In this paper, we select Support Vector Machines (SVMs) as the underlying supervised classifier since it represents the state-of-the-art in the machine learning research community, and there are good implementations of the algorithm available.</S>
			<S sid ="52" ssid = "4">Specifically, we use LIBSVM (Chang et al., 2001), an effective tool for support vector classification, since it supports multi-class classification and provides probability estimation as well.</S>
			<S sid ="53" ssid = "5">For each pair of entity mentions, we extract and compute various lexical and syntactic features, as employed in a state-of-the-art relation extraction system (Zhou et al., 2005).</S>
			<S sid ="54" ssid = "6">(1) Words: According to their positions, four categories of words are considered: a) the words of both the mentions; b) the words between the two mentions; c) the words before M1; and d) the words after M2.</S>
			<S sid ="55" ssid = "7">straightforwardly trained from previously available labeled data as follows: Algorithm self-bootstrapping Require: labeled seed set L Require: unlabeled data set U Require: batch size S Repeat Train a single classifier on L Run the classifier on U Find at most S instances in U that the classifier has the highest prediction confidence Add them into L Until: no data points available or the stoppage condition is reached Figure 1.</S>
			<S sid ="56" ssid = "8">Self-bootstrapping algorithm In order to measure the confidence of the classifier’s prediction, we compute the entropy of the label probability distribution that the classifier assigns to the class label on an example (the lower the entropy, the higher the confidence): n (2) Entity type: This category of features concerns about the entity types of both the H = −∑ pi log pi i (1) mentions.</S>
			<S sid ="57" ssid = "9">(3) Mention Level: This category of features considers the entity level of both the mentions.</S>
			<S sid ="58" ssid = "10">(4) Overlap: This category of features includes the number of other mentions and words between two mentions.</S>
			<S sid ="59" ssid = "11">Typically, the overlap features are usually combined with other features such as entity type and mention level.</S>
			<S sid ="60" ssid = "12">(5) Base phrase chunking: The base phrase chunking is proved to play an important role in semantic relation extraction.</S>
			<S sid ="61" ssid = "13">Most of the chunking features concern about the headwords of the phrases between the two mentions.</S>
			<S sid ="62" ssid = "14">In this paper, we do not employ any deep syntactic or semantic features (such as dependency tree, full parse tree etc.), since they contribute quite limited in relation extraction.</S>
	</SECTION>
	<SECTION title="Bootstrapping &amp; Stratified Sampling. " number = "4">
			<S sid ="63" ssid = "1">We first present the self-bootstrapping algorithm, and then discuss several key problems on bootstrapping in the order of initial seed selection, augmentation of labeled data and stoppage criterion for iteration.</S>
			<S sid ="64" ssid = "2">4.1 Bootstrapping Algorithm.</S>
			<S sid ="65" ssid = "3">Following Zhang (2004), we define a basic self- bootstrapping strategy, which keeps augmenting the labeled data set with the models Where n denotes the total number of relation classes, and pi denotes the probability of current example being classified as the ith class.</S>
			<S sid ="66" ssid = "4">4.2 Stratified Sampling for Initial Seeds.</S>
			<S sid ="67" ssid = "5">Normally, the number of available labeled instances is quite limited (usually less than 100 instances) when the iterative bootstrapping procedure begins.</S>
			<S sid ="68" ssid = "6">If the distribution of the initial seed set fails to approximate the distribution of the test data, the augmented data generated from bootstrapping would not capture the essence of relation types, and the performance on the test set will significantly decrease even only after one or two rounds of iterations.</S>
			<S sid ="69" ssid = "7">Therefore, the selection of initial seed set plays an important role in bootstrapping-based semantic relation extraction.</S>
			<S sid ="70" ssid = "8">Sampling is a part of statistical practice concerned with the selection of individual observations, which is intended to yield some knowledge about a population of interest.</S>
			<S sid ="71" ssid = "9">When dealing with the task of semi-supervised semantic relation classification, the population is the training set of relation instances from the ACE RDC corpora.</S>
			<S sid ="72" ssid = "10">We compare two practical sampling strategies as follows: (1) Randomly sampling, which picks the initial seeds from the training data using a random scheme.</S>
			<S sid ="73" ssid = "11">Each element thus has an equal probability of selection, and the population is not subdivided or partitioned.</S>
			<S sid ="74" ssid = "12">Currently, most work on semi-supervised relation extraction employs this method.</S>
			<S sid ="75" ssid = "13">However, since the size of the initial seed set is very small, they are not guaranteed to capture the statistical properties of the whole training data, let alone of the test data.</S>
			<S sid ="76" ssid = "14">(2) Stratified sampling.</S>
			<S sid ="77" ssid = "15">When the population embraces a number of distinct categories, stratified sampling (Neyman, 1934) can be applied to this case.</S>
			<S sid ="78" ssid = "16">First, the population can be organized by these categories into separate &quot;strata&quot;, then a sample is selected within each &quot;stratum&quot; separately, and randomly.</S>
			<S sid ="79" ssid = "17">Generally, the sample size is normally proportional to the relative size of the strata.</S>
			<S sid ="80" ssid = "18">The main motivation for using a stratified sampling design is to ensure that particular groups within a population are adequately represented in the sample.</S>
			<S sid ="81" ssid = "19">It is well known that the number of the instances for each relation type in the ACE RDC corpora is greatly unbalanced (Zhou et al., 2005) as shown in Table 1 for the ACE RDC 2004 corpus.</S>
			<S sid ="82" ssid = "20">When the relation instances for a specific relation type occurs frequently in the initial seed set, the classifier will achieve good performance on this type, otherwise the classifier can hardly recognize them from the test set.</S>
			<S sid ="83" ssid = "21">In order for every type of relations to be properly represented, the stratified sampling strategy is applied to the seed selection procedure.</S>
			<S sid ="84" ssid = "22">Table 1.</S>
			<S sid ="85" ssid = "23">Numbers of relations on the ACE RDC 2004: break down by relation types and subtypes Figure 2 illustrates the stratified sampling strategy we use in bootstrapping, where RSET denotes the training set, V is the stratification variable, and SeedSET denotes the initial seed set.</S>
			<S sid ="86" ssid = "24">First, we divide the relation instances into different strata according to available properties, such as major relation type (considering reverse relations or not) and relation subtype (considering reverse relations or not).</S>
			<S sid ="87" ssid = "25">Then within every stratum, a certain number of instances are sampled randomly, and this number is normally proportional to the size of that stratum in the whole population.</S>
			<S sid ="88" ssid = "26">However, when this number is 0 due to the rounding of real numbers, it is set to 1.</S>
			<S sid ="89" ssid = "27">Also it must be ensured that the total number of instances being sampled is NS.</S>
			<S sid ="90" ssid = "28">Finally, these instances form the initial seed set and can be used as the input to the underlying supervised learning for the bootstrapping procedure.</S>
			<S sid ="91" ssid = "29">Require: RSET ={R1,R2,…,RN} Require: V = {v1, v2,…,vK} Require: SeedSET with the size of NS (100) Initialization: SeedSET = NULL Steps: z Group RSET into K strata according to the stratified variable V, i.e.: RSET={RSET1,RSET2,…,RSETK} z Calculate the class prior probability for each stratum i={1,2,…,K} Pi = NUM (RSETi ) / NUM (RSET ) z Caculate the number of intances being sampled for each stratum Ni = Pi ∗ N If Ni =0 then Ni=1 z Calculate the difference of numbers as follows: K N∆ = NS − ∑ Ni i =1 z If N△&gt;0 then add Ni (i=1,2,…,|N△|) by 1 If N△&lt;0 then subtract 1 from Ni (i=1,2,...,|N△|) z For each i from 1 to K Select Ni instances from RESTi randomly Add them into SeedSET Figure 2.</S>
			<S sid ="92" ssid = "30">Stratefied Sampling for initial seeds 4.3 Augmentation of labeled data.</S>
			<S sid ="93" ssid = "31">After each round of iteration, some newly classified instances with the highest confidence can be augmented to the labeled training data.</S>
			<S sid ="94" ssid = "32">Nevertheless, just like the selection of initial seed set, we still wish that every stratum would be represented as appropriately as possible in the instances added to the labeled set.</S>
			<S sid ="95" ssid = "33">In this paper, we compare two kinds of augmentation strategies available: (1) Top n method: the classified instances are first sorted in the ascending order by their entropies (i.e. decreasing confidence), and then the top n (usually 100) instances are chosen to be added.</S>
			<S sid ="96" ssid = "34">(2) Stratified method: in order to make the added instances representative for their stratum, we first select m (usually greater than n) instances with the highest confidence, then we choose n instances from them using the stratified strategy.</S>
			<S sid ="97" ssid = "35">4.4 Stoppage of Iterations.</S>
			<S sid ="98" ssid = "36">In a self-bootstrapping procedure, as the iterations go on, both the reliable and unreliable instances are added to the labeled data continuously, hence the performance will fluctuate in a relatively small range.</S>
			<S sid ="99" ssid = "37">The key question here is how we can know when the bootstrapping procedure reaches its best performance on the test data.</S>
			<S sid ="100" ssid = "38">The bootstrapping algorithm by Zhang (2004) stops after it runs out of all the training instances, which may take a relatively long time.</S>
			<S sid ="101" ssid = "39">In this paper, we present a method to determine the stoppage criterion based on the mean entropy as follows: Hi &lt;= p (2) Where Hi denotes the mean entropy of the confidently classified instances being augmented to the labeled data in each iteration, and p denotes a threshold for the mean entropy, which will be fixed through empirical experiments.</S>
			<S sid ="102" ssid = "40">This criterion is based on the assumption that when the mean entropy becomes less than or equal to a certain threshold, the classifier would achieve the most reliable confidence on the instances being added to the labeled set, and it may be impossible to yield better performance since then.</S>
			<S sid ="103" ssid = "41">Therefore, the iteration may stop at that reasonable point.</S>
	</SECTION>
	<SECTION title="Experimentation. " number = "5">
			<S sid ="104" ssid = "1">This section aims to empirically investigate the effectiveness of the bootstrapping-based semi- supervised learning we discussed above for semantic relation classification.</S>
			<S sid ="105" ssid = "2">In particular, different methods for selecting the initial seed set and augmenting the labeled data are evaluated.</S>
			<S sid ="106" ssid = "3">5.1 Experimental Setting.</S>
			<S sid ="107" ssid = "4">We use the ACE corpora as the benchmark data, which are gathered from various newspapers, newswire and broadcasts.</S>
			<S sid ="108" ssid = "5">The ACE 2004 corpus contains 451 documents and 5702 positive relation instances.</S>
			<S sid ="109" ssid = "6">It defines 7 relation types and 23 subtypes between 7 entity types.</S>
			<S sid ="110" ssid = "7">For easy reference with related work in the literature, evaluation is also done on 347 documents (including nwire and bnews domains) and 4305 relation instances using 5-fold cross-validation.</S>
			<S sid ="111" ssid = "8">That is, these relation instances are first divided into 5 sets, then, one of them (about 860 instances) is used as the test data set, while the others are regarded as the training data set, from which the initial seed set is sampled.</S>
			<S sid ="112" ssid = "9">In the ACE 2003 corpus, the training set consists of 674 documents and 9683 positive relation instances while the test data consists of 97 documents and 1386 positive relation instances.</S>
			<S sid ="113" ssid = "10">The ACE RDC 2003 task defines 5 relation types and 24 subtypes between 5 entity types.</S>
			<S sid ="114" ssid = "11">The corpora are first parsed using Collins’s parser (Collins, 2003) with the boundaries of all the entity mentions kept.</S>
			<S sid ="115" ssid = "12">Then, the parse trees are converted into chunklink format using chunklink.pl 1.</S>
			<S sid ="116" ssid = "13">Finally, various useful lexical and syntactic features, as described in Subsection 3.1, are extracted and computed accordingly.</S>
			<S sid ="117" ssid = "14">For the purpose of comparison, we define our task as the classification of the 5 or 7 major relation types in the ACE RDC 2003 and 2004 corpora.</S>
			<S sid ="118" ssid = "15">For LIBSVM parameters, we adopted the polynomial kernel, and c is set to 10, g is set to 0.15.</S>
			<S sid ="119" ssid = "16">Under this setting, we achieved the best classification performance.</S>
			<S sid ="120" ssid = "17">5.2 Experimental Results.</S>
			<S sid ="121" ssid = "18">In this subsection, we compare and discuss the experimental results using various sampling strategies, different augmentation methods, and iteration stoppage criterion.</S>
			<S sid ="122" ssid = "19">Comparison of sampling strategies in selecting the initial seed set Table 2 and Table 3 show the initial and the highest classification performance of Precision/Recall/F-measure for various sampling strategies of the initial seed set on 7 major relation types of the ACE RDC 2004 corpus respectively when the size of initial seed set L is 100, the batch size S is 100, and the top 100 1 http://ilk.kub.nl/~sabine/chunklink/ instances with the highest confidence are added at each iteration.</S>
			<S sid ="123" ssid = "20">Table 2 also lists the number of strata for stratified sampling methods from which the initial seeds are randomly chosen respectively.</S>
			<S sid ="124" ssid = "21">Table 3 additionally lists the time needed to complete the bootstrapping process (on a PC with a Pentium IV 3.0G CPU and 1G memory).</S>
			<S sid ="125" ssid = "22">In this paper, we consider the following five experimental settings when sampling the initial seeds: z Randomly Sampling: as described in Subsection 4.2.</S>
			<S sid ="126" ssid = "23">z Stratified-M Sampling: the strata are grouped in terms of major relation types without considering reverse relations.</S>
			<S sid ="127" ssid = "24">z Stratified-MR Sampling: the strata are grouped in terms of major relation types, including reverse relations.</S>
			<S sid ="128" ssid = "25">z Stratified-S Sampling: the strata are grouped in terms of relation subtypes without considering reverse subtypes.</S>
			<S sid ="129" ssid = "26">z Stratified-SR Sampling: the strata are grouped in terms of relation subtypes, including reverse subtypes.</S>
			<S sid ="130" ssid = "27">For each sampling strategies, we performed 20 trials and computed average scores and the total time on the test set over these 20 trials.</S>
			<S sid ="131" ssid = "28">Table 2.</S>
			<S sid ="132" ssid = "29">The initial performance of applying various sampling strategies to selecting the initial seed set on the ACE RDC 2004 corpus Table 3.</S>
			<S sid ="133" ssid = "30">The highest performance of applying various sampling strategies in selecting the initial seed set on the ACE RDC 2004 corpus These two tables jointly indicate that the self- bootstrapping procedure for all sampling strategies can moderately improve the classification performance by ~1.2 units in F- score, which is also verified by Zhang (2004).</S>
			<S sid ="134" ssid = "31">Furthermore, they show that: z The most improvements in performance come from improvements in precision.</S>
			<S sid ="135" ssid = "32">Actually, for some settings the recalls even decrease slightly.</S>
			<S sid ="136" ssid = "33">The reason may be that due to the nature of self-bootstrapping, the instances augmented at each iteration are always those which are the most similar to the initial seed instances, therefore the models trained from them would exhibit higher precision on the test set, while it virtually does no help for recall.</S>
			<S sid ="137" ssid = "34">z All of the four stratified sampling methods outperform the randomly sampling method to various degrees, both in the initial performance and the highest performance.</S>
			<S sid ="138" ssid = "35">This means that sampling of the initial seed set based on stratification by major/sub relation types can be helpful to relation classification, largely due to the performance improvement of the initial seed set, which is caused by adequate representation of instances for every relation type.</S>
			<S sid ="139" ssid = "36">z Of all the four stratified sampling methods, the Stratified-SR sampling achieves the best performance of 72.9/68.4/70.6 in P/R/F. Moreover, the more the number of strata generated by the sampling strategy, the more appropriately they would be represented in the initial seed set, and the better performance it will yield.</S>
			<S sid ="140" ssid = "37">This also implies that the hierarchy of relation types/subtypes in the ACE RDC 2004 corpus is fairly reasonably defined.</S>
			<S sid ="141" ssid = "38">z An important conclusion, which can be draw accordingly, is that the F-score improvement of Stratified-SR sampling over Randomly sampling in initial performance (3.3 units) is significantly greater than the F-score improvement gained by bootstrapping itself using Randomly sampling (1.4 units).</S>
			<S sid ="142" ssid = "39">This means that the sampling strategy of the initial seed set is even more important than the bootstrapping algorithm itself for relation classification.</S>
			<S sid ="143" ssid = "40">z It is interesting to note that the time needed to bootstrap increases with the number of strata.</S>
			<S sid ="144" ssid = "41">The reason may be that due to more diverse structures in the labeled data for stratified sampling, the SVM needs more time to differentiate between instances, i.e. more time to learn the models.</S>
			<S sid ="145" ssid = "42">Comparison of different augmentation strategies of training data Figure 3 compares the performance of F-score for two augmentation strategies: the Top n method and the stratified method, over various initial seed sampling strategies on the ACE RDC 2004 corpus.</S>
			<S sid ="146" ssid = "43">For each iteration, a variable number (m is ranged from 100 to 500) of classified instances in the decreasing order of confidence are first chosen as the base examples, then at most 100 examples are selected from the base examples to be augmented to the labeled set.</S>
			<S sid ="147" ssid = "44">Specifically, when m is equal to 100, the whole set of the base example is added to the labeled data, i.e. degenerated to the Top n augmentation strategy.</S>
			<S sid ="148" ssid = "45">On the other hand, when m is greater than 100, we wish we would select examples of different major relation types from the base examples according to their distribution in the training set, in order to achieve the performance improvement as much as the stratified sampling does in the selection of the initial seed set.</S>
			<S sid ="149" ssid = "46">probability of extracting instances of more relation types increases with the increase of the number of the base examples.</S>
			<S sid ="150" ssid = "47">These two factors inversely interact with each other, leading to the fluctuation in performance.</S>
			<S sid ="151" ssid = "48">Comparison of different threshold values for stoppage criterion We compare the performance and bootstrapping time (20 trials with the same initial seed set) when applying stoppage criterion in Formula (2) with different threshold p over various sampling strategies on the ACE RDC 2004 corpus in Figure 4 and Figure 5 respectively.</S>
			<S sid ="152" ssid = "49">These two figures jointly show that: 72 Randomly.</S>
			<S sid ="153" ssid = "50">71 70 Stratified-M.</S>
			<S sid ="154" ssid = "51">69 Stratified-MR. 68 Stratified-S.</S>
			<S sid ="155" ssid = "52">67 Stratified-SR. 66 65 71 70 69 68 67 Randomly.</S>
			<S sid ="156" ssid = "53">64 66 Stratified-M Stratified-MR 65 Stratified-S.</S>
			<S sid ="157" ssid = "54"># Base examples Figure 3.</S>
			<S sid ="158" ssid = "55">Comparison of two augmentation strategies over different sampling strategies in selecting the initial seed set.</S>
			<S sid ="159" ssid = "56">This figure shows that, except for randomly sampling strategy, the stratified augmentation strategies improve the performance.</S>
			<S sid ="160" ssid = "57">Nevertheless, this result is far from our expectation in two ways: z The performance improvement in F-score is trivial, at most 0.4 units on average.</S>
			<S sid ="161" ssid = "58">The reason may be that, although we try to add as many as 100 classified instances to the labeled data according to the distribution of every major relation type in the training set, the top m instances with the highest confidence are usually focused on certain relation types (e.g. PHSY and PER-SOC), this leads to the stratified augmentation failing to function effectively.</S>
			<S sid ="162" ssid = "59">Hence, all the following experiments will only adopt Top n method for augmenting the labeled data.</S>
			<S sid ="163" ssid = "60">z With the increase of the number of the base examples, the performance fluctuates slightly, thus it is relatively difficult to recognize where the optima is. We think there are two contradictory factors that affect the performance.</S>
			<S sid ="164" ssid = "61">While the reliability of the instances extracted from the base examples decreases with the increase of the number of base examples, the 64 Stratified-SR. 0 0.2 0.22 0.24 0.26 0.28 0.3 p Figure 4.</S>
			<S sid ="165" ssid = "62">Performance for different p values 90 80 Randomly.</S>
			<S sid ="166" ssid = "63">70 Stratified-M.</S>
			<S sid ="167" ssid = "64">60 Stratified-MR. 50 Stratified-S.</S>
			<S sid ="168" ssid = "65">40 Stratified-SR. 30 20 10 0 0 0.2 0.22 0.24 0.26 0.28 0.3 p Figure 5.</S>
			<S sid ="169" ssid = "66">Bootstrapping time for different p values z The performance decreases slowly while the bootstrapping time decreases dramatically with the increase of p from 0 to 0.3.</S>
			<S sid ="170" ssid = "67">Specifically, when the p equals to 0.3, the bootstrapping time tends to be neglected, while the performance is almost similar to the initial performance.</S>
			<S sid ="171" ssid = "68">It implies that we can find a reasonable point for each sampling strategy, at which the time falls greatly while the performance nearly does not degrade.</S>
			<S sid ="172" ssid = "69">Re lati on typ es Boo tpro ject L P j s Stratified Bootstrap ping P R F P R F P R F R O LE 7 8 . 5 6 9 . 7 7 3 . 8 8 1 . 0 7 4 . 7 7 7 . 7 7 4 . 7 8 6 . 3 8 0 . 1 PA RT 6 5 . 6 3 4 . 1 4 4 . 9 7 0 . 1 4 1 . 6 5 2 . 2 6 6 . 4 4 7 . 0 5 5 . 0 A T 6 1 . 0 8 4 . 8 7 0 . 9 7 4 . 2 7 9 . 1 7 6 . 6 7 4 . 9 6 6 . 1 7 0 . 2 N E A R 1 3 . 7 1 2 . 5 1 3 . 0 1 0 0 . 0 2 . 9 5 . 6 SO C 4 7 . 0 5 7 . 4 5 1 . 7 4 5 . 0 5 9 . 1 5 1 . 0 6 5 . 2 7 9 . 0 7 1 . 4 Av era ge 6 7 . 9 6 7 . 4 6 7 . 6 7 3 . 6 6 9 . 4 7 0 . 9 7 3 . 8 7 3 . 3 7 3 . 5 Table 4.</S>
			<S sid ="173" ssid = "70">Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus z Clearly, if the performance is the primary concern, then p=0.2 may be the best choice in that we can get ~30% saving on the time at the cost of only ~0.08 loss in F-score on average.</S>
			<S sid ="174" ssid = "71">If the time is a primary concern, then p=0.22 is a reasonable threshold in that we get ~50% saving on the time at the cost of ~0.25 units loss in F- score on average.</S>
			<S sid ="175" ssid = "72">This suggests that our proposed stoppage criterion is effective to terminate the bootstrapping procedure with minor performance loss.</S>
			<S sid ="176" ssid = "73">Comparison of Stratified Bootstrapping with Bootproject and Label propagation Table 4 compares Bootproject (Zhang, 2004), Label propagation (Chen et al., 2006) with our Stratified Bootstrapping on the 5 major types of the ACE RDC 2003 corpus.</S>
			<S sid ="177" ssid = "74">Both Bootproject and Label propagation select 100 initial instances randomly, and at each iteration, the top 100 instances with the highest confidence are added to the labeled data.</S>
			<S sid ="178" ssid = "75">Differently, we choose 100 initial seeds using stratified sampling strategy; similarly, the top 100 instances with the highest confidence are augmented to the labeled data at each iteration.</S>
			<S sid ="179" ssid = "76">Due to the lack of comparability followed from the different size of the labeled data used in (Zhou et al., 2008), we omit their results here.</S>
			<S sid ="180" ssid = "77">outperforms both Bootproject and Label Propagation methods on the ACE RDC corpus, with the increase of 5.9/4.1 units in F-score on average respectively.</S>
			<S sid ="181" ssid = "78">Stratified bootstrapping consistently outperforms Bootproject in every major relation type, while it outperforms Label Propagation in three of the major relation types, especially SOC type, with the exception of AT and NEAR types.</S>
			<S sid ="182" ssid = "79">The reasons may be follows.</S>
			<S sid ="183" ssid = "80">Although there are many AT relation instances in the corpus, they are scattered divergently in multi-dimension space so that they tend to be relatively difficult to be recognized via SVM.</S>
			<S sid ="184" ssid = "81">For the NEAR relation instances, they occur least frequently in the whole corpus, so it is very hard for them to be identified via SVM.</S>
			<S sid ="185" ssid = "82">By contrast, even small size of labeled instances can be fully utilized to correctly induce the unlabeled instances via LP algorithm due to its ability to exploit manifold structures of both labeled and unlabeled instances (Chen et al., 2006).</S>
			<S sid ="186" ssid = "83">In general, these results again suggest that the sampling strategy in selecting the initial seed set plays a critical role for relation classification, and stratified sampling can significantly improve the performance due to proper selection of the initial seed set.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="187" ssid = "1">This paper explores several key issues in semi- supervised learning based on bootstrapping for semantic relation classification.</S>
			<S sid ="188" ssid = "2">The application of stratified sampling originated from statistics theory to the selection of the initial seed set contributes most to the performance improvement in the bootstrapping procedure.</S>
			<S sid ="189" ssid = "3">In addition, the more strata the training data is divided into, the better performance will be achieved.</S>
			<S sid ="190" ssid = "4">However, the augmentation of the labeled data using the stratified strategy fails to function effectively largely due to the unbalanced distribution of the confidently classified instances, rather than the stratified sampling strategy itself.</S>
			<S sid ="191" ssid = "5">Furthermore, we also propose a mean entropy-based stoppage criterion in the bootstrapping procedure, which can significantly decrease the training time with little loss in performance.</S>
			<S sid ="192" ssid = "6">Finally, it also shows that our method outperforms other state-of-the-art semi-supervised ones.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="193" ssid = "7">This research is supported by Project 60673041 and 60873150 under the National Natural Science Foundation of China, Project 2006AA01Z147 under the “863” National High- Tech Research and Development of China, Project BK2008160 under the Jiangsu Natural Science Foundation of China, and the National Research Foundation for the Doctoral Program of Higher Education of China under Grant No. 20060285008.</S>
			<S sid ="194" ssid = "8">We would also like to thank the excellent and insightful comments from the three anonymous reviewers.</S>
	</SECTION>
</PAPER>
