<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Recent kernel-based PPI extraction systems achieve promising performance because of their capability to capture structural syntactic information, but at the expense of computational complexity.</S>
		<S sid ="2" ssid = "2">This paper incorporates dependency information as well as other lexical and syntactic knowledge in a feature-based framework.</S>
		<S sid ="3" ssid = "3">Our motivation is that, considering the large amount of biomedical literature being archived daily, feature-based methods with comparable performance are more suitable for practical applications.</S>
		<S sid ="4" ssid = "4">Additionally, we explore the difference of lexical characteristics between biomedical and newswire domains.</S>
		<S sid ="5" ssid = "5">Experimental evaluation on the AIMed corpus shows that our system achieves comparable performance of 54.7 in F1-Score with other state-of-the-art PPI extraction systems, yet the best performance among all the feature-based ones.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">In recent years, automatically extracting biomedical information has been the subject of significant research efforts due to the rapid growth in biomedical development and discovery.</S>
			<S sid ="7" ssid = "7">A wide concern is how to characterize protein interaction partners since it is crucial to understand not only the functional role of individual proteins but also the organization of the entire biological process.</S>
			<S sid ="8" ssid = "8">However, manual collection of relevant Protein-Protein Interaction (PPI) information from thousands of research papers published every day is so time-consuming that automatic extraction approaches with the help of Natural Language Processing (NLP) techniques become necessary.</S>
			<S sid ="9" ssid = "9">Various machine learning approaches for relation extraction have been applied to the biomedical domain, which can be classified into two categories: feature-based methods (Mitsumori et al., 2006; Giuliano et al., 2006; Sætre et al., 2007) and kernel-based methods (Bunescu et al., 2005; Erkan et al., 2007; Airola et al., 2008; Kim et al., 2010).</S>
			<S sid ="10" ssid = "10">Provided a large-scale manually annotated corpus, the task of PPI extraction can be formulated as a classification problem.</S>
			<S sid ="11" ssid = "11">Typically, for featured-based learning each protein pair is represented as a vector whose features are extracted from the sentence involving two protein names.</S>
			<S sid ="12" ssid = "12">Early studies identify the existence of protein interactions by using “bag-of-words” features (usually uni-gram or bi-gram) around the protein names as well as various kinds of shallow linguistic information, such as POS tag, lemma and orthographical features.</S>
			<S sid ="13" ssid = "13">However, these systems do not achieve promising results since they disregard any syntactic or semantic information altogether, which are very useful for the task of relation extraction in the newswire domain (Zhao and Grishman, 2005; Zhou et al., 2005).</S>
			<S sid ="14" ssid = "14">Furthermore, feature-based methods fail to effectively capture the structural information, which is essential to * Corresponding author 757 Coling 2010: Poster Volume, pages 757–765, Beijing, August 2010 With the wide application of kernel-based methods to many NLP tasks, various kernels such as subsequence kernels (Bunescu and Mooney, 2005) and tree kernels (Li et al., 2008), are also applied to PPI detection..</S>
			<S sid ="15" ssid = "15">Particularly, dependency-based kernels such as edit distance kernels (Erkan et al., 2007) and graph kernels (Airola et al., 2008; Kim et al., 2010) show some promising results for PPI extraction.</S>
			<S sid ="16" ssid = "16">This suggests that dependency information play a critical role in PPI extraction as well as in relation extraction from newswire stories (Culotta and Sorensen, 2004).</S>
			<S sid ="17" ssid = "17">In order to appreciate the advantages of both feature-based methods and kernel-based methods, composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b) are further employed to combine structural syntactic information with flat word features and significantly improve the performance of PPI extraction.</S>
			<S sid ="18" ssid = "18">However, one critical challenge for kernel-based methods is their computation complexity, which prevents them from being widely deployed in real-world applications regarding the large amount of biomedical literature being archived everyday.</S>
			<S sid ="19" ssid = "19">Considering the potential of dependency information for PPI extraction and the challenge of computation complexity of kernel-based methods, one may naturally ask the question: “Can the essential dependency information be maximally exploited in featured-based PPI extraction so as to enhance the performance without loss of efficiency?” “If the answer is Yes, then How?” This paper addresses these problems, focusing on the application of dependency information to feature-based PPI extraction.</S>
			<S sid ="20" ssid = "20">Starting from a baseline system in which common lexical and syntactic features are incorporated using Support Vector Machines (SVM), we further augment the baseline with various features related to dependency information, including predicates in the dependency tree.</S>
			<S sid ="21" ssid = "21">Moreover, in order to reveal the linguistic difference between distinct domains we also compare the effects of various features on PPI extraction from biomedical texts with those on relation extraction from newswire narratives.</S>
			<S sid ="22" ssid = "22">Evaluation on the AIMed and other PPI cor The rest of the paper is organized as follows.</S>
			<S sid ="23" ssid = "23">A feature-based PPI extraction baseline system is given in Section 2 while Section 3 describes our dependency-driven method.</S>
			<S sid ="24" ssid = "24">We report our experiments in Section 4, and compare our work with the related ones in Section 5.</S>
			<S sid ="25" ssid = "25">Section 6 concludes this paper and gives some future directions.</S>
	</SECTION>
	<SECTION title="Feature-based	PPI 	extraction:. " number = "2">
			<S sid ="26" ssid = "1">Baseline For feature-based methods, PPI extraction task is recast as a classification problem by first transforming PPI instances into multidimensional vectors with various features, and then applying machine learning approaches to detect whether the potential relationship exists for a particular protein pair.</S>
			<S sid ="27" ssid = "2">In training, a feature-based classifier learning algorithm, such as SVM or MaxEnt, uses the annotated PPI instances to learn a classifier while, in testing, the learnt classifier is in turn applied to new instances to determine their PPI binary classes and thus candidate PPI instances are extracted.</S>
			<S sid ="28" ssid = "3">As a baseline, various linguistic features, such as words, overlap, chunks, parse tree features as well as their combined ones are extracted from a sentence and formed as a vector into the feature-based learner.</S>
			<S sid ="29" ssid = "4">1) Words Four sets of word features are used in our system: 1) the words of both the proteins; 2) the words between the two proteins; 3) the words before M1 (the 1st protein); and 4) the words after M2 (the 2nd protein).</S>
			<S sid ="30" ssid = "5">Both the words before M1 and after M2 are classified into two bins: the first word next to the proteins and the second word next to the proteins.</S>
			<S sid ="31" ssid = "6">This means that we only consider the two words before M1 and after M2.</S>
			<S sid ="32" ssid = "7">Words features include: x MW1: bag-of-words in M1 x MW2: bag-of-words in M2 x BWNULL: when no word in between x BWO: other words in between except first and last words when at least three words in between x BWM1FL: the only word before M1 x BWM1: first and second word before M1 x BWM2FL: the only word after M2 x BWM2F: first word after M2 x BWM2L: second word after M2 x BWM2: first and second word after M2 2) Overlap The numbers of other protein names as well as the words that appear between two protein names are included in the overlap features.</S>
			<S sid ="33" ssid = "8">This category of features includes: x #MB: number of other proteins in between x #WB: number of words in between x E-Flag: flag indicating whether the two proteins are embedded or not 3) Chunks It is well known that chunking plays an important role in the task of relation extraction in the ACE program (Zhou et al., 2005).</S>
			<S sid ="34" ssid = "9">However, its significance in PPI extraction has not fully investigated.</S>
			<S sid ="35" ssid = "10">Here, the Stanford Parser1 is first employed for full parsing, and then base phrase chunks are derived from full parse trees using the Perl script2 . The chunking features usually concern about the head words of the phrases between the two proteins, which are further classified into three bins: the first phrase head in between, the last phrase head in between and other phrase heads in between.</S>
			<S sid ="36" ssid = "11">In addition, the path of phrasal labels connecting two proteins is also a common syntactic indicator of the polarity of the PPI instance, just as the path NP_VP_PP_NP in the sentence “The ability of PROT1 to interact with the PROT2 was investigated.” is likely to suggest the positive interaction between two proteins.</S>
			<S sid ="37" ssid = "12">These base phrase chunking features contain: x CPHBNULL: when no phrase in between.</S>
			<S sid ="38" ssid = "13">x CPHBFL: the only phrase head when only one phrase in between x CPHBF: the first phrase head in between when at least two phrases in between.</S>
			<S sid ="39" ssid = "14">1 http://nlp.stanford.edu/software/lex-parser.shtml 2 http://ilk.kub.nl/~sabine/chunklink/ tween.</S>
			<S sid ="40" ssid = "15">x CPHBO: other phrase heads in between except first and last phrase heads when at least three phrases in between.</S>
			<S sid ="41" ssid = "16">x CPP: path of phrase labels connecting the two entities in the chunking Furthermore, we also generate a set of bi-gram features which combine the above chunk features except CPP with their corresponding chunk types.</S>
			<S sid ="42" ssid = "17">4) Parse Tree It is obvious that full pares trees encompass rich structural information of a sentence.</S>
			<S sid ="43" ssid = "18">Nevertheless, it is much harder to explore such information in featured-based methods than in kernel-based ones.</S>
			<S sid ="44" ssid = "19">Thus so far only the path connecting two protein names in the full-parse tree is considered as a parse tree feature.</S>
			<S sid ="45" ssid = "20">x PTP: the path connecting two protein names in the full-parse tree.</S>
			<S sid ="46" ssid = "21">Again, take the sentence “The ability of PROT1 to interact with the PROT2 was investigated.” as an example, the parse path between PROT1 and PROT2 is NP_S_VP_PP_NP, which is slightly different from the CPP feature in the chunking feature set.</S>
	</SECTION>
	<SECTION title="Dependency-Driven PPI Extraction. " number = "3">
			<S sid ="47" ssid = "1">The potential of dependency information for PPI extraction lies in the fact that the dependency tree may well reveal non-local or long-range dependencies between the words within a sentence.</S>
			<S sid ="48" ssid = "2">In order to capture the necessary information inherent in the depedency tree for identifying their relationship, various kernels, such as edit distance kernel based on dependency path (Erkan et al., 2007), all-dependency-path graph kernel (Airola et al., 2008), and walk-weighted subsequence kernels (Kim et al., 2010) as well as other composite kernels (Miyao et al., 2008; Miwa et al., 2009a; Miwa et al., 2009b), have been proposed to address this problem.</S>
			<S sid ="49" ssid = "3">It’s true that these methods achieve encouraging results, neverthless, they suffer from prohibitive computation burden.</S>
			<S sid ="50" ssid = "4">dependency information back into flat features in a feature-based framework so as to speed up the learning process while retaining nsubj ccomp comparable performance.</S>
			<S sid ="51" ssid = "5">This is what we refer to as dependency-driven PPI extraction.</S>
			<S sid ="52" ssid = "6">First, we construct dependency trees from grammatical relations generated by the Stanford Parser.</S>
			<S sid ="53" ssid = "7">Every grammatical relation has the form of dependent-type (word1, word2), PROT1 nsubj motif binds prep_to PROT2 Where word1 is the head word, word2 is dependent on word1, and dependent-type denotes the predefined type of dependency.</S>
			<S sid ="54" ssid = "8">Then, from these grammatical relations the following features called DependenecySet1 are taken into consideration as illustrated in Figure 1: x DP1TR: a list of words connecting PROT1 and the dependency tree root.</S>
			<S sid ="55" ssid = "9">x DP2TR: a list of words connecting PROT2 and the dependency tree root.</S>
			<S sid ="56" ssid = "10">x DP12DT: a list of dependency types connecting the two proteins in the dependency tree.</S>
			<S sid ="57" ssid = "11">x DP12: a list of dependent words combined with their dependency types connecting the two proteins in the dependency tree.</S>
			<S sid ="58" ssid = "12">x DP12S: the tuple of every word combined with its dependent type in DP12.</S>
			<S sid ="59" ssid = "13">x DPFLAG: a boolean value indicating whether the two proteins are directly dependent on each other.</S>
			<S sid ="60" ssid = "14">The typed dependencies produced by the Stanford Parser for the sentence “PROT1 contains a sequence motif binds to PROT2.” are listed as follows: nsubj(contains-2,PROT11) det(motif-5, a-3) nn(motif-5, sequence-4) nsubj(binds-6, motif-5) ccomp(contains-2, binds-6) prep_to(binds-6, PROT28)Each word in a dependency tuple is fol lowed by its index in the original sentence, ensuring accurate positioning of the head word and dependent word.</S>
			<S sid ="61" ssid = "15">Figure 1 shows the dependency tree we construct from the above grammatical relations.</S>
			<S sid ="62" ssid = "16">det nn a sequence Figure 1: Dependency tree for the sentence “PROT1 contains a sequence motif binds to PROT2.” Erkan et al.</S>
			<S sid ="63" ssid = "17">(2007) extract the path information between PROT1 and PROT2 in the dependency tree for kernel-based PPI extraction and report promising results, neverthless, such path is so specific for feature-based methods that it may incure higher precision but lower recall.</S>
			<S sid ="64" ssid = "18">Thus we alleviate this problem by collapsing the feature into multiple ones with finer granularity, leading to the features such as DP12S.</S>
			<S sid ="65" ssid = "19">It is widely acknowledged that predicates play an important role in PPI extraction.</S>
			<S sid ="66" ssid = "20">For example, the change of a pivot predicate between two proteins may easily lead to the polarity reversal of a PPI instance.</S>
			<S sid ="67" ssid = "21">Therefore, we extract the predicates and their positions in the dependency tree as predicate features called DependencySet2: x FVW: the predicates in the DP12 feature occurring prior to the first protein.</S>
			<S sid ="68" ssid = "22">x LVW: the predicates in the DP12 feature occurring next to the second entity.</S>
			<S sid ="69" ssid = "23">x MVW: other predicates in the DP12 features.</S>
			<S sid ="70" ssid = "24">x #FVW: the number of FVW x #LVW: the number of LVW x #MVW: the number of MVW</S>
	</SECTION>
	<SECTION title="Experimentation. " number = "4">
			<S sid ="71" ssid = "1">This section systematically evaluates our feature-based method on the AIMed corpus as well as other commonly used corpus and reports our experimental results.</S>
			<S sid ="72" ssid = "2">4.1 Data Sets.</S>
			<S sid ="73" ssid = "3">We use five corpora3 with the AIMed corpus as the main experimental data, which contains 177 Medline abstracts with interactions be-.</S>
			<S sid ="74" ssid = "4">tween two interactions, and 48 abstracts without any PPI within single sentences.</S>
			<S sid ="75" ssid = "5">There are 4,084 protein references and around 1,000 annotated interactions in this data set.</S>
			<S sid ="76" ssid = "6">For corpus pre-procession, we first rename two proteins of a pair as PROT1 and PROT2 respectively in order to blind the learner for fair comparison with other work.</S>
			<S sid ="77" ssid = "7">Then, all the instances are generated from the sentences which contain at least two proteins, that is, if a sentence contains n different proteins, there are (n )different pairs of proteins and these pairs are considered untyped and undirected.</S>
			<S sid ="78" ssid = "8">For the purpose of comparison with previous work, all the self-interactions (59 instances) are removed, while all the PPI instances with nested protein names are retained (154 instances).</S>
			<S sid ="79" ssid = "9">Finally, 1002 positive instances and 4794 negative instances are generated and their corresponding features are extracted.</S>
			<S sid ="80" ssid = "10">We select Support Vector Machines (SVM) as the classifier since SVM represents the state-of-the-art in the machine learning research community.</S>
			<S sid ="81" ssid = "11">In particular, we use the binary-class SVMLigh 4 developed by Joachims (1998) since it satisfies our requirement of detecting potential PPI instances.</S>
			<S sid ="82" ssid = "12">Evaluation is done using 10-fold document-level cross-validation.</S>
			<S sid ="83" ssid = "13">Particularly, we apply the extract same 10-fold split that was used by Bunescu et al.</S>
			<S sid ="84" ssid = "14">(2005) and Giuliano et al.</S>
			<S sid ="85" ssid = "15">(2006).</S>
			<S sid ="86" ssid = "16">Furthermore, OAOD (One Answer per Occurrence in the Document) strategy is adopted, which means that the correct interaction must be extracted for each occurrence.</S>
			<S sid ="87" ssid = "17">This guarantees the maximal use of the available data, and more important, allows fair comparison with earlier relevant work.</S>
			<S sid ="88" ssid = "18">The evaluation metrics are commonly used Precision (P), Recall (R) and harmonic F1-score (F1).</S>
			<S sid ="89" ssid = "19">As an alternative to F1-score, the AUC (area under the receiver operating characteristics curve) measure is proved to be invariant to the class distribution of the training dataset.</S>
			<S sid ="90" ssid = "20">Thus we also provide AUC scores 3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html 4 http://svmlight.joachims.org/ for our system as Airola et al.</S>
			<S sid ="91" ssid = "21">(2008) and Miwa et al.</S>
			<S sid ="92" ssid = "22">(2009a).</S>
			<S sid ="93" ssid = "23">4.2 Results and.</S>
			<S sid ="94" ssid = "24">Discussion Features P(%) R(%) F1 Baseline features Words 59.4 40.6 47.6 +Overlap 60.4 39.9 47.4 +Chunk 59.2 44.5 50.6 +Parse 60.9 44.8 51.4 Dependency-driven features +Dependency Set1 62.9 48.0 53.9 +Dependency Set2 63.4 48.8 54.7 Table 1: Performance of PPI extraction with various features in the AIMed corpus We present in Table 1 the performance of our system using document-wise evaluation strategies and 10-fold cross-validation with different features in the AIMed corpus, where the plus sign before a feature means it is incrementally added to the feature set.</S>
			<S sid ="95" ssid = "25">Table 1 reports that our system achieves the best performance of 63.4/48.8/54.7 in P/R/F scores.</S>
			<S sid ="96" ssid = "26">It also shows that: x Words features alone achieve a relatively low performance of 59.4/40.9/47.6 in P/R/F, particularly with fairly low recall score.</S>
			<S sid ="97" ssid = "27">This suggests the difficulty of PPI extraction and words features alone can’t effectively capture the nature of protein interactions.</S>
			<S sid ="98" ssid = "28">x Overlap features slightly decrease the performance.</S>
			<S sid ="99" ssid = "29">Statistics show that both the distributions of #MB and #WB between positives and negatives are so similar that they are by no means the discriminators for PPI extraction.</S>
			<S sid ="100" ssid = "30">Hence, we exclude the overlap features in the succeeding experiments.</S>
			<S sid ="101" ssid = "31">x Chunk features significantly improves the F-measure by 3 units largely due to the increase of recall by 3.9%, though at the slight expense of precision.</S>
			<S sid ="102" ssid = "32">This suggests the effectiveness of shallow parsing information in the form of headwords captured by chunking on PPI extraction.</S>
			<S sid ="103" ssid = "33">x The usefulness of the parse tree features is quite limited.</S>
			<S sid ="104" ssid = "34">It only improves the F-measure by 0.8 units.</S>
			<S sid ="105" ssid = "35">The main reason may be that these paths are usually long and specific, thus they suffer from the problem of data sparsity.</S>
			<S sid ="106" ssid = "36">Furthermore, some of the parse tree features are already involved in the chunk features.</S>
			<S sid ="107" ssid = "37">x The DependencySet1 features are very effective in that it can increase the preci Giuliano et al., 20065 60.9 57.2 59.0 sion and recall by 2.0 and 3.2 units respectively, leading to the increase of F1 score by 2.5 units.</S>
			<S sid ="108" ssid = "38">This means that the dependency-related features can effectively retrieve more PPI instances without introducing noise that will severely harm the precision.</S>
			<S sid ="109" ssid = "39">According to our statistics, there are over 60% sentences with more than 5 words between their protein entities in the AIMed corpus.</S>
			<S sid ="110" ssid = "40">Therefore, dependency information exhibit great potential to PPI extraction since they can capture long-range dependencies within sentences.</S>
			<S sid ="111" ssid = "41">Take the aforementioned sentence “PROT1 contains a sequence motif binds to PROT2.” as an example, although the two proteins step over a relatively long distance, the dependency path between them is concise and accurate, reflecting the essence of the interaction.</S>
			<S sid ="112" ssid = "42">x The predicate features also contribute to the F1-score gain of 0.8 units.</S>
			<S sid ="113" ssid = "43">It is not surprising since some predicates, such as “interact”, “activate” and “inhibit” etc, are strongly suggestive of the interaction polarity between two proteins.</S>
			<S sid ="114" ssid = "44">We compare in Table 2 the performance of our system with other systems in the AIMed corpus using the same 10-fold cross validation strategy.</S>
			<S sid ="115" ssid = "45">These systems are grouped into three distinct classes: feature-based, kernel-based and composite kernels.</S>
			<S sid ="116" ssid = "46">Except for Airola et al.</S>
			<S sid ="117" ssid = "47">(2008) Miwa et al.</S>
			<S sid ="118" ssid = "48">(2009a) and Kim et al.</S>
			<S sid ="119" ssid = "49">(2010), which adopt graph kernels, our system performs comparably with other systems.</S>
			<S sid ="120" ssid = "50">In particular, our dependency-driven system achieves the best F1-score of 54.7 among all feature-based systems.</S>
			<S sid ="121" ssid = "51">In order to measure the generalization ability of our dependency-driven PPI extraction Sætre et al., 2007 64.3 44.1 52.0 Mitsumori et al., 2006 54.2 42.6 47.7 Yakushiji et al., 2005 33.7 33.1 33.4 Kernel-based methods Kim et al., 2010 61.4 53.3 56.7 Airola et al., 2008 52.9 61.8 56.4 Bunescu et al., 2006 65.0 46.4 54.2 Composite kernels Miwa et al., 2009a - - 62.0 Miyao et al., 20086 51.8 58.1 54.5 Table 2: Comparison with other PPI extraction systems in the AIMed corpus The corresponding performance of F1-score and AUC metrics as well as their standard deviations is present in Table 3.</S>
			<S sid ="122" ssid = "52">Comparative available results from Airola et al.</S>
			<S sid ="123" ssid = "53">(2008) and Miwa et al.</S>
			<S sid ="124" ssid = "54">(2009a) are also included in Table 3 for comparison.</S>
			<S sid ="125" ssid = "55">This table shows that our system performs almost consistently with the other two systems, that is, the LLL corpus gets the best performance yet with the greatest variation, while the AIMed corpus achieves the lowest performance with reasonable variation.</S>
			<S sid ="126" ssid = "56">It is well known that biomedical texts exhibit distinct linguistic characteristics from newswire narratives, leading to dramatic performance gap between PPI extraction and relation detection in the ACE corpora.</S>
			<S sid ="127" ssid = "57">However, no previous work has ever addressed this problem and empirically characterized this difference.</S>
			<S sid ="128" ssid = "58">In this paper, we devise a series of experiments over the ACE RDC corpora using our dependency-driven feature-based method as a touchstone task.</S>
			<S sid ="129" ssid = "59">In order to do that, a sub system across different corpora, we further apply our method to other four publicly available PPI corpora: BioInfer, HPRD50, IEPA and LLL.</S>
	</SECTION>
	<SECTION title="Airola et al. (2008) repeat the method published. " number = "5">
			<S sid ="130" ssid = "1">by Giuliano et al.</S>
			<S sid ="131" ssid = "2">(2006) with a correctly preprocessed AIMed and reported an F1-score of 52.4%.</S>
	</SECTION>
	<SECTION title="The results from Table 1 (Miyao et al., 2009) with. " number = "6">
			<S sid ="132" ssid = "1">the most similar settings to ours (Stanford Parser with SD representation) are reported.</S>
			<S sid ="133" ssid = "2">set of 5796 relation instances is randomly sampled from the ACE 2003 and 2004 corpora respectively.</S>
			<S sid ="134" ssid = "3">The same cross-validation and evaluation metrics are applied to these two sets as PPI extraction in the AIMed corpus.</S>
			<S sid ="135" ssid = "4">O u r s y s t e m A i r o l a e t a l .</S>
			<S sid ="136" ssid = "5">( 2 0 0 8 ) 7 M i w a e t a l .</S>
			<S sid ="137" ssid = "6">( 2 0 0 9 a ) C or pu s F 1 ıF1 AUC ıAUC F 1 ıF1 AUC ıAUC F 1 ıF1 AUC ıAUC AI M ed Bi oI nf er H P R D 50 I E P A L L L 54 .7 4.5 82.4 3.5 59 .8 3.5 80.9 3.3 64 .9 13.4 79.8 8.5 62 .1 6.2 74.8 6.6 78 .1 15.8 85.1 8.3 56 .4 5.0 84.8 2.3 61 .3 5.3 81.9 6.5 63 .4 11.4 79.7 6.3 75 .1 7.0 85.1 5.1 76 .8 17.8 83.4 12.2 60 .8 6.6 86.8 3.3 68 .1 3.2 85.9 4.4 70 .9 10.3 82.2 6.3 71 .7 7.8 84.4 4.2 80.</S>
			<S sid ="138" ssid = "7">1 14.1 86.3 10.8 Table 3: Comparison of performance across the five PPI corpora Fe at ur es A I M e d A C E 2 0 0 3 A C E 2 0 0 4 P( % ) R(%) F1 P( % ) R(%) F1 P( % ) R(%) F1 W or ds + O ve rla p + C hu nk +P ar se + D ep en de nc y Se t1 + D ep en de nc y Se t2 5 9.</S>
			<S sid ="139" ssid = "8">4 40.6 47.6 + 1.</S>
			<S sid ="140" ssid = "9">00.70.2 1.</S>
			<S sid ="141" ssid = "10">7 +4.6 +3.2 + 1.</S>
			<S sid ="142" ssid = "11">7 +0.3 +0.8 + 2.</S>
			<S sid ="143" ssid = "12">0 +3.2 +2.5 + 0.</S>
			<S sid ="144" ssid = "13">5 +0.8 +0.8 6 6.</S>
			<S sid ="145" ssid = "14">5 51.6 57.9 + 5.</S>
			<S sid ="146" ssid = "15">4 +1.8 +3.2 + 2.</S>
			<S sid ="147" ssid = "16">3 +5.1 +4.0 + 0.</S>
			<S sid ="148" ssid = "17">3 +0.6 +0.5 + 0.</S>
			<S sid ="149" ssid = "18">8 +0.7 +0.7 + 0.</S>
			<S sid ="150" ssid = "19">3 +0.2 +0.3 6 8.</S>
			<S sid ="151" ssid = "20">1 59.6 63.4 + 4.</S>
			<S sid ="152" ssid = "21">6 +1.2 +2.7 + 1.</S>
			<S sid ="153" ssid = "22">5 +1.9 +1.7 + 0.</S>
			<S sid ="154" ssid = "23">6 +0.4 +0.5 + 0.</S>
			<S sid ="155" ssid = "24">5 +0.9 +0.7 + 0.</S>
			<S sid ="156" ssid = "25">2 +0.4 +0.3 Table 4: Comparison of contributions of different features to relation detection across multiple domains Table 4 compares the performance of our method over different domains.</S>
			<S sid ="157" ssid = "26">The table reports that the words features alone achieve the best F1-score of 63.4 in ACE2004 but the lowest F1-score of 47.6 in AIMed.</S>
			<S sid ="158" ssid = "27">This suggests the wide difference of lexical distribution between these domains.</S>
			<S sid ="159" ssid = "28">We extract the words appearing before the 1st mention, between the two mentions and after the 2nd mention from the training sets of these corpora respectively, and summarize the statistics (the number of tokens, the number of occurrences) in Table 5, where the KL divergence between positives and negatives is summed over the distribution of the 500 most frequently occurring words.</S>
			<S sid ="160" ssid = "29">Statistics AIMed ACE2003 ACE2004 # of tokens 2,340 2,064 2,099 # of occurrences 69,976 53,744 49,570 KL divergence 0.22 0.28 0.33 Table 5: Lexical statistics on three corpora The table shows that AIMed uses the most kinds of words and the most words around the two mentions than the other two.</S>
			<S sid ="161" ssid = "30">More important, AIMed has the least distribution difference between the words appearing in positives and negatives, as indicated by its least KL divergence.</S>
			<S sid ="162" ssid = "31">Therefore, the lexical words in AIMed are less discriminative for relation detection than they do in the other two.</S>
			<S sid ="163" ssid = "32">This naturally explains the reason why the performance by words feature alone is AIMed&lt;ACE2003&lt;ACE2004.</S>
			<S sid ="164" ssid = "33">In addition, Table 4 also shows that: x The overlap features significantly improve the performance in ACE while slightly deteriorating that in AIMed.</S>
			<S sid ="165" ssid = "34">The reason is that, as indicated in Zhou et al.</S>
			<S sid ="166" ssid = "35">(2005), most of the positive relation instances in ACE exist in local contexts, while the positive interactions in AIMed occur in relative long range just as the negatives, therefore these features are not discriminative for AIMed.</S>
			<S sid ="167" ssid = "36">x The chunk features consistently greatly boost the performance across multiple corpora.</S>
			<S sid ="168" ssid = "37">This implies that the headwords in chunk phrases can well capture the partial nature of relation instances regardless of their genre.</S>
			<S sid ="169" ssid = "38">x It’s not surprising that the parse feature attain moderate performance gain in all domains since these parse paths are usually 7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa et.</S>
			<S sid ="170" ssid = "39">al.</S>
			<S sid ="171" ssid = "40">(2009b) long and specificity, leading to data sparseness problem.</S>
			<S sid ="172" ssid = "41">x It is interesting to note that the dependency-related features exhibit more significant improvement in AIMed than that in ACE.</S>
			<S sid ="173" ssid = "42">The reason may be that, these dependency features can effectively capture long-range relationships prevailing in AIMed, while in ACE a large number of local relationships dominate the corpora.</S>
			<S sid ="174" ssid = "43">5 Related Work.</S>
			<S sid ="175" ssid = "44">Among feature-based methods, the PreBIND system (Donaldson et al., 2003) uses words and word bi-grams features to identify the existence of protein interactions in abstracts and such information is used to enhance manual expert reviewing for the BIND database.</S>
			<S sid ="176" ssid = "45">Mitsumori et al.</S>
			<S sid ="177" ssid = "46">(2006) use SVM to extract protein-protein interactions, where bag-of-words features, specifically the words around the protein names, are employed.</S>
			<S sid ="178" ssid = "47">Sugiyama et al.</S>
			<S sid ="179" ssid = "48">(2003) extract various features from the sentences based on the verbs and nouns in the sentences such as the verbal forms, and the part-of-speech tags of the 20 words surrounding the verb.</S>
			<S sid ="180" ssid = "49">In addition to word features, Giuliano et al.</S>
			<S sid ="181" ssid = "50">(2006) extract shallow linguistic information such as POS tag, lemma, and orthographic features of tokens for PPI extraction.</S>
			<S sid ="182" ssid = "51">Unlike our dependency-driven method, these systems do not consider any syntactic information.</S>
			<S sid ="183" ssid = "52">For kernel-based methods, there are several systems which utilize dependency information.</S>
			<S sid ="184" ssid = "53">Erkan et al.</S>
			<S sid ="185" ssid = "54">(2007) defines similarity functions based on cosine similarity and edit distance between dependency paths, and then incorporate them in SVM and KNN learning for PPI extraction.</S>
			<S sid ="186" ssid = "55">Airola et al.</S>
			<S sid ="187" ssid = "56">(2008) introduce all-dependency-paths graph kernel to capture the complex dependency relationships between lexical words and attain significant performance boost at the expense of computational complexity.</S>
			<S sid ="188" ssid = "57">Kim et al.</S>
			<S sid ="189" ssid = "58">(2010) adopt walk-weighted subsequence kernel based on dependency paths to explore various substructures such as e-walks, partial match, and non-contiguous paths.</S>
			<S sid ="190" ssid = "59">Essentially, their kernel is also a graph-based one.</S>
			<S sid ="191" ssid = "60">For composite kernel methods, Sætre et al.</S>
			<S sid ="192" ssid = "61">(2007) combine a “bag-of-words” kernel with dependency and PAS (Predicate Argument Structure) tree kernels to exploit both the words features and the structural syntactic information.</S>
			<S sid ="193" ssid = "62">Hereafter, Miyao et al.</S>
			<S sid ="194" ssid = "63">(2008) investigate the contribution of various syntactic features using different representations from dependency parsing, phrase structure parsing and deep parsing by different parsers.</S>
			<S sid ="195" ssid = "64">Miwa et al.</S>
			<S sid ="196" ssid = "65">(2009a) integrate “bag-of-words” kernel, PAS tree kernel and all-dependency-paths graph kernel to achieve the higher performance.</S>
			<S sid ="197" ssid = "66">They (Miwa et al., 2009b) also use similar composite kernels for corpus weighting learning across multiple PPI corpora.</S>
			<S sid ="198" ssid = "67">6 Conclusion and Future Work.</S>
			<S sid ="199" ssid = "68">In this paper, we have combined various lexical and syntactic features, particularly dependency information, into a feature-based PPI extraction system.</S>
			<S sid ="200" ssid = "69">We find that the dependency information as well as the chunk features contributes most to the performance improvement.</S>
			<S sid ="201" ssid = "70">The predicate features involved in the dependency tree can also moderately enhance the performance.</S>
			<S sid ="202" ssid = "71">Furthermore, comparative study between biomedical domain and the ACE newswire domain shows that these domains exhibit different lexical characteristics, rendering the task of PPI extraction much more difficult than that of relation detection from the ACE corpora.</S>
			<S sid ="203" ssid = "72">In future work, we will explore more syntactic features such as PAS information for feature-based PPI extraction to further boost the performance.</S>
	</SECTION>
	<SECTION title="Acknowledgment">
			<S sid ="204" ssid = "73">This research is supported by Projects 60873150 and 60970056 under the National Natural Science Foundation of China and Project BK2008160 under the Natural Science Foundation of Jiangsu, China.</S>
			<S sid ="205" ssid = "74">We are also very grateful to Dr. Antti Airola from Truku University for providing partial experimental materials .</S>
	</SECTION>
</PAPER>
