<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the BioNLP 2011 shared tasks.</S>
		<S sid ="2" ssid = "2">Bacteria, geographical locations and host entities were processed by a pattern-based approach and domain lexical resources.</S>
		<S sid ="3" ssid = "3">For the extraction of environment locations, we propose a framework based on semantic analysis supported by an ontology of the biotope domain.</S>
		<S sid ="4" ssid = "4">Domain-specific rules were developed for dealing with Bacteria anaphora.</S>
		<S sid ="5" ssid = "5">Official results show that our Alvis system achieves the best performance of participating systems.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Given a set of Web pages, the information extraction goal of the Bacteria Biotope (BB) task is to precisely identify bacteria and their locations and to relate them.</S>
			<S sid ="7" ssid = "7">The type of the predicted locations has to be selected among eight types.</S>
			<S sid ="8" ssid = "8">Among them the host and host-part locations have to be related by the part-of relation.</S>
			<S sid ="9" ssid = "9">Three teams participated in the challenge.</S>
			<S sid ="10" ssid = "10">BB task example Ureaplasma parvum is a mycoplasma and a pathogenic biology challenges (Kim et al., 2010) and geographical locations (Zhou et al., 2005).</S>
			<S sid ="11" ssid = "11">Locations include natural environments and hosts as well as food and medical locations.</S>
			<S sid ="12" ssid = "12">In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) terminoontology.</S>
			<S sid ="13" ssid = "13">This mapping derives the type of location terms and filters out non-location terms.</S>
			<S sid ="14" ssid = "14">Large external dictionaries of host names (i.e. NCBI taxonomy) and geographical names (i.e. Agrovoc thesaurus) complete the lexical resources.</S>
			<S sid ="15" ssid = "15">The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty.</S>
			<S sid ="16" ssid = "16">Our Alvis system implements an anaphora resolution algorithm that takes into consideration the anaphoric distance and the position of the antecedent in the sentence.</S>
			<S sid ="17" ssid = "17">Alvis predicts the bacteria names and their relation to the locations with the help of handmade patterns based on linguistic analysis and lexical resources.</S>
			<S sid ="18" ssid = "18">The methods for predicting and typing locations (section 2) and bacteria (section 3) are first described.</S>
			<S sid ="19" ssid = "19">Section 4 details the method for relating them.</S>
			<S sid ="20" ssid = "20">Section 5 comments the experimental results.</S>
	</SECTION>
	<SECTION title="Location. " number = "2">
			<S sid ="21" ssid = "1">Our system handles separately the recognition of host and geographical names by dictionary mappings, while the recognition of locations of the environment and host part types is based on linguistic analysis and ontology inference.</S>
			<S sid ="22" ssid = "2">Localization ureolytic mollicute which colonises Host names and geographic al names appeared to be easier to predict by using a named entity recog the urogenital tracts of humans.</S>
			<S sid ="23" ssid = "3">Part-of One of the specificities of the BB task is that the bacteria location vocabulary is very large and various as opposed to protein subcellular locations in nition strategy than the other types of location.</S>
			<S sid ="24" ssid = "4">They are less subject to variation than environmental locations, which can include any physical feature.</S>
			<S sid ="25" ssid = "5">For host name extraction, we used the NCBI taxonomy as the major source.</S>
			<S sid ="26" ssid = "6">Only the eukaryote subtree was considered for host detection.</S>
			<S sid ="27" ssid = "7">102 Proceedings of BioNLP Shared Task 2011 Workshop, pages 102â€“111, Portland, Oregon, USA, 24 June, 2011.</S>
			<S sid ="28" ssid = "8">Qc 2011 Association for Computational Linguistics Our system filters out the ambiguous names such as Indicator (honeyguides) or Dialysis (xylophage insect) by comparing them to a list of common words in English.</S>
			<S sid ="29" ssid = "9">The host name list was enriched with additional common names including non- taxonomic host groups (e.g. herbivores), progeny names (e.g. calf) and human categories (e.g. patient).</S>
			<S sid ="30" ssid = "10">The resulting host name list contains more than 1,800,000 scientific names and 60,000 common names.</S>
			<S sid ="31" ssid = "11">The geographical name recognition component uses a small dictionary of all geographic terms from the Agrovoc thesaurus sub- vocabularies.</S>
			<S sid ="32" ssid = "12">At first, we considered using the very rich resource GeoNames.</S>
			<S sid ="33" ssid = "13">However, it contains too many ambiguous names to be directly usable by short-term development.</S>
			<S sid ="34" ssid = "14">2.1 Location of Environment type.</S>
			<S sid ="35" ssid = "15">The identification of environment locations is done in two steps.</S>
			<S sid ="36" ssid = "16">First, the automatic extraction of all candidate terms from the test corpus, then the assignment of a location type to these terms with the help of the Bacteria Biotope (BB) terminoontology.</S>
			<S sid ="37" ssid = "17">The type assigned to a given term is the type of the closest concept label in the ontology.</S>
			<S sid ="38" ssid = "18">Since the BB terminoontology was originally not structured according to the eight types, in order to be usable it first had to be enriched by the new concepts and then mapped to this topology.</S>
			<S sid ="39" ssid = "19">Corpus term extraction.</S>
			<S sid ="40" ssid = "20">The corpus terms were automatically extracted by the AlvisNLP/ML pipeline (Nedellec et al., 2008) with BioYatea (Nedellec et al., 2010).</S>
			<S sid ="41" ssid = "21">BioYatea is the version of Yatea (Hamon &amp; Aubin, 2006) adapted to the biology domain.</S>
			<S sid ="42" ssid = "22">We modified BioYatea setting according to the training dataset study.</S>
			<S sid ="43" ssid = "23">We observed that most of the location terms in the training dataset are noun phrases with adjective modifiers (e.g. rodent nests) while prepositional phrases are rather rare (e.g. breaks in the skin).</S>
			<S sid ="44" ssid = "24">We set the term boundaries of BioYatea to include all prepositions except the of preposition.</S>
			<S sid ="45" ssid = "25">Considering other prepositions such as with may yield syntactic attachment errors, thus we prefer the risk of incomplete terms to incorrect prepositional attachments.</S>
			<S sid ="46" ssid = "26">Bacteria Biotope ontology.</S>
			<S sid ="47" ssid = "27">We used the Bacteria Biotope (BB) terminoontology for typing the extracted terms.</S>
			<S sid ="48" ssid = "28">It is under development for the study of bacteria phenotypes and habitats.</S>
			<S sid ="49" ssid = "29">The high level of the habitat part is structured in a manner similar to that proposed by the one level classifica tion by Floyd (Floyd et al., 2005).</S>
			<S sid ="50" ssid = "30">It has a fine- grained structure with the same goal as the general- ist EnvO habitat ontology (Field et al., 2008), but it focuses on bacteria phenotype and biotope modeling.</S>
			<S sid ="51" ssid = "31">It includes a terminological level that records lexical forms of the concepts including terms, synonyms and variations.</S>
			<S sid ="52" ssid = "32">For the purpose of the challenge, the initial on- tology was manually completed using location concepts.</S>
			<S sid ="53" ssid = "33">The training corpus, as well as the habitat and isolation site fields of the GOLD database on sequenced prokaryotes (Liolios et al., 2009) are the main sources of location terms and synonyms.</S>
			<S sid ="54" ssid = "34">The analysis of the training corpus mainly led to the addition of adjectival forms of host parts (e.g. lymphatic, intracellular) and human references (e.g. patient, infant, progeny).</S>
			<S sid ="55" ssid = "35">The GOLD database isolation site field is a very rich source of bacteria location terms.</S>
			<S sid ="56" ssid = "36">It is filled by natural language descriptions of matters, natural habitats, hosts and geographical locations.</S>
			<S sid ="57" ssid = "37">For instance, the isolation site of Anoxybacillus flavithermus bacterium is waste water drain at the Wairakei geothermal power station in New Zea- land.</S>
			<S sid ="58" ssid = "38">The term analysis of GOLD isolation site entries yielded 3,415 location terms including 1,050 geographical names.</S>
			<S sid ="59" ssid = "39">Hundreds of these terms were manually added to the BB terminoontology.</S>
			<S sid ="60" ssid = "40">The lack of time as well as the full sentence structure of the GOLD resource prevented us from correctly handling them in a fully automatic way.</S>
			<S sid ="61" ssid = "41">We are currently developing a method for the automatic alignment of the terms extracted from GOLD to the BB terminoontology.</S>
			<S sid ="62" ssid = "42">Additionally, the GOLD habitat field provided around a hundred different terms that have been directly integrated into the BB terminoontology.</S>
			<S sid ="63" ssid = "43">The current version of the habitat subpart of the BB terminoontology contains 1,247 concepts and 266 synonyms.</S>
			<S sid ="64" ssid = "44">Location types in Bacteria Biotope ontology.</S>
			<S sid ="65" ssid = "45">The BB terminoontology has been developed previous to the BB task and the structure of its habitat subpart does not reflect the eight location types of the task.</S>
			<S sid ="66" ssid = "46">In order to reuse the ontology for the BB task, we assigned types to each location concept.</S>
			<S sid ="67" ssid = "47">We manually associated the high level nodes of the location hierarchies to the eight BB task types.</S>
			<S sid ="68" ssid = "48">The types of the lower level concepts were then automatically inferred.</S>
			<S sid ="69" ssid = "49">For instance, the concept aquatic environment is tagged Water in the ontol ogy and all of its descendants lake, sea, ocean are of type Water as well.</S>
			<S sid ="70" ssid = "50">Local type exceptions were manually tagged.</S>
			<S sid ="71" ssid = "51">For instance, the waste tree includes water-carried wastes of type Water and solid industrial residues of type Environment.</S>
			<S sid ="72" ssid = "52">This way all concepts in the resulting typed ontology were assigned a unique type.</S>
			<S sid ="73" ssid = "53">The concept types are then propagated to their associated term classes at the terminological level.</S>
			<S sid ="74" ssid = "54">For instance, underground water and its synonym subterranean water are both typed as Water.</S>
			<S sid ="75" ssid = "55">The resulting typed BB terminoontology is then usable for deriving the types of the terms extracted from the test corpus.</S>
			<S sid ="76" ssid = "56">Derivation of location type.</S>
			<S sid ="77" ssid = "57">The BB terminoontology scope is too limited for the correct prediction of all candidate term types by Boolean and exact comparison.</S>
			<S sid ="78" ssid = "58">From the 2,290 candidate terms of the test corpus, only 152 belong as such to the BB terminoontology.</S>
			<S sid ="79" ssid = "59">We propose a method based on the head comparison of the candidate and BB terms for the derivation of the candidate term type.</S>
			<S sid ="80" ssid = "60">The quality of the ontology-based annotation depends to a large extent on an accurate match between the resource and the terms extracted from the corpus.</S>
			<S sid ="81" ssid = "61">Our method targets the syntactic structure of terms (candidate and BB terms) in order to gather the most of semantically similar terms.</S>
			<S sid ="82" ssid = "62">This approach differs from the ontology alignment and population methods that also use the information from the ontology structure in order to infer semantic relationships (e.g. hyponyms, meronyms) (Euzenat, 2007).</S>
			<S sid ="83" ssid = "63">It also differs from semantic annotation supported by context analysis such as distributional semantics (Grefenstette, 1994) or Hearst patterns (Hearst, 1992).</S>
			<S sid ="84" ssid = "64">It belongs to the class of methods that focus on the morphology of the corpus terms, which use string-based (Levensthein, 1966, Jaro, 1989) or linguistic-based methods (Jacquemin &amp; Tzoukermann, 1999).</S>
			<S sid ="85" ssid = "65">Even though the context-based approach should produce very good results, we chose a less time- consuming method that is easier and faster to set up, which is based on morphosyntactic analysis.</S>
			<S sid ="86" ssid = "66">In our case, string similarity measures turn out to be irrelevant (laboratory rat does not mean rat laboratory).</S>
			<S sid ="87" ssid = "67">We observed that in candidate and BB terms, the head is very often the most informative element.</S>
			<S sid ="88" ssid = "68">Thus, the linguistic-based analysis of terms, in particular the head-similarity analysis (Hamon &amp; Nazarenko, 2001), represents a promising alternative.</S>
			<S sid ="89" ssid = "69">Our method is inspired by MetaMap (Aronson, 2001).</S>
			<S sid ="90" ssid = "70">MetaMap tags bio- medical corpora with the UMLS Metathesaurus by syntactic analysis that takes into account lexical heads of terms.</S>
			<S sid ="91" ssid = "71">The similarity scores computed by linguistically-based metrics are higher for terms whose heads have previously been analyzed.The MetaMap method includes a variant compu tation that maps acronyms, abbreviations, synonyms as well as derivational, inflectional and spelling variants.</S>
			<S sid ="92" ssid = "72">Our term typing method is less sophisticated and uses a few lexical variants due to the lack of a complete resource.</S>
			<S sid ="93" ssid = "73">Some ontology enrichment applications also use head-supported term matching, as in Desmontils (Desmontils et all, 2003).</S>
			<S sid ="94" ssid = "74">In Desmontils, new concepts belonging to WordNet (Fellbaum, 1998) are automatically added to the ontology in order to improve the indexing process.</S>
			<S sid ="95" ssid = "75">However, the analysis of the results shows that a great number of concepts found in the texts are not considered because they do not exist in WordNet.</S>
			<S sid ="96" ssid = "76">Our typing task uses a similar head- based method, but only for type derivation.Our system derives the location type of candi date terms in several steps.</S>
			<S sid ="97" ssid = "77">First, if there is a term in the BB terminoontology that is strictly equal to the candidate term, it is assigned the same type.</S>
			<S sid ="98" ssid = "78">Then, the other candidate terms are assigned types according to the comparison of their heads to the BB term heads.</S>
			<S sid ="99" ssid = "79">We assume that in most of the cases the term head conveys the information about the type and is non-ambiguous.</S>
			<S sid ="100" ssid = "80">A given head H is non-ambiguous if all BB terms with head H are of the same type.</S>
			<S sid ="101" ssid = "81">The location term head set is the set of all habitat term heads found in the BB terminoontology.</S>
			<S sid ="102" ssid = "82">The current version contains 693 different heads.</S>
			<S sid ="103" ssid = "83">Let Te denote the extracted term to be typed.</S>
			<S sid ="104" ssid = "84">If the head of Te does not belong to the BB term head set, then the type of Te is simply not Location (e.g. high metabolic diversity).</S>
			<S sid ="105" ssid = "85">If Te head does belong to the BB term head set and the head is non-ambiguous, then Te is assigned the associated type.</S>
			<S sid ="106" ssid = "86">For instance, the head of the extracted term stratified lake is lake.</S>
			<S sid ="107" ssid = "87">The type of all the BB terms with lake head is Water (e.g. meromictic lake).</S>
			<S sid ="108" ssid = "88">Stratified lake is therefore typed as Water.</S>
			<S sid ="109" ssid = "89">Specific processing is applied to terms with ambiguous heads.</S>
			<S sid ="110" ssid = "90">The associative set of BB term heads and types exhibits some cases of ambiguous heads with multiple types that we analyzed in detail.</S>
			<S sid ="111" ssid = "91">There are two kinds of ambiguities that were processed in different ways.</S>
			<S sid ="112" ssid = "92">In the first, multiple types reflect different roles of the same object.</S>
			<S sid ="113" ssid = "93">In the second, the head is non-informative with respect to the type.</S>
			<S sid ="114" ssid = "94">In the latter case the type is conveyed by the subterm (term after head removal).</S>
			<S sid ="115" ssid = "95">We qualify non-informative BB term heads as neutral.</S>
			<S sid ="116" ssid = "96">They mainly denote habitats (habitat, environment, medium, zone) and extracts (sample, surface, isolate, material, content).</S>
			<S sid ="117" ssid = "97">In this case, the type is derived from the subterm.</S>
			<S sid ="118" ssid = "98">For instance, the head isolate of the extracted term marine isolate is neutral.</S>
			<S sid ="119" ssid = "99">After head removal, it is assigned the type Water since marine is of type Water.</S>
			<S sid ="120" ssid = "100">Freshwater has the same type as freshwater medium or freshwater environment since medium and environment are neutral heads.</S>
			<S sid ="121" ssid = "101">Some heads have more than one type although they denote specific locations.</S>
			<S sid ="122" ssid = "102">Their multiple types reflect different uses or states.</S>
			<S sid ="123" ssid = "103">For instance, the head bottle has two types: Food and Medical.</S>
			<S sid ="124" ssid = "104">The type Food is derived from the BB concept water bottle and the type Medical is derived from bedside water bottles in a hospital environment.</S>
			<S sid ="125" ssid = "105">The correct type for the extracted terms is then selected by a set of patterns based on the context of the term in the document.</S>
			<S sid ="126" ssid = "106">For instance, many vegetables and meats could be either of type Host or Food.</S>
			<S sid ="127" ssid = "107">The type is Host by default.</S>
			<S sid ="128" ssid = "108">One pattern states that if a term includes or is preceded by a food processing- related word (e.g. cooked, grilled, fermented), then the term is reassigned the type Food.</S>
			<S sid ="129" ssid = "109">Another pattern states that if a host is preceded by a death- related adjective (dead, decaying), then its type should be revised as Environment.</S>
			<S sid ="130" ssid = "110">Our system currently includes nine disambiguation/retyping patterns.</S>
			<S sid ="131" ssid = "111">The first version of the type derivation method was automatically applied to the 1,263 GOLD terms after head analysis.</S>
			<S sid ="132" ssid = "112">Manual examination of the results yielded an extension of the two lists of neutral heads and heads with ambiguous types.</S>
			<S sid ="133" ssid = "113">There are 20 neutral heads and 21 ambiguous heads in the current version of the BB terminoontology.</S>
			<S sid ="134" ssid = "114">The head-matching algorithm appears to be quite productive for the biotope terms.</S>
			<S sid ="135" ssid = "115">The procedure applied to the test corpus yielded the following figures: BioYatea extracted2,290 terms.</S>
			<S sid ="136" ssid = "116">416 terms matching the post processing filters were discarded.</S>
			<S sid ="137" ssid = "117">This includes terms which are too general (i.e. approach, diversity), terms containing irrelevant or non desirable adjectives (i.e. numerous deficiencies, known spe cies) and terms containing forbidden words according to the annotation location rules (i.e. bacteria, pathogen, contaminated, parasite).</S>
			<S sid ="138" ssid = "118">Finally, 1,873 candidate terms were kept.</S>
			<S sid ="139" ssid = "119">Among these figures: - 152 terms belong to the BB terminoontology - 90 terms were typed using the ontology heads - 6 terms with several types were handled by disambiguation patterns.We plan to extend the list of neutral heads and dis criminate adjectives for type disambiguation by machine learning classification applied to the BB terminoontology modifiers.</S>
			<S sid ="140" ssid = "120">Location entity boundary.</S>
			<S sid ="141" ssid = "121">The analysis of term extraction result from the training corpus shows that the predicted boundaries of locations were not fully consistent with the task annotation guidelines.</S>
			<S sid ="142" ssid = "122">Post-processing adjusts incorrect boundaries by filtering irrelevant words, packing and merging terms.</S>
			<S sid ="143" ssid = "123">Irrelevant words (e.g. contaminated, infected, host species, disease, inflammation) were removed from the location candidate terms independently of their types (e.g. contaminated Bach- man Road site vs. Bachman Road ; host plant vs. plant).</S>
			<S sid ="144" ssid = "124">Note that BioYatea extracts not only the maximum terms (e.g. contaminated Bachman Road site), but also their constituents (Bachman Road site, Bachman Road and site).</S>
			<S sid ="145" ssid = "125">Boundary adjustment often consists in selecting the relevant alternative among the subterms.</S>
			<S sid ="146" ssid = "126">Other boundary issues are handled by several patterns, which are applied after the typing stage.</S>
			<S sid ="147" ssid = "127">These patterns are type-dependent: each pattern only applies to one type or a subset of location types.</S>
			<S sid ="148" ssid = "128">When necessary, they shift the boundaries in order to include relevant modifiers.</S>
			<S sid ="149" ssid = "129">They also split location terms or join adjacent location terms.</S>
			<S sid ="150" ssid = "130">BioYatea may have missed relevant modifiers because of POS-tagging errors.</S>
			<S sid ="151" ssid = "131">For instance, if a nationality name precedes a location, then it is included (e.g. German oil field).</S>
			<S sid ="152" ssid = "132">Also, it frequently happens that hosts are modifiers of host parts (e.g. insect gut).</S>
			<S sid ="153" ssid = "133">BioYatea extracts the whole term and its constituents.</S>
			<S sid ="154" ssid = "134">The term is correctly typed as Host-part and the host modifier as Host.</S>
			<S sid ="155" ssid = "135">In order to avoid embedded locations, a specific pattern is devoted to the splitting of these terms.</S>
			<S sid ="156" ssid = "136">In this way insect gut (Host-part) becomes insect (Host) and gut (Host-part).</S>
			<S sid ="157" ssid = "137">Most of these patterns involve several specific lexicons, including cardinal directions, relevant and irrelevant modifiers for each type of location, as well as types, which can be merged and split.</S>
			<S sid ="158" ssid = "138">The current resources were manually built by examining the location terms of the training set and GOLD isolation fields.</S>
			<S sid ="159" ssid = "139">The acquisition of relevant and irrelevant modifiers could be automated by machine learning.</S>
			<S sid ="160" ssid = "140">Some linguistic phenomena could be better handled by the customization of BioYa- tea.</S>
			<S sid ="161" ssid = "141">For instance BioYatea considers the preposition with as a term boundary so it cannot extract terms containing with, like areas with high sulfur and salt concentrations.</S>
	</SECTION>
	<SECTION title="Extraction of Bacteria names. " number = "3">
			<S sid ="162" ssid = "1">We observed in the training corpus that not only were bacteria names tagged, but also higher level taxa (families) and lower level taxa (strains).</S>
			<S sid ="163" ssid = "2">We used the NCBI taxonomy as the main bacteria tax- on resource since it includes all organism levels and is kept up-to-date.</S>
			<S sid ="164" ssid = "3">This bacteria dictionary was enriched by taxa from the training corpus, in particular by non standard abbreviations (e.g. Chl.</S>
			<S sid ="165" ssid = "4">= Chlorobium, ssp.</S>
			<S sid ="166" ssid = "5">= subsp) and plurals, (Vibrios as the plural for Vibrio) that were hopefully rather rare.</S>
			<S sid ="167" ssid = "6">Determining the boundaries of the bacteria names was one of the main issues because corpus strain names do not always follow conventional nomenclature rules.</S>
			<S sid ="168" ssid = "7">Also, the recognition of bacteria name is evaluated using a strict exact match.</S>
			<S sid ="169" ssid = "8">Patterns were developed to account for such cases.</S>
			<S sid ="170" ssid = "9">They handle inversion (LB400 of Burkholderia xenovorans instead of Burkholderia xenovorans LB400) and parenthesis (Tropheryma whipplei (the Twist strain) instead of Tropheryma whipplei strain Twist).</S>
			<S sid ="171" ssid = "10">The corpus also mentions names of bacteria that contain modifiers not found in the NCBI dictionary, such as antimicrobial-resistant C. coli or L. pneumophila serogroup 1.</S>
			<S sid ="172" ssid = "11">Such cases, as well as abbreviations (e.g. GSB for green sulfur bacteria) and partial strain names (e.g. strain DSMZ 245 T for Chlorobium limicola strain DSMZ 245 T) were also specifically handled.</S>
			<S sid ="173" ssid = "12">The main source of error in bacteria name prediction is due to the mixture of family names and strain name abbreviations in the same text.</S>
			<S sid ="174" ssid = "13">It frequently happens that the strain name is abbreviated into the first word of the name.</S>
			<S sid ="175" ssid = "14">For instance Bar- tonella henselae is abbreviated as Bartonella.</S>
			<S sid ="176" ssid = "15">Unfortunately, Bartonella is a genus mentioned in the same text, thus yielding ambiguities between the anaphora and the family name, which are identical.</S>
			<S sid ="177" ssid = "16">3.1 Bacteria anaphora resolution.</S>
			<S sid ="178" ssid = "17">Anaphors are frequent in the text, especially for bacteria reference and to a smaller extent for host reference.</S>
			<S sid ="179" ssid = "18">Our effort focused on bacteria anaphora resolution ignoring host anaphora.</S>
			<S sid ="180" ssid = "19">The extraction method of location relations (section 4) assumes that the relation arguments, location and bacterium (or anaphora of the bacterium) occur in the same sentence.</S>
			<S sid ="181" ssid = "20">From a total of 2,296 sentences in the training corpus, only 363 sentences contain both the location and the explicit bacterium, while 574 mention only the location.</S>
			<S sid ="182" ssid = "21">Two thirds of the locations do not co-occur with bacteria.</S>
			<S sid ="183" ssid = "22">This demonstrates the importance of recovering the bacteria for these cases, which is potentially referred to by an explicit anaphora.</S>
			<S sid ="184" ssid = "23">The manual examination of the training corpus showed that the most frequent anaphora of bacteria are not pronouns but higher level taxa, often preceded by a demonstrative determinant, (i.e. This bacteria, This Clostridium) and sortal anaphora (i.e. genus, organism, species and strain), both of which are commonly found in biological texts (Torri &amp; VijayShanker, 2007).</S>
			<S sid ="185" ssid = "24">The style of some of the documents is rather relaxed and the antecedent may be ambiguous even for a human reader.</S>
			<S sid ="186" ssid = "25">We observed three types of anaphora in the corpus.</S>
			<S sid ="187" ssid = "26">First, the standard anaphora which includes both pronouns and sortal anaphora, which requires a unique bacterial antecedent.</S>
			<S sid ="188" ssid = "27">Second, bianaphora or an anaphora that requires two bacteria antecedents.</S>
			<S sid ="189" ssid = "28">This happens when the properties of two strains are compared in the document.</S>
			<S sid ="190" ssid = "29">Finally, the case of a higher taxon being used to refer to a lower taxon, which we named name taxon anaphora.</S>
			<S sid ="191" ssid = "30">Anaphora with a unique antecedent C. coli is pathogenic in animals and humans.</S>
			<S sid ="192" ssid = "31">People usually get infected by eating poultry that contained the bacteria, eating raw food, drinking raw milk, and drinking bottle water [â€¦].</S>
			<S sid ="193" ssid = "32">Anaphora with two antecedents C. coli is usually found hand in hand with its bacteria relative, C. jejuni.</S>
			<S sid ="194" ssid = "33">These two organisms are recognized as the two most leading causes of acute inflammation of intestine in the United States and other nations.</S>
			<S sid ="195" ssid = "34">Name taxon anaphora Ticks become infected with Borrelia duttonii while feeding on an infected rodent.</S>
			<S sid ="196" ssid = "35">Borrelia then multiplies rapidly, causing a generalized infection throughout the tick.</S>
			<S sid ="197" ssid = "36">For anaphora detection and resolution a pattern- based approach was preferred to machine learning because the constraints for relating anaphora to antecedent candidates of the same taxonomy level were mainly semantic and domain-dependent and the annotation of anaphora was not provided in the training corpus.</S>
			<S sid ="198" ssid = "37">Anaphora detection consists of identifying potential anaphora in the corpus, given a list of pronouns, sortal anaphora and taxa and then filtering out irrelevant cases (SeguraBedmar et al., 2010, Lin &amp; Lian, 2004) before anaphora resolution.</S>
			<S sid ="199" ssid = "38">Not all the pronouns, sortal anaphora terms and higher taxon bacteria are anaphoric.</S>
			<S sid ="200" ssid = "39">For example, if a higher taxon is preceded or followed by the word genus, this signals that it is not anaphoric but that the text is actually about the higher taxon.</S>
			<S sid ="201" ssid = "40">Non-anaphoric higher taxon Burkholderia cenocepacia HI2424[â€¦] The genus Burkholderia consists of some 35 bacterial species, most of which are soil saprophytes and phytopathogens that occupy a wide range of environmental niches.</S>
			<S sid ="202" ssid = "41">The anaphora resolution algorithm takes into account two features: the distance to the antecedent candidate and its position in the sentence.</S>
			<S sid ="203" ssid = "42">The antecedent is usually found in proximity to the anaphora, in order to maintain the coherence of the text.</S>
			<S sid ="204" ssid = "43">Therefore, our method ranks the antecedent candidates according to the anaphoric distance counted in sentences.</S>
			<S sid ="205" ssid = "44">If more than one bacterium is found in a given sentence, their position is discriminate.</S>
			<S sid ="206" ssid = "45">Centering theory states that in a sentence the most prominent entities and therefore the most probable antecedent candidates are in the order: subject &gt; object &gt; other position (Grosz et al., 1995).</S>
			<S sid ="207" ssid = "46">In English, due to the SVO order of the language the subject is most often found at the beginning of the sentence, followed by the object and the others.</S>
			<S sid ="208" ssid = "47">Therefore, the method retains the leftmost bacterium in the sentence when searching for the best antecedent candidate.</S>
			<S sid ="209" ssid = "48">More precisely, the method selects the first antecedent that it finds according to the following precedence list: - First bacterium in the current sentence (s) - First bacterium in the previous sentence (s-1) - First bacterium in sentence s-2 - First bacterium in sentence s-3 - First bacterium in the current paragraph - Last bacterium in the previous paragraph - First bacterium in the first sentence of the document - The first bacterium ever mentioned.</S>
			<S sid ="210" ssid = "49">The method only relates anaphora to antecedents that are found before.</S>
			<S sid ="211" ssid = "50">It does not handle cataphors since they are rarely found in the corpus.</S>
			<S sid ="212" ssid = "51">For anaphors that require two antecedents we use the same criteria but search for two bacteria in each sentence or paragraph, instead of one.</S>
			<S sid ="213" ssid = "52">For taxon anaphora we look for the presence of a lower taxon in the document found before the anaphora that is compatible according to the species taxonomy.</S>
			<S sid ="214" ssid = "53">The counts of anaphora detected by the patterns are given in Table 1.</S>
			<S sid ="215" ssid = "54">Co rp us Si ng le an te B i a n t e T a x o n a nt e Tr ain 9 3 3 4 1 2 9 De v 2 0 4 3 2 2 Te st 2 4 0 0 1 8 To tal 1 , 3 7 7 7 1 6 9 Table 1.</S>
			<S sid ="216" ssid = "55">The count of the types of anaphora per corpus.</S>
			<S sid ="217" ssid = "56">The anaphora resolution algorithm allowed us to retrieve more sentences that contain both a bacterium and a location.</S>
			<S sid ="218" ssid = "57">Out of the 574 sentences that contain only a location, 436 were found to contain an anaphora related to at least one bacterium.</S>
			<S sid ="219" ssid = "58">The remaining 138 sentences are cases where there is no bacterial anaphora or the bacterium name is implicit.</S>
			<S sid ="220" ssid = "59">It frequently happens that the bacterium is referred to through its action.</S>
			<S sid ="221" ssid = "60">For example in the sentence below, the bacterium name could be derived from the name of the disease that it causes.</S>
			<S sid ="222" ssid = "61">In the 1600s anthrax was known as the &quot;Black bane&quot; and killed over 60,000 cows.</S>
			<S sid ="223" ssid = "62">One of the questions we had about the resolution of anaphora is whether anaphora that are found in the same sentence together with a bacterium (therefore potentially its antecedent) should be consid ered or not.</S>
			<S sid ="224" ssid = "63">We tested this on the development set.</S>
			<S sid ="225" ssid = "64">We found that removing such anaphora from consideration improved the overall score.</S>
			<S sid ="226" ssid = "65">It yielded an F-score of 53.22% (precision: 46.17%, recall: 62.81%), compared to the original F-score of 50.15% (precision: 41.06%, recall: 64.44%).</S>
			<S sid ="227" ssid = "66">This improvement in F-score is solely due to an increase in precision, which shows that while resolving anaphora is important and required, the incorrect recognition of terms as anaphora and incorrect 5 Results.</S>
			<S sid ="228" ssid = "67">Table 2 summarizes the official scores that the Bib- liome Alvis system achieved for the Bacteria Biotope Task.</S>
			<S sid ="229" ssid = "68">It ranked first among three participants.</S>
			<S sid ="230" ssid = "69">The first column gives the recall of entity prediction.</S>
			<S sid ="231" ssid = "70">The prediction of hosts and bacteria named-entities achieved a good recall of 84 and 82, respectively.</S>
			<S sid ="232" ssid = "71">anaphora resolution can introduce noise.</S>
			<S sid ="233" ssid = "72">Entity recall Event recall Event Precis.</S>
			<S sid ="234" ssid = "73">F-score</S>
	</SECTION>
	<SECTION title="Relation extraction. " number = "4">
			<S sid ="235" ssid = "1">In this work we concentrated most of our effort on the prediction of entities.</S>
			<S sid ="236" ssid = "2">For the prediction of events we used a strategy based on the co- occurrence of arguments and trigger words within a sentence: - If a bacteria name, a location and a trigger word are present in a sentence, then the system predicts a Localization event between the bacterium and the location.</S>
			<S sid ="237" ssid = "3">- If a bacteria anaphora, a location and a trigger word are present in a sentence, then the system predicts a Localization event between each anaphora antecedent and the location.</S>
			<S sid ="238" ssid = "4">- If a host, a host part, a bacterium and at least one trigger word are present in a sentence, then the system predicts a PartOf event between the host and the host part.</S>
			<S sid ="239" ssid = "5">The list of trigger words contains 20 verbs (e.g. inhabit, colonize, but also discover, isolate), 16 disease markers (e.g. chronic, pathogen) and 19 other relevant words (e.g. ingest, environment, niche).</S>
			<S sid ="240" ssid = "6">This list was designed by ranking words in the sentences of the training corpus containing both a bacteria name and a location.</S>
			<S sid ="241" ssid = "7">The ranking criterion used was the information gain with respect to whether the sentence contained an event or not.</S>
			<S sid ="242" ssid = "8">The ranked list was adjusted by removing spurious words and adding domain knowledge words.</S>
			<S sid ="243" ssid = "9">By removing the constraint of the occurrence of a trigger word in the sentence, we can determine that the maximum recall the method can achieve with this strategy is 47% (precision: 41%, F-score: 44%).</S>
			<S sid ="244" ssid = "10">The selected trigger word list yielded a recall close to the maximum, thus it seems that the trigger words do not affect the recall and are suitable for the task.</S>
			<S sid ="245" ssid = "11">Bacteria 84 - - - Host 82 61 48 53 Host part 72 53 42 47 Env.</S>
			<S sid ="246" ssid = "12">53 29 24 26 Geo.</S>
			<S sid ="247" ssid = "13">29 13 38 19 Food - - 29 41 Medical 100 50 33 40 Water 83 60 55 57 Soil 86 69 59 63 Total 45 45 45 Table 2.</S>
			<S sid ="248" ssid = "14">Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011.</S>
			<S sid ="249" ssid = "15">However, geographical locations based on a similar strategy were poorly predicted (29%).</S>
			<S sid ="250" ssid = "16">Our system predicted only 15 countries.</S>
			<S sid ="251" ssid = "17">A more appropriate resource of geographical names than the Agrovoc thesaurus would certainly increase the recall of geographical locations.</S>
			<S sid ="252" ssid = "18">The host parts, medical, water and soil locations predicted with the same ontology-based method were surprisingly good with a recall of 72, 100, 83 and 86, respectively.</S>
			<S sid ="253" ssid = "19">The small size of the ontology and the small number of different term heads (i.e. 51 different heads) initially appeared as a limitation factor for reuse on new corpora.</S>
			<S sid ="254" ssid = "20">The good recall shows that the location vocabulary of the test set has similarities with the training set compared to potential space of location names.</S>
			<S sid ="255" ssid = "21">The potential space is reflected by the richness of the GOLD isolation site field.</S>
			<S sid ="256" ssid = "22">This demonstrates the robustness of the type derivation approach based on term heads.</S>
			<S sid ="257" ssid = "23">The correctness of the derivation type cannot be calculated without a corpus where all the locations and not only bacteria ones are annotated.</S>
			<S sid ="258" ssid = "24">The recall of the environment location prediction is a little bit lower, 53%.</S>
			<S sid ="259" ssid = "25">The environment type in cludes many different types that cannot all be anticipated.</S>
			<S sid ="260" ssid = "26">Therefore the coverage of the BB terminoontology environment part is limited except for water and soil, which are more focused topics.</S>
			<S sid ="261" ssid = "27">The localization event recall (column 2) is on average 20% lower for all types than the location entity recall.</S>
			<S sid ="262" ssid = "28">The regularity of the difference may suggest that once the argument is identified, the localization relation is equally harder to find by our method independently of the type.</S>
			<S sid ="263" ssid = "29">The localization event precision (column 3) is more difficult to analyze because many sources of error may be involved, such as an incorrect arguments, incorrect anaphora resolution, relation to the wrong bacterium among several or the absence of a relation.</S>
			<S sid ="264" ssid = "30">The prediction precision of localization events involving soil, water and host is better than environment and food.</S>
			<S sid ="265" ssid = "31">The manual analysis of the test corpus shows that in some cases environmental locations were mentioned as potential sources of industrial applications without actually being bacteria isolation places.</S>
			<S sid ="266" ssid = "32">For instance, in Other fields of application for thermostable enzymes are starch- processing, organic synthesis, diagnostics, waste treatment, pulp and paper manufacture, and animal feed and human food, the Alvis system erroneously predicted waste treatment, paper manufacture, animal feed and human food.</S>
			<S sid ="267" ssid = "33">This is due to the fact that the system does not handle modalities.</S>
			<S sid ="268" ssid = "34">Such hypotheses are specific to the BB task text genre, i.e. Bacteria sequencing projects.</S>
			<S sid ="269" ssid = "35">Such projects contain details for potential industrial applications, which are absent from academic literature.Ambiguous types are also a source of error.</S>
			<S sid ="270" ssid = "36">De spite the host dictionary cleaning, some ambiguities remained.</S>
			<S sid ="271" ssid = "37">For example, the head canal in tooth root canal is erroneously typed as water and should be disambiguated with its tooth host-part modifier.</S>
			<S sid ="272" ssid = "38">After test publication we measured the gain of anaphora resolution by using the online service.</S>
			<S sid ="273" ssid = "39">The anaphora resolution algorithm was found to have a strong impact on the final result.</S>
			<S sid ="274" ssid = "40">Running the test set using all of the modules except for the anaphora resolution algorithm yielded a decrease in the F-score by almost 13% (F-score: 32.5%, precision: 48.5%, 24.4%).</S>
			<S sid ="275" ssid = "41">This shows that the addition of an anaphora resolution algorithm significantly increases the precision and that a resolution algorithm adapted to the Bacteria domain is necessary for the Biotope corpus.</S>
			<S sid ="276" ssid = "42">The part-of event prediction relies on the strict co-occurrence of a bacterium, trigger word, host and host part within a sentence.</S>
			<S sid ="277" ssid = "43">An additional run with the more relaxed constraint where the bacterium can be denoted by an anaphora as well yielded a gain of 6 recall points, a loss of 5 precision points with a net benefit of 1 F-measure point.</S>
			<S sid ="278" ssid = "44">6 Discussion.</S>
			<S sid ="279" ssid = "45">The use of trigger words for the selection of sentences for relation extraction does not take into account the structure or syntax of the sentence for the prediction of relation arguments.</S>
			<S sid ="280" ssid = "46">The system predicts all combinations of bacteria and locations as localization events and all combination of host and host parts as part-of event.</S>
			<S sid ="281" ssid = "47">This has a negative effect on the precision measure since some pairs are irrelevant as in the sentence below.</S>
			<S sid ="282" ssid = "48">Baumannia cicadellinicola.</S>
			<S sid ="283" ssid = "49">This newly discovered organism is an obligate endosymbiont of the leafhopper insect Homalodisca coagulata (Say), also known as the Glassy-Winged Sharpshooter, which feeds on the xylem of plants.</S>
			<S sid ="284" ssid = "50">It has been shown that the use of syntactic dependencies to extract biological events (such as protein-protein interactions) improves the results of such systems (Erkan et al., 2007, Manine et al., 2008, Airola et al. 2008).</S>
			<S sid ="285" ssid = "51">The use of syntactic dependencies could offer a more in depth examination of the syntax and the semantics and therefore allow for a more refined extraction of bacteria- localization and host-host part relations.</S>
			<S sid ="286" ssid = "52">Term extraction appears to be a good method for predicting locations including unseen terms, but it is limited by the typing strategy that filters out all terms with unknown heads (with respect to the BB terminoontology).</S>
			<S sid ="287" ssid = "53">In the future, we will study the effect of linguistic markers such as enumeration and exemplification structures for recovering additional location terms.</S>
			<S sid ="288" ssid = "54">For instance, in heated organic materials such as compost heaps, rotting hay, manure piles or mushroom growth medium, our system has correctly typed heated organic materials as environment but not the other examples because of their unknown heads.</S>
			<S sid ="289" ssid = "55">The promising performance of the Alvis system on the BB task shows that a combination of semantic analysis and domain-adapted resources is a good strategy for information extraction in the biology domain.</S>
	</SECTION>
</PAPER>
