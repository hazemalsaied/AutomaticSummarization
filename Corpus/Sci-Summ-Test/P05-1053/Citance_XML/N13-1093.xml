<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents an approach that exploits the scope of negation cues for relation extraction (RE) without the need of using any specifically annotated dataset for building a separate negation scope detection classifier.</S>
		<S sid ="2" ssid = "2">New features are proposed which are used in two different stages.</S>
		<S sid ="3" ssid = "3">These also include non-target entity specific features.</S>
		<S sid ="4" ssid = "4">The proposed RE approach outperforms the previous state of the art for drug-drug interaction (DDI) extraction.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Negation is a linguistic phenomenon where a negation cue (e.g. not) can alter the meaning of a particular text segment or of a fact.</S>
			<S sid ="6" ssid = "6">This text segment (or fact) is said to be inside the scope of that negation (cue).</S>
			<S sid ="7" ssid = "7">In the context of RE, there is not much work that aims to exploit the scope of negations.1 The only work on RE that we are aware of is SanchezGraillet and Poesio (2007) where they used various heuristics to extract negative protein interaction.</S>
			<S sid ="8" ssid = "8">Despite the recent interest on automatically detecting the scope of negation2 till now there seems to be no empirical evidence supporting its exploitation for the purpose of RE.</S>
			<S sid ="9" ssid = "9">Even if we could manage to obtain highly accurate automatically detected 1 In the context of event extraction (a closely related task of.</S>
			<S sid ="10" ssid = "10">RE), there have been efforts in BioNLP shared tasks of 2009 and 2011 for (non-mandatory sub-task of) event negation detection (3 participants in 2009; 2 in 2011) (Kim et al., 2009; Kim et al., 2011).</S>
			<S sid ="11" ssid = "11">The participants approached the sub-task using either predefined patterns or some heuristics.</S>
	</SECTION>
	<SECTION title="This task is popularized by various recently held shared. " number = "2">
			<S sid ="12" ssid = "1">tasks (Farkas et al., 2010; Morante and Blanco, 2012).</S>
			<S sid ="13" ssid = "2">negation scopes, it is not clear how to feed this information inside the RE approach.</S>
			<S sid ="14" ssid = "3">Simply considering whether a pair of candidate mentions falls under the scope of a negation cue might not be helpful.</S>
			<S sid ="15" ssid = "4">In this paper, we propose that the scope of negations can be exploited at two different levels.</S>
			<S sid ="16" ssid = "5">Firstly, the system would check whether all the target entity3 mentions inside a sentence along with possible relation clues (or trigger words), if any, fall (directly or indirectly) under the scope of a negation cue.</S>
			<S sid ="17" ssid = "6">If such a sentence is found, then it should be discarded (i.e. candidate mention pairs4 inside that sentence would not be considered).</S>
			<S sid ="18" ssid = "7">Secondly, for each of the remaining pairs of candidate mentions, the system should exploit features related to the scope of negation (rather than simply adding a feature for negation cue, approach adopted in various RE systems) that can provide indication (if any such evidence exists) that the corresponding relation of interest actually does not hold in that particular context.</S>
			<S sid ="19" ssid = "8">In the subsequent sections, we describe our approach.</S>
			<S sid ="20" ssid = "9">The RE task considered is drug-drug interaction (DDI) extraction.</S>
			<S sid ="21" ssid = "10">The task has significant importance for public health safety.5 We used 3 The target entities, for example, for DDI extraction and for EMPORG relation extraction would be {DRUG} and {PER, GPE, ORG} respectively.</S>
			<S sid ="22" ssid = "11">Any entity other than the target entities (w.r.t. the particular RE task) belongs to non-target entities.</S>
			<S sid ="23" ssid = "12">4 Candidate mention pairs for RE are taken from target entity.</S>
			<S sid ="24" ssid = "13">mentions.</S>
			<S sid ="25" ssid = "14">5 After the death of pop star Michael Jackson, allegedly due to DDI, it was reported that about 2.2 million people in USA, age 57 to 85, were taking potentially dangerous combinations of drugs (Landau, 2009).</S>
			<S sid ="26" ssid = "15">An earlier report mentioned that deaths from accidental drug interactions rose 68 percent between 1999 and 2004 (Payne, 2007).</S>
			<S sid ="27" ssid = "16">765 Proceedings of NAACLHLT 2013, pages 765–771, Atlanta, Georgia, 9–14 June 2013.</S>
			<S sid ="28" ssid = "17">Qc 2013 Association for Computational Linguistics the DDIExtraction2011 challenge corpus (SeguraBedmar et al., 2011).</S>
			<S sid ="29" ssid = "18">The official training and test data of the corpus contain 4,267 and 1,539 sentences, and 2,402 and 755 DDI annotations respectively.</S>
			<S sid ="30" ssid = "19">2 Proposed Approach.</S>
			<S sid ="31" ssid = "20">2.1 Stage 1: Exploiting scope of negation to.</S>
			<S sid ="32" ssid = "21">filter out sentences We propose a two stage RE approach.</S>
			<S sid ="33" ssid = "22">In the first stage, our goal is to exploit the scope of negations to reduce the number of candidate mention pairs by discarding sentences.</S>
			<S sid ="34" ssid = "23">For this purpose, we propose the following features to train a binary classifier: • has2TM: If the sentence has exactly 2 target entity mentions (i.e. drug mentions for DDI extraction).</S>
			<S sid ="35" ssid = "24">• has3OrMoreTM: Whether the sentence has more than 2 target entity mentions.</S>
			<S sid ="36" ssid = "25">• allTMonRight: Whether all target entity mentions inside the sentence appear after the negation cue.</S>
			<S sid ="37" ssid = "26">• neitherAllTMonLeftOrRight: Whether some but not all target entity mentions appear after the negation cue.</S>
			<S sid ="38" ssid = "27">• negCue: The negation cue itself.</S>
			<S sid ="39" ssid = "28">• immediateGovernor: The word on which the cue is directly syntactically dependent.</S>
			<S sid ="40" ssid = "29">• nearestVerbGovernor: The nearest verb in the dependency graph on which the cue is syntactically dependent.</S>
			<S sid ="41" ssid = "30">• isVerbGovernorRoot: Whether the nearestVerb- Governor is root of the dependency graph of the sentence.</S>
			<S sid ="42" ssid = "31">• allTMdependentOnNVG: Whether all target entity mentions are syntactically dependent (directly/indirectly) on the nearestVerbGovernor.</S>
			<S sid ="43" ssid = "32">• allButOneTMdependentOnNVG: Whether all but one target entity mentions are syntactically dependent on the nearestVerbGovernor.</S>
			<S sid ="44" ssid = "33">• although*PrecedeCue: Whether the syntactic clause containing the negation cue begins with “although / though / despite / in spite”.</S>
			<S sid ="45" ssid = "34">• commaBeforeNextTM: Whether there is a comma in the text between the negation cue and the next target entity mention after the cue.</S>
			<S sid ="46" ssid = "35">• commaAfterPrevTM: Whether there is a comma in the text between the previous target entity mention before the negation cue and the cue itself.</S>
			<S sid ="47" ssid = "36">• sentHasBut: Whether the sentence contains the word “but”.</S>
			<S sid ="48" ssid = "37">The objective of the classifier is to decide whether all of the target entity mentions (i.e. drugs) as well as any possible evidence of the relation of interest (for which we assume the immediate and the nearest verb governors of the negation cue would be good candidates) inside the corresponding sentence fall under the scope of a negation cue in such a way that the sentence is unlikely to contain a DDI.</S>
			<S sid ="49" ssid = "38">At present, we limit our focus only on the first occurrence of the following negation cues: “no”, “n’t” or “not”.6 In the Stage 1, any sentence that contains at least one DDI is considered by the classifier as a positive (training/test) instance.</S>
			<S sid ="50" ssid = "39">Other sentences are considered as negative instances.</S>
			<S sid ="51" ssid = "40">We rule out any sentence (i.e. we do not consider as training/test instance for the classifier that filters less informative sentences) during both training and testing if any of the following conditions holds: • The sentence contains less than two target entity mentions (such sentence would not contain the relation of interest anyway).</S>
			<S sid ="52" ssid = "41">• It has any of the following phrases – “not recommended”, “should not be” or “must not be”.7 • There is no “no”, “n’t” or “not” in the sentence.</S>
			<S sid ="53" ssid = "42">• No target entity mention appears in the sentence after “no”, “n’t” or “not”.</S>
			<S sid ="54" ssid = "43">To assess the effectiveness of the proposed Stage 1 classifier, we defined a baseline classifier that filters any sentence that contains “no”, “n’t” or “not”.</S>
			<S sid ="55" ssid = "44">2.2 Stage 2.</S>
			<S sid ="56" ssid = "45">Once the sentences which are likely to have no DDI are identified and removed, the next step is to apply a state-of-the-art RE approach on the remaining sentences.</S>
			<S sid ="57" ssid = "46">In this section, we propose a new hybrid kernel, KH ybrid, for this purpose.</S>
			<S sid ="58" ssid = "47">It is defined as follows: KH ybrid (R1, R2) = KH F (R1, R2) + KSL (R1, R2) + w * KP ET (R1, R2) 6 These cues usually occur more frequently and generally have larger negation scope than other negation cues.</S>
			<S sid ="59" ssid = "48">7 These expressions often provide clues that one of the bio-.</S>
			<S sid ="60" ssid = "49">entity mentions negatively influences the level of activity of the other.</S>
			<S sid ="61" ssid = "50">Here, KH F stands for a new feature based kernel (proposed in this paper) that uses a heterogeneous set of features.</S>
			<S sid ="62" ssid = "51">KSL stands for the Shallow Linguistic (SL) kernel proposed by Giuliano et al.</S>
			<S sid ="63" ssid = "52">(2006).</S>
			<S sid ="64" ssid = "53">KP ET stands for the Path-enclosed Tree (PET) kernel (Moschitti, 2004).</S>
			<S sid ="65" ssid = "54">w is a multiplicative constant used for the PET kernel.</S>
			<S sid ="66" ssid = "55">It allows the hybrid kernel to assign more (or less) weight to the information obtained using tree structures depending on the corpus.</S>
			<S sid ="67" ssid = "56">The proposed kernel composition is valid according to the closure properties of kernels.</S>
			<S sid ="68" ssid = "57">We exploit the SVM-Light-TK toolkit (Moschitti, 2006; Joachims, 1999) for kernel computation.</S>
			<S sid ="69" ssid = "58">In Stage 2, each candidate drug mention pair represents an instance.</S>
			<S sid ="70" ssid = "59">2.2.1 Proposed KH F kernel As mentioned earlier, this proposed kernel uses heterogeneous features.</S>
			<S sid ="71" ssid = "60">The first version of the heterogeneous feature set (henceforth, HF v1) combines features proposed by two previous RE works.</S>
			<S sid ="72" ssid = "61">The former is Zhou et al.</S>
			<S sid ="73" ssid = "62">(2005), which uses 51 different features.</S>
			<S sid ="74" ssid = "63">We select the following 27 of their features for our feature set: WBNULL, WBFL, WBF, WBL, WBO, BM1F, BM1L, AM2F, AM2L, #MB, #WB, CPHBNULL, CPHBFL, CPHBF, CPHBL, CPHBO, CPHBM1F, CPHBM1L, CPHAM2F, CPHAM2F, CPP, CPPH, ET12SameNP, ET12SamePP, ET12SameVP, PTP, PTPH The latter is the TPWF kernel (Chowdhury and Lavelli, 2012a) from which we use following features: HasTriggerWord, Trigger-X, DepPattern-i, e- walk, v-walk The TPWF kernel extracts the HasTriggerWord, Trigger-X and DepPattern-i features from a sub- graph called reduced graph.</S>
			<S sid ="75" ssid = "64">We also follow this approach with one minor difference.</S>
			<S sid ="76" ssid = "65">Unlike Chowdhury and Lavelli (2012a), we look for trigger words in the whole reduced graph instead of using only the root of the sub-graph.</S>
			<S sid ="77" ssid = "66">Due to space limitation we refer the readers to the corresponding papers for the description of the above mentioned features and the definition of reduced graph.</S>
			<S sid ="78" ssid = "67">In addition, HF v1 also includes surrounding tokens within the window of {-2,+2} for each candidate mention.</S>
			<S sid ="79" ssid = "68">We are unaware of any available list of trigger words for drug-drug interaction.</S>
			<S sid ="80" ssid = "69">So, we created such a list.8 We extend the heterogeneous feature set by adding features related to the scope of negation (henceforth, HF v2).</S>
			<S sid ="81" ssid = "70">We use a list of 13 negation cues9 to search inside the reduced graph of a candidate pair.</S>
			<S sid ="82" ssid = "71">If the reduced graph contains any of the negation cues or their morphological variants then we add the following features: • negCue: The corresponding negation cue.</S>
			<S sid ="83" ssid = "72">• immediateNegatedWord: If the word following the negation cue is neither a preposition nor a “be verb”, then that word, otherwise the word after the next word.10 Furthermore, if the corresponding matched negation cue is either “no”, “n’t” or “not”, then we add additional features related to negation scope: • bothEntDependOnImmediateGovernor: Whether the immediate governor (if any) of the negation cue is also governor of a dependency sub-tree (of the dependency graph of the corresponding sentence) that includes both of the candidate mentions.</S>
			<S sid ="84" ssid = "73">• immediateGovernorIsVerbGovernor: Whether the immediate governor of the negation cue is a verb.</S>
			<S sid ="85" ssid = "74">• nearestVerbGovernor: The closest verb governor (i.e. parent or grandparent inside the dependency graph), if any, of the negation cue.</S>
			<S sid ="86" ssid = "75">We further extend the heterogeneous feature set by adding features related to relevant non-target entities (with respect to the relation of interest; henceforth, HF v3).</S>
			<S sid ="87" ssid = "76">For the purpose of DDI extraction, we deem the presence of DISEASE mentions (which might result as a consequence of a DDI) can provide some clues.</S>
			<S sid ="88" ssid = "77">So, we use a publicly available state-of-the-art disease NER system called BioEnEx (Chowdhury and Lavelli, 2010) to annotate the DDIExtraction2011 challenge corpus.</S>
			<S sid ="89" ssid = "78">For 8 The RE system developed for this work and the created list of trigger words for DDI can be downloaded from https://github.com/fmchowdhury/HyREX . 9 No, not, neither, without, lack, fail, unable, abrogate, ab-.</S>
			<S sid ="90" ssid = "79">sence, prevent, unlikely, unchanged, rarely.</S>
			<S sid ="91" ssid = "80">10 For example, “interested” from “... not interested ...”, and.</S>
			<S sid ="92" ssid = "81">“confused” from “... not to be confused ...”.</S>
			<S sid ="93" ssid = "82">each candidate (drug) mention pair, we add the following features in HF v3: • NTEMinsideSentence: Whether the corresponding sentence contains important non-target entity mention(s) (e.g. disease for DDI).</S>
			<S sid ="94" ssid = "83">• immediateGovernorIsVerbGovernorOfNTEM: The immediate governor (if any) of the non-target entity mention, only if such governor is also governing a dependency sub-tree that includes both of the target candidate entity mentions.</S>
			<S sid ="95" ssid = "84">• nearestVerbGovernorOfNTEM: The closest verb governor (if any) of the non-target entity mention, only if it also governs the candidate entity mentions.</S>
			<S sid ="96" ssid = "85">• immediateGovernorIsVerbGovernorOfNTEM: Whether the immediate governor is a verb.</S>
	</SECTION>
	<SECTION title="Results and Discussion. " number = "3">
			<S sid ="97" ssid = "1">We train a linear SVM classifier in Stage 1 and tune the hyper-parameters (by doing 5-fold cross- validation) for obtaining maximum possible recall.</S>
			<S sid ="98" ssid = "2">In this way we minimize the number of false negatives (i.e. sentences that contain DDIs but are wrongly identified as not having any).</S>
			<S sid ="99" ssid = "3">During the cross-validation experiments on the training data, 334 sentences (7.83% of the total sentences) containing at least 2 drug mentions were identified by our proposed classifier (in Section 2.1) as unlikely to have any DDI and hence are candidates for discarding.</S>
			<S sid ="100" ssid = "4">Only 19 of these sentences were incorrectly identified.</S>
			<S sid ="101" ssid = "5">When we trained on the training data and tested on the official test data of DDIExtraction2011 challenge corpus, 121 sentences (7.86% of the total test sentences) were identified by the classifier as candidates for discarding.</S>
			<S sid ="102" ssid = "6">Only 5 of them were incorrectly identified.</S>
			<S sid ="103" ssid = "7">Unlike Stage 1, in Stage 2 where we train the hybrid kernel based RE classifier and use it for RE (i.e. DDI extraction) from the test data, sentences are not the RE training/test instances.</S>
			<S sid ="104" ssid = "8">Instead, a RE instance corresponds to a candidate mention pair.</S>
			<S sid ="105" ssid = "9">All the DDIs (i.e. positive RE instances) of the incorrectly identified sentences in Stage 1 (i.e. the sentences which are incorrectly labelled as not having any DDI and filtered) are automatically considered as false negatives during the calculation of DDI extraction results in Stage 2.</S>
			<S sid ="106" ssid = "10">To verify whether our proposed hybrid kernel achieves state-of-the-art results without taking benefits of the output of Stage 1, we did some experiments without discarding any sentence.</S>
			<S sid ="107" ssid = "11">These experiments are done using Zhou et al.</S>
			<S sid ="108" ssid = "12">(2005), TPWF kernel, SL kernel, different versions of proposed KH F kernel and KH ybrid kernel.</S>
			<S sid ="109" ssid = "13">Table 1 shows the results of 5-fold cross-validation experiments (hyper-parameters are tuned for obtaining maximum F-score).</S>
			<S sid ="110" ssid = "14">As the results show, there is a gain +0.9 points in F-score (mainly due to the boost in recall) after the addition of features related to negation scope.</S>
			<S sid ="111" ssid = "15">There is also some minor improvement due to the proposed non-target entity specific features.</S>
			<S sid ="112" ssid = "16">We also performed (5-fold cross validation) experiments by combining the Stage 1 classifier with each of the Zhou et al.</S>
			<S sid ="113" ssid = "17">(2005), TPWF kernel, SL kernel, PET kernel, KH F kernel and KH ybrid kernel separately (only the results of KH ybrid are reported in Table 1 due to space limitation).</S>
			<S sid ="114" ssid = "18">In each case, there were improvements in precision, recall and F- score.</S>
			<S sid ="115" ssid = "19">The gain in F-score ranged from 1.0 to 1.4 points.</S>
			<S sid ="116" ssid = "20">P / R / F-score Using SL kernel (Giuliano et al., 2006) 51.3 / 64.7 / 57.3 Using (Zhou et al., 2005) 58.7 / 37.1 / 45.5 Using PET kernel (Moschitti, 2004) 46.8 / 602 / 52.7 TPWF (Chowdhury and Lavelli, 2012a) 43.7 / 60.7 / 50.8 Proposed approaches Proposed KH F v1 53.4 / 51.5 / 52.4 KH F v2 (i.e. + neg scope feat.)</S>
			<S sid ="117" ssid = "21">53.9 / 52.6 / 53.3 (+0.9) KH F v3 (i.e. + non-target entity feat.)</S>
			<S sid ="118" ssid = "22">53.6 / 53.5 / 53.6 (+0.3) Proposed KH ybrid 56.3 / 68.5 / 61.8 Proposed KH ybrid with Stage 1 57.3 / 69.4 / 62.8 (+1.0) Table 1: 5-fold cross-validation results on training data.</S>
			<S sid ="119" ssid = "23">Table 2 reports the results of the previously published studies that used the same corpus.</S>
			<S sid ="120" ssid = "24">Our proposed KH ybrid kernel obtains an F-score that is higher than that of the previous state of the art.</S>
			<S sid ="121" ssid = "25">When the Stage 1 classifier (based on negation scope features) is exploited before using the KH ybrid kernel, the F-score reaches up to 67.4.</S>
			<S sid ="122" ssid = "26">This is +1.0 points higher than without exploiting the Stage 1 classifier and +1.7 higher than previous state of the art.</S>
			<S sid ="123" ssid = "27">We did separate experiments (also reported in Table 2) to assess the performance improvement when the output of Stage 1 is used to filter sentences from either training or test data only.</S>
			<S sid ="124" ssid = "28">The results remain the same when only training sentences are filtered; while there are some improvements when only test sentences are filtered.</S>
			<S sid ="125" ssid = "29">Filtering both training and test sentences provides the larger gain which is statistically significant.</S>
			<S sid ="126" ssid = "30">Usually, the number of negative instances in a corpus is much higher than that of the positive instances.</S>
			<S sid ="127" ssid = "31">In a recent work, Chowdhury and Lavelli (2012b) showed that by removing less informative P RF sco re (Th om as et al., 201 1) 60 .5 71.</S>
			<S sid ="128" ssid = "32">9 65.</S>
			<S sid ="129" ssid = "33">7 (Ch ow dhu ry et al., 201 1) 58 .6 70.</S>
			<S sid ="130" ssid = "34">5 64.</S>
			<S sid ="131" ssid = "35">0 (Ch ow dhu ry and Lav elli, 201 1) 58 .4 70.</S>
			<S sid ="132" ssid = "36">1 63.</S>
			<S sid ="133" ssid = "37">7 (Bj orn e et al., 201 1) 58 .0 68.</S>
			<S sid ="134" ssid = "38">9 63.</S>
			<S sid ="135" ssid = "39">0 Pro pos ed KH ybri d 60 .0 74.</S>
			<S sid ="136" ssid = "40">3 66.</S>
			<S sid ="137" ssid = "41">4 KH ybri d + Sta ge 1 bas elin e 61 .8 68.</S>
			<S sid ="138" ssid = "42">9 65.</S>
			<S sid ="139" ssid = "43">1 KH ybri d + pro pos ed Sta ge 1 60 .0 74.</S>
			<S sid ="140" ssid = "44">2 66.</S>
			<S sid ="141" ssid = "45">4 (only training sentences are filtered) KH ybrid + proposed Stage 1 61.4 73.8 67.0 (only test sentences are filtered) KH ybrid + proposed Stage 1 62.1 73.8 67.4 stat.</S>
			<S sid ="142" ssid = "46">sig.</S>
			<S sid ="143" ssid = "47">(both training and test sentences are filtered) (negative) instances (henceforth, LIIs), not only the skewness in instance distribution could be reduced but it also leads to a better result.</S>
			<S sid ="144" ssid = "48">The proposed Proposed KH ybrid + LII filtering 61.1 75.1 67.4 stat.</S>
			<S sid ="145" ssid = "49">sig.</S>
			<S sid ="146" ssid = "50">Stage 1 classifier, presented in this work, also reduces skewness in instance distribution.</S>
			<S sid ="147" ssid = "51">This is because we are only removing those sentences that are unlikely to contain any positive instance.</S>
			<S sid ="148" ssid = "52">So, in principle, the Stage 1 classifier is focused on removing only negative instances (although the classifier mistakenly discards few positive instances, too).</S>
			<S sid ="149" ssid = "53">We wanted to study how the Stage 1 classifier would contribute if we use it on top of the techniques that were proposed in Chowdhury and Lavelli (2012b) to remove LIIs.</S>
			<S sid ="150" ssid = "54">As Table 2 shows, by using the Stage 1 classifier along with LLI filtering, we could further improve the results (+3.2 points difference in F-score with the previous state of the art).</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "4">
			<S sid ="151" ssid = "1">A major flexibility in the proposed approach is that it does not require a separate dataset (which needs to match the genre of the text to be used for RE) annotated with negation scopes.</S>
			<S sid ="152" ssid = "2">Instead, the proposed Stage 1 classifier uses the RE training data (which do not have negation scope annotations) to self-supervise itself.</S>
			<S sid ="153" ssid = "3">Various new features have been exploited (both in stages 1 and 2) that can provide strong indications of the scope of negation cues with respect to the relation to be extracted.</S>
			<S sid ="154" ssid = "4">The only thing needed is the list of possible negation cues (Morante (2010) includes such a comprehensive list).</S>
			<S sid ="155" ssid = "5">Our proposed kernel, which has a component that exploits a heterogeneous set of features including negation scope and presence of non-target entities, already obtains better results than previous studies.</S>
			<S sid ="156" ssid = "6">Proposed KH ybrid + LII filtering 63.5 75.2 68.9 stat.</S>
			<S sid ="157" ssid = "7">sig.</S>
			<S sid ="158" ssid = "8">+ proposed Stage 1 Table 2: Results obtained on the official test set of the 2011 DDI Extraction challenge.</S>
			<S sid ="159" ssid = "9">LII filtering refers to the.</S>
			<S sid ="160" ssid = "10">techniques proposed in Chowdhury and Lavelli (2012b) for reducing skewness in RE data distribution.</S>
			<S sid ="161" ssid = "11">stat.</S>
			<S sid ="162" ssid = "12">sig.</S>
			<S sid ="163" ssid = "13">indicates that the improvement of F-score, due to usage of Stage 1 classifier, is statistically significant (verified using Approximate Randomization Procedure (Noreen, 1989); number of iterations = 1,000, confidence level = 0.01).</S>
			<S sid ="164" ssid = "14">The results considerably improve when possible irrelevant sentences from both training and test data are filtered by exploiting features related to the scope of negations.</S>
			<S sid ="165" ssid = "15">In future, we would like to exploit the scope of more negation cues, apart from the three cues that are used in this study.</S>
			<S sid ="166" ssid = "16">We believe our approach would help to improve RE in other genres of text (such as newspaper) as well.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="167" ssid = "17">This work was carried out in the context of the project “eOnco - Pervasive knowledge and data management in cancer care”.</S>
	</SECTION>
</PAPER>
