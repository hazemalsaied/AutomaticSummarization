<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">The Clinical E-Science Framework (CLEF) project has built a system to extract clinically significant information from the textual component of medical records, for clinical research, evidence-based healthcare and genotype-meets-phenotype informatics.</S>
		<S sid ="2" ssid = "2">One part of this system is the identification of relationships between clinically important entities in the text.</S>
		<S sid ="3" ssid = "3">Typical approaches to relationship extraction in this domain have used full parses, domain-specific grammars, and large knowledge bases encoding domain knowledge.</S>
		<S sid ="4" ssid = "4">In other areas of biomedical NLP, statistical machine learning approaches are now routinely applied to relationship extraction.</S>
		<S sid ="5" ssid = "5">We report on the novel application of these statistical techniques to clinical relationships.</S>
		<S sid ="6" ssid = "6">We describe a supervised machine learning system, trained with a corpus of oncology narratives hand-annotated with clinically important relationships.</S>
		<S sid ="7" ssid = "7">Various shallow features are extracted from these texts, and used to train statistical classifiers.</S>
		<S sid ="8" ssid = "8">We compare the suitability of these features for clinical relationship extraction, how extraction varies between inter- and intrasentential relationships, and examine the amount of training data needed to learn various relationships.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">The application of Natural Language Processing (NLP) is widespread in biomedicine.</S>
			<S sid ="10" ssid = "10">Typically, it is applied to improve access to the ever-burgeoning research literature.</S>
			<S sid ="11" ssid = "11">Increasingly, biomedical researchers need to relate this literature to phenotypic data: both to populations, and to individual clinical subjects.</S>
			<S sid ="12" ssid = "12">The computer applications used in biomedical research, including NLP applications, therefore need to support genotype-meets- phenotype informatics and the move towards translational biology.</S>
			<S sid ="13" ssid = "13">Such support will undoubtedly include linkage to the information held in individual medical records: both the structured portion, and the unstructured textual portion.</S>
			<S sid ="14" ssid = "14">The Clinical E-Science Framework (CLEF) project (Rector et al., 2003) is building a framework for the capture, integration and presentation of this clinical information, for research and evidence- based health care.</S>
			<S sid ="15" ssid = "15">The project’s data resource is a repository of the full clinical records for over 20000 cancer patients from the Royal Marsden Hospital, Europe’s largest oncology centre.</S>
			<S sid ="16" ssid = "16">These records combine structured information, clinical narratives, and free text investigation reports.</S>
			<S sid ="17" ssid = "17">CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research.</S>
			<S sid ="18" ssid = "18">The CLEF IE system analyses the textual records to extract entities, events and the relationships between them.</S>
			<S sid ="19" ssid = "19">These relationships give information that is often not available in the structured record.</S>
			<S sid ="20" ssid = "20">Why was a drug given?</S>
			<S sid ="21" ssid = "21">What were the results of a physical examination?</S>
			<S sid ="22" ssid = "22">What problems were not present?</S>
			<S sid ="23" ssid = "23">We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b).</S>
			<S sid ="24" ssid = "24">This paper examines relationship extraction.</S>
			<S sid ="25" ssid = "25">Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system.</S>
			<S sid ="26" ssid = "26">Several such systems have been described.</S>
			<S sid ="27" ssid = "27">They generally use a syntactic parse with domain- specific grammar rules.</S>
			<S sid ="28" ssid = "28">The Linguistic String project (Sager et al., 1994) used a full syntactic and 10 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, Columbus, Ohio, USA, June 2008.</S>
			<S sid ="29" ssid = "29">Qc 2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical statements.</S>
			<S sid ="30" ssid = "30">These were mapped to a database model incorporating medical facts and the relationships between them.</S>
			<S sid ="31" ssid = "31">MedLEE (Friedman et al., 1994), and more recently BioMedLEE (Lussier et al., 2006) used a semantic lexicon and grammar of domain-specific semantic patterns.</S>
			<S sid ="32" ssid = "32">The patterns encode the possible relationships between entities, allowing both entities and the relationships between them to be directly matched in the text.</S>
			<S sid ="33" ssid = "33">Other systems have incorporated large- scale domain-specific knowledge bases.</S>
			<S sid ="34" ssid = "34">MEDSYNDIKATE (Hahn et al., 2002) employed a rich discourse model of entities and their relationships, built using a dependency parse of texts and a description logic knowledge base re-engineered from existing terminologies.</S>
			<S sid ="35" ssid = "35">MENELAS (Zweigenbaum et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base.</S>
			<S sid ="36" ssid = "36">In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text.</S>
			<S sid ="37" ssid = "37">Such approaches have typically been applied to journal texts.</S>
			<S sid ="38" ssid = "38">They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)).</S>
			<S sid ="39" ssid = "39">This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)).</S>
			<S sid ="40" ssid = "40">Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition.</S>
			<S sid ="41" ssid = "41">The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications.</S>
			<S sid ="42" ssid = "42">To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text.</S>
			<S sid ="43" ssid = "43">This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narratives.</S>
			<S sid ="44" ssid = "44">The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE, such as querying to support clinical research.</S>
			<S sid ="45" ssid = "45">We apply Support Vector Machine (SVM) classifiers to learn these relationships.</S>
			<S sid ="46" ssid = "46">The classifiers are trained and evaluated using novel data: a gold standard corpus of clinical text, hand-annotated with semantic entities and relationships.</S>
			<S sid ="47" ssid = "47">In order to test the applicability of this method to the clinical domain, we train classifiers using a number of comparatively simple text features, and look at the contribution of these features to system performance.</S>
			<S sid ="48" ssid = "48">Clinically interesting relationships may span several sentences, and so we compare classifiers trained for both intra- and inter- sentential relationships (spanning one or more sentence boundaries).</S>
			<S sid ="49" ssid = "49">We also examine the influence of training corpus size on performance, as hand annotation of training data is the major expense in supervised machine learning.</S>
	</SECTION>
	<SECTION title="Relationship Schema. " number = "2">
			<S sid ="50" ssid = "1">Relationship Argument 1 Argument 2 has target Investigation Locus Intervention Locus has finding Investigation Condition Investigation Result has indication Drug or device Condition Intervention Condition Investigation Condition has location Condition Locus negation modifies Negation modifier Condition laterality modifies Laterality modifier Intervention Laterality modifier Locus sub-location modifies Sub-location modifier Locus Table 1: Relationship types and their argument type constraints.</S>
			<S sid ="51" ssid = "2">The CLEF application extracts entities, relationships and modifiers from text.</S>
			<S sid ="52" ssid = "3">By entity, we mean some real-world thing, event or state referred to in the text: the drugs that are mentioned, the tests that were carried out, etc. Modifiers are words that qualify an entity in some way, referring e.g. to the laterality of an anatomical locus, or the negation of a condition (“no sign of inflammation”).</S>
			<S sid ="53" ssid = "4">Entities are connected to each other and to modifiers by relationships: e.g. linking a drug entity to the condition entity for which it is indicated, linking an investigation to its results, or linking a negating phrase to a condition.</S>
			<S sid ="54" ssid = "5">The entities, modifiers, and relationships are described by both a formal XML schema, and by a set of detailed definitions.</S>
			<S sid ="55" ssid = "6">These were developed by a group of clinical experts through an iterative process, until acceptable agreement was reached.</S>
			<S sid ="56" ssid = "7">Entity types are mapped to types from the UMLS semantic network (Lindberg et al., 1993), each CLEF en tity type covering several UMLS types.</S>
			<S sid ="57" ssid = "8">Relationship types are those that were felt necessary to capture the essential clinical dependencies between entities referred to in patient documents, and to support CLEF end user applications.</S>
			<S sid ="58" ssid = "9">Each relationship type is constrained to exist between limited pairs of entity types.</S>
			<S sid ="59" ssid = "10">For example, the has location relationship can only exist between a Condition entity and a Locus entity.</S>
			<S sid ="60" ssid = "11">Some relationships can exist between multiple type pairs.</S>
			<S sid ="61" ssid = "12">The full set of relationships and their argument type constraints are shown in Table 1.</S>
			<S sid ="62" ssid = "13">Examples of each relationship are given in Roberts et al (2008a).</S>
			<S sid ="63" ssid = "14">Some of the relationships considered important by the clinical experts were not obvious without domain knowledge.</S>
			<S sid ="64" ssid = "15">For example, He is suffering from nausea and severe headaches.</S>
			<S sid ="65" ssid = "16">Dolasteron was prescribed.</S>
			<S sid ="66" ssid = "17">Without domain knowledge, it is not clear that there is a has indication relationship between the “Dolasteron” Drug or device entity and the “nausea” Condition entity.</S>
			<S sid ="67" ssid = "18">As in this example, many of this type of relationship are intrasentential.</S>
			<S sid ="68" ssid = "19">A single real-world entity may be referred to several times in the same text.</S>
			<S sid ="69" ssid = "20">Each of these co- referring expressions is a mention of the entity.</S>
			<S sid ="70" ssid = "21">The gold standard includes annotation of co-reference between different textual mentions of the same entity.</S>
			<S sid ="71" ssid = "22">For the work reported in this paper, however, co-reference is not considered.</S>
			<S sid ="72" ssid = "23">Each entity is assumed to have a single mention.</S>
			<S sid ="73" ssid = "24">Relationships between entities can be considered, by extension, as relationships between the single mentions of those entities.</S>
			<S sid ="74" ssid = "25">The implications of this are discussed further below.</S>
	</SECTION>
	<SECTION title="Gold Standard Corpus. " number = "3">
			<S sid ="75" ssid = "1">The schema and definitions were used to hand- annotate the entities and relationships in 77 oncology narratives, to provide a gold standard for system training and evaluation.</S>
			<S sid ="76" ssid = "2">Corpora of this size are typical in supervised machine learning, and reflect the expense of hand annotation.</S>
			<S sid ="77" ssid = "3">Narratives were carefully selected and annotated according to a best practice methodology, as described in Roberts et al (2008a).</S>
			<S sid ="78" ssid = "4">Narratives were annotated by two independent, clinically trained, annotators, and a consensus created by a third.</S>
			<S sid ="79" ssid = "5">We will refer to this corpus as C77.</S>
			<S sid ="80" ssid = "6">Annotators were asked to first mark the mentions of entities and modifiers, and then to go through each of these in turn, deciding if any had relationships with mentions of other entities.</S>
			<S sid ="81" ssid = "7">Although the annotators were marking co-reference between mentions of the same entity, they were asked to ignore this with respect to relationship annotation.</S>
			<S sid ="82" ssid = "8">Both the annotation tool that they were using and their annotation guidelines, enforced the creation of relationships between mentions, and not between entities.</S>
			<S sid ="83" ssid = "9">The gold standard is thus analogous to the style of relationship extraction reported here, in which we extract relations between single mention entities, and do not consider co-reference.</S>
			<S sid ="84" ssid = "10">Annotators were further told that relationships could span multiple sentences, and that it was acceptable to use clinical domain knowledge to infer that a relationship existed between two mentions.</S>
			<S sid ="85" ssid = "11">Counts of all relationships annotated in C77 are shown in Table 2, subdivided by the number of sentence boundaries spanned by a relationship.</S>
	</SECTION>
	<SECTION title="Relationship Extraction. " number = "4">
			<S sid ="86" ssid = "1">The system we have built uses the GATE NLP toolkit (Cunningham et al., 2002) 1.</S>
			<S sid ="87" ssid = "2">The system is shown in Figure 1, and is described below.</S>
			<S sid ="88" ssid = "3">Narratives are first pre-processed using standard GATE modules.</S>
			<S sid ="89" ssid = "4">Narratives were tokenised, sentences found with a regular expression-based sentence splitter, part-of-speech (POS) tagged, and morphological roots found for tokens.</S>
			<S sid ="90" ssid = "5">Each token was also labelled with a generalised POS tag, the first two characters of the full POS tag.</S>
			<S sid ="91" ssid = "6">This takes advantage of the Penn Treebank tagset used by GATE’s POS tagger, in which related POS tags share the first two characters.</S>
			<S sid ="92" ssid = "7">For example, all six verb POS tags start with the letters “VB”.</S>
			<S sid ="93" ssid = "8">After pre-processing, mentions of entities within the text are annotated.</S>
			<S sid ="94" ssid = "9">In the experiments reported, we assume perfect entity recognition, as given by the entities in the human annotated gold standard 1 We used a development build of GATE 4.0, downloadable from http://gate.ac.uk Sentence boundaries between arguments To tal 0 1 2 3 4 5 6 7 8 9 &gt;9 has fin din g 26 5 46 25 7 5 4 3 2 2 2 0 36 1 has ind ica tio n 13 9 85 35 32 14 11 6 4 5 5 12 34 8 has loc ati on 36 0 4 1 1 1 1 1 0 0 0 4 37 3 has tar get 12 2 14 4 2 2 4 3 1 0 1 0 15 3 lat era lity mo difi es 12 8 0 0 0 0 0 0 0 0 0 0 12 8 neg atio n mo difi es 10 0 1 0 0 0 0 0 0 0 0 0 10 1 sub loc atio n mo difi es 76 0 0 0 0 0 0 0 0 0 0 76 Tot al 11 90 15 0 65 42 22 20 13 7 7 8 16 15 40 Cu mu lati ve tot al 11 90 13 40 14 05 14 47 14 69 14 89 15 02 15 09 15 16 15 24 15 40 Table 2: Count of relationships in 77 gold standard documents.</S>
			<S sid ="95" ssid = "10">described above.</S>
			<S sid ="96" ssid = "11">Our results are therefore higher than would be expected in a system with automatic entity recognition.</S>
			<S sid ="97" ssid = "12">It is useful and usual to fix entity recognition in this way, to allow tuning specific to relationship extraction, and to allow the isolation of relation-specific problems.</S>
			<S sid ="98" ssid = "13">We accept, however, that ultimately, relation extraction does depend on the quality of entity recognition.</S>
			<S sid ="99" ssid = "14">The relation extraction described here is used as part of an operational IE system in which clinical entity recognition is performed by a combination of lexical lookup and supervised machine learning.</S>
			<S sid ="100" ssid = "15">We have described our entity extraction system elsewhere (Roberts et al., 2008b).</S>
			<S sid ="101" ssid = "16">4.1 Classification.</S>
			<S sid ="102" ssid = "17">We treat clinical relationship extraction as a classification task, training classifiers to assign a relationship type to an entity pair.</S>
			<S sid ="103" ssid = "18">An entity pair is a pairing of entities that may or may not be the arguments of a relation.</S>
			<S sid ="104" ssid = "19">For a given document, we create all possible entity pairs within two constraints.</S>
			<S sid ="105" ssid = "20">First, entities that are paired must be within n sentences of each other.</S>
			<S sid ="106" ssid = "21">For all of the work reported here, unless stated, n ≤ 1 (crossing 0 or 1 sentence boundaries).</S>
			<S sid ="107" ssid = "22">Second, we can constrain the entity pairs created by argument type (Rindflesch and Fiszman, 2003).</S>
			<S sid ="108" ssid = "23">For example, there is little point in creating an entity pair between a Drug or device entity and a Result entity, as no relationships, as specified by the schema, exist between entities of these types.</S>
			<S sid ="109" ssid = "24">Entity pairing is carried out by a GATE component developed specifically for clinical relationship extraction.</S>
			<S sid ="110" ssid = "25">In addition to pairing entities according to the above constraints, this component also assigns features to each pair that characterise its lexical and syntactic qualities (described further in Section 4.2).</S>
			<S sid ="111" ssid = "26">Entity pairs correspond to classifier training and test instances.</S>
			<S sid ="112" ssid = "27">In classifier training, if an entity pair corresponds to the arguments of a relationship present in the gold standard, then it is assigned a class of that relationship type.</S>
			<S sid ="113" ssid = "28">If it does not correspond to such a relation, then it is assigned the class null.</S>
			<S sid ="114" ssid = "29">The classifier builds a model of these entity pair training instances, from their features.</S>
			<S sid ="115" ssid = "30">In classifier application, entity pairs are created from unseen text, under the above constraints.</S>
			<S sid ="116" ssid = "31">The classifier assigns one of our seven relationship types, or null, to each entity pair.</S>
			<S sid ="117" ssid = "32">We use Support Vector machines (SVMs) as train- able classifiers, as these have proved to be robust and efficient for a range of NLP tasks, including relation extraction.</S>
			<S sid ="118" ssid = "33">We use an SVM implementation developed within our own group, and provided as part of the GATE toolkit.</S>
			<S sid ="119" ssid = "34">This is a variant on the original SVM algorithm, SVM with uneven margins, in which classification may be biased towards positive training examples.</S>
			<S sid ="120" ssid = "35">This is particularly suited to NLP applications, in which positive training examples are often rare.</S>
			<S sid ="121" ssid = "36">Full details of the classifier are given in Li et al (2005).</S>
			<S sid ="122" ssid = "37">We used the implementation “out of the box”, with default parameters as determined in experiments with other data sets.</S>
			<S sid ="123" ssid = "38">SVMs are binary classifiers: the multi-class problem of classifying entity pairs must therefore be mapped to a number of binary classification problems.</S>
			<S sid ="124" ssid = "39">There are several ways in which a multi- class problem can be recast as binary problems.</S>
			<S sid ="125" ssid = "40">The commonest are one-against-one in which one classifier is trained for every possible pair of classes, and one-against-all in which a classifier is trained for a binary decision between each class and all other classes, including null, combined.</S>
			<S sid ="126" ssid = "41">We have carried out extensive experiments (not reported here), with these two strategies, and have found little difference between them for our data.</S>
			<S sid ="127" ssid = "42">We have chosen to use one-against-all, as it needs fewer classifiers (for an n class problem, it needs n classifiers, as op posed to (n−1)!</S>
			<S sid ="128" ssid = "43">for one-against-one).</S>
			<S sid ="129" ssid = "44">The resultant class assignments by multiple binary classifiers must be post-processed to deal with ambiguity.</S>
			<S sid ="130" ssid = "45">In application to unseen text, it is possible that several classifiers assign different classes to an entity pair (test instance).</S>
			<S sid ="131" ssid = "46">To disambiguate these cases, the output of each one-against-all classifier is transformed into a probability, and the class with the highest probability is assigned.</S>
			<S sid ="132" ssid = "47">Recasting the multi-class relation problem as a number of binary problems, and post-processing to resolve ambiguities, is handled by the GATE Learning API.</S>
			<S sid ="133" ssid = "48">Figure 1: The relationship extraction system.</S>
			<S sid ="134" ssid = "49">4.2 Features for Classification.</S>
			<S sid ="135" ssid = "50">The SVM classification model is built from lexical and syntactic features assigned to tokens and entity pairs prior to classification.</S>
			<S sid ="136" ssid = "51">We use features developed in part from those described in Zhou et al (2005) and Wang et al (2006).</S>
			<S sid ="137" ssid = "52">These features are split into 11 sets, as described in Table 3.</S>
			<S sid ="138" ssid = "53">The tokN features are POS and surface string taken from a window of N tokens on each side of each paired entity’s mention.</S>
			<S sid ="139" ssid = "54">For N = 6, this gives 48 features.</S>
			<S sid ="140" ssid = "55">The rationale behind these simple features is that there is useful information in the words surrounding two mentions, that helps deter mine any relationship between them.</S>
			<S sid ="141" ssid = "56">The gentokN features generalise tokN to use morphological root and generalised POS.</S>
			<S sid ="142" ssid = "57">The str features are a set of 14 surface string features, encoding the full surface strings of both entity mentions, their heads, their heads combined, the surface strings of the first, last and other tokens between the mentions, and of the two tokens immediately before and after the leftmost and rightmost mentions respectively.</S>
			<S sid ="143" ssid = "58">The pos, root, and genpos feature sets are similarly constructed from the POS tags, roots, and generalised POS tags of the entity mentions and their surrounding tokens.</S>
			<S sid ="144" ssid = "59">These four feature sets differ from tokN and gentokN, in that they provide more fine- grained information about the position of features relative to the paired entity mentions.</S>
			<S sid ="145" ssid = "60">For the event feature set, the main entities were divided into events (Investigation and Intervention) and nonevents (all others).</S>
			<S sid ="146" ssid = "61">Features record whether the entity pair consists of two events, two nonevents, one of each, and whether there are any intervening events and nonevents.</S>
			<S sid ="147" ssid = "62">This feature set gives similar information to atype (semantic types of arguments) and inter (intervening entities), but at a coarser level of typing.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "5">
			<S sid ="148" ssid = "1">We used a standard tenfold cross validation methodology and standard evaluation metrics.</S>
			<S sid ="149" ssid = "2">Metrics are defined in terms of true positive, false positive and false negative matches between relationships in a system annotated response document and a gold standard key document.</S>
			<S sid ="150" ssid = "3">A response relationship is a true positive if a relationship of the same type, and with the exact same arguments, exists in the key.</S>
			<S sid ="151" ssid = "4">Corresponding definitions apply for false positive and false negative.</S>
			<S sid ="152" ssid = "5">Counts of these matches are used to calculate standard metrics of Recall (R), Precision (P ) and F 1 measure.</S>
			<S sid ="153" ssid = "6">The metrics do not say how hard relationship extraction is. We therefore provide a comparison with Inter Annotator Agreement (IAA) scores from the gold standard.</S>
			<S sid ="154" ssid = "7">The IAA score gives the agreement between the two independent double annotators.</S>
			<S sid ="155" ssid = "8">It is equivalent to scoring one annotator against the other using the F 1 metric.</S>
			<S sid ="156" ssid = "9">IAA scores are not directly comparable here, as relationship annotation is Fea tur e set Siz e De sc rip tio n tok N 8N Sur fac e stri ng an d PO S of tok ens sur rou ndi ng the arg um ent s, win do we d − N to + N , N = 6 by def aul t ge nto kN 8N Ro ot an d ger en alis ed PO S of tok ens sur rou ndi ng the arg um ent enti ties , win do we d − N to + N , N = 6 by def aul t aty pe 1 Co nca ten ate d se ma ntic typ e of arg um ent s, in arg1 arg 2 or de r dir 1 Dir ecti on: line ar tex t ord er of the arg um ent s (is arg 1 bef ore arg 2, or vic e ver sa?</S>
			<S sid ="157" ssid = "10">) dist 2 Dis tan ce: abs olut e nu mb er of sen ten ce and par agr aph bou nda ries bet we en arg um ent s str 14 Sur fac e stri ng feat ure s bas ed on Zh ou et al (20 05), see text for full de scr ipti on po s 14 PO S feat ure s, as abo ve roo t 14 Ro ot feat ure s, as abo ve ge np os 14 Ge ner alis ed PO S feat ure s, as abo ve int er 11 Int erv eni ng me ntio ns: nu mb ers an d typ es of inte rve nin g enti ty me ntio ns bet we en arg um ent s eve nt 5 Ev ent s: are any of the arg um ent s, or inte ven ing ent itie s, eve nts ? allg en 96 All fea tur es in roo t an d ge ner alis ed PO S for ms, i.e. ge nto k6 +at yp e+ dir +di st+ ro ot+ ge np os +in ter +e ve nt not ok 48 All exc ept tok N fea tur es, oth ers in stri ng an d PO S for ms, i.e. aty pe +di r+ dis t+s tr+ po s+i nte r+ ev ent Table 3: Feature sets used for learning relationships.</S>
			<S sid ="158" ssid = "11">The size of a set is the number of features in that set.</S>
			<S sid ="159" ssid = "12">a slightly different task for the human annotators.</S>
			<S sid ="160" ssid = "13">The relationship extraction system is given entities, and finds relationships between them.</S>
			<S sid ="161" ssid = "14">Human an- notators must find both the entities and the relationships.</S>
			<S sid ="162" ssid = "15">Therefore, were one human annotator to fail to find a particular entity, they could never find relationships with that entity.</S>
			<S sid ="163" ssid = "16">The raw IAA score does not take this into account: if an annotator fails to find an entity, then they will also be penalised for all relationships with that entity.</S>
			<S sid ="164" ssid = "17">We therefore give a Corrected IAA, CIAA, in which annotators are only compared on those relations for which they have both found the entities involved.</S>
			<S sid ="165" ssid = "18">Both forms of IAA are shown in Table 4.</S>
			<S sid ="166" ssid = "19">It is clear that it is hard for annotators to reach agreement on relationships, and that this is compounded massively by lack of perfect agreement on entities.</S>
			<S sid ="167" ssid = "20">Note that the gold standard used in training and evaluation reflects a further consensus annotation, to correct this poor agreement.</S>
	</SECTION>
	<SECTION title="Results. " number = "6">
			<S sid ="168" ssid = "1">6.1 Feature Selection.</S>
			<S sid ="169" ssid = "2">The first group of experiments reported looks at the performance of relation extraction with various feature sets.</S>
			<S sid ="170" ssid = "3">We followed an additive strategy for feature selection.</S>
			<S sid ="171" ssid = "4">Starting with basic features, we added further features one set at a time.</S>
			<S sid ="172" ssid = "5">We measured the performance of the resulting classifier each time we added a new feature set.</S>
			<S sid ="173" ssid = "6">Results are shown in Table 4.</S>
			<S sid ="174" ssid = "7">The initial classifier used a tok6+atype feature set.</S>
			<S sid ="175" ssid = "8">Addition of both dir and dist features give significant improvements in all metrics, of around 10% F 1 overall, in each case.</S>
			<S sid ="176" ssid = "9">This suggests that the linear text order of arguments, and whether relations are intra- or inter-sentential is important to classification.</S>
			<S sid ="177" ssid = "10">Addition of the str features also give good improvement in most metrics, again 10% F 1 overall.</S>
			<S sid ="178" ssid = "11">Addition of part-of-speech information, in the form of pos features, however, leads to a drop in some metrics, overall F 1 dropping by 1%.</S>
			<S sid ="179" ssid = "12">Unexpectedly, POS seems to provide little extra information above that in surface string.</S>
			<S sid ="180" ssid = "13">Errors in POS tagging cannot be dismissed, and could be the cause of this.</S>
			<S sid ="181" ssid = "14">The existence of intervening entities, as coded in feature set inter, provides a small benefit.</S>
			<S sid ="182" ssid = "15">The inclusion of information about events, in the event feature set, is less clear-cut.</S>
			<S sid ="183" ssid = "16">We were interested to see if generalising features could improve performance, as this had benefited our previous work in entity extraction.</S>
			<S sid ="184" ssid = "17">We replaced all surface string features with their root form, and POS features with their generalised POS form.</S>
			<S sid ="185" ssid = "18">This gave the results shown in column allgen.</S>
			<S sid ="186" ssid = "19">Results are not clear cut, in some cases better and in some worse than the previous best.</S>
			<S sid ="187" ssid = "20">Overall, there is no difference in F 1.</S>
			<S sid ="188" ssid = "21">There is a slight increase in overall recall, and a corresponding drop in precision — as might be expected.</S>
			<S sid ="189" ssid = "22">Both the tokN, and the str and pos feature sets provide surface string and POS information about tokens surrounding and between relationship arguments.</S>
			<S sid ="190" ssid = "23">The former gives features from a window around each argument.</S>
			<S sid ="191" ssid = "24">The latter two give a greater amount of positional information.</S>
			<S sid ="192" ssid = "25">Do these two provide enough information on their own, without the windowed features?</S>
			<S sid ="193" ssid = "26">To test this, we removed the tokN features from the full cumulative feature set, from column +event.</S>
			<S sid ="194" ssid = "27">Results are given in column Rel ati on Me tric to k6 +a ty pe +di r +di st +st r +p os +in ter +e ve nt all ge n not ok IA A CI AA has fin din g P 44 49 58 63 62 64 65 63 63 R 39 63 78 80 80 81 81 82 82 F1 39 54 66 70 69 71 72 71 71 46 80 has ind ica tio n P 37 23 38 42 40 41 42 37 44 R 14 14 46 44 44 47 47 45 47 F1 18 16 39 39 38 41 42 38 41 26 50 has loc ati on P 36 36 50 68 71 72 72 73 73 R 28 28 74 79 79 81 81 83 83 F1 30 30 58 72 74 76 75 77 76 55 80 has tar get P 9 9 32 63 57 60 62 60 59 R 11 11 51 68 67 67 66 68 68 F1 9 9 38 64 60 63 63 63 62 42 63 lat era lity mo difi es P 21 38 73 84 83 84 84 86 86 R 9 55 82 89 86 88 88 87 89 F1 12 44 76 85 83 84 84 84 85 73 94 neg atio n mo difi es P 19 54 85 81 80 79 79 77 81 R 12 82 97 98 93 92 93 93 93 F1 13 63 89 88 85 84 85 83 85 66 93 sub loc atio n mo difi es P 2 2 55 88 86 86 88 88 87 R 1 1 62 94 92 95 95 95 95 F1 1 1 56 90 86 89 91 91 90 49 96 Ov era ll P 33 38 50 63 62 64 65 64 64 R 22 36 70 74 73 75 75 76 76 F1 26 37 58 68 67 69 69 69 70 47 75 Table 4: Variation in performance by feature set.</S>
			<S sid ="195" ssid = "28">Features sets are abbreviated as in Table 3.</S>
			<S sid ="196" ssid = "29">For the first seven columns, features were added cumulatively to each other.</S>
			<S sid ="197" ssid = "30">The next two columns, allgen and notok, are as described in Table 3.</S>
			<S sid ="198" ssid = "31">The final two columns give inter annotator agreement and corrected inter annotator agreement, for comparison.</S>
			<S sid ="199" ssid = "32">notok.</S>
			<S sid ="200" ssid = "33">There is no clear change in performance, some relationships improving, and some worsening.</S>
			<S sid ="201" ssid = "34">Overall, there is a 1% improvement in F 1.</S>
			<S sid ="202" ssid = "35">It appears that the bulk of performance is attained through entity type and distance features, with some contribution from positional surface string information.</S>
			<S sid ="203" ssid = "36">Performance is between 1% and 9% lower than CIAA for the same relationship, with a best overall F 1 of 70%, compared to a CIAA of 75%.</S>
			<S sid ="204" ssid = "37">6.2 Sentences Spanned.</S>
			<S sid ="205" ssid = "38">Table 2 shows that although most relationships are intrasentential, 23% are inter-sentential, 10% of all relationships being between arguments in adjacent sentences.</S>
			<S sid ="206" ssid = "39">If we consider a relationship to cross n sentence boundaries, then the classifiers described in the previous section were all trained on relationshipscrossing n ≤ 1 sentence boundaries, i.e. with argu ments in the same or adjacent sentences.</S>
			<S sid ="207" ssid = "40">What effect does including more distant relationships have on performance?</S>
			<S sid ="208" ssid = "41">We trained classifiers on only intrasentential relationships, and on relationships span ning up to n sentence boundaries, for n ∈ {1...5}.</S>
			<S sid ="209" ssid = "42">We also trained a classifier on relationships with 1 ≤ n ≤ 5, comprising 85% of all inter-sentential relationships.</S>
			<S sid ="210" ssid = "43">In each case, the cumulative feature set +event from Table 4 was used.</S>
			<S sid ="211" ssid = "44">Results are shown in Table 5.</S>
			<S sid ="212" ssid = "45">It is clear from the results that the feature sets used do not perform well on inter- sentential relationships.</S>
			<S sid ="213" ssid = "46">There is a 6% drop in overall F 1 when including relationships with n = 1 together with n &lt; 1.</S>
			<S sid ="214" ssid = "47">Performance continues to drop as more inter-sentential relationships are included, and is very poor for just inter-sentential relationships.</S>
			<S sid ="215" ssid = "48">A preliminary error analysis suggests that the more distant relationship arguments are from each other, the more likely clinical knowledge is required to extract the relationship.</S>
			<S sid ="216" ssid = "49">This raises additional difficulties for extraction, which the simple features described here are unable to address.</S>
			<S sid ="217" ssid = "50">6.3 Size of Training Corpus.</S>
			<S sid ="218" ssid = "51">The provision of sufficient training data for supervised learning algorithms is a limitation on their use.</S>
			<S sid ="219" ssid = "52">We examined the effect of training corpus size on relationship extraction.</S>
			<S sid ="220" ssid = "53">The C77 corpus, compris Number of sentence boundaries between arguments Corpus size i n t e r intra inter- and intra sentential Rel ati on Me tric 1 ≤ n ≤ 5 n &lt; 1 n ≤ 1 n ≤ 2 n ≤ 3 n ≤ 4 n ≤ 5 C2 5 C5 0 C7 7 has fin din g P 24 68 65 62 60 61 61 66 63 65 R 18 89 81 79 78 78 77 74 74 81 F1 18 76 72 69 67 68 67 67 67 72 has ind ica tio n P 18 49 42 42 36 32 30 22 25 42 R 17 59 47 42 42 39 38 30 31 47 F1 16 51 42 39 37 34 33 23 25 42 has loc ati on P 0 74 72 73 72 72 72 72 71 72 R 0 83 81 81 81 82 82 76 80 81 F1 0 77 75 76 75 76 76 73 74 75 has tar get P 3 64 62 59 60 59 58 65 49 62 R 1 75 66 64 62 61 61 60 65 66 F1 2 68 63 61 60 60 59 59 54 63 lat era lity mo difi es P 0 86 84 86 86 86 87 77 78 84 R 0 89 88 88 88 87 88 69 68 88 F1 0 85 84 85 86 85 86 72 69 84 neg atio n mo difi es P 0 80 79 79 80 80 80 78 79 79 R 0 94 93 91 93 93 93 80 93 93 F1 0 86 85 84 85 86 85 78 84 85 sub loc atio n mo difi es P 0 89 88 88 89 89 89 64 91 88 R 0 95 95 95 95 95 95 64 85 95 F1 0 91 91 91 91 91 91 64 86 91 Ov era ll P 22 69 65 64 62 61 60 62 63 65 R 17 83 75 73 71 70 70 65 71 75 F1 19 75 69 68 66 65 65 63 66 69 Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.</S>
			<S sid ="221" ssid = "54">ing 77 narratives and used in the previous experiments, was subsetted to give corpora of 25 and 50 narratives, which will be referred to as C25 and C50 respectively.</S>
			<S sid ="222" ssid = "55">We trained two further classifiers on these new corpora.</S>
			<S sid ="223" ssid = "56">Again, the cumulative feature set +event from Table 4 was used.</S>
			<S sid ="224" ssid = "57">Results are shown in Table 5.</S>
			<S sid ="225" ssid = "58">Overall, performance improves as training corpus size increases (F 1 rising from 63% to 69%).</S>
			<S sid ="226" ssid = "59">We were struck however, by the fact that increasing from 50 to 77 documents has little effect on a few relationships (negation modifies and has location).</S>
			<S sid ="227" ssid = "60">It may well be that the amount of training data required has plateaued for those relationships.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "7">
			<S sid ="228" ssid = "1">We have shown that it is possible to extract clinical relationships from text, using shallow features, and supervised statistical machine learning.</S>
			<S sid ="229" ssid = "2">Judging from poor inter annotator agreement, the task is hard.</S>
			<S sid ="230" ssid = "3">Our system achieves a reasonable performance, with an overall F 1 just 5% below a corrected inter annotator agreement.</S>
			<S sid ="231" ssid = "4">This performance is reached largely by using features of the text that encode entity type, distance between arguments, and some surface string information.</S>
			<S sid ="232" ssid = "5">Performance does, however, vary with the number of sentences spanned by the relationships.</S>
			<S sid ="233" ssid = "6">Learning inter-sentential relationships does not seem amenable to this approach, and may require the use of domain knowledge.</S>
			<S sid ="234" ssid = "7">A major concern when using supervised learning algorithms is the expense and availability of training data.</S>
			<S sid ="235" ssid = "8">We have shown that while this concern is justified in some cases, larger training corpora may not improve performance for all relationships.</S>
			<S sid ="236" ssid = "9">The technology used has proved scalable.</S>
			<S sid ="237" ssid = "10">The full CLEF IE system, including automatic entity recognition, is able to process a document in sub- second time on a commodity workstation.</S>
			<S sid ="238" ssid = "11">We have used the system to extract 6 million relations from over half a million patient documents, for use in downstream CLEF applications (Roberts et al., 2008a).</S>
			<S sid ="239" ssid = "12">Our future work on relationship extraction in CLEF includes integration of a dependency parse into the feature set, further analysis to determine what knowledge may be required to learn inter- sentential relations, and integration of relationship extraction with a co-reference algorithm.</S>
			<S sid ="240" ssid = "13">Availability All of the software described here is open source and can be downloaded as part of GATE, with the exception of the entity pairing component, which will be released shortly.</S>
			<S sid ="241" ssid = "14">We are currently preparing a UK research ethics committee application, requesting permission to release our annotated corpus.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="242" ssid = "15">CLEF is funded by the UK Medical Research Council.</S>
			<S sid ="243" ssid = "16">We would like to thank the Royal Marsden Hospital for providing the corpus, and our clinical partners in CLEF for assistance in developing the schema, and for gold standard annotation.</S>
	</SECTION>
</PAPER>
