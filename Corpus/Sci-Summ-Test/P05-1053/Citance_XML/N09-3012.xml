<PAPER>
	<ABSTRACT>
	<SECTION title="Introduction. " number = "1">
			<S sid ="1" ssid = "1">One of the contemporary demanding NLP tasks is information extraction, which is the procedure of extracting structured information such as entities, relations, and events from free text documents.</S>
			<S sid ="2" ssid = "2">As an information extraction sub-task, semantic relation extraction is the procedure of finding predefined semantic relations between textual entity mentions.</S>
			<S sid ="3" ssid = "3">For instance, assuming a semantic relation with type Physical and subtype Located between an entity of type Person and another entity of type Location, the sentence &quot;Police arrested Mark at the airport last week.&quot; conveys two mentions of this relation between &quot;Mark&quot; and Kernel functions can implicitly capture a large amount of features efficiently; thus, they have been widely used in various NLP tasks.</S>
			<S sid ="4" ssid = "4">Various types of features have been exploited so far for relation extraction.</S>
			<S sid ="5" ssid = "5">In (Bunescu and Mooney, 2005b) sequence of words features are utilized using a sub-sequence kernel.</S>
			<S sid ="6" ssid = "6">In (Bunescu and Mooney, 2005a) dependency graph features are exploited, and in (Zhang et al., 2006a) syntactic features are employed for relation extraction.</S>
			<S sid ="7" ssid = "7">Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction.</S>
			<S sid ="8" ssid = "8">66 Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 66–71, Boulder, Colorado, June 2009.</S>
			<S sid ="9" ssid = "9">Qc 2009 Association for Computational Linguistics In CD’01 (Collins and Duffy, 2001) a convolution syntactic tree kernel is proposed that generally measures the syntactic similarity between parse trees.</S>
			<S sid ="10" ssid = "10">In this paper, a generalized version of CD’01 convolution tree kernel is proposed by associating generic weights to the nodes and sub-trees of the parse tree.</S>
			<S sid ="11" ssid = "11">These weights can be used to incorporate domain knowledge into the kernel and make it more flexible and customizable.</S>
			<S sid ="12" ssid = "12">The generalized kernel can be conveniently used to generate a variety of syntactic sub-kernels (including the original CD’01 kernel), by adopting appropriate weighting mechanisms.As a result, in this paper, novel syntactic sub finally found to be the best performing tree portion.</S>
			<S sid ="13" ssid = "13">PT is a portion of parse tree that is enclosed by the shortest path between the two relation arguments.</S>
			<S sid ="14" ssid = "14">Moreover, this tree kernel is combined with an entity kernel to form a reportedly high quality composite kernel in (Zhang et al., 2006b).</S>
			<S sid ="15" ssid = "15">3 CD’01 Convolution Tree Kernel.</S>
			<S sid ="16" ssid = "16">In (Collins and Duffy, 2001), a convolution tree kernel has been introduced that measures the syntactic similarity between parse trees.</S>
			<S sid ="17" ssid = "17">This kernel computes the inner products of the following feature vector.</S>
			<S sid ="18" ssid = "18">kernels are generated from the generalized kernel for the task of relation extraction.</S>
			<S sid ="19" ssid = "19">Evaluations size1 H (T ) = (λ 2 sizen sizei # subTree1 (T ),..., λ 2 # subTreei (T ),..., demonstrate that these kernels outperform theoriginal CD’01 kernel in the extraction of ACE 2005 main relation types λ 2 0 &lt; λ ≤ 1 # subTreen (T )) (2) The remainder of this paper is structured as follows.</S>
			<S sid ="20" ssid = "20">In section 2, the most related works are briefly reviewed.</S>
			<S sid ="21" ssid = "21">In section 3, CD’01 tree kernel is described.</S>
			<S sid ="22" ssid = "22">The proposed generalized convolution tree kernel is explained in section 4 and its produced sub-kernels for relation extraction are Each feature of this vector is the occurrence count of a sub-tree type in the parse tree decayed exponentially by the parameter λ . Without this decaying mechanism used to retain the kernel values within a fairly small range, the value of the kernel for identical trees becomes far higher than illustrated in section 5.</S>
			<S sid ="23" ssid = "23">The experimental results are discussed in section 6.</S>
			<S sid ="24" ssid = "24">Our work is concluded its value for different trees.</S>
			<S sid ="25" ssid = "25">Term sizei is defined in section 7 and some possible future works are presented in section 8.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="26" ssid = "1">In (Collins and Duffy, 2001), a convolution parse tree kernel has been introduced.</S>
			<S sid ="27" ssid = "2">This kernel is generally designed to measure syntactic similarity to be the number of rules or internal nodes of the ith sub-tree type.</S>
			<S sid ="28" ssid = "3">Samples of such sub-trees are shown in Fig.</S>
			<S sid ="29" ssid = "4">1 for a simple parse tree.</S>
			<S sid ="30" ssid = "5">Since the number of sub-trees of a tree is exponential in its size (Collins and Duffy, 2001), direct inner product calculation is computationally infeasible.</S>
			<S sid ="31" ssid = "6">Consequently, Collins and Duffy (2001) proposed an ingenious kernel function that implicitly between parse trees and is especially exploited for parsing English sentences in their paper.</S>
			<S sid ="32" ssid = "7">Since calculates the inner product in on the trees of size N and N . O( N1 × N 2 ) time then, the kernel has been widely used in different 1 2 applications such as semantic role labeling (Moschitti, 2006b) and relation extraction (Zhang et al., 2006a; Zhang et al., 2006b; Zhou et al., 2007; Li et al. 2008).</S>
			<S sid ="33" ssid = "8">For the first time, in (Zhang et al., 2006a), this convolution tree kernel was used for relation extraction.</S>
			<S sid ="34" ssid = "9">Since the whole syntactic parse tree of the sentence that holds the relation arguments 4 A Generalized Convolution Tree Kernel In order to describe the kernel, a feature vector over the syntactic parse tree is firstly defined in eq.</S>
			<S sid ="35" ssid = "10">(3), in which the ith feature equals the weighted sum of the number of instances of sub-tree type ith in the tree.</S>
			<S sid ="36" ssid = "11">contains a plenty of misleading features, several parse tree portions are studied to find the most Function I subtreei (n) is an indicator function that feature-rich portion of the syntactic tree for returns 1 if the subtreei occurs with its root at relation extraction, and Path-Enclosed Tree (PT) is node n and 0 otherwise.</S>
			<S sid ="37" ssid = "12">As described in eq.</S>
			<S sid ="38" ssid = "13">(4), function tw(T) (which stands for &quot;tree weight&quot;) assigns a weight to a tree T which is equal to the product of the weights of all its nodes.</S>
			<S sid ="39" ssid = "14">H (T ) = ( ∑ [ I subtree (n ) × tw( subtree 1 (n ))],..., n∈T a Cgc(n1,n2 ) function over all tree node pairs of T1 and T2.</S>
			<S sid ="40" ssid = "15">Function Cgc(n1,n2 ) is the weighted sum of the common sub-trees rooted at n1 and n2, and can be recursively computed in a similar way to ∑ [ I subtree i (n ) × tw( subtree i (n ))],..., (3) function C (n , n ) of (Collins and Duffy, 2001) as n∈T 1 2 follows.</S>
			<S sid ="41" ssid = "16">∑ [ I subtree m (n ) × tw( subtree m (n ))]) n∈T (1) if the production rules of nodes n1 and n2 are tw(T ) = ∏ inw( n) n∈InternalNode s (T ) × ∏ enw(n) n∈Extern alNodes (T ) (4) diff eren t then Cgc (n1 , n2 ) = 0 (2) els e if n1 an d n2 ar e th e sa me pre ter mi na ls (th e sa me pa rt of sp ee ch es) th en (1) (2) (3) NP NP NP PP DT the PP IN NP C gc (n1 , n2 ) = inw(n1 ) × enw (child (n1 )) × inw(n2 ) × enw(child (n2 )) NP PP (4) (5) DT NN at (3) else if both n1 and n2 have the same producti on NNP IN NP DT NN NP NP PP NN airport the airport rules then C gc (n1 , n2 ) = inw(n1 ) × inw(n2 ) × Mark at (6) (7) the airport NNP IN Mark NP NP DT NN NP NP PP ∏[enw(chil di (n1 )) × enw(childi (n2 )) +C gc (childi (n1 ), childi (n2 ))] i In the first case, when the two nodes repres ent the airport NNP Figure 1.</S>
			<S sid ="42" ssid = "17">Samples of sub-trees used in convolution tree kernel calculation.</S>
			<S sid ="43" ssid = "18">Since each node of the whole syntactic tree can either happen as an internal node or as an external node of a supposed sub-tree (presuming its existence in the sub-tree), two types of weights are respectively associated to each node by the different production rules they can&apos;t accordingly have any sub-trees in common.</S>
			<S sid ="44" ssid = "19">In the second case, there is exactly one common sub-tree of size two.</S>
			<S sid ="45" ssid = "20">It should be noted that all the leaf nodes of the tree (or words of the sentence) are considered identical in the calculation of the tree kernel.</S>
			<S sid ="46" ssid = "21">The value of the function in this case is the weight of this common sub-tree.</S>
			<S sid ="47" ssid = "22">In the third case, when the nodes functions inw(n) and enw(n) (which respectivel y generally represent the same productio n rules the stand for &quot;internal node weight&quot; and &quot;external node weight&quot;).</S>
			<S sid ="48" ssid = "23">For instance, in Fig.</S>
			<S sid ="49" ssid = "24">1, the node with label PP is an external node for sub-trees (1) and (7) while it is an internal node of sub-trees (3) and (4).</S>
			<S sid ="50" ssid = "25">K (T1 ,T2 ) = &lt; H (T1 ), H (T2 ) &gt; weighted sum of the common sub-trees are calculated recursively.</S>
			<S sid ="51" ssid = "26">The equation holds because the existence of common sub-trees rooted at n1 and n2 implies the existence of common sub trees rooted at their corresponding children, which can be combined multiplicatively to form their parents&apos; common sub-trees.</S>
			<S sid ="52" ssid = "27">= ∑[ i [ ∑ I subtreei (n1 ) × tw(subT reei (n1 ))] n1∈T1 D ue to the equi valen t proc edur e of kern el calcu latio n, this gene raliz ed versi on of the tree × [ ∑ I subtreei (n2 ) × tw(subTreei (n2 ))]] n2∈T2 (5) kernel preserves the nice O( N1 × N2 ) time = ∑ ∑[∑ I subtreei (n1 ) × I subtreei (n2 ) × complexity property of the original kernel.</S>
			<S sid ="53" ssid = "28">It is worthy of note that in (Moschitti, 2006b) a sorting n1∈T1 n2∈T2 i tw(subTre ei (n1 )) × tw(subTre ei (n2 ))] based method is proposed for the fast implementation of such tree kernels that reduces = ∑ ∑Cgc (n1 , n2 ) n1∈T1 n2∈T2 the average running time to O( N1 + N2 ) . As shown in eq.</S>
			<S sid ="54" ssid = "29">(5), A similar procedure to The generalized kernel can be converted to (Collins and Duffy, 2001) can be employed to CD’01 kernel by defining inw(n) = λ and develop a kernel function for the calculation of dot products on H(T) vectors.</S>
			<S sid ="55" ssid = "30">According to eq.</S>
			<S sid ="56" ssid = "31">(5) the calculation of the kernel finally leads to the sum of enw(n) = 1 . Likewise, other definitions can be utilized to produce other useful sub-kernels.</S>
			<S sid ="57" ssid = "32">5 Kernels for Relation Extraction.</S>
			<S sid ="58" ssid = "33">In this section, three sub-kernels of the generalized convolution tree kernel will be proposed for relation extraction.</S>
			<S sid ="59" ssid = "34">Using the embedded weights of the generalized kernel, these sub-kernels differentiate among sub-trees based on their ancestor path, the less it is decayed by this weighting method.</S>
			<S sid ="60" ssid = "35">S NP VP expected relevance to semantic relations.</S>
			<S sid ="61" ssid = "36">More specifically, the sub-trees are weighted according to how their nodes interact to the arguments of the relation.</S>
			<S sid ="62" ssid = "37">NNP Police VBN arrested NP NP PP NP JJ NN NNP IN NP last week 5.1 Argument Ancestor Path Kernel (AAP).</S>
			<S sid ="63" ssid = "38">AAPDist(airport, NP)=1 Mark at DT NN Definition of weighting functions is shown in eq. the airport (6) and (7).</S>
			<S sid ="64" ssid = "39">Parameter parameter similar to λ . 0 &lt; α ≤ 1 is a decaying Figure 2.</S>
			<S sid ="65" ssid = "40">The syntactic parse tree of the sentence &quot;Police arrested Mark at the airport last week&quot; that conveys a Phys.Located(Mark, airport) relation.</S>
			<S sid ="66" ssid = "41">The α inw(n) =   0 1 enw(n) =   0 if n is on the argument ancestor path or a direct child of a node on it otherwi se if n is on the argument ancestor path or a direct child of a node on it otherwise (6) (7) ancestor path of the argume nt &quot;airport &quot; (dashed curve) and the distance of the node NP of &quot;Mark&quot; from it (dotted curve) is shown.</S>
			<S sid ="67" ssid = "42">5.3 Threshold Sensitive Argument Ancestor Path Dista nce Kern el (TSA APD ) This weighting method is equivalent to applying CD’01 tree kernel (by setting λ = α 2 ) on a portion of the parse tree that exclusively includes the arguments ancestor nodes and their direct children.</S>
			<S sid ="68" ssid = "43">5.2 Argument Ancestor Path Distance Kernel.</S>
			<S sid ="69" ssid = "44">This kernel is intuitively similar to the previous kernel but uses a rough threshold based decaying technique instead of a smooth one.</S>
			<S sid ="70" ssid = "45">The definition of weighting functions is shown in eq.</S>
			<S sid ="71" ssid = "46">(10) and (11).</S>
			<S sid ="72" ssid = "47">Both functions are again identical in this case.</S>
			<S sid ="73" ssid = "48">(AAPD) Min ( AAPDist ( n ,arg1 ), AAPDist ( n ,arg 2 )) 1 inw(n) =  α AAPDist (n) ≤ Threshold AAPDist (n) ≥ Threshold (10) inw(n ) = α enw (n ) = α M A X _ D I S T Min ( AAPDist ( n ,arg1 ), AAPDist ( n ,arg 2 )) M A X _ D I S T (8) (9) 1 enw(n) =  α AAPDist (n) ≤ Threshold AAPDist (n) ≥ Threshold (11) Definition of weighting functions is shown in eq.</S>
			<S sid ="74" ssid = "49">(8) and (9).</S>
			<S sid ="75" ssid = "50">Both functions have identical definitions for this kernel.</S>
			<S sid ="76" ssid = "51">Function AAPDist(n,arg) calculates the distance of the node n from the argument arg on the parse tree as illustrated by Fig.</S>
			<S sid ="77" ssid = "52">2.</S>
			<S sid ="78" ssid = "53">MAX_DIST is used for normalization, and is the maximum of AAPDist(n,arg) in the whole tree.</S>
			<S sid ="79" ssid = "54">In this way, the closer a tree node is to one of the arguments 6 Experiments.</S>
			<S sid ="80" ssid = "55">6.1 Experiments.</S>
			<S sid ="81" ssid = "56">Setting The proposed kernels are evaluated on ACE-2005 multilingual corpus (Walker et al., 2006).</S>
			<S sid ="82" ssid = "57">In order to avoid parsing problems, the more formal parts of the corpus in &quot;news wire&quot; and &quot;broadcast news&quot; sections are used for evaluation as in (Zhang et al., 2006b) . R e l a t i o n K e r n e l P E R S O C A R T G E N A F F O R G A F F PA RT W H OL E P H Y S C D ’ 0 1 0 . 6 2 0 . 5 1 0 . 0 9 0 . 4 3 0 . 3 0 0 . 3 2 A A P 0 . 5 8 0 . 4 9 0 . 1 0 0 . 4 3 0 . 2 8 0 . 3 6 A A P D 0 . 7 0 0 . 5 0 0 . 1 2 0 . 4 3 0 . 2 9 0 . 2 9 T S A A P D 0 0 . 6 3 0 . 4 8 0 . 1 1 0 . 4 3 0 . 3 0 0 . 3 3 T S A A P D 1 0 . 7 3 0 . 4 7 0 . 1 1 0 . 4 5 0 . 2 8 0 . 3 3 Table 1: The F1-Measure value is shown for every kernel on each ACE-2005 main relation type.</S>
			<S sid ="83" ssid = "58">For every relation type the best result is shown in bold font.</S>
			<S sid ="84" ssid = "59">We have used LIBSVM (Chang and Lin 2001) java source for the SVM classification and Stanford NLP package1 for tokenization, sentence segmentation and parsing.</S>
			<S sid ="85" ssid = "60">Following [Bunescu and Mooney, 2007], every pair of entities within a sentence is regarded as a negative relation instance unless it is annotated as a positive relation in the corpus.</S>
			<S sid ="86" ssid = "61">The total number of negative training instances, constructed in this way, is about 20 times more than the number of annotated positive instances.</S>
			<S sid ="87" ssid = "62">Thus, we also imposed the restriction of maximum argument distance of 10 words.</S>
			<S sid ="88" ssid = "63">This constraint eliminates half of the negative constructed instances while slightly decreases positive instances.</S>
			<S sid ="89" ssid = "64">Nevertheless, since the resulted training set is still unbalanced, we used LIBSVM weighting mechanism.</S>
			<S sid ="90" ssid = "65">Precisely, if there are P positive and N negative instances in the training set, a weight value of except for AAP are computed on the PT portion described in section 2.</S>
			<S sid ="91" ssid = "66">AAP is computed over the MCT tree portion which is also proposed by (Zhang et al., 2006a) and is the sub-tree rooted at the first common ancestor of relation arguments.</S>
			<S sid ="92" ssid = "67">For the proposed kernels α is set to 0.44 which is tuned on a development set that contained 5000 instances of type PHYS.</S>
			<S sid ="93" ssid = "68">The λ parameter of CD’01 kernel is set to 0.4 according to (Zhang et al., 2006a).</S>
			<S sid ="94" ssid = "69">The C parameter of SVM classification is set to 2.4 for all the kernels after tuning it individually for each kernel on the mentioned development set.</S>
			<S sid ="95" ssid = "70">6.2 Experiments Results.</S>
			<S sid ="96" ssid = "71">The results of the experiments are shown in Table 1.</S>
			<S sid ="97" ssid = "72">The proposed kernels outperform the original.</S>
			<S sid ="98" ssid = "73">CD’01 kernel in four of the six relation types.</S>
			<S sid ="99" ssid = "74">The N / P is used for positiv e instan ces while the perfor manc e of TSAAPD1 is especially default weight value of 1 is used for negative ones.</S>
			<S sid ="100" ssid = "75">A binary SVM is trained for every relation type separately, and type compatible annotated and constructed relation instances are used to train it.</S>
			<S sid ="101" ssid = "76">For each relation type, only type compatible relation instances are exploited for training.</S>
			<S sid ="102" ssid = "77">For example to learn an ORGAFF relation (which applies to (PER, ORG) or (ORG, ORG) argument types) it is meaningless to use a relation instance between two entities of type PERSON.</S>
			<S sid ="103" ssid = "78">Moreover, the total number of training instances used for training every relation type is restricted to 5000 instances to shorten the duration of the evaluation process.</S>
			<S sid ="104" ssid = "79">The reported results are achieved using a 5-fold cross validation method.</S>
			<S sid ="105" ssid = "80">The kernels AAP, AAPD and TSAAPD0 (TSAAPD with threshold = 0) and TSAAPD1 (TSAAPD with threshold = 1) are compared with CD’01 convolution tree kernel.</S>
			<S sid ="106" ssid = "81">All the kernels 1 http://nlp.stanford.edu/software/index.shtmlremarkable because it is the best kernel in ORG AFF and PER-SOC relations.</S>
			<S sid ="107" ssid = "82">It particularly performs very well in the extraction of PER-SOC relation with an F1-measure of 0.73.</S>
			<S sid ="108" ssid = "83">It should be noted that the general low performance of all the kernels on the GENAFF type is because of its extremely small number of annotated instances in the training set (40 in 5000).</S>
			<S sid ="109" ssid = "84">The AAPD kernel has the best performance with a remarkable improvement over the Collins kernel in GENAFF relation type.</S>
			<S sid ="110" ssid = "85">The results clearly demonstrate that the nodes closer to the ancestor path of relation arguments contain the most useful syntactic features for relation extraction 7 Conclusion.</S>
			<S sid ="111" ssid = "86">In this paper, we proposed a generalized convolution tree kernel that can generate various syntactic sub-kernels including the CD’01 kernel.</S>
			<S sid ="112" ssid = "87">The kernel is generalized by assigning weights to the sub-trees.</S>
			<S sid ="113" ssid = "88">The weight of a sub-tree is the product of the weights assigned to its nodes by two types of weighting functions.</S>
			<S sid ="114" ssid = "89">In this way, impacts of the tree nodes on the kernel value can be discriminated purposely based on the application.</S>
			<S sid ="115" ssid = "90">Context information can also be injected to the kernel via context sensitive weighting mechanisms.</S>
			<S sid ="116" ssid = "91">Using the generalized kernel, various sub- kernels can be produced by different definitions of the two weighting functions.</S>
			<S sid ="117" ssid = "92">We consequently used the generalized kernel for systematic generation of useful kernels in relation extraction.</S>
			<S sid ="118" ssid = "93">In these kernels, the closer a node is to the relation arguments ancestor paths, the less it is decayed by the weighting functions.</S>
			<S sid ="119" ssid = "94">Evaluation on the ACE 2005 main relation types demonstrates the effectiveness of the proposed kernels.</S>
			<S sid ="120" ssid = "95">They show remarkable performance improvement over CD’01 kernel.</S>
			<S sid ="121" ssid = "96">8 Future Work.</S>
			<S sid ="122" ssid = "97">Although the path-enclosed tree portion (PT) (Zhang et al., 2006a) seems to be an appropriate portion of the syntactic tree for relation extraction, it only takes into account the syntactic information between the relation arguments, and discards many useful features (before and after the arguments features).</S>
			<S sid ="123" ssid = "98">It seems that the generalized kernel can be used with larger tree portions that contain syntactic features before and after the arguments, because it can be more easily targeted to related features.</S>
			<S sid ="124" ssid = "99">Currently, the proposed weighting mechanisms are solely based on the location of the tree nodes in the parse tree; however other useful information such as labels of nodes can also be used in weighting.</S>
			<S sid ="125" ssid = "100">Another future work can be utilizing the generalized kernel for other applicable NLP tasks such as co-reference resolution.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="126" ssid = "101">This work is supported by Iran Telecommunication Research Centre under contract No. 5007725.</S>
	</SECTION>
</PAPER>
