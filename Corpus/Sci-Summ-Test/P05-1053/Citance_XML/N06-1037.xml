<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper proposes to use a convolution kernel over parse trees to model syntactic structure information for relation extraction.</S>
		<S sid ="2" ssid = "2">Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these features can be well captured by the convolution tree kernel.</S>
		<S sid ="3" ssid = "3">Evaluation on the ACE 2003 corpus shows that the convolution kernel over parse trees can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes.</S>
		<S sid ="4" ssid = "4">It also shows that our method significantly outperforms the previous two dependency tree kernels on the 5 ACE relation major types.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Relation extraction is a subtask of information extraction that finds various predefined semantic relations, such as location, affiliation, rival, etc., between pairs of entities in text.</S>
			<S sid ="6" ssid = "6">For example, the sentence “George Bush is the president of the United States.” conveys the semantic relation “President” between the entities “George Bush” (PER) and “the United States” (GPE: a GeoPolitical Entity --- an entity with land and a government (ACE, 2004)).</S>
			<S sid ="7" ssid = "7">Prior feature-based methods for this task (Kambhatla 2004; Zhou et al., 2005) employed a large amount of diverse linguistic features, varyingfrom lexical knowledge, entity mention informa tion to syntactic parse trees, dependency trees and semantic features.</S>
			<S sid ="8" ssid = "8">Since a parse tree contains rich syntactic structure information, in principle, the features extracted from a parse tree should contribute much more to performance improvement for relation extraction.</S>
			<S sid ="9" ssid = "9">However it is reported (Zhou et al., 2005; Kambhatla, 2004) that hierarchical structured syntactic features contributes less to performance improvement.</S>
			<S sid ="10" ssid = "10">This may be mainly due to the fact that the syntactic structure information in a parse tree is hard to explicitly describe by a vector of linear features.</S>
			<S sid ="11" ssid = "11">As an alternative, kernel methods (Collins and Duffy, 2001) provide an elegant solution to implicitly explore tree structure features by directly computing the similarity between two trees.</S>
			<S sid ="12" ssid = "12">But to our surprise, the sole two-reported dependency tree kernels for relation extraction on the ACE corpus (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004) showed much lower performance than the feature-based methods.</S>
			<S sid ="13" ssid = "13">One may ask: are the syntactic tree features very useful for relation extraction?</S>
			<S sid ="14" ssid = "14">Can tree kernel methods effectively capture the syntactic tree features and other various features that have been proven useful in the feature-based methods?</S>
			<S sid ="15" ssid = "15">In this paper, we demonstrate the effectiveness of the syntactic tree features for relation extraction and study how to capture such features via a convolution tree kernel.</S>
			<S sid ="16" ssid = "16">We also study how to select the optimal feature space (e.g. the set of sub-trees to represent relation instances) to optimize the system performance.</S>
			<S sid ="17" ssid = "17">The experimental results show that the convolution tree kernel plus entity features achieves slightly better performance than the previous best-reported feature-based methods.</S>
			<S sid ="18" ssid = "18">It also shows that our method significantly outperforms the two dependency tree kernels (Bunescu and Mooney, 2005; Culotta and Sorensen, 2004) on the 5 ACE relation types..</S>
			<S sid ="19" ssid = "19">The rest of the paper is organized as follows.</S>
			<S sid ="20" ssid = "20">In Section 2, we review the previous work.</S>
			<S sid ="21" ssid = "21">Section 3 discusses our tree kernel based learning algorithm.</S>
			<S sid ="22" ssid = "22">288 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 288–295, New York, June 2006.</S>
			<S sid ="23" ssid = "23">Qc 2006 Association for Computational Linguistics Section 4 shows the experimental results and compares our work with the related work.</S>
			<S sid ="24" ssid = "24">We conclude our work in Section 5.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="25" ssid = "1">The task of relation extraction was introduced as a part of the Template Element task in MUC6 and formulated as the Template Relation task in MUC7 (MUC, 19871998).</S>
			<S sid ="26" ssid = "2">Miller et al.</S>
			<S sid ="27" ssid = "3">(2000) address the task of relation extraction from the statistical parsing viewpoint.</S>
			<S sid ="28" ssid = "4">They integrate various tasks such as POS tagging, NE tagging, template extraction and relation extraction into a generative model.</S>
			<S sid ="29" ssid = "5">Their results essentially depend on the entire full parse tree.</S>
			<S sid ="30" ssid = "6">Kambhatla (2004) employs Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text for relation extraction.</S>
			<S sid ="31" ssid = "7">Zhou et al.</S>
			<S sid ="32" ssid = "8">(2005) explore various features in relation extraction using SVM.</S>
			<S sid ="33" ssid = "9">They conduct exhaustive experiments to investigate the incorporation and the individual contribution of diverse features.</S>
			<S sid ="34" ssid = "10">They report that chunking information contributes to most of the performance improvement from the syntactic aspect.</S>
			<S sid ="35" ssid = "11">The features used in Kambhatla (2004) and Zhou et al.</S>
			<S sid ="36" ssid = "12">(2005) have to be selected and carefully calibrated manually.</S>
			<S sid ="37" ssid = "13">Kambhatla (2004) use the path of non-terminals connecting two mentions in a parse tree as the parse tree features.</S>
			<S sid ="38" ssid = "14">Besides, Zhou et al.</S>
			<S sid ="39" ssid = "15">(2005) introduce additional chunking features to enhance the parse tree features.</S>
			<S sid ="40" ssid = "16">However, the hierarchical structured information in the parse trees is not well preserved in their parse tree- related features.</S>
			<S sid ="41" ssid = "17">As an alternative to the feature-based methods, kernel methods (Haussler, 1999) have been proposed to implicitly explore features in a high dimensional space by employing a kernel function to calculate the similarity between two objects directly.</S>
			<S sid ="42" ssid = "18">In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004).</S>
			<S sid ="43" ssid = "19">This is because a kernel can measure the similarity between two discrete structured objects directly using the original representation of the objects instead of explicitly enumerating their features.</S>
			<S sid ="44" ssid = "20">Zelenko et al.</S>
			<S sid ="45" ssid = "21">(2003) develop a tree kernel for relation extraction.</S>
			<S sid ="46" ssid = "22">Their tree kernel is recursively defined in a top-down manner, matching nodes from roots to leaf nodes.</S>
			<S sid ="47" ssid = "23">For each pair of matching nodes, a subsequence kernel on their child nodes is invoked, which matches either contiguous or sparse subsequences of node.</S>
			<S sid ="48" ssid = "24">Culotta and Sorensen (2004) generalize this kernel to estimate similarity between dependency trees.</S>
			<S sid ="49" ssid = "25">One may note that their tree kernel requires the matchable nodes must be at the same depth counting from the root node.</S>
			<S sid ="50" ssid = "26">This is a strong constraint on the matching of syntax so it is not surprising that the model has good precision but very low recall on the ACE corpus (Zhao and Grishman, 2005).</S>
			<S sid ="51" ssid = "27">In addition, according to the top-down node matching mechanism of the kernel, once a node is not matchable with any node in the same layer in another tree, all the sub-trees below this node are discarded even if some of them are matchable to their counterparts in another tree.</S>
			<S sid ="52" ssid = "28">Bunescu and Mooney (2005) propose a shortest path dependency kernel for relation extraction.</S>
			<S sid ="53" ssid = "29">They argue that the information to model a relationship between entities is typically captured by the shortest path between the two entities in the dependency graph.</S>
			<S sid ="54" ssid = "30">Their kernel is very straightforward.</S>
			<S sid ="55" ssid = "31">It just sums up the number of common word classes at each position in the two paths.</S>
			<S sid ="56" ssid = "32">We notice that one issue of this kernel is that they limit the two paths must have the same length, otherwise the kernel similarity score is zero.</S>
			<S sid ="57" ssid = "33">Therefore, although this kernel shows nontrivial performance improvement than that of Culotta and Sorensen (2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.</S>
			<S sid ="58" ssid = "34">Zhao and Grishman (2005) define a feature- based composite kernel to integrate diverse features.</S>
			<S sid ="59" ssid = "35">Their kernel displays very good performance on the 2004 version of ACE corpus.</S>
			<S sid ="60" ssid = "36">Since this is a feature-based kernel, all the features used in the kernel have to be explicitly enumerated.</S>
			<S sid ="61" ssid = "37">Similar with the feature-based method, they also represent the tree feature as a link path between two entities.</S>
			<S sid ="62" ssid = "38">Therefore, we wonder whether their performance improvement is mainly due to the explicitly incorporation of diverse linguistic features instead of the kernel method itself.</S>
			<S sid ="63" ssid = "39">The above discussion suggests that the syntactic features in a parse tree may not be fully utilized in the previous work, whether feature-based or kernel-based.</S>
			<S sid ="64" ssid = "40">We believe that the syntactic tree features could play a more important role than that reported in the previous work.</S>
			<S sid ="65" ssid = "41">Since convolution kernels aim to capture structural information in terms of substructures, which providing a viable alternative to flat features, in this paper, we propose to use a convolution tree kernel to explore syntactic features for relation extraction.</S>
			<S sid ="66" ssid = "42">To our knowledge, convolution kernels have not been explored for relation extraction1.</S>
	</SECTION>
	<SECTION title="Tree Kernels for Relation Extraction. " number = "3">
			<S sid ="67" ssid = "1">In this section, we discuss the convolution tree kernel associated with different relation feature spaces.</S>
			<S sid ="68" ssid = "2">In Subsection 3.1, we define seven different relation feature spaces over parse trees.</S>
			<S sid ="69" ssid = "3">In Subsection 3.2, we introduce a convolution tree kernel for relation extraction.</S>
			<S sid ="70" ssid = "4">Finally we compare our method with the previous work in Subsection 3.3.</S>
			<S sid ="71" ssid = "5">3.1 Relation Feature Spaces.</S>
			<S sid ="72" ssid = "6">In order to study which relation feature spaces (i.e., which portion of parse trees) are optimal for relation extraction, we define seven different relation feature spaces as follows (as shown in Figure 1): (1) Minimum Complete Tree (MCT): It is the complete sub-tree rooted by the node of the nearest common ancestor of the two entities under consideration.</S>
			<S sid ="73" ssid = "7">(2) Path-enclosed Tree (PT): It is the smallest common sub-tree including the two entities.</S>
			<S sid ="74" ssid = "8">In other words, the sub-tree is en closed by the shortest path linking the two entities in the parse tree (this path is also typically used asthe path tree features in the feature-based meth ods).</S>
			<S sid ="75" ssid = "9">(3) Chunking Tree (CT): It is the base phrase list extracted from the PT.</S>
			<S sid ="76" ssid = "10">We prune out all the internal structures of the PT and only keep the root node and the base phrase list for generating the chunking tree.</S>
			<S sid ="77" ssid = "11">1 Convolution kernels were proposed as a concept of kernels.</S>
			<S sid ="78" ssid = "12">for a discrete structure by Haussler (1999) in machine learning study.</S>
			<S sid ="79" ssid = "13">This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects.</S>
			<S sid ="80" ssid = "14">Convolution kernels are abstract concepts, and the instances of them are determined by the definition of “sub-kernels”.</S>
			<S sid ="81" ssid = "15">The Tree Kernel (Collins and Duffy, 2001), String Subsequence Kernel (SSK) (Lodhi et al., 2002) and Graph Kernel (HDAG Kernel) (Suzuki et al., 2003) are examples of convolution kernels instances in the NLP field.</S>
			<S sid ="82" ssid = "16">(4) Context-Sensitive Path Tree (CPT): It is the PT extending with the 1st left sibling of the node of entity 1 and the 1st right sibling of the node of entity 2.</S>
			<S sid ="83" ssid = "17">If the sibling is unavailable, then we move to the parent of current node and repeat the same process until the sibling is available or the root is reached.</S>
			<S sid ="84" ssid = "18">(5) Context-Sensitive Chunking Tree (CCT): It is the CT extending with the 1st left sibling of the node of entity 1 and the 1st right sibling of the node of entity 2.</S>
			<S sid ="85" ssid = "19">If the sibling is unavailable, the same process as generating the CPT is applied.</S>
			<S sid ="86" ssid = "20">Then we do a further pruning process to guarantee that the context structures of the CCT is still a list of base phrases.</S>
			<S sid ="87" ssid = "21">(6) Flattened PT (FPT): We define two criteria to flatten the PT in order to generate the Flattened Parse tree: if the in and out arcs of a non-terminal node (except POS node) are both single, the node is to be removed; if a node has the same phrase type with its father node, the node is also to be removed.</S>
			<S sid ="88" ssid = "22">(7) Flattened CPT (FCPT): We use the above two criteria to flatten the CPT tree to generate the Flattened CPT.</S>
			<S sid ="89" ssid = "23">Figure 1 in the next page illustrates the different sub-tree structures for a relation instance in sentence “Akyetsu testified he was powerless to stop the merger of an estimated 2000 ethnic Tutsi&apos;s in the district of Tawba.”.</S>
			<S sid ="90" ssid = "24">The relation instance is an example excerpted from the ACE corpus, where an ACE-defined relation “AT.LOCATED” exists between the entities “Tutsi&apos;s” (PER) and “district” (GPE).</S>
			<S sid ="91" ssid = "25">We use Charniak’s parser (Charniak, 2001) to parse the example sentence.</S>
			<S sid ="92" ssid = "26">Due to space limitation, we do not show the whole parse tree of the entire sentence here.</S>
			<S sid ="93" ssid = "27">Tree T1 in Figure 1 is the MCT of the relation instance example, where the substructure circled by a dashed line is the PT.</S>
			<S sid ="94" ssid = "28">For clarity, we redraw the PT as in T2.</S>
			<S sid ="95" ssid = "29">The only difference between the MCT and the PT lies in that the MCT does not allow the partial production rules.</S>
			<S sid ="96" ssid = "30">For instance, the most-left two-layer sub-tree [NP [DT … E1-O-PER]] in T1 is broken apart in T2.</S>
			<S sid ="97" ssid = "31">By comparing the performance of T1 and T2, we can test whether the substructures with partial production rules as in T2 will decrease performance.</S>
			<S sid ="98" ssid = "32">T3 is the CT. By comparing the performance of T2 and T3, we want to study whether the chunk- ing information or the parse tree is more effective T1): MCT T2): PT T3): CT T4):CPT T5):CCT T7):FCPT T6):FPT Figure 1.</S>
			<S sid ="99" ssid = "33">Relation Feature Spaces of the Example Sentence “…… to stop the merger of an estimated 2000 ethnic Tutsi&apos;s in the district of Tawba.”, where the phrase type “E1-O-PER” denotes that the current phrase is the 1st entity, its entity type is “PERSON” and its mention level is “NOMIAL”, and likewise for the other two phrase types “E2-O-GPE” and “E-N-GPE”.</S>
			<S sid ="100" ssid = "34">for relation extraction.</S>
			<S sid ="101" ssid = "35">T4 is the CPT, where the two structures circled by dashed lines are the so- called context structures.</S>
			<S sid ="102" ssid = "36">T5 is the CCT, where the additional context structures are also circled by dashed lines.</S>
			<S sid ="103" ssid = "37">We want to study if the limited context information in the CPT and the CCT can help boost performance.</S>
			<S sid ="104" ssid = "38">Moreover, we illustrate the other two flattened trees in T6 and T7.</S>
			<S sid ="105" ssid = "39">The two circled nodes in T2 are removed in the flattened trees.</S>
			<S sid ="106" ssid = "40">We want to study if the eliminated small structures are noisy features for relation extraction.</S>
			<S sid ="107" ssid = "41">3.2 The Convolution Tree Kernel.</S>
			<S sid ="108" ssid = "42">Given the relation instances defined in the previous section, we use the same convolution tree kernel as the parse tree kernel (Collins and Duffy, 2001) and the semantic kernel (Moschitti, 2004).</S>
			<S sid ="109" ssid = "43">Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors):φ(T ) = (# of sub-trees of type 1, …, # of sub trees of type i, …, # of sub-trees of type n) This results in a very high dimensionality since the number of different sub-trees is exponential in its size.</S>
			<S sid ="110" ssid = "44">Thus it is computational infeasible to directlyuse the feature vector φ(T ) . To solve the compu tational issue, we introduce the tree kernel function which is able to calculate the dot product between the above high dimensional vectors efficiently.</S>
			<S sid ="111" ssid = "45">The kernel function is defined as follows: K (T1 , T2 ) =&lt; φ (T1 ),φ (T2 ) &gt;= ∑i φ (T1 )[i],φ (T2 )[i] sider the entire substructure types and their occurring frequencies.</S>
			<S sid ="112" ssid = "46">In this way, on the one hand, the parse tree-related features in the flat feature set2 are embedded in the feature space of our method: “Base Phrase Chunking” and “Parse Tree” features explicitly appear as substructures of a parse = ∑ n ∈N ∑ n ∈N ∑i Ii (n1 ) ∗Ii (n2 )tree.</S>
			<S sid ="113" ssid = "47">A few of entity related features in the flat feature set are also captured by our feature space: “en where N1 and N2 are the sets of all nodes in trees T1and T2, respectively, and Ii(n) is the indicator func tion that is 1 iff a sub-tree of type i occurs with root at node n and zero otherwise.</S>
			<S sid ="114" ssid = "48">Collins and Duffy (2002) show that K (T1,T2 ) is an instance of convolution kernels over tree structures, and whichcan be computed in O(| N1 | × | N2 |) by the follow tity type” and “mention level” explicitly appear as phrase types in a parse tree.</S>
			<S sid ="115" ssid = "49">On the other hand, the other features in the flat feature set, such as “word features”, “bigram word features”, “overlap” and “dependency tree” are not contained in our feature space.</S>
			<S sid ="116" ssid = "50">From the syntactic viewpoint, the tree representation in our feature space is more robust than “Parse Tree Path” feature in the flat feature set ing recursive definitions (Let ∑i Ii (n1 ) ∗Ii (n2 ) ): ∆(n1 , n2 ) = since the path feature is very sensitive to the small changes of parse trees (Moschitti, 2004) and it also does not maintain the hierarchical information of a (1) if n1 and n2 do not have the same syntactic tag or their children are different then ∆(n1, n2 ) = 0 ; (2) else if their children are leaves (POS tags), then ∆(n1, n2 ) = 1× λ ; nc(n1 )parse tree.</S>
			<S sid ="117" ssid = "51">Due to the extensive exploration of syn tactic features by kernel, our method is expected to show better performance than the previous feature- based methods.</S>
			<S sid ="118" ssid = "52">It is also worth comparing our method with the previous relation kernels.</S>
			<S sid ="119" ssid = "53">Since our method only (3) else ∆(n1, n2 ) = λ ∏ (1+∆(ch(n 1, j), ch(n2 , j))) , j = 1 counts the occurre nce of each sub-tree without conside ring its ancestor s, our method is not limited where nc(n1 ) is the number of the children of n1 , by the constraints in Culotta and Sorensen (2004) ch(n, j) is the jth child of node n and and that in Bunescu and Mooney (2005) as dis λ ( 0 &lt; λ &lt; 1) is the decay factor in order to make the kernel value less variable with respect to the tree sizes.</S>
			<S sid ="120" ssid = "54">3.3 Comparison with Previous Work.</S>
			<S sid ="121" ssid = "55">It would be interesting to review the differences between our method and the feature-based methods.</S>
			<S sid ="122" ssid = "56">The basic difference between them lies in the relation instance representation and the similarity calculation mechanism.</S>
			<S sid ="123" ssid = "57">A relation instance in our method is represented as a parse tree while it is represented as a vector of features in the feature- based methods.</S>
			<S sid ="124" ssid = "58">Our method estimates the similarity between two relation instances by only counting the number of substructures that are in common while the feature methods calculate the dot-product between the feature vectors directly.</S>
			<S sid ="125" ssid = "59">The main difference between them is the different feature spaces.</S>
			<S sid ="126" ssid = "60">By the kernel method, we implicitly represent a parse tree by a vector of integer counts of each substructure type.</S>
			<S sid ="127" ssid = "61">That is to say, we con cussed in Section 2.</S>
			<S sid ="128" ssid = "62">Compared with Zhao and Grishman’s kernel, our method directly uses the original representation of a parse tree while they flatten a parse tree into a link and a path.</S>
			<S sid ="129" ssid = "63">Given the above improvements, our method is expected to outperform the previous relation kernels.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "4">
			<S sid ="130" ssid = "1">The aim of our experiment is to verify the effectiveness of using richer syntactic structures and the convolution tree kernel for relation extraction.</S>
			<S sid ="131" ssid = "2">4.1 Experimental Setting.</S>
			<S sid ="132" ssid = "3">Corpus: we use the official ACE corpus for 2003 evaluation from LDC as our test corpus.</S>
			<S sid ="133" ssid = "4">The ACE corpus is gathered from various newspaper, news- wire and broadcasts.</S>
			<S sid ="134" ssid = "5">The same as previous work 2 For the convenience of discussion, without losing generality,.</S>
			<S sid ="135" ssid = "6">we call the features used in Zhou et al.</S>
			<S sid ="136" ssid = "7">(2005) and Kambhatla (2004) flat feature set.</S>
			<S sid ="137" ssid = "8">(Zhou et al., 2005), our experiments are carried out on explicit relations due to the poor inter-annotator agreement in annotation of implicit relations and their limited numbers.</S>
			<S sid ="138" ssid = "9">The training set consists of 674 annotated text documents and 9683 relation instances.</S>
			<S sid ="139" ssid = "10">The test set consists of 97 documents and 1386 relation instances.</S>
			<S sid ="140" ssid = "11">The 2003 evaluation defined 5 types of entities: Persons, Organizations, Locations, Facilities and GPE.</S>
			<S sid ="141" ssid = "12">Each mention of an entity is associated with a mention type: proper name, nominal or pronoun.</S>
			<S sid ="142" ssid = "13">They further defined 5 major relation types and 24 subtypes: AT (Base-In, Located…), NEAR (Relative-Location), PART (Part-of, Subsidiary …), ROLE (Member, Owner 4.2 Experimental.</S>
			<S sid ="143" ssid = "14">Results In order to study the impact of the sole syntactic structure information embedded in parse trees on relation extraction, we remove the entity information from parse trees by replacing the entity-related phrase type (“E1-O-PER”, etc., in Figure 1) with “NP”.</S>
			<S sid ="144" ssid = "15">Then we carry out a couple of preliminary experiments on the test set using parse trees regardless of entity information.</S>
			<S sid ="145" ssid = "16">Feature Spaces P R F Minimum Complete Tree 77.45 38.39 51.34 Path-enclosed Tree (PT) 72.77 53.80 61.87 relation (except 1 symmetric type)) and 43 classes on the subtypes (2 for each relation subtype and a “NONE” class for non-relation (except 6 symmetric subtypes)).</S>
			<S sid ="146" ssid = "17">In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators).</S>
			<S sid ="147" ssid = "18">Classifier: we select SVM as the classifier used in this paper since SVM can naturally work with kernel methods and it also represents the state-of-the- art machine learning algorithm.</S>
			<S sid ="148" ssid = "19">We adopt the one vs. others strategy and select the one with largest margin as the final answer.</S>
			<S sid ="149" ssid = "20">The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)).</S>
			<S sid ="150" ssid = "21">In our implementation, we use the binary SVMLight developed by Joachims (1998) and Tree Kernel Toolkits developed by Moschitti (2004).</S>
			<S sid ="151" ssid = "22">Kernel Normalization: since the size of a parse tree is not constant, we normalize K (T , T ) by divid Table 1.</S>
			<S sid ="152" ssid = "23">Performance of seven relation feature spaces over the 5 ACE major types using parse tree information only Table 1 reports the performance of our defined seven relation feature spaces over the 5 ACE major types using parse tree information regardless of any entity information.</S>
			<S sid ="153" ssid = "24">This preliminary experiments show that: • Overall the tree kernel over different relation feature spaces is effective for relation extraction since we use the parse tree information only.</S>
			<S sid ="154" ssid = "25">We will report the detailed performance comparison results between our method and previous work later in this section.</S>
			<S sid ="155" ssid = "26">• Using the PTs achieves the best performance.</S>
			<S sid ="156" ssid = "27">This means the portion of a parse tree enclosed by the shortest path between entities can model relations better than other sub-trees.</S>
			<S sid ="157" ssid = "28">• Using the MCTs get the worst performance.</S>
			<S sid ="158" ssid = "29">This is because the MCTs introduce too much ing it by K (T , T ) • K (T , T ) . left and right context inform ation, which may be noisy feature s, as shown in Figure 1.</S>
			<S sid ="159" ssid = "30">It.</S>
			<S sid ="160" ssid = "31">suggest s Evaluation Method: we parse the sentence using Charniak parser and iterate over all pair of mentions occurring in the same sentence to generate potential instances.</S>
			<S sid ="161" ssid = "32">We find the negative samples are 10 times more than the positive samples.</S>
			<S sid ="162" ssid = "33">Thus data imbalance and sparseness are potential problems.</S>
			<S sid ="163" ssid = "34">Recall (R), Precision (P) and F-measure (F) are adopted as the performance measure.</S>
			<S sid ="164" ssid = "35">that only allowing complete (not partial) production rules in the MCTs does harm performance.• The performance of using CTs drops by 5 in F measure compared with that of using the PTs.</S>
			<S sid ="165" ssid = "36">This suggests that the middle and high-level structures beyond chunking is also very useful for relation extraction.</S>
			<S sid ="166" ssid = "37">• The context-sensitive trees show lower performance than the corresponding original PTs and CTs.</S>
			<S sid ="167" ssid = "38">In some cases (e.g. in sentence “the merge of company A and company B….”, “merge” is the context word), the context information is performance of our tree kernel method for relation extraction is 76.32/62.99/69.02 in precision/recall/F-measure over the 5 ACE major types.</S>
			<S sid ="168" ssid = "39">Methods P R F helpful.</S>
			<S sid ="169" ssid = "40">However the effective scope of context is hard to determine.</S>
			<S sid ="170" ssid = "41">• The two flattened trees perform worse than the original trees, but better than the corresponding context-sensitive trees.</S>
			<S sid ="171" ssid = "42">This suggests that the removed structures by the flattened trees contribute nontrivial performance improvement.</S>
			<S sid ="172" ssid = "43">In the above experiments, the path-enclosed tree displays the best performance among the seven feature spaces when using the parse tree structural information only.</S>
			<S sid ="173" ssid = "44">In the following incremental experiments, we incorporate more features into the Ours: convolution kernel over parse trees Kambhatla (2004): feature-based ME Zhou et al.</S>
			<S sid ="174" ssid = "45">(2005): feature-based SVM Culotta and Sorensen (2004): dependency kernel Bunescu and Mooney (2005): shortest path dependency kernel 76.32 (64.6) - (63.5) 77.2 (63.1) 67.1 (-) 65.5 (-) 62.99 (50.76) - (45.2) 60.7 (49.5) 35.0 (-) 43.8 (-) 69.02 (56.83) - (52.8) 68.0 (55.5) 45.8 (-) 52.5 (-) path-enclosed parse trees and it shows significant performance improvement.</S>
			<S sid ="175" ssid = "46">Path-enclosed Tree (PT) P R F Table 3.</S>
			<S sid ="176" ssid = "47">Performance comparison, the numbers in parentheses report the performance over the 24 ACE subtypes while the numbers outside parentheses is for the 5 ACE major types Parse tree structure information only 72.77 53.80 61.87 Table 3 compares the performance of different methods on the ACE corpus3 . It shows that our method achieves the best-reported performance on +Entity information 76.14 62.85 68.86 +Semantic features 76.32 62.99 69.02 Table 2.</S>
			<S sid ="177" ssid = "48">Performance of Path-enclosed Trees with different setups over the 5 ACE major types Table 2 reports the performance over the 5 ACE major types using Path-enclosed trees enhanced with more features in nodes.</S>
			<S sid ="178" ssid = "49">The 1st row is the baseline performance using structural information only.</S>
			<S sid ="179" ssid = "50">We then integrate entity information, including Entity type and Mention level features, into the corresponding nodes as shown in Figure 1.</S>
			<S sid ="180" ssid = "51">The 2nd row in Table 2 reports the performance of this setup.</S>
			<S sid ="181" ssid = "52">Besides the entity information, we further incorporate the semantic features used in Zhou et al.</S>
			<S sid ="182" ssid = "53">(2005) into the corresponding leaf nodes.</S>
			<S sid ="183" ssid = "54">The 3rd row in Table 2 reports the performance of this setup.</S>
			<S sid ="184" ssid = "55">Please note that in the 2nd and 3rd setups, we still use the same tree kernel function with slight modification on the rule (2) in calculating ∆(n1 , n2 ) (see subsection 3.2) to make it consider more features associated with each individual node: ∆(n1 ,n2 ) = feature weight × λ . From Table 2, we can see that the basic feature of entity information is quite useful, which largely boosts performance by 7 in F-measure.</S>
			<S sid ="185" ssid = "56">The final both the 24 ACE subtypes and the 5 ACE major types.</S>
			<S sid ="186" ssid = "57">It also shows that our tree kernel method significantly outperform the previous two dependency kernel algorithms by 16 in F-measure on the</S>
	</SECTION>
	<SECTION title="ACE relation types4. This may be due to two rea-. " number = "5">
			<S sid ="187" ssid = "1">sons: one reason is that the dependency tree lacks the hierarchical syntactic information, and another reason is due to the two constraints of the two dependency kernels as discussed in Section 2 and Subsection 3.3.</S>
			<S sid ="188" ssid = "2">The performance improvement by our method suggests that the convolution tree kernel can explore the syntactic features (e.g. parse tree structures and entity information) very effectively and the syntactic features are also particu 3 Zhao and Grishman (2005) also evaluated their algorithm on.</S>
			<S sid ="189" ssid = "3">the ACE corpus and got good performance.</S>
			<S sid ="190" ssid = "4">But their experimental data is for 2004 evaluation, which defined 7 entity types with 44 entity subtypes, and 7 relation major types with 27 subtypes, so we are not ready to compare with each other.</S>
			<S sid ="191" ssid = "5">4 Bunescu and Mooney (2005) used the ACE 2002 corpus,.</S>
			<S sid ="192" ssid = "6">including 422 documents, which is known to have many in consistencies than the 2003 version.</S>
			<S sid ="193" ssid = "7">Culotta and Sorensen (2004) used an ACE corpus including about 800 documents, and they did not specify the corpus version.</S>
			<S sid ="194" ssid = "8">Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.</S>
			<S sid ="195" ssid = "9">Thus Table 3 is only for reference purpose.</S>
			<S sid ="196" ssid = "10">We just hope that we can get a few clues from this table.</S>
			<S sid ="197" ssid = "11">larly effective for the task of relation extraction.</S>
			<S sid ="198" ssid = "12">In addition, we observe from Table 1 that the feature space selection (the effective portion of a parse tree) is also critical to relation extraction.</S>
	</SECTION>
</PAPER>
