Distant Supervision for Relation Extraction with an Incomplete Knowledge Base


Bonan Min, Ralph Grishman, Li Wan
New York University
New York, NY 10003
{min,grishman,wanli}
@cs.nyu.edu


Chang Wang, David Gondek IBM 
T. J. Watson Research Center 
Yorktown Heights, NY 10598
{wangchan,dgondek}
@us.ibm.com


Abstract


Distant supervision, heuristically labeling a 
corpus using a knowledge base, has emerged 
as a popular choice for training relation ex- 
tractors.  In this paper, we show that a sig- 
nificant number of “negative“ examples gen- 
erated by the labeling process are false neg- 
atives because the knowledge base is incom- 
plete.  Therefore the heuristic for generating 
negative examples has a serious flaw. Building 
on a state-of-the-art distantly-supervised ex- 
traction algorithm, we proposed an algorithm 
that learns from only positive and unlabeled 
labels at the pair-of-entity level. Experimental 
results demonstrate its advantage over existing 
algorithms.


1   Introduction

Relation  Extraction  is   a   well-studied  problem
(Miller et al., 2000; Zhou et al., 2005; Kambhatla,
2004; Min et al., 2012a).  Recently, Distant Super- 
vision (DS) (Craven and Kumlien, 1999; Mintz et 
al., 2009) has emerged to be a popular choice for 
training relation extractors without using manually 
labeled data. It automatically generates training ex- 
amples by labeling relation mentions1 in the source 
corpus according to whether the argument pair is 
listed in the target relational tables in a knowledge 
base (KB). This method significantly reduces human 
efforts for relation extraction.
  The labeling heuristic has a serious flaw. Knowl- 
edge bases are usually highly incomplete. For exam-

1 An occurrence of a pair of entities with the source sentence.


ple, 93.8% of persons from Freebase2 have no place 
of birth, and 78.5% have no nationality (section 3). 
Previous work typically assumes that if the argument 
entity pair is not listed in the KB as having a re- 
lation, all the corresponding relation mentions are 
considered negative examples.3 This crude assump- 
tion labeled many entity pairs as negative when in 
fact some of their mentions express a relation. The 
number of such false negative matches even exceeds 
the number of positive pairs, by 3 to 10 times, lead- 
ing to a significant problem for training.  Previous 
approaches (Riedel et al., 2010; Hoffmann et al.,
2011; Surdeanu et al., 2012) bypassed this problem 
by heavily under-sampling the “negative“ class.
  We instead deal with a learning scenario where we 
only have entity-pair level labels that are either posi- 
tive or unlabeled. We proposed an extension to Sur- 
deanu et al. (2012) that can train on this dataset. Our 
contribution also includes an analysis on the incom- 
pleteness of Freebase and the false negative match 
rate in two datasets of labeled examples generated 
by DS. Experimental results on a realistic and chal- 
lenging dataset demonstrate the advantage of the al- 
gorithm over existing solutions.

2   Related Work

Distant supervision was first proposed by Craven 
and  Kumlien  (1999)  in  the  biomedical  domain.

  2 Freebase is a large collaboratively-edited KB. It is available 
at http://www.freebase.com.
3 There are variants of labeling heuristics. For example, Sur-
deanu et al. (2011) and Sun et al. (2011) use a pair < e, v > 
as a negative example, when it is not listed in Freebase, but e is 
listed with a different v′ . These assumptions are also problem- 
atic in cases where the relation is not functional.



777

Proceedings of NAACL-HLT 2013, pages 777–782,
Atlanta, Georgia, 9–14 June 2013. Qc 2013 Association for Computational Linguistics


Since then, it has gain popularity (Mintz et al., 2009; 
Bunescu and Mooney, 2007; Wu and Weld, 2007; 
Riedel et al., 2010; Hoffmann et al., 2011; Sur- 
deanu et al., 2012; Nguyen and Moschitti, 2011). 
To tolerate noisy labels in positive examples, Riedel 
et al. (2010) use Multiple Instance Learning (MIL), 
which assumes only at-least-one of the relation men- 
tions in each “bag“ of mentions sharing a pair of ar- 
gument entities which bears a relation, indeed ex- 
presses the target relation.   MultiR (Hoffmann et 
al., 2011) and Multi-Instance Multi-Label (MIML) 
learning (Surdeanu et al., 2012) further improve it 
to support multiple relations expressed by different 
sentences in a bag.  Takamatsu et al. (2012) mod- 
els the probabilities of a pattern showing relations, 
estimated from the heuristically labeled dataset. 
Their algorithm removes mentions that match low- 
probability patterns.  Sun et al. (2011) and Min et 
al. (2012b) also estimate the probablities of patterns 
showing relations, but instead use them to relabel ex- 
amples to their most likely classes. Their approach 
can correct highly-confident false negative matches.

3   Problem Definition
Distant Supervision: Given a KB D (a collection


nor stay current. We took frequent relations, which 
involve an entity of type PERSON, from Freebase 
for analysis. We define the incompleteness ∂(r) of a 
relation r as follows:
∂(r) = |{e}|−|{e|∃e′ ,s.t.r(e,e′ )ǫD}|
|{e}|
  ∂(r) is the percentage of all persons {e} that do 
not have an attribute e′ (with which r(e, e′) holds).
Table 1 shows that 93.8% of persons have no place 
of birth, and 78.5% of them have no nationality. 
These are must-have attributes for a person.  This 
shows that Freebase is highly incomplete.
Freebase relation types 	Incompleteness
/people/person/education	0.792
/people/person/employment history 	0.923
/people/person/nationality*	0.785
/people/person/parents* 	0.988
/people/person/place  of birth* 	0.938
/people/person/places  lived* 	0.966
Table 1:  The incompleteness of Freebase (* are must- 
have attributes for a person).
  We further investigate the rate of false negative 
matches,  as the percentage of entity-pairs that are 
not listed in Freebase but one of its mentions gen- 
erated by DS does express a relation in the tar- 
get set of types.   We randomly picked 200 unla-
5


of relational tables r(e1, e2), in which rǫR (R is the


beled bags


from each of the two datasets (Riedel


set of relation labels), and < e1, e2  > is a pair of 
entities that is known to have relation r) and a cor- 
pus C , the key idea of distant supervision is that we
align D to C , label each bag4 of relation mentions
that share argument pair < e1, e2  > with r, other-
wise OTHER. This generates a dataset that has labels 
on entity-pair (bag) level. Then a relation extractor 
is trained with single-instance learning (by assum- 
ing all mentions have the same label as the bag), or 
Multiple-Instance Learning (by assuming at-least- 
one of the mentions expresses the bag-level label), 
or Multi-Instance Multi-Label learning (further as- 
suming a bag can have multiple labels) algorithms. 
All of these works treat the OTHER class as exam- 
ples that are labeled as negative.
  The incomplete KB problem: KBs are usually 
incomplete because they are manually constructed, 
and it is not possible to cover all human knowledge


et al., 2010; Surdeanu et al., 2012) generated by DS,
and we manually annotate all relation mentions in 
these bags.  The result is shown in Table 2, along 
with a few examples that indicate a relation holds in 
the set of false negative matches (bag-level).  Both 
datasets have around 10% false negative matches in 
the unlabeled set of bags.  Taking into considera- 
tion that the number of positive bags and unlabeled 
bags are highly imbalanced (1:134 and 1:37 in the 
Riedel and KBP dataset respectively, before under- 
sampling the unlabeled class), the number of false 
negative matches are 11 and 4 times the number 
of positive bags in Reidel and KBP dataset, respec- 
tively.  Such a large ratio shows false negatives do 
have a significant impact on the learning process.

4   A semi-supervised MIML algorithm

Our goal is to model the bag-level label noise, 
caused by the incomplete KB problem, in addition


4 A bag is defined as a set of relation mentions sharing the	 	


same entity pair as relation arguments. We will use the terms
bag and entity pair interchangeably in this paper.
  

5 85% and 95.7% of the bags in the Riedel and KBP datasets 
have only one relation mention.


Dataset 
(train- 
ing)


#    pos- 
itive bags


# positive :
# unlabeled


% 	are 
false 
negatives


# positive
:   # false 
negative


has  human 
assessment


Examples 
of false 
negative 
mentions


Riedel    4,700 	1:134(BD*)    8.5% 	1:11.4 	no 	(/location/location/contains)...  in Brooklyn ’s Williamsburg. 
(/people/person/place  lived)  Cheryl Rogowski , a farmer from 
Orange County ...
KBP	183,062 	1:37(BD*) 	11.5% 	1:4 	yes 	(per:city of birth)  Juan  Martn  Maldacena  (born  September
10, 1968) is a theoretical physicist born in Buenos Aires
(per:employee of)Dave Matthews, from the ABC News, ...
Table 2:  False negative matches on the Riedel (Riedel et al., 2010) and KBP dataset (Surdeanu et al., 2012).  All 
numbers are on bag (pairs of entities) level. BD* are the numbers before downsampling the negative set to 10% and
5% in Riedel and KBP dataset, respectively.


to modeling the instance-level noise using a 3-layer 
MIL or MIML model (e.g., Surdeanu et al. (2012)). 
We propose a 4-layer model as shown in Figure 1.
  The input to the model is a list of n bags with a 
vector of binary labels, either Positive (P), or Un- 
labled (U) for each relation r.  Our model can be


• ℓr ǫ{P, N }: 	a  hidden  variable  that  denotes
whether r holds for the ith bag.
• θ is an observed constant controlling the total
number of bags whose latent label is positive.
We define the following conditional probabilities:
 1/2	if yr  = P ∧ ℓr = P ;


i	i
viewed as a semi-supervised6  framework that ex-	
 1/2	if yr  = U ∧ ℓr = P ;


tends a state-of-the-art Multi-Instance Multi-Label


• p(yr    r	i	i


i |ℓi ) =


1	if yr  = U ∧ ℓr = N ;


(MIML) model (Surdeanu et al., 2012).  Since the
input to previous MIML models are bags with per- 
relation binary labels of either Positive (P) or Neg-


 0	i	;	i
otherwise
It encodes the constraints between true bag-
level labels and the entity pair labels in the KB.


n	r


ative (N), we add a set of latent variables ℓ which
models the true bag-level labels, to bridge the ob- 
served bag labels y and the MIML layers. We con- 
sider this as our main contribution to the model. Our 
hierarchical model is shown in Figure 1.

 
Figure 1: Plate diagram of our model.
  Let i, j be the index in the bag and mention level, 
respectively. Following Surdeanu et al. (2012), we
model mention-level extraction p(zr |xij ; wz ) and
multi-instance multi-label aggregation p(ℓr |zi; wr )
in the bottom 3 layers. We define:


• p(θ|ℓ)	∼	N ( 	i=1	rǫR	i	,  1 )  where
n 	k
δ(x, y) = 1 if x = y, 0 otherwise. k is a large
number.  θ is the fraction of the bags that are 
positive.  It is an observed parameter that de- 
pends on both the source corpus and the KB 
used.
  Similar to Surdeanu et al. (2012), we also define 
the following parameters and conditional probabili- 
ties (details are in Surdeanu et al. (2012)):
• zij ǫR ∪ {OT H ER}: a latent variable that de-
notes the relation type of the jth mention in the
ith bag.
• xij  is the feature representation of the jth rela-
tion mention in the ith bag. We use the set of
features in Surdeanu et al. (2012).
• wz is the weight vector for the multi-class rela-
tion mention-level classifier.
• wℓ is the weight vector for the rth binary top-


i 	ℓ
level aggregation classifier (from mention la-


• r is a relation label.   rǫR ∪ {OT H ER},  in
which OTHER denotes no relation expressed.
• yr ǫ{P, U }:  r holds for ith  bag or the bag is



bels to bag-level prediction). We use wℓ to rep- 
resent w1, w2, ...w|R|.
ℓ	ℓ	ℓ
r	r	r


i
unlabeled.

    6 We use the term semi-supervised  because the algorithm 
uses unlabeled bags but existing solutions requires bags to be 
labeled either positive or negative.


• p(ℓi |zi; wℓ ) ∼ Bern (fℓ(wℓ , zi)) where fℓ is
probability produced by the rth top-level clas-
sifier, from the mention-label level to the bag- 
label level.
• p(zr |xij ; wz ) ∼ Multi (fz (wz , xij )) where fz


is probability produced by the mention-level 
classifier, from the mentions to the mention-


In the E-step, we do a greedy search (steps 5-8
in algorithm 1) in all p(ℓr |xi; wz , wℓ) and update ℓr


label level.7


i
until the second 
term is 
maximized.


wz , wℓ


i
are the



4.1   Training

We use hard Expectation-Maximization (EM) algo- 
rithm for training the model. Our objective function 
is to maximize log-likelihood:
L(wz , wℓ) = logp(y, θ|x; wz , wℓ)


model weights learned from the previous iteration.
After fixed ℓ, we seek to maximize:
n
logp(ℓ|xi; wz , wℓ) = ) logp(ℓi|xi; wz , wℓ)
i=1
n
= ) log ) p(ℓi, zi|xi; wz , wℓ)


= log ) p(y, θ, ℓ|x; wz , wℓ)
ℓ



which	can


i=1
be


zi
solved	with	an	approxi-


Since solving it exactly involves exploring an expo- 
nential assignment space for ℓ, we approximate and
iteratively set ℓ∗ = argℓ max p(ℓ|y, θ, x; wz , wℓ)
p(ℓ|y, θ, x; wz , wℓ) ∝ p(y, θ, ℓ|x; wz , wℓ)
= p(y, θ|ℓ, x)p(ℓ|x; wz , wℓ)
= p(y|ℓ)p(θ|ℓ)p(ℓ|x; wz , wℓ)
Rewriting in log form:
logp(ℓ|y, θ, x; wz , wℓ)
= logp(y|ℓ) + logp(θ|ℓ) + logp(ℓ|x; wz , wℓ)


mate	solution	in	Surdeanu	et	al.	(2012)
(step   9-11):	update   zi	independently   with:
zi   = arg maxzi  p(zi|ℓi, xi; wz , wℓ).  More details
can be found in Surdeanu et al. (2012).
  In the M-step, we retrain both of the mention- 
level and the aggregation level classifiers.
The full EM algorithm is shown in algorithm 1.

4.2	Inference
Inference on a bag xi is trivial. For each mention:


n
n 	� � δ(ℓr , P )


∗ = arg


zij ǫR∪{OT H ER}


max p(zij |xij , wz )


i	Followed by the aggregation (directly with wℓ):


= ) ) logp(yr |ℓr ) − k( i=1 rǫR


− θ)2


r(∗)	r	r


i  i	n


yi	= argyr


max p(y |zi; w )


i=1  rǫR
n
+ ) ) logp(ℓr |xi; wz , wℓ) + const
i
i=1  rǫR
Algorithm 1 Training (E-step:2-11; M-step:12-15)
1:  for i = 1, 2 to T do
2:	ℓr ← N for all yr  = U and rǫR


i ǫ{P,N }	i 	ℓ

4.3   Implementation details
We implement our model on top of the 
MIML(Surdeanu et  al.,  2012)  code  base.8      We 
use the same mention-level and aggregate-level 
feature sets as Surdeanu et al. (2012).   We adopt


i
3:	ℓr ← P


i
for all yr  = P 
and rǫR


the same idea 
of using cross 
validation for 
the E


I = {< i, r > |ℓr  = N }; I ′ = {< i, r > |ℓr  = P }


and M steps to avoid overfitting. We initialize our


4:	i	i


5:	for k = 0, 1 to θn − |I ′| do
6:	< i′, r′ >= arg max<i,r>ǫI p(ℓr |xi; wz , wℓ)
7:	ℓr′   ← P ; I = I \{< i′, r′ >}
8:	end for
9:	for i = 1, 2 to n do


algorithm by sampling 5% unlabeled examples as
negative,  in essence using 1 epoch of MIML to 
initialize. Empirically it performs well.
5	Experiments


10:	z∗ = arg maxzi
end for


p(zi|ℓi, xi; wz , wℓ)


Data  set:	We  use  the  
KBP  (Ji  et  al.,  2011)
dataset9 prepared and 
publicly released by 
Surdeanu


11:
12:	w∗ = arg maxw 	n


xi    logp(zij |xij , wz )



et al. (2012) for our 
experiment since it is 1) 
large


�	�|   |
z 	z 	i=1	=


j 1
13:	for all rǫR do
14:	wr(∗)	�n


and  realistic,  2)  publicly available,  
3)  most  im-
portantly, it is the only dataset that 
has associated


ℓ	= arg maxwr


i=1 p(ℓr |zi, wr )


ℓ 	i 	ℓ
15:	end for
16:  end for
17:  return wz, wℓ

  7 All classifiers are implemented with L2-regularized logistic 
regression with Stanford CoreNLP package.


human-labeled ground truth. Any KB held-out eval- 
uation without manual assessment will be signif- 
icantly affected by KB incompleteness.   In KBP

8 Available at http://nlp.stanford.edu/software/mimlre.shtml
    9 Available   from   Linguistic   Data   Consortium   (LDC). 
http://projects.ldc.upenn.edu/kbp/data/



 
Figure 2: Performance on the KBP dataset.  The figures on the left, middle and right show MIML, Hoffmann, and 
Mintz++ compared to the same MIML-Semi curve, respectively. MIML-Semi is shown in red curves (lighter curves in 
black and white) while other algorithms are shown in black curves (darker curves in black and white).


dataset, the training bags are generated by mapping 
Wikipedia (http://en.wikipedia.org) infoboxes (after 
merging similar types following the KBP 2011 task 
definition) into a large unlabeled corpus (consisting 
of 1.5M documents from the KBP source corpus and 
a complete snapshot of Wikipedia). The KBP shared 
task provided 200 query named entities with their as- 
sociated slot values (in total several thousand pairs). 
We use 40 queries as development dataset (dev), and 
the rest (160 queries) as evaluation dataset. We set 
θ = 0.25 by tuning on the dev set and use it in the 
experiments. For a fair comparison, we follow Sur- 
deanu et al. (2012) and begin by downsampling the 
“negative“ class to 5%.  We also set T=8 and use 
the following noisy-or (for ith bag) of mention-level 
probability to rank predicted types (r) of pairs and 
plot the precision-recall curves for all experiments.
P robi(r) = 1 − n (1 − p(zij  = r|xij ; wz ))
j

  Evaluation: We compare our algorithm (MIML- 
semi)  to three algorithms:  1) MIML (Surdeanu et 
al., 2012), the Multiple-Instance Multiple Label al- 
gorithm which labels the bags directly with the KB 
(y = ℓ). 2) MultiR (denoted as Hoffmann) (Hoff- 
mann et al., 2011), a Multiple-Instance algorithm 
that supports overlapping relations. It also imposes 
y = ℓ. 3) Mintz++ (Surdeanu et al., 2012), a vari- 
ant of the single-instance learning algorithm (section
3).  The first two are stat-of-the-art Multi-Instance 
Multi-Label algorithms. Mintz++ is a strong base- 
line (Surdeanu et al., 2012) and an improved ver- 
sion of Mintz et al. (2009).   Figure 2 shows that 
our algorithm consistently outperforms all three al-


gorithms at almost all recall levels (with the excep- 
tion of a very small region in the PR-curve).  This 
demonstrates that by treating unla-beled data set dif- 
ferently and leveraging the missing positive bags, 
MIML-semi is able to learn a more accurate model 
for extraction. Although the proposed solution is a 
specific algorithm, we believe the idea of treating 
unlabeled data differently can be incorporated into 
any of these algorithms that only use unlabeled data 
as negative examples.

6   Conclusion

We show that the distant-supervision labeling pro- 
cess generates a significant number of false nega- 
tives because the knowledge base is incomplete. We 
proposed an algorithm that learns from only positive 
and unlabeled bags.  Experimental results demon- 
strate its advantage over existing algorithms.

Acknowledgments

Supported in part by the Intelligence Advanced Re- 
search Projects Activity (IARPA) via Department of 
Interior National Business Center contract number 
D11PC20154.  The U.S. Government is authorized 
to reproduce and distribute reprints for Governmen- 
tal purposes notwithstanding any copyright annota- 
tion thereon. The views and conclusions contained 
herein are those of the authors and should not be 
interpreted as necessarily representing the official 
policies or endorsements, either expressed or im- 
plied, of IARPA, DoI/NBC, or the U.S. Government.


References

Razvan Bunescu and Raymond Mooney. 2007. Learning 
to extract relations from the web using minimal super- 
vision. In Proceedings of the 45th Annual Meeting of 
the Association for Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing bi- 
ological knowledge bases by extracting information 
from text sources. In Proceedings of the Seventh Inter- 
national Conference on Intelligent Systems for Molec- 
ular Biology.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke 
Zettlemoyer, and Daniel S. Weld. 2011. Knowledge- 
based weak supervision for information extraction of 
overlapping relations. In Proceedings  of the Annual 
Meeting of the Association for Computational Linguis- 
tics.
Heng  Ji,  Ralph  Grishman,  and  Hoa  T.  Dang.  2011.
Overview of the TAC 2011 knowledge base popula- 
tion track. In Proceedings of the Text Analytics Con- 
ference.
Jing Jiang and ChengXiang Zhai. 2007. A systematic ex- 
ploration of the feature space for relation extraction. In 
Proceedings of HLT-NAACL-2007.
Nanda Kambhatla. 2004. Combining lexical, syntactic, 
and semantic features with maximum entropy mod- 
els for information extraction. In Proceedings of ACL-
2004.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph 
Weischedel. 2000. A novel use of statistical parsing 
to extract information from text. In Proceedings of 
NAACL-2000.
Bonan Min, Shuming Shi, Ralph Grishman and Chin- 
Yew Lin. 2012a. Ensemble Semantics for Large-scale 
Unsupervised Relation Extraction. In Proceedings of 
EMNLP-CoNLL 2012.
Bonan Min, Xiang Li, Ralph Grishman and Ang Sun.
2012b. New York University 2012 System for KBP 
Slot Filling. In Proceedings of the Text Analysis Con- 
ference (TAC) 2012.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf- 
sky. 2009. Distant supervision for relation extraction 
without labeled data. In Proceedings of the 47th An- 
nual Meeting  of the Association  for Computational 
Linguistics.
Truc Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi- 
sion from external semantic repositories. In Proceed- 
ings of the 49th Annual Meeting of the Association for 
Computational Linguistics:  Human Language Tech- 
nologies.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without


labeled text. In Proceedings of the European 
Confer- ence on Machine Learning and Knowledge 
Discovery in Databases (ECML PKDD 10).
Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.
2011. New York University 2011 system for KBP 
slot filling. In Proceedings of the Text Analytics 
Confer- ence.
Mihai Surdeanu, Sonal Gupta, John Bauer, David 
Mc- Closky, Angel X. Chang, Valentin I. 
Spitkovsky, and Christopher D. Manning. 2011. 
Stanfords distantly- supervised slot-filling system. 
In Proceedings of the Text Analytics Conference.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, 
Christopher D. Manning. 2012. Multi-instance 
Multi- label Learning for Relation Extraction. In 
Proceed- ings of the 2012 Conference on Empirical 
Methods in Natural Language Processing and 
Natural Language Learning.
TAC KBP 2011 task definition. 2011. http://nlp
.cs.qc.cuny.edu/kbp/2011/KBP2011 
TaskDefinition.pdf
Shingo Takamatsu, Issei Sato, Hiroshi Nakagawa. 2012.
Reducing Wrong Labels in Distant Supervision for 
Re- lation Extraction. In Proceedings of 50th Annual 
Meet- ing of the Association for Computational 
Linguistics.
Fei Wu and Daniel S. Weld. 2007. Autonomously 
seman- tifying wikipedia. In Proceedings of the 
International Conference on Information and 
Knowledge Manage- ment (CIKM-2007).
Guodong Zhou, Jian Su, Jie Zhang and Min Zhang. 2005.
Exploring various knowledge in relation extraction. 
In
Proceedings of ACL-
2005.

