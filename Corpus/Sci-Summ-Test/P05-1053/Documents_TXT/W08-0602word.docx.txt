Extracting Clinical Relationships from Patient Narratives



Angus Roberts, Robert Gaizauskas, Mark Hepple 
Department of Computer Science, University  of Sheffield, 
Regent Court, 211 Portobello, Sheffield S1 4DP
{initial.surname}@dcs.shef.ac.uk







Abstract

The Clinical E-Science Framework  (CLEF) 
project has  built a  system  to extract  clin- 
ically significant information from the tex- 
tual component of medical records, for clin- 
ical research, evidence-based healthcare and 
genotype-meets-phenotype informatics. One 
part of this system is the identification of rela- 
tionships between clinically important entities 
in the text. Typical  approaches to relationship 
extraction in this domain have used full parses, 
domain-specific  grammars, and large knowl- 
edge bases encoding  domain  knowledge. In 
other areas of biomedical NLP, statistical ma- 
chine learning  approaches are now routinely 
applied to relationship extraction.  We report 
on the novel application of these statistical 
techniques to clinical relationships.
We describe a supervised machine learning 
system, trained with a corpus of oncology nar- 
ratives hand-annotated with clinically impor- 
tant relationships. Various shallow features 
are extracted from these  texts, and used to 
train statistical classifiers. We compare the 
suitability of these features  for clinical re- 
lationship extraction,  how extraction  varies 
between  inter- and intra-sentential  relation- 
ships, and examine the amount of training data 
needed to learn various relationships.

1   Introduction

The application  of Natural Language  Processing 
(NLP) is widespread in biomedicine.  Typically, it 
is applied to improve  access to the ever-burgeoning 
research literature. Increasingly, biomedical re- 
searchers  need  to relate this literature to pheno- 
typic data: both to populations,  and to individ- 
ual clinical subjects. The computer applications


used in biomedical research, including  NLP appli- 
cations, therefore need to support genotype-meets- 
phenotype informatics and the move towards trans- 
lational biology. Such support will undoubtedly in- 
clude linkage to the information  held in individual 
medical records: both the structured portion, and the 
unstructured textual portion.
  The  Clinical  E-Science Framework (CLEF) 
project (Rector et al., 2003) is building a  frame- 
work for the capture, integration and presentation of 
this clinical information, for research and evidence- 
based health care. The project’s  data resource is a 
repository of the full clinical records for over 20000 
cancer patients from the Royal Marsden Hospital, 
Europe’s largest oncology centre. These records 
combine structured information, clinical narratives, 
and free text investigation reports. CLEF uses infor- 
mation extraction (IE) technology to make informa- 
tion from the textual portion of the medical record 
available for integration with the structured record, 
and thus available for clinical care and research. The 
CLEF IE system analyses the textual records to ex- 
tract entities,  events and the relationships between 
them. These relationships give information  that is 
often not available in the structured record. Why 
was a drug given? What were the results of a physi- 
cal examination? What problems were not present? 
We have previously reported entity extraction in the 
CLEF IE system (Roberts et al., 2008b). This paper 
examines relationship extraction.
  Extraction of relationships from clinical text is 
usually carried out as part of a full clinical IE sys- 
tem.  Several such systems have been described. 
They  generally  use a syntactic  parse with domain- 
specific grammar rules.   The Linguistic String 
project (Sager et al., 1994) used a full syntactic and



10
BioNLP 2008: Current Trends in Biomedical  Natural  Language Processing, pages 10–18, 
Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics


clinical sublanguage parse to fill template data struc- 
tures corresponding  to medical  statements.  These 
were mapped  to a  database  model incorporating 
medical facts and the relationships between them. 
MedLEE (Friedman et al., 1994), and more recently 
BioMedLEE (Lussier et al., 2006) used a semantic 
lexicon and grammar  of domain-specific  semantic 
patterns. The patterns encode the possible relation- 
ships between entities, allowing  both entities and the 
relationships between them to be directly matched 
in the text. Other systems have incorporated large- 
scale domain-specific  knowledge  bases. MEDSYN- 
DIKATE  (Hahn et al., 2002)  employed  a rich dis- 
course model of entities and their relationships, built 
using a dependency  parse  of texts and a descrip- 
tion logic  knowledge  base re-engineered from exist- 
ing terminologies. MENELAS (Zweigenbaum  et al.,
1995) also used a full parse, a conceptual represen- 
tation of the text, and a large scale knowledge  base.
  In other applications of biomedical  NLP,  a sec- 
ond paradigm  has become widespread:   the appli- 
cation of statistical machine learning techniques to 
feature-based models of the text. Such approaches 
have typically been applied to journal texts. They 
have been used both for entity recognition  and ex- 
traction of various relations, such as protein-protein 
interactions  (see, for example, Grover et al (2007)). 
This follows on from the success of these methods 
in general NLP (see for example Zhou et al (2005)). 
Statistical  machine learning has also been applied to 
clinical text, but its use has generally  been limited 
to entity recognition. The Mayo Clinic text analysis 
system (Pakhomov et al., 2005), for example,  uses a 
combination of dictionary lookup and a Na¨ıve Bayes 
classifier to identify entities for information retrieval 
applications. To the best of our knowledge, statisti- 
cal methods have not been previously  applied to ex- 
traction of clinical relationships from text.
  This paper describes experiments in the statistical 
machine learning of relationships from a novel text 
type: oncology  narratives.  The set of relationships 
extracted are considered to be of interest for clinical 
and research applications  down line of IE, such as 
querying to support clinical research. We apply Sup- 
port Vector Machine (SVM) classifiers to learn these 
relationships.  The classifiers are trained  and eval- 
uated using novel data: a gold standard corpus of 
clinical text, hand-annotated with semantic entities


and relationships.   In order to test the applicability 
of this method to the clinical domain, we train clas- 
sifiers using a number of comparatively simple text 
features, and look at the contribution of these fea- 
tures to system performance.  Clinically interesting 
relationships  may span several sentences, and so we 
compare classifiers trained for both intra- and inter- 
sentential relationships (spanning one or more sen- 
tence boundaries). We also examine the influence of 
training corpus size on performance, as hand anno- 
tation of training data is the major expense in super- 
vised machine learning.

2   Relationship Schema


Relationship
Argument 1
Argument 2
has target
Investigation
Locus

Intervention
Locus
has finding
Investigation
Condition

Investigation
Result
has indication
Drug or device
Condition

Intervention
Condition

Investigation
Condition
has location
Condition
Locus
negation modifies
Negation modifier
Condition
laterality modifies
Laterality modifier
Intervention

Laterality modifier
Locus
sub-location modifies
Sub-location modifier
Locus

Table 1: Relationship types and their argument type con- 
straints.

  The CLEF application extracts entities, relation- 
ships and modifiers  from text. By entity, we mean 
some real-world  thing, event or state referred to in 
the text: the drugs that are mentioned, the tests that 
were carried out, etc. Modifiers are words that qual- 
ify an entity in some way, referring e.g. to the lat- 
erality of an anatomical locus, or the negation of a 
condition (“no sign of inflammation”).   Entities are 
connected to each other  and to modifiers by rela- 
tionships:  e.g. linking a drug entity to the condition 
entity for which it is indicated, linking an investiga- 
tion to its results, or linking a negating  phrase to a 
condition.
  The entities, modifiers,  and relationships are de- 
scribed by both a formal XML schema, and by a 
set of detailed definitions.  These were developed by 
a group of clinical experts through an iterative pro- 
cess, until acceptable agreement was reached. Entity 
types are mapped to types from the UMLS seman- 
tic network (Lindberg et al., 1993), each CLEF en-


tity type covering several UMLS types. Relationship 
types are those that were felt necessary to capture the 
essential clinical dependencies between entities re- 
ferred to in patient documents, and to support CLEF 
end user applications.
  Each relationship type is constrained to exist be- 
tween limited pairs of entity types. For example, 
the has location relationship can only exist be- 
tween  a Condition entity and  a Locus  entity. 
Some relationships can exist between multiple  type 
pairs. The full set of relationships  and their argu- 
ment type constraints  are shown  in Table 1.  Ex- 
amples of each relationship  are given in Roberts et 
al (2008a).
  Some of the relationships considered important 
by the clinical experts were not obvious without do- 
main knowledge. For example,

He is suffering from nausea  and severe 
headaches. Dolasteron  was prescribed.

Without domain knowledge, it is not clear that there 
is a  has indication relationship  between the 
“Dolasteron” Drug  or device entity and the 
“nausea” Condition entity. As in this example, 
many of this type of relationship are intra-sentential.
  A single real-world entity may be referred to sev- 
eral times in the same  text.   Each of these  co- 
referring  expressions is a mention of the entity. The 
gold standard includes  annotation of co-reference 
between different textual mentions of the same en- 
tity.  For the work reported in this paper, however, 
co-reference is not considered.   Each entity is as- 
sumed to have a single  mention. Relationships be- 
tween entities can be considered, by extension,  as 
relationships between the single mentions of those 
entities. The implications of this are discussed fur- 
ther below.

3   Gold Standard Corpus

The schema  and definitions were used  to hand- 
annotate the entities and relationships  in 77 oncol- 
ogy narratives, to provide  a gold standard for sys- 
tem training and evaluation.   Corpora of this size 
are typical in supervised machine learning,  and re- 
flect the expense  of hand annotation. Narratives 
were carefully  selected and annotated according to 
a best practice methodology,  as described in Roberts


et al (2008a). Narratives were annotated by two in- 
dependent, clinically trained, annotators, and a con- 
sensus created by a third. We will refer to this corpus 
as C77.
  Annotators were asked to first mark the mentions 
of entities and modifiers, and then to go through 
each of these in turn, deciding if any had relation- 
ships with mentions of other entities. Although the 
annotators were marking co-reference between men- 
tions of the same entity, they were asked to ignore 
this with respect to relationship annotation. Both 
the annotation tool that they were using and their 
annotation guidelines, enforced the creation of rela- 
tionships between mentions, and not between enti- 
ties. The gold standard is thus analogous to the style 
of relationship  extraction  reported here, in which 
we extract relations between single mention entities, 
and do not consider co-reference. Annotators were 
further told that relationships could span multiple 
sentences, and that it was acceptable to use clini- 
cal domain knowledge to infer that a relationship 
existed between two mentions.  Counts of all rela- 
tionships annotated in C77 are shown  in Table 2, 
sub-divided by the number of sentence boundaries 
spanned by a relationship.

4   Relationship Extraction

The system  we have  built uses  the GATE NLP 
toolkit (Cunningham et al., 2002) 1. The system is 
shown in Figure 1, and is described below.
  Narratives  are first pre-processed using standard 
GATE modules. Narratives were tokenised,  sen- 
tences found with a regular  expression-based  sen- 
tence splitter, part-of-speech  (POS) tagged, and 
morphological roots found for tokens.    Each to- 
ken was also labelled with a generalised  POS tag, 
the first two characters of the full POS tag. This 
takes advantage of the Penn Treebank  tagset used 
by GATE’s POS tagger, in which related POS tags 
share the first two characters.  For example, all six 
verb POS tags start with the letters “VB”.
  After pre-processing, mentions of entities within 
the text are annotated. In the experiments reported, 
we assume perfect  entity recognition,   as given by 
the entities in the human annotated gold standard

  1 We used a development  build of GATE 4.0, downloadable 
from http://gate.ac.uk




Sentence boundaries between 
arguments

To
tal

0
1
2
3
4
5
6
7
8
9
>9

has 
fin
din
g
26
5
46
25
7
5
4
3
2
2
2
0
36
1
has 
ind
ica
tio
n
13
9
85
35
32
14
11
6
4
5
5
12
34
8
has 
loc
ati
on
36
0
4
1
1
1
1
1
0
0
0
4
37
3
has 
tar
get
12
2
14
4
2
2
4
3
1
0
1
0
15
3
lat
era
lity 
mo
difi
es
12
8
0
0
0
0
0
0
0
0
0
0
12
8
neg
atio
n 
mo
difi
es
10
0
1
0
0
0
0
0
0
0
0
0
10
1
sub 
loc
atio
n 
mo
difi
es
76
0
0
0
0
0
0
0
0
0
0
76
Tot
al
11
90
15
0
65
42
22
20
13
7
7
8
16
15
40
Cu
mu
lati
ve 
tot
al
11
90
13
40
14
05
14
47
14
69
14
89
15
02
15
09
15
16
15
24
15
40


Table 2: Count of relationships in 77 gold standard documents.




described above.   Our results are therefore  higher 
than would  be expected in a system with automatic 
entity recognition.  It is useful and usual to fix en- 
tity recognition in this way, to allow tuning specific 
to relationship extraction, and to allow the isolation 
of relation-specific problems.  We accept, however, 
that ultimately,  relation extraction does depend on 
the quality of entity recognition. The relation extrac- 
tion described here is used as part of an operational 
IE system in which clinical entity recognition is per- 
formed by a combination  of lexical lookup and su- 
pervised machine learning. We have described our 
entity extraction  system elsewhere (Roberts et al.,
2008b).


4.1   Classification

We treat clinical relationship extraction as a classi- 
fication task, training classifiers to assign a relation- 
ship type to an entity pair. An entity pair is a pairing 
of entities that may or may not be the arguments of 
a relation.  For a given document, we create all pos- 
sible entity pairs within two constraints.  First, en- 
tities that are paired must be within n sentences of 
each other. For all of the work reported here, unless
stated, n ≤ 1 (crossing 0 or 1 sentence boundaries).
Second, we can constrain the entity pairs created
by argument type (Rindflesch and Fiszman, 2003). 
For example, there is little point in creating an en- 
tity pair between  a Drug  or device entity and 
a Result entity, as no relationships, as specified 
by the schema, exist between entities of these types. 
Entity pairing is carried out by a GATE component 
developed specifically  for clinical relationship ex- 
traction. In addition to pairing entities according to 
the above constraints, this component  also assigns 
features to each pair that characterise its lexical and


syntactic qualities (described further in Section 4.2).
  Entity pairs correspond to classifier training and 
test instances. In classifier training, if  an entity 
pair corresponds to the arguments of a relationship 
present in the gold standard, then it is assigned  a 
class of that relationship type. If it does not corre- 
spond to such a relation, then it is assigned the class 
null.  The classifier builds a model of these entity 
pair training instances, from their features. In classi- 
fier application,  entity pairs are created from unseen 
text, under the above constraints.  The classifier as- 
signs one of our seven relationship types, or null, 
to each entity pair.
  We use Support Vector machines (SVMs) as train- 
able classifiers, as these have proved  to be robust and 
efficient for a range of NLP tasks, including relation 
extraction.  We use an SVM implementation devel- 
oped within our own group,  and provided  as part 
of the GATE toolkit. This is a variant  on the orig- 
inal SVM algorithm, SVM with uneven margins, in 
which classification may be biased towards positive 
training examples. This is particularly suited to NLP 
applications, in which positive training examples are 
often rare. Full details of the classifier are given in 
Li et al (2005). We used the implementation “out of 
the box”, with default  parameters as determined in 
experiments with other data sets.
  SVMs are binary classifiers: the multi-class prob- 
lem of classifying entity pairs must therefore be 
mapped to a number  of binary classification prob- 
lems.  There  are several  ways in which a  multi- 
class problem  can be recast as binary  problems.  The 
commonest are one-against-one in which one classi- 
fier is trained for every possible pair of classes, and 
one-against-all in which a classifier  is trained for 
a binary  decision  between each class and all other


classes, including  null, combined.  We have car- 
ried out extensive experiments (not reported here), 
with these two strategies, and have found little dif- 
ference between them for our data. We have chosen 
to use one-against-all,  as it needs fewer classifiers 
(for an n class problem, it needs n classifiers,  as op-
posed to (n−1)! for one-against-one).
  The resultant  class assignments by multiple bi- 
nary classifiers must be post-processed to deal with 
ambiguity. In application to unseen text, it is possi- 
ble that several classifiers assign different classes to 
an entity pair (test instance). To disambiguate these 
cases, the output  of each one-against-all classifier is 
transformed into a probability, and the class with 
the highest probability  is assigned.  Re-casting the 
multi-class  relation  problem  as a number of binary 
problems, and post-processing to resolve ambigui- 
ties, is handled by the GATE Learning API.


 

Figure 1: The relationship extraction system.


4.2   Features for Classification

The SVM classification model is built from lexical 
and syntactic  features assigned to tokens and en- 
tity pairs prior to classification. We  use features 
developed in part from those described in Zhou et 
al (2005) and Wang et al (2006).  These features are 
split into 11 sets, as described in Table 3.
  The tokN  features are POS and surface string 
taken from a window of N tokens on each side of 
each paired  entity’s  mention.  For N   = 6, this 
gives 48 features.  The rationale behind these sim- 
ple features is that there is useful information in the 
words surrounding two mentions, that helps deter-


mine any relationship between them. The gentokN 
features generalise tokN  to use morphological  root 
and generalised POS. The str features  are a set 
of 14 surface string features, encoding the full sur- 
face  strings of both entity mentions,  their heads, 
their heads combined,  the surface strings of the first, 
last and other tokens  between the mentions,  and 
of the two tokens immediately  before and after the 
leftmost and rightmost mentions respectively.  The 
pos, root, and genpos feature sets are similarly 
constructed from the POS tags, roots, and gener- 
alised POS tags of the entity mentions and their sur- 
rounding  tokens. These four feature sets differ from 
tokN and gentokN, in that they provide more fine- 
grained information  about the position of features 
relative to the paired entity mentions.
  For the event feature set,  the main entities 
were divided into events (Investigation  and 
Intervention) and non-events (all others). Fea- 
tures record whether the entity pair consists of two 
events, two non-events,  one of each, and whether 
there are  any intervening  events and non-events. 
This feature set gives similar information  to atype 
(semantic types of arguments) and inter (inter- 
vening entities), but at a coarser level of typing.

5   Evaluation

We   used   a   standard ten-fold  cross validation 
methodology and standard evaluation metrics. Met- 
rics are defined in terms of true positive, false pos- 
itive and false negative  matches between relation- 
ships in a system annotated response document and 
a gold standard key document.  A response relation- 
ship is a true positive  if a relationship  of the same 
type, and with the exact same arguments, exists in 
the key.  Corresponding definitions apply for false 
positive and false negative. Counts of these matches 
are used to calculate standard metrics of Recall (R), 
Precision (P ) and F 1 measure.
  The metrics do not say how hard relationship ex- 
traction is. We therefore provide a comparison with 
Inter Annotator Agreement (IAA) scores from the 
gold standard.  The IAA score gives the agreement 
between the two independent double annotators. It 
is equivalent to scoring one annotator against the 
other using the F 1 metric. IAA scores are not di- 
rectly  comparable here, as relationship annotation is



Fea
tur
e 
set
Siz
e
De
sc
rip
tio
n
tok
N
8N
Sur
fac
e 
stri
ng 
an
d 
PO
S 
of 
tok
ens 
sur
rou
ndi
ng 
the 
arg
um
ent
s, 
win
do
we
d 
−
N 
to 
+
N , 
N 
= 
6 
by 
def
aul
t
ge
nto
kN
8N
Ro
ot 
an
d 
ger
en
alis
ed 
PO
S 
of 
tok
ens 
sur
rou
ndi
ng 
the 
arg
um
ent 
enti
ties
, 
win
do
we
d 
−
N 
to 
+
N , 
N 
= 
6 
by 
def
aul
t
aty
pe
1
Co
nca
ten
ate
d 
se
ma
ntic 
typ
e 
of 
arg
um
ent
s, 
in 
arg
1-
arg
2 
or
de
r
dir
1
Dir
ecti
on: 
line
ar 
tex
t 
ord
er 
of 
the 
arg
um
ent
s 
(is 
arg
1 
bef
ore 
arg
2, 
or 
vic
e 
ver
sa?
)
dist
2
Dis
tan
ce: 
abs
olut
e 
nu
mb
er 
of 
sen
ten
ce 
and 
par
agr
aph 
bou
nda
ries 
bet
we
en 
arg
um
ent
s
str
14
Sur
fac
e 
stri
ng 
feat
ure
s 
bas
ed 
on 
Zh
ou 
et 
al 
(20
05),  
see 
text 
for 
full 
de
scr
ipti
on
po
s
14
PO
S 
feat
ure
s, 
as 
abo
ve
roo
t
14
Ro
ot 
feat
ure
s, 
as 
abo
ve
ge
np
os
14
Ge
ner
alis
ed 
PO
S 
feat
ure
s, 
as 
abo
ve
int
er
11
Int
erv
eni
ng 
me
ntio
ns: 
nu
mb
ers 
an
d 
typ
es 
of 
inte
rve
nin
g 
enti
ty 
me
ntio
ns 
bet
we
en 
arg
um
ent
s
eve
nt
5
Ev
ent
s: 
are 
any 
of 
the 
arg
um
ent
s, 
or 
inte
ven
ing 
ent
itie
s, 
eve
nts
?
allg
en
96
All 
fea
tur
es 
in 
roo
t 
an
d 
ge
ner
alis
ed 
PO
S 
for
ms, 
i.e. 
ge
nto
k6
+at
yp
e+
dir
+di
st+
ro
ot+
ge
np
os
+in
ter
+e
ve
nt
not
ok
48
All 
exc
ept 
tok
N 
fea
tur
es, 
oth
ers 
in 
stri
ng 
an
d 
PO
S 
for
ms, 
i.e. 
aty
pe
+di
r+
dis
t+s
tr+
po
s+i
nte
r+
ev
ent

Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.




a slightly different task for the human annotators. 
The relationship extraction system is given entities, 
and finds relationships between them. Human an- 
notators must find both the entities and the relation- 
ships. Therefore, were one human annotator to fail 
to find a particular entity, they could never find rela- 
tionships with that entity. The raw IAA score does 
not take this into account:  if an annotator fails to 
find an entity, then they will also be penalised for 
all relationships with that entity. We therefore give a 
Corrected IAA, CIAA, in which annotators are only 
compared on those relations  for which they have 
both found the entities involved. Both forms of IAA 
are shown in Table 4. It is clear that it is hard for 
annotators to reach agreement on relationships, and 
that this is compounded massively by lack of perfect 
agreement on entities. Note that the gold standard 
used in training  and evaluation reflects a further con- 
sensus annotation,  to correct this poor agreement.

6   Results

6.1   Feature Selection
The first group of experiments reported looks at the 
performance of relation extraction with various fea- 
ture sets. We followed  an additive strategy for fea- 
ture selection. Starting with basic features, we added 
further  features one set at a time. We measured the 
performance of the resulting classifier each time we 
added a new feature set.  Results are shown in Ta- 
ble 4.  The initial classifier  used a tok6+atype 
feature set.  Addition of both dir and dist fea- 
tures give significant improvements in all metrics, of 
around 10% F 1 overall, in each case. This suggests 
that the linear text order of arguments, and whether


relations are intra- or inter-sentential is important to 
classification. Addition of the str features also give 
good improvement in most metrics, again 10% F 1 
overall. Addition of part-of-speech information,  in 
the form of pos features, however, leads to a drop 
in some metrics, overall F 1 dropping by 1%. Unex- 
pectedly, POS seems to provide little extra informa- 
tion above that in surface string. Errors in POS tag- 
ging cannot be dismissed, and could be the cause of 
this. The existence of intervening  entities, as coded 
in feature set inter, provides a small benefit. The 
inclusion of information about events, in the event 
feature set, is less clear-cut.
  We were interested to see if generalising features 
could improve  performance,  as this had benefited 
our previous work in entity extraction. We replaced 
all surface string features with their root form, and 
POS features with their generalised POS form. This 
gave the results shown in column allgen. Results 
are not clear cut, in some cases better and in some 
worse than the previous best. Overall,  there is no 
difference in F 1. There is a slight increase in over- 
all recall, and a corresponding  drop in precision — 
as might be expected.
  Both the tokN, and the str and pos feature sets 
provide surface string and POS information  about 
tokens surrounding and between relationship  argu- 
ments. The former gives features from a window 
around each argument. The latter two give a greater 
amount of positional information. Do these two pro- 
vide enough information  on their own, without the 
windowed  features?  To test this, we removed the 
tokN  features from the full cumulative  feature set, 
from column +event. Results are given in column



Rel
ati
on
Me
tric
to
k6
+a
ty
pe
+di
r
+di
st
+st
r
+p
os
+in
ter
+e
ve
nt
all
ge
n
not
ok
IA
A
CI
AA
has 
fin
din
g
P
44
49
58
63
62
64
65
63
63



R
39
63
78
80
80
81
81
82
82



F1
39
54
66
70
69
71
72
71
71
46
80
has 
ind
ica
tio
n
P
37
23
38
42
40
41
42
37
44



R
14
14
46
44
44
47
47
45
47



F1
18
16
39
39
38
41
42
38
41
26
50
has 
loc
ati
on
P
36
36
50
68
71
72
72
73
73



R
28
28
74
79
79
81
81
83
83



F1
30
30
58
72
74
76
75
77
76
55
80
has 
tar
get
P
9
9
32
63
57
60
62
60
59



R
11
11
51
68
67
67
66
68
68



F1
9
9
38
64
60
63
63
63
62
42
63
lat
era
lity 
mo
difi
es
P
21
38
73
84
83
84
84
86
86



R
9
55
82
89
86
88
88
87
89



F1
12
44
76
85
83
84
84
84
85
73
94
neg
atio
n 
mo
difi
es
P
19
54
85
81
80
79
79
77
81



R
12
82
97
98
93
92
93
93
93



F1
13
63
89
88
85
84
85
83
85
66
93
sub 
loc
atio
n 
mo
difi
es
P
2
2
55
88
86
86
88
88
87



R
1
1
62
94
92
95
95
95
95



F1
1
1
56
90
86
89
91
91
90
49
96
Ov
era
ll
P
33
38
50
63
62
64
65
64
64



R
22
36
70
74
73
75
75
76
76



F1
26
37
58
68
67
69
69
69
70
47
75

Table 4: Variation in performance by feature set. Features sets are abbreviated  as in Table 3.  For the first seven 
columns, features were added cumulatively  to each other. The next two columns, allgen and notok, are as de- 
scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for 
comparison.




notok.  There is no clear change in performance, 
some relationships improving, and some worsening. 
Overall, there is a 1% improvement in F 1.
  It appears that the bulk of performance is attained 
through entity type and distance features, with some 
contribution from positional surface string informa- 
tion. Performance is between 1% and 9% lower than 
CIAA for the same relationship,  with a best overall 
F 1 of 70%, compared to a CIAA of 75%.

6.2   Sentences Spanned
Table 2 shows that although most relationships are 
intra-sentential, 23% are inter-sentential, 10% of all 
relationships being between arguments in adjacent 
sentences.  If we consider a relationship  to cross n 
sentence boundaries, then the classifiers described in 
the previous section were all trained on relationships
crossing n ≤ 1 sentence boundaries, i.e. with argu-
ments in the same or adjacent sentences. What effect
does including more distant relationships  have on 
performance?  We trained classifiers on only intra- 
sentential relationships,  and on relationships  span-
ning up to n sentence boundaries, for n ∈ {1...5}.


We also trained  a classifier  on relationships with
1 ≤ n ≤ 5, comprising 85% of all inter-sentential
relationships.  In each case, the cumulative  feature
set +event from Table 4 was used. Results are 
shown in Table 5.  It is clear from the results that 
the feature sets used do not perform well on inter- 
sentential relationships. There is a 6% drop in over- 
all F 1 when including relationships with n = 1 to- 
gether with n < 1. Performance continues to drop as 
more inter-sentential relationships are included, and 
is very poor for just inter-sentential relationships.
  A  preliminary error analysis  suggests that the 
more distant relationship  arguments are from each 
other, the more likely clinical knowledge is required 
to extract the relationship.  This raises additional  dif- 
ficulties for extraction, which the simple features de- 
scribed here are unable to address.

6.3   Size of Training  Corpus
The provision of sufficient training data for super- 
vised learning algorithms is a limitation on their use. 
We examined the effect of training corpus size on 
relationship extraction.  The C77 corpus, compris-




Number of sentence 
boundaries  between 
arguments

Corpus 
size

i
n
t
e
r
-
int
ra-
inter- and 
intra-
sentential

Rel
ati
on
Me
tric
1 
≤ 
n 
≤ 
5
n 
< 
1
n 
≤ 
1
n 
≤ 
2
n 
≤ 
3
n 
≤ 
4
n 
≤ 
5
C2
5
C5
0
C7
7
has 
fin
din
g
P
24
68
65
62
60
61
61
66
63
65

R
18
89
81
79
78
78
77
74
74
81

F1
18
76
72
69
67
68
67
67
67
72
has 
ind
ica
tio
n
P
18
49
42
42
36
32
30
22
25
42

R
17
59
47
42
42
39
38
30
31
47

F1
16
51
42
39
37
34
33
23
25
42
has 
loc
ati
on
P
0
74
72
73
72
72
72
72
71
72

R
0
83
81
81
81
82
82
76
80
81

F1
0
77
75
76
75
76
76
73
74
75
has 
tar
get
P
3
64
62
59
60
59
58
65
49
62

R
1
75
66
64
62
61
61
60
65
66

F1
2
68
63
61
60
60
59
59
54
63
lat
era
lity 
mo
difi
es
P
0
86
84
86
86
86
87
77
78
84

R
0
89
88
88
88
87
88
69
68
88

F1
0
85
84
85
86
85
86
72
69
84
neg
atio
n 
mo
difi
es
P
0
80
79
79
80
80
80
78
79
79

R
0
94
93
91
93
93
93
80
93
93

F1
0
86
85
84
85
86
85
78
84
85
sub 
loc
atio
n 
mo
difi
es
P
0
89
88
88
89
89
89
64
91
88

R
0
95
95
95
95
95
95
64
85
95

F1
0
91
91
91
91
91
91
64
86
91
Ov
era
ll
P
22
69
65
64
62
61
60
62
63
65

R
17
83
75
73
71
70
70
65
71
75

F1
19
75
69
68
66
65
65
63
66
69

Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.




ing 77 narratives  and used in the previous experi- 
ments, was subsetted to give corpora of 25 and 50 
narratives, which will be referred to as C25 and C50 
respectively.  We trained two further classifiers on 
these new corpora. Again, the cumulative  feature 
set +event from Table 4 was used. Results are 
shown in Table 5. Overall, performance improves as 
training  corpus size increases (F 1 rising from 63% 
to 69%). We were struck however, by the fact that 
increasing from 50 to 77 documents has little effect 
on a few relationships  (negation modifies and 
has location).  It may well be that the amount 
of training  data required has plateaued for those re- 
lationships.

7   Conclusion

We have shown that it is possible to extract clini- 
cal relationships from text, using shallow features, 
and supervised statistical  machine learning. Judg- 
ing from poor inter annotator agreement, the task 
is hard. Our system achieves  a reasonable  perfor- 
mance, with an overall F 1 just 5% below a cor- 
rected inter annotator agreement. This performance 
is reached largely by using features of the text that


encode entity type, distance between arguments, and 
some surface string information.  Performance does, 
however, vary with the number of sentences spanned 
by the relationships.  Learning inter-sentential rela- 
tionships does not seem amenable to this approach, 
and may require the use of domain knowledge.
  A major concern when using supervised learning 
algorithms  is the expense and availability of training 
data. We have shown that while this concern is jus- 
tified in some cases, larger training corpora may not 
improve performance for all relationships.
  The technology  used has proved  scalable. The 
full  CLEF IE system,  including automatic entity 
recognition, is able to process a document  in sub- 
second  time on a  commodity workstation.  We 
have used the system to extract 6 million relations 
from over half a million patient documents, for use 
in downstream CLEF applications (Roberts et al.,
2008a). Our future work on relationship extrac- 
tion in CLEF includes integration of a dependency 
parse into the feature set, further  analysis to deter- 
mine what knowledge may be required to learn inter- 
sentential relations,  and integration  of relationship 
extraction with a co-reference  algorithm.


Availability  All  of the software  described  here 
is open source and can be downloaded   as part of 
GATE, with the exception of the entity pairing com- 
ponent, which will be released shortly. We are cur- 
rently preparing a UK research ethics committee ap- 
plication, requesting permission to release our anno- 
tated corpus.

Acknowledgements

CLEF is funded by the UK Medical  Research Coun- 
cil.   We would like  to thank the Royal Marsden 
Hospital for providing the corpus, and our clinical 
partners in CLEF for assistance in developing the 
schema, and for gold standard annotation.


References

H.  Cunningham, D.  Maynard, K.  Bontcheva,   and 
V. Tablan. 2002. GATE: A framework  and graphi- 
cal development environment for robust NLP tools and 
applications.  In Proceedings of the 40th Anniversary 
Meeting of the Association for Computational Linguis- 
tics, pages 168–175, Philadelphia, PA, USA, July.
C. Friedman,   P. Alderson, J.  Austin, J.  Cimino, and 
S. Johnson. 1994. A general natural-language text 
processor for clinical radiology. Journal of the Amer- 
ican Medical Informatics Association, 1(2):161–174, 
March.
C.  Grover,  B.  Haddow,  E.  Klein,   M.  Matthews, 
L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting 
a relation  extraction  pipeline  for the BioCreAtIvE II 
task. In Proceedings of the BioCreAtIvE  II Workshop
2007, Madrid, Spain.
U. Hahn, M. Romacker, and S. Schulz.  2002. MEDSYN- 
DIKATE  — a  natural language  system  for the ex- 
traction of medical information from findings reports. 
International Journal of Medical Informatics, 67(1–
3):63–74, December.
Y.  Li,  K.  Bontcheva,  and H. Cunningham.   2005.
SVM based learning  system for information extrac- 
tion. In Deterministic and statistical methods in ma- 
chine learning: first international  workshop, number
3635 in Lecture  Notes in Computer  Science,  pages
319–339. Springer.
D. Lindberg, B. Humphreys, and A. McCray. 1993. The 
Unified Medical Language System. Methods of Infor- 
mation in Medicine, 32(4):281–291.
Y. Lussier, T. Borlawsky, D. Rappaport,  Y. Liu,  and 
C. Friedman. 2006. PhenoGO: Assigning phenotypic 
context to Gene Ontology  annotations with natural lan- 
guage processing. In Biocomputing  2006, Proceed-


ings of the Pacific  Symposium, pages 64–75, Hawaii, 
USA, January.
S. Pakhomov,  J. Buntrock,  and P. Duffy.  2005. High 
throughput modularized NLP system for clinical text. 
In Proceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics (ACL’05), 
interactive poster and demonstration  sessions, pages
25–28, Ann Arbor, MI, USA, June.
A. Rector,  J. Rogers,  A. Taweel, D. Ingram, D. Kalra, 
J.  Milan, P. Singleton,   R. Gaizauskas,  M. Hepple, 
D. Scott, and R. Power. 2003. CLEF — joining up 
healthcare with clinical and post-genomic research. In 
Proceedings of UK e-Science All Hands Meeting 2003, 
pages 264–267, Nottingham, UK.
T. Rindflesch and M. Fiszman. 2003. The interaction of 
domain knowledge and linguistic structure in natural 
language processing: interpreting hypernymic propo- 
sitions in biomedical text. Journal of Biomedical In- 
formatics, 36(6):462–477.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou, 
Y. Guo, A. Setzer, and I. Roberts. 2008a. Seman- 
tic annotation of clinical text: The CLEF corpus. In 
Proceedings of Building and evaluating resources for 
biomedical text mining:  workshop  at LREC 2008, 
Marrakech, Morocco, May. In press.
A. Roberts,  R. Gaizauskas,  M. Hepple, and Y. Guo.
2008b. Combining  terminology  resources and statis- 
tical methods for entity recognition: an evaluation. 
In Proceedings of the Sixth International  Conference 
on Language Resources and Evaluation,  LREC 2008, 
Marrakech, Morocco, May. In press.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.
1994. Natural language processing and the representa- 
tion of clinical data. Journal of the American Medical 
Informatics Association, 1(2):142–160, March-April.
T. Wang, Y. Li,  K. Bontcheva,  H. Cunningham,  and 
J. Wang. 2006. Automatic extraction of hierarchical 
relations from text.  In The Semantic Web: Research 
and Applications. 3rd European  Semantic Web Con- 
ference, ESWC 2006, number 4011 in Lecture Notes 
in Computer Science, pages 215–229. Springer.
G. Zhou, J. Su, J. Zhang,  and M. Zhang. 2005. Ex- 
ploring Various Knowledge in Relation Extraction. In 
Proceedings of the 43rd Annual Meeting of the Asso- 
ciation for Computational Linguistics (ACL’05), pages
427–434, Ann Arbor, MI, USA, June.
P. Zweigenbaum,  B. Bachimont,  J. Bouaud, J. Charlet, 
and J-F. Boisvieux. 1995. A multi-lingual architec- 
ture for building a normalised  conceptual representa- 
tion from medical language. In Proceedings of the An- 
nual Symposium on Computer Applications in Medical 
Care, pages 357–361, New York, NY, USA.

