An Error Analysis of Relation Extraction in Social Media Documents




Gregory Ichneumon Brown 
University of Colorado at Boulder 
Boulder, Colorado 
browngp@colorado.edu









Abstract

Relation extraction in documents allows the 
detection of how entities being discussed in a 
document are related to one another (e.g. part- 
of).  This paper presents an analysis of a re- 
lation extraction  system based on prior work 
but applied to the J.D. Power and Associates 
Sentiment Corpus to examine how the system 
works on documents from a range  of social 
media. The results are examined on three dif- 
ferent  subsets of the JDPA Corpus, showing 
that the system performs much worse on doc- 
uments from certain sources. The proposed 
explanation is that the features used are more 
appropriate to text with strong editorial stan- 
dards than the informal  writing style of blogs.


1   Introduction

To summarize accurately, determine the sentiment, 
or answer questions about a document it is often nec- 
essary to be able to determine the relationships be- 
tween entities being discussed in the document (such 
as part-of  or member-of).  In the simple sentiment 
example

Example 1.1: I bought a new car yesterday.  I love 
the powerful engine.

determining the sentiment the author is expressing 
about the car requires knowing  that the engine is a 
part of the car so that the positive sentiment being 
expressed about the engine can also be attributed to 
the car.
  In this paper we examine our preliminary  results 
from applying a relation extraction  system to the


J.D. Power and Associates (JDPA)  Sentiment Cor- 
pus (Kessler et al., 2010).  Our system  uses lex- 
ical features from prior work to classify relations, 
and we examine how the system works on different 
subsets from the JDPA Sentiment Corpus, breaking 
the source documents down into professionally writ- 
ten reviews, blog reviews, and social networking re- 
views. These three document types represent quite 
different writing styles,  and we see significant differ- 
ence in how the relation extraction system performs 
on the documents from different sources.

2   Relation Corpora

2.1   ACE-2004 Corpus

The Automatic  Content Extraction (ACE) Corpus 
(Mitchell, et al., 2005) is one of the most common 
corpora for performing relation extraction. In addi- 
tion to the co-reference annotations, the Corpus is 
annotated to indicate 23 different relations between 
real-world  entities that are mentioned  in the same 
sentence. The documents consist of broadcast news 
transcripts and newswire articles from a variety  of 
news organizations.

2.2   JDPA Sentiment Corpus

The JDPA Corpus consists of 457 documents con- 
taining  discussions about cars, and 180 documents 
discussing cameras (Kessler  et al., 2010). In this 
work we only use the automotive  documents.  The 
documents  are drawn from a  variety of sources, 
and we particularly  focus on the 24% of the doc- 
uments from the JDPA Power Steering blog, 18% 
from Blogspot, and 18% from LiveJournal.




64


Proceedings of the ACL-HLT 2011 Student Session, pages 64–68,
Portland, OR, USA 19-24 June 2011. Qc 2011 Association for Computational Linguistics


  The annotated mentions in the Corpus are single 
or multi-word expressions which refer to a particu- 
lar real world or abstract entity. The mentions are 
annotated to indicate  sets of mentions which con- 
stitute co-reference groups referring to the same en- 
tity. Five relationships  are annotated between these 
entities: PartOf, FeatureOf, Produces, InstanceOf, 
and MemberOf.  One significant difference between 
these relation  annotations and those in the ACE Cor- 
pus is that the former  are relations between sets of 
mentions (the co-reference groups) rather than be- 
tween individual mentions. This means that these 
relations are not limited to being between mentions 
in the same sentence. So in Example 1.1, “engine” 
would  be marked as a part of “car” in the JDPA Cor- 
pus annotations, but there would be no relation an- 
notated in the ACE Corpus. For a more direct com- 
parison to the ACE Corpus results, we restrict our- 
selves only to mentions within the same sentence 
(we discuss this decision further in section 5.4).

3   Relation Extraction System

3.1   Overview

The system extracts all pairs of mentions in a sen- 
tence, and then classifies  each pair of mentions  as 
either having a relationship,  having an inverse rela- 
tionship, or having no relationship.  So for the PartOf 
relation in the JDPA Sentiment Corpus we consider 
both the relation “X is part of Y” and “Y is part of 
X”. The classification of each mention  pair is per- 
formed using a support vector machine implemented 
using libLinear  (Fan et al., 2008).
  To generate the features for each of the mention 
pairs a proprietary JDPA Tokenizer is used for pars- 
ing the document and the Stanford Parser (Klein and 
Manning, 2003) is used to generate parse trees and 
part of speech tags for the sentences in the docu- 
ments.

3.2   Features

We used Zhou et al.’s lexical features (Zhou et al.,
2005) as the basis for the features of our system sim- 
ilar to what other researchers have done (Chan and 
Roth, 2010). Additional work has extended  these 
features (Jiang and Zhai, 2007) or incorporated other 
data sources (e.g. WordNet), but in this paper we fo- 
cus solely on the initial step of applying  these same


lexical features to the JDPA Corpus.
  The Mention Level, Overlap, Base Phrase Chunk- 
ing, Dependency  Tree, and Parse Tree features are 
the same as Zhou et al. (except for using the Stan- 
ford Parser rather than the Collins  Parser). The mi- 
nor changes we have made are summarized below:

• Word Features:  Identical,  except rather than 
using a heuristic to determine the head word of 
the phrase it is chosen to be the noun (or any 
other word if there are no nouns in the men- 
tion) that is the least deep in the parse tree. This 
change has minimal impact.
• Entity Types: Some of the entity types in the 
JDPA Corpus indicate the type of the relation 
(e.g. CarFeature,  CarPart)  and so we replace 
those entity types with “Unknown”.
• Token Class: We added an additional  feature 
(TC12+ET12) indicating the Token Class of 
the head words (e.g. Abbreviation, DollarAm- 
mount, Honorific) combined  with the entity 
types.
• Semantic  Information:  These  features  are 
specific to the ACE relations and so are not 
used. In Zhou et al.’s work, this set of features 
increases the overall  F-Measure by 1.5.

4   Results

4.1   ACE Corpus Results
We ran our system on the ACE-2004  Corpus as a 
baseline to prove that the system worked properly 
and could approximately duplicate Zhou et al.’s re- 
sults. Using 5-fold cross validation on the newswire 
and broadcast news  documents  in the dataset we 
achieved an average overall F-Measure of 50.6 on 
the fine-grained relations. Although a bit lower than 
Zhou et al.’s result of 55.5 (Zhou et al., 2005), we 
attribute the difference to our use of a different  tok- 
enizer, different  parser, and having not used the se- 
mantic information features.

4.2   JDPA Sentiment Corpus Results
We randomly divided the JDPA Corpus into train- 
ing (70%), development  (10%), and test (20%) 
datasets.  Table 1 shows relation extraction results 
of the system  on the test portion of the corpus. 
The results are further broken out by three differ- 
ent source types to highlight the differences caused




R
el
ati
o
n
All 
Docum
ents
LiveJo
urnal
Blog
spot
J
D
P
A

P
R
F
P
R
F
P
R
F
P
R
F
F
EA
T
U
R
E 
O
F
4
4.
8
42
.3
43
.5
2
6.
8
35
.8
30
.6
4
4.
1
40
.0
42
.0
5
9.
0
55
.0
56
.9
M
E
M
B
E
R 
O
F
3
4.
1
10
.7
16
.3
0
.
0
0.
0
0.
0
3
6.
0
13
.2
19
.4
3
6.
4
13
.7
19
.9
PA
R
T 
O
F
4
6.
5
34
.7
39
.8
4
1.
4
17
.5
24
.6
4
8.
1
35
.6
40
.9
4
8.
8
43
.9
46
.2
P
R
O
D
U
C
E
S
5
1.
7
49
.2
50
.4
0
5.
0
36
.4
08
.8
4
3.
7
36
.0
39
.5
6
6.
5
64
.6
65
.6
IN
ST
A
N
C
E 
O
F
3
7.
1
16
.7
23
.0
4
4.
8
14
.9
22
.4
4
2.
1
13
.0
19
.9
3
0.
9
29
.6
30
.2
O
ve
ra
ll
4
6.
0
36
.2
40
.5
2
7.
1
22
.6
24
.6
4
5.
2
33
.3
38
.3
5
3.
7
46
.5
49
.9

Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.


Li
ve
Jo
ur
na
l
Bl
og
sp
ot
J
D
P
A
A
C
E
To
ke
ns 
Pe
r 
S
en
te
nc
e
1
9
.
2
1
8
.
6
1
6
.
5
1
9
.
7
Re
lati
on
s 
Pe
r 
S
en
te
nc
e
1
.
0
8
1
.
7
1
2
.
5
6
0
.
5
6
Re
lati
on
s 
N
ot 
In 
Sa
me 
S
en
te
nc
e
3
3
%
3
0
%
2
7
%
0
%
Tr
ai
ni
ng 
M
en
tio
n 
Pa
irs 
in 
On
e 
Se
nt
en
ce
5
8
,
4
5
2
5
4
,
4
8
0
95
,6
30
7
7,
5
7
2
M
en
tio
ns 
Pe
r 
S
en
te
nc
e
4
.
2
6
4
.
3
2
4
.
0
3
3
.
1
6
M
en
tio
ns 
Pe
r 
E
nti
ty
1
.
7
3
1
.
6
3
1
.
3
3
2
.
3
6
M
en
tio
ns 
Wi
th 
O
nly 
O
ne 
To
ke
n
7
7
.
3
%
7
3
.
2
%
6
1.
2
%
5
6.
2
%

Table 2: Selected document statistics for three JDPA Corpus document sources.




by the writing styles from different types of media: 
LiveJournal (livejournal.com),  a social media site 
where users comment and discuss stories with each 
other; Blogspot (blospot.com), Google’s blogging 
platform; and JDPA (jdpower.com’s Power Steering 
blog), consisting of reviews of cars written by JDPA 
professional  writers/analysts.    These subsets were 
selected because they provide the extreme (JDPA 
and LiveJournal) and average (Blogspot)  results for 
the overall dataset.

5   Analysis

Overall the system is not performing  as well as it 
does on the ACE-2004  dataset.  However,  there is 
a 25 point F-Measure difference between the Live- 
Journal and JDPA authored documents.  This sug- 
gests that the informal style of the LiveJournal doc- 
uments may be reducing the effectiveness of the 
features developed by Zhou et al., which were de- 
veloped on newswire and broadcast news transcript 
documents.
  In the remainder of this section we look at a sta- 
tistical analysis of the training portion of the JDPA 
Corpus, separated by document source, and suggest


areas where improved  features may be able to aid 
relation extraction on the JDPA Corpus.


5.1   Document Statistic Effects on Classifier

Table 2 summarizes some important  statistical dif- 
ferences between  the documents from different 
sources. These differences  suggest two reasons why 
the instances being used to train the classifier could 
be skewed disproportionately  towards the JDPA au- 
thored documents.
  First, the JDPA written documents express a much 
larger number of relations between entities. When 
training  the classifier, these differences will cause a 
large share of the instances that have a relation to be 
from a JDPA  written document, skewing the clas- 
sifier towards any language clues specific to these 
documents.
  Second, the number of mention pairs occurring 
within one sentence is significantly higher in the 
JDPA  authored  documents  than the other docu- 
ments. This disparity is even true on a per sentence 
or per document basis. This provides the classifier 
with significantly  more negative examples written in 
a JDPA written style.




Word Percent of All Tokens in Documents

LiveJourna
l
Bl
og
sp
ot
JD
P
A
A
C
E
c
a
r
I
 
i
t
 
I
t
 
i
t
s
t
h
e
0
.
8
6
1
.
9
1
1
.
4
2
0
.
3
3
0
.
2
5
4
.
4
3
0
.
7
1
1
.
2
8
0
.
9
7
0
.
2
7
0
.
1
8
4
.
6
0
0.
2
0
0.
2
4
0.
2
3
0.
3
5
0.
2
2
3.
5
4
0.
0
1
0.
2
1
0.
6
3
0.
0
9
0.
1
9
4.
8
1

Table 4: Frequency of some common words per token.












Table 3: Top 10 phrases in mention pairs whose relation 
was incorrectly  classified, and the total percentage of er- 
rors from the top ten.


5.2   Common Errors
Table 3  shows the mention phrases  that occur 
most commonly in the incorrectly  classified men- 
tion pairs. For the LiveJournal and Blogspot data, 
many more of the errors are due to a few specific 
phrases being classified  incorrectly   such as “car”, 
“Maybach”,  and various forms of “it”.  The top four 
phrases constitute  17% of the errors for LiveJour- 
nal and 14% for Blogspot.  Whereas the JDPA doc- 
uments have the errors spread more evenly  across 
mention phrases, with the top 10 phrases constitut- 
ing 13.6% of the total errors.
  Furthermore,  the phrases causing  many of the 
problems for the LiveJournal and Blogspot relation 
detection are generic nouns and pronouns such as 
“car” and “it”.    This suggests  that the classifier 
is having difficulty determining relationships when 
these less descriptive  words are involved.

5.3   Vocabulary
To investigate where these variations in phrase error 
rates comes from, we performed two analyses of the 
word frequencies in the documents: Table 4 shows 
the frequency of some common words in the docu- 
ments; Table 5 shows the frequency of a select set of 
parts-of-speech per sentence in the document.





Table 5: Frequency of select part-of-speech tags.


  We find that despite all the documents discussing 
cars, the JDPA reviews use the word “car” much less 
often, and use proper nouns significantly more often. 
Although “car” also appears in the top ten errors on 
the JDPA documents, the total percentage of the er- 
rors is one fifth of the error rate on the LiveJour- 
nal documents. The JDPA authored documents also 
tend to have more multi-word  mention phrases (Ta- 
ble 2) suggesting that the authors use more descrip- 
tive language when referring  to an entity.  77.3% 
of the mentions in LiveJournal documents use only 
a single  word while 61.2% of mentions JDPA au- 
thored documents are a single word.
  Rather than descriptive  noun phrases, the Live- 
Journal and Blogspot  documents make more use of 
pronouns. LiveJournal especially uses pronouns of- 
ten, to the point of averaging one per sentence, while 
JDPA uses only one every five sentences.

5.4   Extra-Sentential Relations

Many relations in the JDPA Corpus occur between 
entities which are not mentioned in the same sen- 
tence. Our system only detects relations  between 
mentions in the same sentence, causing about 29% 
of entity relations to never be detected (Table  2).


The LiveJournal documents are more likely to con- 
tain relationships between entities that are not men- 
tioned in the same sentence.   In the semantic role 
labeling (SRL) domain, extra-sentential arguments 
have been shown to significantly improve SRL per- 
formance (Gerber and Chai, 2010). Improvements 
in entity relation extraction could likely be made by 
extending Zhou et al.’s features across sentences.

6   Conclusion

The above analysis shows that at least some of the 
reason for the system performing  worse on the JDPA 
Corpus than on the ACE-2004 Corpus is that many 
of the documents in the JDPA  Corpus have a dif- 
ferent writing style from the news articles  in the 
ACE Corpus.  Both the ACE news documents, and 
the JDPA authored documents are written by profes- 
sional writers with stronger editorial  standards than 
the other JDPA Corpus documents, and the relation 
extraction system performs much better on profes- 
sionally  edited documents.  The heavy use of pro- 
nouns and less descriptive  mention  phrases in the 
other documents seems to be one cause of the re- 
duction in relation extraction performance. There is 
also some evidence that because of the greater num- 
ber of relations in the JPDA authored documents that 
the classifier training data could be skewed more to- 
wards those documents.
  Future work  needs to explore  features that can ad- 
dress the difference  in language usage that the dif- 
ferent authors  use.   This work also does not ad- 
dress whether  the relation extraction task is being 
negatively impacted by poor tokenization or pars- 
ing of the documents rather than the problems being 
caused by the relation classification itself. Further 
work is also needed to classify extra-sentential rela- 
tions, as the current methods look only at relations 
occurring within a single  sentence thus ignoring  a 
large percentage of relations between entities.

Acknowledgments

This work was partially funded  and supported by 
J. D. Power and Associates.  I would like to thank 
Nicholas Nicolov, Jason Kessler, and Will Headden 
for their help in formulating this work, and my the- 
sis advisers: Jim Martin, Rodney Nielsen, and Mike 
Mozer.


References

Chan, Y. S. and Roth D. Exploiting Background Knowl- 
edge for Relation Extraction.  Proceedings of the 23rd 
International  Conference on Computational Linguis- 
tics (Coling 2010).
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and 
C.-J. Lin.   LIBLINEAR: A library for large linear 
classification.  Journal of Machine Learning Research
9(2008), 1871-1874. 2008.
Gerber, M. and Chai, J.  Beyond NomBank:  A Study of 
Implicit Arguments for Nominal Predicates. Proceed- 
ings of the 48th Annual Meeting of the Association for 
Computational Linguistics, pages 1583-1592.  2010.
Jiang, J. and Zhai, C.X. A systematic exploration  of the 
feature space for relation extraction. In The Proceed- 
ings of NAACL/HLT. 2007.
Kessler J., Eckert  M., Clark L., and Nicolov N..  The 
ICWSM 2010 JDPA Sentiment Corpus for the Auto- 
motive Domain International AAAI Conference on 
Weblogs and Social Media Data Challenge Workshop.
2010.
Klein D. and Manning C. Accurate Unlexicalized Pars- 
ing.  Proceedings of the 41st Meeting of the Asso- 
ciation for Computational Linguistics, pp. 423-430.
2003.
Mitchell A., et al. ACE 2004 Multilingual Training Cor- 
pus. Linguistic  Data Consortium, Philadelphia. 2005. 
Zhou G., Su J., Zhang J., and Zhang M. Exploring var- 
ious knowledge in relation extraction. Proceedings of
the 43rd Annual Meeting of the ACL. 2005.

