Tree Kernel-based Relation Extraction
with Context-Sensitive Structured Parse Tree Information


GuoDong ZHOU 12 	Min ZHANG 2 	Dong Hong JI 2 	QiaoMing ZHU 1
1School of Computer Science & Technology 	2  Institute for Infocomm Research
Soochow Univ. 	Heng Mui Keng Terrace
Suzhou, China 215006 	Singapore 119613
Email: {gdzhou,qmzhu}@suda.edu.cn 	Email: {zhougd, mzhang, dhji}@i2r.a-star.edu.sg





Abstract

This paper proposes a tree kernel  with context- 
sensitive structured parse tree information for re- 
lation extraction. It resolves two critical problems 
in previous tree kernels for relation extraction  in 
two ways. First, it automatically determines a dy- 
namic context-sensitive  tree span for relation ex- 
traction   by  extending  the  widely -used  Shortest 
Path-enclosed Tree (SPT) to include necessary 
context information outside SPT. Second, it pro- 
poses a context-sensitive convolution tree kernel, 
which enumerates both context-free and context- 
sensitive  sub-trees by consid ering  their  ancestor 
node paths as their contexts. Moreover, this paper 
evaluates the complementary nature between our 
tree kernel and a state-of-the-art linear kernel. 
Evaluation on the ACE RDC corpora shows that 
our dynamic context-sensitive tree span is much 
more s uitable for relation extraction than SPT and 
our tree kernel outperforms the state-of-the-art 
Collins  and  Duffy’s  convolution  tree  kernel.  It 
also shows that our tree kernel achieves much bet- 
ter performance than the state-of-the-art linear 
kernels .  Finally, it shows that  feature-based and 
tree kernel-based methods much complement each 
other and the composite kernel can well integrate 
both flat and structured features.

1     Introduction

Relation extraction  is to find various predefined  se- 
mantic relations between pairs of entities in text. The 
research in relation extraction has been pr omoted by 
the Message Understanding Conferences (MUCs) 
(MUC, 1987-1998) and the NIST Automatic Content 
Extraction (ACE) program (ACE, 2002-2005). Ac- 
cording to the ACE Program, an entity is an object or 
a set of objects in the world and a relation is an ex- 
plicitly or implicitly stated relationship among enti- 
ties.  For  example,  the  sentence  “Bill  Gates is   the


chairman  and chief software  architect  of Microsoft 
Corporation.”   c onveys 	the 	ACE-style 	relation 
“EMPLOYMENT.exec” between  the  entities  “Bill 
Gates” (person  name)  and  “Microsoft  Corporation” 
(organization name). Extraction of semantic relations 
between entities can be very useful in many applic a- 
tions such as question answering, e.g. to answer the 
query “Who  is the president of the United States?”, 
and information   retrieval, e.g. to expand the query 
“George W. Bush”with “the pres ident of the United 
States”via his relationship with “the United States”.
  Many researches have been done in relation extrac- 
tion.  Among them,   feature-based  methods  (Kamb- 
hatla 2004; Zhou et al., 2005) achieve certain success 
by employing a large amount of diverse linguistic 
features,  varying  from  lexical  knowledge,   entity- 
related information to syntactic parse trees, depend- 
ency trees and semantic information . How ever, it is 
difficult  for  them  to  effectively  capture  struc tured 
parse tree information (Zhou et al 2005), which is 
critical for further performance improvement in rela- 
tion extraction.
As  an  alternative  to  feature-based  methods,  tree
kernel-based methods provide an elegant solution to 
explore   implic itly   structured   features   by   directly 
computing the simila rity between two trees. Although 
earlier researches (Zelenko et al 2003; Culotta and 
Sorensen 2004; Bunescu and Mooney 2005a) only 
achieve success on simple tasks and fail on complex 
tasks, such as the ACE RDC task, tree kernel-based 
methods  achieve  much  progress  recently.  As  the 
state-of-the-art, Zhang et al (2006) applied the c onvo- 
lution  tree  kernel   (Collins  and  Duffy  2001)  and 
achieved comparable performance with a state-of-the- 
art linear kernel (Zhou et al 2005)  on the 5 relation 
types in the ACE RDC 2003 corpus.
However,  there are two problems in  Collins and
Duffy’s convolution tree kernel for relation extrac tion. 
The first is that the sub-trees enumerated in the tree 
kernel  computation  are  context-free.  That  is,  each 
sub-tree enumerated in the  tree  kernel computation




728
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural  Language Learning,  pp. 728–736, Prague, June 2007. Qc 2007 Association for Computational Linguistics



does not consider the context information outside the 
sub-tree. The second is to dec ide a proper tree span in 
relation extraction. Zhang et al (2006) explored  five 
tree spans in relation extraction and  it was a bit sur- 
prising  to find that  the Shortest Path-enclosed  Tree 
(SPT, i.e. the sub-tree enclosed by the shortest path 
linking  two involved  entities  in  the  parse  tree)  per- 
formed best. This is contrast to our intuition. For ex- 
ample,  “got  married”   is  critical  to  determine  the 
relationship between “John” and “Mary”in  the sen- 
tence  “John  and Mary  got  married… ” as shown in 
Figure  1(e). It is  obvious  that  the information  con- 
tained in SPT (“John  and Marry”) is not  enough to 
determine their relationship.
This paper proposes  a context-sensitive  convolu-
tion tree kernel for relation extraction to resolve  the 
above two problems. It first automatically determines 
a dynamic context-sensitive tree span for relation ex- 
traction by extending the Shortest Path-enclosed Tree 
(SPT) to  include necessary context information out- 
side SPT. Then it proposes a context -sensitive convo- 
lution tree kernel, whic h not only enumerates context- 
free  sub-trees but also context -sensitive sub-trees by
considering their ancestor node paths as their contexts. 
Moreover,  this  paper  evaluates  the  complementary 
nature of different linear kernels and tree kernels via a 
composite kernel.
  The layout of this paper is as follows. In Section 2, 
we review related work in more details. Then, the 
dynamic context-sensitive tree span and the context- 
sensitive convolution tree ker nel are proposed in Sec- 
tion 3 while Section 4 shows the experimental results. 
Finally, we conclude our work in Sec tion 5.

2 	Related Work

The  relation  extraction  task  was  first  introduced  as 
part of the Template Element task in MUC6 and then 
formulated as the Template Relation task in MUC7. 
Since then, many methods, such as  feature-based 
(Kambhatla 2004; Zhou et al 2005, 2006),  tree ker- 
nel-based (Zelenko et al 2003;  Culotta and Sorensen
2004; Bunescu and Mooney 2005a; Zhang et al 2006)
and   composite   kernel-based  (Zhao   and  Gris hman
2005; Zhang et al 2006), have been proposed in lit- 
erature.
  For the feature-based methods, Kambhatla (2004) 
employed Maximum Entropy models to combine di- 
verse lexical, syntactic and semantic features in rela- 
tion extrac tion, and achieved  the F-measure of 52.8 
on the 24 relation subtypes in the ACE RDC 2003 
corpus. Zhou et al (2005) further systematically ex- 
plored diverse features through a linear kernel and 
Support   Vector   Machines ,  and   achieved   the   F-



measures of 68.0 and 55.5 on the 5 relation types and 
the 24 relation subtypes in the ACE RDC 2003 cor- 
pus respectively. One problem with the feature-based 
methods is that they need extensive feature engineer- 
ing. Another problem is that, although  they can ex- 
plore  some  structured  information  in  the  parse  tree 
(e.g.  Kambhatla  (2004)  used the  non-terminal  path c 
onnecting the given two entities in a parse tree while 
Zhou et al. (2005) introduced additional chunking 
features to enhance the performance), it is found dif- 
ficult to well preserve structured information in the 
parse trees using the feature-based methods. Zhou et 
al (2006) further improved the performance by ex- 
ploring the commonality among related classes in a 
class hierarchy using hierarchical learning strategy.
  As an alternative to the feature-based methods, the 
kernel-based methods (Haussler, 1999) have been 
proposed to implicitly explore   various  features  in a 
high dimensional space by employing a kernel to cal- 
culate the similarity between two objects directly. In 
particular, the kernel-based methods could be very 
effective at reducing the burden  of feature engineer- 
ing for structured objects in NLP researches, e.g. the 
tree structure in relation extraction.
Zelenko  et  al.  (2003)  proposed a kernel  between
two  parse  trees,  which  recursively  matches  nodes 
from roots to leaves in a top-down manner . For each 
pair of matched nodes, a subsequence kernel on their 
child nodes is invoked. They  achieved quite success 
on two simple relation extraction tasks.  Culotta and 
Sorensen (2004) extended this work to estimate  simi- 
larity   between   augmented   dependency   trees   and 
achieved  the  F-measure  of  45.8  on  the  5  relation 
types  in  the  ACE  RDC  2003  corpus.  One problem 
with the above two tree kernels is that matched nodes 
must be at the same height and have the same path to 
the  root  node.   Bunescu  and  Mooney  (2005a)  pr o- 
posed a shortest path dependency tree kernel, which
just sums up the number of common word classes
at each position in the two paths, and achieved the 
F-measure of 52.5 on the 5 relation types in the ACE 
RDC 2003 corpus. They argued that the information 
to model a  relationship between two entities can be 
typically captured by the shortest path between them
in the dependency graph.  While the shortest  path
may not be able to well preserve structured de- 
pendency  tree  information,  another  problem  with 
their  kernel is that the two paths should have same 
length. This makes  it s uffer from the similar behavior 
with that of Culotta and Sorensen (2004): high preci- 
sion but very low recall.
  As the  state-of-the-art tree kernel-based method, 
Zhang et al (2006) explored various structured feature



spaces  and  used  the  convolution   tree  kernel  over 
parse trees (Collins and Duffy 2001) to model syntac- 
tic   structured   information   for   relation  extraction. 
They achieved the F -measures of 61.9 and 63.6 on the
5 relation types of the ACE RDC 2003 corpus and the
7 relation types of the ACE RDC 2004 corpus respec- 
tively without entity-related information while the F- 
measure  on the 5 relation  types  in the ACE  RDC
2003 corpus reached 68.7 when entity-related infor-
mation was included in the parse tree.  One problem 
with  Collins  and  Duffy’s convolution tree kernel is 
that the sub-trees involved in the tree kernel computa- 
tion are context-free, that is, they do not consider the 
information  outside  the  sub-trees.  This is  different 
from the tree kernel in Culota and Sorensen (2004), 
where the sub-trees involved in the  tree kernel com- 
putation are context -sensitive (that is,  with the path 
from the tree root node to the sub-tree root node in 
consideration).  Zhang et al (2006) also  showed that 
the widely-used Shortest Path-enclosed Tree (SPT) 
performed best. One problem with SPT is that it fails 
to capture the contextual information outside the 
shortest path, which is important  for relation extrac- 
tion in many cases . Our random selection of 100 pos i- 
tive  training  instances  from  the  ACE  RDC  2003 
training corpus shows that ~25% of the cases need 
contextual   information   outside   the   shortest   path. 
Among other kernels , Bunescu and Mooney (2005b) 
proposed a subsequence kernel and applied it in pro- 
tein interaction and ACE relation extraction tasks.
In  order  to  integrate  the  advantages  of  feature-
based and tree kernel-based methods, some researc h- 
ers have turned to composite kernel-based methods. 
Zhao and Grishman (2005) defined several feature- 
based composite kernels to integrate diverse features 
for relation extraction and achieved the F-measure of
70.4 on the  7 relation types of the ACE RDC  2004
corpus . Zhang et al (2006) proposed two composite 
kernels to integrate a linear kernel and Collins and 
Duffy’s convolution  tree  kernel.  It  achieved  the  F- 
measure of 70.9/57.2 on the 5 relation types/24 rela- 
tion subtypes in  the ACE RDC 2003 corpus and the 
F-measure of 72.1/63.6 on the 7 relation types/23 
relation subtypes in the ACE RDC 2004 corpus.
The above discussion suggests that structured  in-
formation in the parse tree may not be fully utilized in 
the previous works, regardless of feature-based, tree 
kernel-based or composite kernel-based methods. 
Compared with the previous works,  this paper pro- 
poses a dynamic context-sensitive tree span trying to
cover necessary structured information and a c ontext-



more, a composite kernel is applied to combine our 
tree kernel and a state-of-the-art linear kernel for in- 
tegrating both flat and structured features in relation 
extraction as well as validating their complementary 
nature.

3    Context    Sensitive    Convolution    Tree
Kernel for Relation Extraction

In this section, we first propose an algorithm to  dy- 
namically determine a proper context-sensitive tree 
span and then a context-sensitive convolution tree 
kernel for relation extraction.

3.1 Dynamic   Context-Sensitive   Tree   Span   in
Relation Extraction

A relation instance  between two entities  is encaps u- 
lated by a parse tree. Thus, it is critical to understand 
which portion of a parse tree is important in the tree 
kernel calculation.  Zhang et al (2006) systematically 
explored  seven  different  tree  spans ,  including  the 
Shortest  Path -enclosed  Tree  (SPT)  and  a  Context-
Sensitive Path-enclosed Tree1 (CSPT), and found that
SPT per formed best. That is,  SPT even outperforms 
CSPT. This is contrary to our intuition. For example, 
“got married” is critical to determine the relationship 
between  “John”  and “Mary” in  the  sentence “John 
and  Mary  got  married… ” as  shown  in  Figure  1(e), 
and  the  information  contained  in  SPT  (“John  and 
Mary”) is not enough to determine their relationship. 
Obviously,  context -sensitive tree spans should have 
the  potential  for  better  performance.  One  problem 
with the context-sensitive tree span explored in Zhang 
et al (2006) is that it only considers the availability of 
entities’siblings and fails to consider following two 
factors:
1)   Whether  is  the  information  contained  in  SPT
enough  to  determine  the  relationship  between 
two entities? It depends. In the embedded cases, 
SPT  is  enough.  For  example,  “John’s  wife” is 
enough to determine the relationship between 
“John” and “John’s wife”in the sentence “John’s 
wife got a good job… ”as shown in Figure 1(a) . 
However, SPT is not enough  in the coordinated 
cases, e.g. to determine the relationship between 
“John”  and “Mary” in  the  sentence  “John  and 
Mary got married… ”as shown in Figure 1(e).



1  CSPT means SPT extending with the 1st left sibling of
st


sensitive  convolution  tree  kernel  considering  both


the node of entity 1 and the 1


right sibling o f the node



context-free and context-sensitive  sub-trees. Further-


of entity 2. In the case of no available sibling, it moves 
to the parent of current node and repeat the same pro c-
ess until a sibling is avai lable or the root is reached.



2)  How can we extend SPT to include necessary 
context information if there is no enough infor- 
mation in SPT for relation extraction?
  To answer  the above two questions, we randomly 
chose  100  positive  instances  from  the  ACE  RDC
2003 training data  and  studied  their  necessary  tree 
spans. It was observed that we can classify them into
5 categories: 1) embedded (37 instances), where one



sentence “John and Mary got married…”as shown in
Figure 1(e);
Based on the above observations, we implement an
algorithm to determine the necessary tree span for the 
relation extract task. The idea behind the algorithm is 
that the  necessary tree span for a relation  should be 
determined dynamically according to  its tree span 
category  and  context.  Given a parsed tree and two


entity is embedded in another entity, e.g. “John”and


entities  in  consideration,


ti first  determin es  the  tree


“John’s wife”as shown in Figure 1(a) ; 2) PP-linked 
(21 instances), where one entity is linked to another 
entity via PP attachment, e.g. “CEO”and “Microsoft” 
in the sentence “CEO of Microsoft announced … ”as 
shown  in  Figure  1(b);  3)  semi-structured  (15  in- 
stances), where the sentence consists of a s equence of 
noun phrases  (including the two given entities),  e.g. 
“Jane” and “ABC  news”in the sentence “Jane, ABC 
news,  California .” as shown  in Figure  1(c); 4)  de- 
scriptive (7  instances),  e.g.  the  citizenship between 
“his mother” and  “Lebanese” in  the  sentence  “his 
mother  Lebanese  landed  at  …”as  shown in Figure
1(d); 5) predicate-linked and others (19 instances, 
including coordinated cases), where the predicate 
information is necessary to determine the relationship 
between two entities, e.g.  “John” and “Mary” in the
S


span category and then extends the tree span accor d- 
ingly.   By   default,   we   adopt   the   Shortest   Path- 
enclosed Tree  (SPT) as our tree span. We only ex- 
pand the tree span when the tree span belongs to the 
“predicate-linked”category.  This is based on our ob- 
servation that the tree spans belonging to the “predi- 
cate-linked” category  vary  much  syntactically  and 
majority  (~70%)  of  them  need information  outside 
SPT while it is quite safe  (>90%) to use SPT as the 
tree span for the remaining categories. In our alg o- 
rithm, the expansion is done by first moving up until 
a predicate-headed  phrase is found  and then moving 
down along the  predicated-headed path to the  predi- 
cate terminal node. Figure 1(e) shows an example for 
the “predicate-linked” category  where the lines with 
arrows indicate the expansion path.
S                                                                                      PP(IN)-subtoot



NP -E2-PER	VP 



NP 
PP 	VP


NP-E2-ORG

IN 	NNP


NP-E1-PER


NP
N
P
 -
E
1
-
P
E
R
 	NP-E2-ORG


of 	Microsoft


NNP 	PO S 	NN       V 	DT     JJ       NN



NN 	NNP 	VBD 	…



NP(NN)


a) context -free


John 	’s	wife      found 	a
a) embedded

NP 


good   job


CEO 	of       Microsoft 	a 
nnounced
b)  PP -linked

S



PP(IN)-subroot

NP -E2-ORG




NP -E1-PER	NP -E2-ORG	NP 



NP 	VP


IN 	NNP

of 	Microsoft



, 	, 	.


NP -E1-PER       NP -E2-GPE	PP 


b ) context -sensitive


NNP	,  NNP	S      ,  N 


PRP$ 	NN


NNP 	VBD 	…


S(VBD)


Jane


, 	ABC      
news      ,   
California 	.
c) 
semi-
struct
ured



His	mother      
Leba nese       
landed       at 
d
)
 
d
e
s
c
r
i
p
t
i
v
e
S



NP(NN)




PP(IN)-
subroot
N
P
 
-
E
2
-
O
R
G



NP 	VP

NP -E1-PER 	NP -E2-PER 	VP 	…
NNP 	CC 	NNP 	VBD



NP 

NP -E1-PER 		NP -E2-PER 		VP NNP 
	CC	NNP 	VBD



VP

…
VBN    …


IN 	NNP
of 	Microsoft
c) context -sensitive
Figure 2: Examples of context-
free and context-sensitive sub-


John 	and 	Mary 	got


married


John 	and 	Mary 	got


married


trees related with Figure 
1(b).


e) predicate-linked: SPT and the dynamic context-sensitive tree span
Figure 1: Different tree span categories with SPT (dotted circle) and an ex- 
ample of the dynamic context-sensitive tree span (solid circle)


Note: the bold node is the root for a sub-tree.


  A problem with our algorithm is how to deter- 
mine whether an entity pair belongs to the “predi- 
cate -linked”  category.   In  this   paper,   a  simple 
method is applied by regarding the “predicate- 
linked” category  as  the  default  category. That is,


those entity pairs, which do not belong to the four 
well defined and easily detected categories (i.e. 
embedded, PP-liked, semi-structured and descrip- 
tive), are classified into the “predicate -linked”cate- 
gory.


Since  “predicate -linked” instances  only  occupy
~20%  of cases,  this explains  why SPT performs 
better  than  the  Context-Sensitive   Path-enclosed
Tree (CSPT) as described in Zhang et al (2006): 
consistently   adopting   CSPT  may  introduce   too 
much  noise/unnecessary   information   in  the  tree 
kernel.

3.2 Context-Sensitive Convolution Tree Kernel



the  tree  kernel  proposed  in  Culota  and  Sorensen 
(2004) which is c ontext-sensitive, that is, it considers 
the path from the tree root node to the sub-tree root 
node. In order to integrate the advantages of both tree 
kernels  and  resolve   the  problem   in  Collins   and 
Duffy’s convolution tree kernel, this paper proposes a 
context-sensitive convolution tree kernel. It works by 
taking ancestral information (i.e. the root node path) 
of sub-trees into consideration:
m


Given  any  tree  span,  e.g.  the  dynamic  context-


K  (T [1], T [2])


(n i [1], n i [2])


(3)


sensitive  tree  span  in  the  last  subsection,  we  now 	C


1 	1
i  1 ni [1]  N i [1], ni [2]  N i [ 2]


study  how  to  measure  the  similarity  between  two



Where


1 	1 	1 	1


trees, using a convolution tree kernel.A convolution
kernel (Haussler D., 1999 ) aims to capture structured 
information in terms of substructures . As a special- 
ized convolution kernel, Collins and Duffy’s convolu-



N i [ j] is the set of root node paths with length i in 
tree T[j] while the maximal length of a root node 
path is defined by m.
i


tion  tree  kernel


KC (T1,T2 )


(‘C’ for  
convolution)


n1 [ j]


(n1 n2 ...ni )[ j] is 
a root node path 
with


counts   the   number   of   common   sub-trees   (sub- 
structures)  as  the  syntactic  structure  similarity  be-
tween two parse trees T1 and  T2  (Collins and Duffy


length i in tree T[j] , which takes into account the
i-1 ancestr al nodes n i  [j] of n [j] in T[j].  Here,
2 	1


2001):


n k  1[ j]


is the 
parent 
of


n k [ j 
]and 
n1[ j] is 
the


K C (T1, T2 )




n1   N1 ,n2


(n1 , n2 )
N 2


(1)


root node of 
a context-free 
sub-tree in 
T[j]. For
better 
differentiation
,  the label of  
each ancestral
ni


where Nj  is the set of nodes in tree Tj  , and


(n1 ,n2 )


node in


1 [j] is augmented with the POS tag of


evaluates the common sub-trees rooted at n1 and n2 2


its head word.


i 	i


and is computed recursive ly as follows :


(n1 [1], n1 [2])


measures the common context-


1)   If   the  context -free   productio ns   (Context -Free


sensitive   sub-trees  rooted  at  root  node  paths


Grammar(CFG) rules) at n1 and n2


are different,


n1 [1]


and n1 [2] . In our tree kernel, a sub-
tree


(n1, n2 )   0 ; Otherwise go to 2.


becomes context-sensitive with its 
dependence on
the root node path instead of the 
root node itself.


2) If  both n1  and n2
Otherwise go to 3.


are POS tags,


(n1, n2)   1  A ;



Figure  2  shows  a  
few  examples  of  
context - sensitive  
sub-trees with 
comparison to 
context -


3)  Calculate


(n1 ,n2 ) 
recursively as:
#ch( n1 )


free sub-
trees.
Similar to 
Collins and 
Duffy (2001),   
our tree ker-


(n1 , n2 ) 	A


(1 	(ch(n1 , k 
),ch(n2 , k ))


(2)


nel computes


(  1 [1],


1 [2])


n i	n i


recursively as follows:


k  1
where # ch(n) is the number of children of node n ,
ch(n, k ) is the k th child of node n and A (0< A <1) is


1)   If  the  context -sensitive  productions   (Context- 
Sensitive  Grammar (CSG)  rules with  root  node
paths as their left hand sides) rooted at n i [1] and


the decay factor in order to make the kernel value less


ni	are different,   return


(ni [1], ni [2]) =0;


variable with respect to different sub-tree sizes.
This convolution tree kernel has been successfully


1[2]	1	1
Otherwise go to Step 2.


applied by Zhang et al (2006) in relation extraction.


2)   If 	both


n1 [1]


and


n1[2]


are    POS    tags,


However, there is one problem with  this tree kernel:


(ni [1], ni [2])


A ; Otherwise go to Step 3.


the sub-trees involved in the tree ker nel computation 	1	1
are  context-free  (That  is,  they  do  not  consider  the


information outside the sub-trees). This is contrast to



3 That is, each root node path



1  encodes the identity


2 That is, each node n encodes the identity of a sub- 
tree rooted at n and,  if there are two nodes in the 
tree with the same label, the summation will go over 
both of them.


of a context-sensitive sub-tree rooted at n i  and, if 
there are two root node paths in the tree with the 
same label sequence, the summation will go over 
both of them.



i 	i 	major relation types and  24 relation subtypes. All the


3)   Calculate


(n1 [1], n1 [2]) 
recursively as:



reported 
performances 
in this paper 
on the ACE 
RDC


(n1 [1], n1 [2])
#ch(n1 [1])
A	(1




(4)
(ch(n1 [1], k ), 
ch(n1[2], k ))


2003 corpus are 
evaluated on the 
test data. The 2004 
corpus  contains  
451  documents  
and  5702   positive
relation instances. It 
redefines  7 entity 
types, 7 major


i	i

k  1
i 	th


relation types and 23 relation subtypes. 
For compari-
son, we use the same setting as Zhang et 
al (2006) by


where ch(n1 [ j], k ])


is the  k


context-sensitive



apply ing a  5-fold cross-
validation on a subset of 
the



i 	i


n1[ j] with # ch(n1[ j]) the number of the con-
text-sensitive  children.  Here,  A (0< A <1) is  the 
decay factor in order to make the kernel value 
less variable with respect to different sizes of the 
context-sensitive sub-trees.
It is worth comparing our tree kernel with previous
tree kernels. Obviously, our tree kernel is an exten- 
sion of Collins and Duffy’s convolution tree kernel, 
which is a special case of our tree kernel  (if m=1 in 
Equation (3)). Our tree kernel not  only counts the 
occurrence of each context-free sub-tree, which does 
not consider its ances tors, but also counts the occur- 
rence of each context-sensitive sub-tree, which con- 
siders its ancestors. As a result, our tree kernel  is not 
limited by the constraints in previous tree kernels (as 
discussed  in Sec tion  2), such as  Collins  and Duffy


tion instances. That is, all the  reported performances 
in this paper on the ACE RDC 2004 corpus are evalu-
ated using 5-fold cross validation on the entire corpus .
  Both  corpo ra  are  parsed  using  Charniak’s parser 
(Charniak, 2001) with the boundaries of all the entity 
mentions  kept 4 . We  iterate  over  all  pairs  of entity
mentions occurring in the same sentence to generate 
potential relation  instances5 . In our experimentation, 
SVM (SVMLight, Joachims(1998))  is selected as our 
cla ssifier. For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to separate 
one class from all others. The training parameters are 
chosen using cross-validation on the ACE RDC 2003 
training  data.  In particular, A in our  tree  kernel is 
fine-tuned to 0.5. This suggests  that about 50% dis- 
count  is  done  as our  tree  kernel  moves  down  one


(2001),  Zhang  et  al  (2006),   Culotta  and  Sorensen


level in computing


(  1 [1],


1 [2])


(2004) and  Bunescu  and  Mooney  (2005a).  Finally,
let’s study the computational issue with our tree ker- 
nel.   Although   our  tree  kernel  takes  the  context- 
sensitive sub-trees into consideration, it only slightly 
increases the computational burden, compared with 
Collins and Duffy’s  convolution tree kernel. This is


n i 	n i 	.

4.2 Experimental Results

First, we systematically evaluate the context-sensitive 
convolution  tree  kernel  and  the  dynamic  context - 
sensitive tree span proposed in this paper.
Then, we evaluate the complementary nature be-


due to that


(n1[1], n1[2])


0 holds for 
the major-


tween our 
tree kernel 
and a state-
of-the-art 
linear ker-


ity of  context-free sub-tree pairs (Collins and Duffy
2001) and that computation for context-sensitive sub-
tree 	pairs 	is 	necessary 	only 	when


nel   via   a  composite   kernel.   Generally   different 
feature-based methods and tree kernel-based methods 
have their own merits. It is usually easy to build a


(n1[1], n1[2])


0 and  the  
context-sensitive  
sub-


system using a 
feature-based 
method and 
achieve the


tree   pairs   have   the   same   root   node   path(i.e.


state-of-the-art performance, while tree kernel-based


n i [1]


n i [2] 
in 
Equatio
n (3)).


method
s  hold 
the 
potenti
al for 
further  
perfor
mance
improv
ement. 
Theref
ore, it 
is 
always 
a good 
idea to



4 	Experimentation


integrate them via a 
composite kernel.



This  paper  uses  the  ACE  RDC  2003  and  2004  cor-	 	


pora provided by LDC in all our experiments.

4.1 Experimental Setting

The ACE  RDC corpora are gathered from various 
newspapers, newswire and broadcasts. In the 2003 
corpus , the training set consists of 674 documents and
9683 positive relation instances w hile the test set con-
sists of 97 documents and 1386 positive relation in- 
stances. The 2003 corpus  defines  5 entity types,  5


4  This can be done by first representing all entity me n- 
tions with their head words and then restoring all the 
entity mentions after parsing. Moreover, please note 
that the final performance of relation extraction may 
change much with different range of parsing errors. 
We will study this issue in the near future.
5 In this paper, we only measure the performance of rela-
tion extraction on “true” mentions wit h “true”chain-
ing   of   co-reference   (i.e.   as   annotated   by   LDC
annotators ). Moreover, we only model explicit relations and 
explicitly model the argument order of the two mentions in- 
volved.



   Finally, we compare our system with the state-of- 
the-art systems in the literature.

Context-Sensitive Convolution Tree Kernel

In this paper, the m parameter of our context-sensitive 
convolution tree kernel as shown in Equation (3) 
indicates the maximal length of root node paths and is 
optimized  to  3  using  5-fold  cross  validation  on  the 
ACE RDC 2003 training data. Table 1 compares the 
impact of different m in context -sensitive convolution 
tree  kernels  using  the  Shortest  Path-enclosed  Tree 
(SPT)  (as  described  in  Zhang  et  al  (2006))  on  the 
major relation types of the ACE RDC 2003 and 2004 
corpora, in details. It also shows that our tree kernel 
achieves best performance on the test data using SPT 
with m = 3, which outperforms the one with m = 1 by
~2.3  in  F-measure.  This  suggests  the  parent  and
grandparent nodes of a sub-tree   contains much 
information for relation extraction while considering 
more ancestral nodes may not help. This may be due



m 	P(%) 	R(%) 	F
1 	72.3(72.7) 	56.6(53.8) 	63.5(61.8)
2 	74.9(75.2) 	57.9(54.7) 	65.3(63.5)
3 	75.7(76.1) 	58.3(55.1) 	65.9(64.0)
4 	76.0(75.9) 	58.3(55.3) 	66.0(63.9)
a) without entity-related information







b) with entity-related information
Table 1: Evaluation of context-sensitive 
convolution
tree kernels using SPT on the major relation types 
of the ACE RDC 2003 (inside the parentheses) and 
2004 (outside the parentheses) corpora.

Tree Span 	P(%) 	R(%) 
	F


to   that,   although   our   experimentation    on   the
training data indicates that   more than 80% (on 
average)  of subtrees  has a root node path longer 
than 3 (since most of the subtrees are deep from the


Shortest Path -
enclosed Tree Dynamic Context- Sensitive Tee


79.6
(79.4)
 81.1 (80.1)


65.6
(62.5)
 66.7 (63.8)


71.9
(69.9)
 73.2 (71.0)


root node and more than 90% of the parsed trees in
the   training   data   are   deeper   than   6   levels), 
including a root node path longer than 3 may be 
vulnerable   to  the  full  parsing   errors  and  have 
negative impact. Table 1 also evaluates the impact of
entity-related   information   in   our   tree   kernel   by 
attaching entity type information (e.g.  “PER”in the 
entity node 1 of Figure 1(b)) into both entity nodes.
It shows that such information can significantly 
improve the performance by ~6.0 in F-measure. In all 
the  following  experiments,  we  will  apply  our  tree 
kernel with m=3 and entity-related information by 
default.
Table  2  compares  the  dynamic  context-sensitive
tree span with SPT using our tree kernel. It shows that 
the   dynamic   tree   span   can   futher   improve   the


Table  2:  Comparison  of  dynamic  context-
sensitive tree span with SPT using our context-
sensitive convolution tree kernel on the major 
relation types of the ACE RDC 2003 (inside the 
parentheses) and 2004 (outside the parentheses) 
corpora. 18% of positive instances in the ACE RDC 
2003 test data belong to the predicate-linked 
category.



Composite 
Kernel

In this paper,  a composite kernel via polynomial in- 
terpolation,  as  described  Zhang  et  al  (2006),  is  ap- 
plied   to   integrate   the   proposed  c ontext-
sensitive convolution tree kernel with a state -of-
the-art linear kernel (Zhou et al 2005) 7 :
P


per formance by ~1.2 in F-measure6 . This suggests the


K1 ( , )


a  K L ( , )


(1   a )


K C ( , )


(5)


usefulness of extending the tree span beyond SPT for
the  “predicate-linked” tree  span  category.  In  the


Here,


K L ( , )


and


K C ( , )


indicates the normal-


future  work,  we will further  explore  expanding  the


ized  linear  kernel  and  context -sensitive convolution


dynamic tree span beyond SPT for the remaining tree


tree  kernel  respectively  while


K p ( , )  is the poly-


span categories.


nomial  expansion  
of


K ( , ) with  
degree  d=2,  i.e.







6 Significance test shows that the dynamic tree span per- 
forms s tatistically significantly better than SPT with p- 
values smaller than 0.05.



K p ( , )   ( K( , )   1)2  and a is the coefficient (a 
is set to 0.3 using cross -validation).

7 Here, we use the same set of flat features (i.e. word, 
entity type, mention level, overlap, base phrase 
chunk- ing, dependency tree, parse tree and semantic 
informa- tion) as Zhou et al (20 05).



Table 	3   evaluates 	the 	performance 	of 	the



ACE RDC 2004 	P(%) 	R(%) 	F


composite kernel. It shows that the composite kernel
much further improves the performance beyond that
of either  the state-of-the-art linear  kernel or our tree 
kernel and achieves the F-measures of 74.1 and 75.8 
on the major relation types of the ACE RDC 2003 
and 2004 corpora respectively. This suggests that our 
tree  kernel  and  the  state-of-the-art linear  kernel  are 
quite complementary, and that our composite kernel 
can   effectively   integrate   both   flat  and  structured
f  eatures.


Ours:
composite kernel
Zhang et al (2006): composite kernel Zhao et al 
(2005):8 composite ke rnel
Ours: context-sensitive convolution tree kernel Zhang et 
al (2006): convolution tree kernel


82.2 (70.3)
76.1 (68.6)
69.2 (-)
81.1 (68.8)
72.5 (-)


70.2 (62.2 )
68.4 (59.3)
70.5 (-)
66.7 (60.3 )
56.7 (-)


75.8 (66.0)
72.1 (63.6)
70.4 (-)
73.2 (64.3)
63.6 (-)


  System 	P(%) 	R(%) 	F


Table  5:  Comparison  of  difference  systems  on  
the


Linear Kernel 		78.2 
(77.2)


63.4
(60.7)


70.1
(68.0)


ACE RDC 2004 corpus over both 7 
types (outside the
parentheses ) and 23 subtypes ( 
inside the parentheses)


Context-Sensitive  Con- 
volution Tree Kernel
 

81.1 (80.1)
 

66.7 (63.8)
 

73.2 (71.0)



Finally,  Tables  4  and  5  
comp are our system 
with other state-of-the-art 
systems9 on the ACE 
RDC 2003


Composite Kernel 	82.2
(80.8)


70.2
(68.4)


75.8
(74.1)


and 2004 corpora, respectively. 
They show that our tree  kernel-
based  system outperforms 
previous tree


Table  3:  Performance  of  the  compos ite  kernel  via
polynomial interpolation on the major relation types
of the ACE RDC 2003 (inside the parentheses) and
2004 (outside the parentheses) corpora


Comparison with Other Systems

ACE RDC 2003 	P(%) 	R(%) 	F


kernel-based systems. This is largely due to the con- 
text-sensitive nature of our tree kernel which resolves 
the limitations of the previous tree kernels. They also 
show that our tree kernel-based system outperforms 
the state-of-the-art feature-based system. This proves 
the great potential inherent in the parse tree structure 
for relation extraction and our tree kernel takes a big 
stride towards the right direction. Finally, they also


Ours:
composite kernel
Zhang et al (2006):
composite kernel
Ours: context -sensitive 
convolution tree ke rnel 
Zhang et al (2006): 
convolution tree ke rnel 
Bunescu et al (2005): 
shortest path 
dependency kernel 
Culotta et al (2004): 
dependency kernel 
Zhou et al. (2005): 
feature-based 
Kambhatla (2004): 
feature-based


80.8 (65.2)
77.3 (64.9)
80.1 (63.4)
76.1 (62.4)
65.5 (-)

67.1 (-)
77.2 (63.1)
- (63.5)


68.4 (54.9 )
65.6 (51.2)
63.8 (51.9 )
62.6 (48.5)
43.8 (-)

35.0 (-)
60.7 (49.5)
- (45.2)


74.1 (59.6)
70.9 (57.2)
71.0 (57.1)
68.7 (54.6)
52.5 (-)

45.8 (-)
68.0 (55.5)
- (52.8)


show that our composite 
kernel-based system 
outper-
forms other 
composite ke 
rnel-based 
systems.

5
     
C
o
n
c
l
u
s
i
o
n

Structured parse tree 
information holds great 
potential for relation 
extraction. This paper 
proposes a context- 
sensitive convolution 
tree kernel  to resolve 
two criti- cal problems 
in previous tree kernels 
for relation ex- traction 
by first automatically 
determining a dynamic 
context-sensitive tree 
span and then applying a 
con- text-sensitive 
convolution tree kernel. 
Moreover, this paper  
evaluates  the  
complementary  nature  
between our tree kernel 
and a state-of-the-art 
linear kernel. Evaluation 
on the ACE RDC 
corpora shows that our 
dynamic context-
sensitive tree span is 
much more suitable for 
relation extraction than  
the widely -used
Shortest Path-enclosed 
Tree  and our tree kernel 
out-


Table  4:  Comparison  of  difference  systems  on  the
ACE RDC 2003 corpus over both 5 types ( outside the
parentheses ) and 24 subtypes (inside the parentheses )


performs the state-of-the-art Collins and Duffy’s con- 
volution tree kernel.  It also shows that feature-based

8 There might be some typing errors for the performance 
reported in Zhao and Grishman(2005) since P, R and F 
do not match.
9  All the state-of-the -art systems apply the entity-related
information. It is not supervising: our experiments 
show that using the entity-related information gives a 
large performance improvement.



and tree kernel-based methods well complement each 
other and the composite kernel can effectively inte- 
grate both flat and structured features.
  To our knowledge, this is the first research to dem- 
onstrate  that, without  extensive  feature  enginee r ing, 
an individual tree kernel can achieve much better per- 
formance than the state-of-the-art linear kernel in re- 
lation extraction. This shows  the great potential of 
structured parse tree  information for relation extrac- 
tion and our tree kernel takes a big stride towards the 
right direc tion.
  For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by ex- 
ploring more useful context information. Moreover, 
we will explore more entity-related information in the 
parse tree. Our preliminary work of including the en- 
tity type information significantly improves the per- 
formance. Finally , we will study how to  resolve the 
data imbalance and sparseness issues from the  learn- 
ing algorithm view point.

Acknowledgement

This research is supported by Project 60673041 under 
the  National  Natural  Science  Foundation  of  China 
and Project 2006AA01Z147 under the “863”National 
High-Tech Research and Develo pment of China. We 
would also like to thank the critical and insightful 
comments from the four anonymous reviewers.


References

ACE.   (20 00-2005).   Automatic   Content   Extraction. 
http://www.ldc.upenn.edu/Projects/ACE/
Bunescu R. & Mooney R.J. (2005a). A shortest path 
dependency	kernel     for     relation     extraction. 
HLT/EMNLP’2005 :  724- 731.  6- 8  Oct  2005.  Va n- 
cover, B.C.
Bunescu R. & Mooney R.J. (2005b). Subsequence Ke r- 
nels for Relation Extraction  NIPS’2005. Va ncouver, 
BC, December 2005
Charniak E.  (2001). Immediate-head Parsing for La n- 
guage Models. ACL’2001: 129- 137. Toulouse, France
Collins M. and Duffy N.  ( 2001) . Convolution Ke rnels 
for  Natural  Language.  NIPS’2001:   625- 632.  Ca m- 
bridge, MA
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction.  ACL’2004 . 423- 429.
21-26 July 2004. Ba rcelona, Spain .
Haussler D.  (1999).  Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Un i- 
versity of California, Santa Cruz



Joachims T.  ( 1998). Text Categorization with Su pport 
Vector  Machine:  learning  with  many  relevant  fe a- 
tures. ECML-1998 : 137-142. Chemnitz, Ge r many
Kambhatla N. (2004). Combining lexical, syntactic and 
semantic features with Maximum Entropy models for 
extracting relations. ACL’2004(Poster). 178 -181. 21-
26 July 2004. Ba rcelona, Spain.
MUC.  (1987-1998).  The  NIST  MUC  website:   http:
//www.itl.nist.gov/iaui/894.02/related_projects/muc/
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083- 1106.
Zhang M ., Zhang J.,  Su  J. and  Zhou  G. D.  ( 2006). A 
Composite Kernel to Extract Relations between Enti- 
ties with both Flat and Structured Features . COLING- 
ACL-2006: 825- 832. Sydney, Australia
Zhao S.B. and Gris hman R. (2005 ). Extracting relations 
with integrated information using kernel methods. 
ACL’2005: 419-426. Univ of Michigan-Ann Arbor,
USA , 25-30 June 2005.
Zhou G.D., Su J. Zhang J. and Zhang M.  (2005). Ex- 
plo ring  various  knowledge  in  relation  extraction. 
ACL’2005. 427-434. 25- 30 June, Ann Arbor, Mic h- 
gan, USA.
Zhou G.D., Su J. and Zhang M. (2006). Modeling co m- 
monality among related classes in relation extraction, 
COLING- ACL’2006: 121- 128. Sy dney, Australia.

