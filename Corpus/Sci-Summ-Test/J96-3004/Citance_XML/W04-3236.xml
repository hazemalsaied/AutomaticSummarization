<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence.</S>
		<S sid ="2" ssid = "2">However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite.</S>
		<S sid ="3" ssid = "3">We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at- once approach).</S>
		<S sid ="4" ssid = "4">Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based).</S>
		<S sid ="5" ssid = "5">This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.</S>
		<S sid ="6" ssid = "6">We found that while the all-at-once, character- based approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run.</S>
		<S sid ="7" ssid = "7">As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Most corpus-based language processing research has focused on the English language.</S>
			<S sid ="9" ssid = "9">Theoretically, we should be able to just port corpus-based, machine learning techniques across different languages since the techniques are largely language independent.</S>
			<S sid ="10" ssid = "10">However, in practice, the special characteristics of different languages introduce complications.</S>
			<S sid ="11" ssid = "11">For Chinese in particular, words are not demarcated in a Chinese sentence.</S>
			<S sid ="12" ssid = "12">As such, we need to perform word segmentation before we can proceed with other tasks such as part-of-speech (POS) tagging and parsing, since one POS tag is assigned to each Chinese word (i.e., all characters in a Chinese word have the same POS tag), and the leaves of a parse tree for a Chinese sentence are words.</S>
			<S sid ="13" ssid = "13">To build a Chinese POS tagger, the following questions naturally arise: (1) Should we perform Chinese POS tagging strictly after word segmentation in two separate phases (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-at-once approach)?(2) Should we assign POS tags on a word-by word basis (like in English), making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based)?</S>
			<S sid ="14" ssid = "14">This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework.</S>
			<S sid ="15" ssid = "15">We analyze the performance of the different approaches in our attempt to find the best approach.</S>
			<S sid ="16" ssid = "16">To our knowledge, our work is the first to systematically investigate such issues in Chinese POS tagging.</S>
	</SECTION>
	<SECTION title="Word Segmentation. " number = "2">
			<S sid ="17" ssid = "1">As a first step in our investigation, we built a Chinese word segmenter capable of performing word segmentation without using POS tag information.</S>
			<S sid ="18" ssid = "2">Since errors in word segmentation will propagate to the subsequent POS tagging phase in the one-at-a-time approach, in order for our study to give relevant findings, it is important that the word segmenter we use gives state-of- the-art accuracy.</S>
			<S sid ="19" ssid = "3">C-2 C −1 =新华 C-1C0 =华社 C0 C1 =社记 The word segmenter we built is similar to the maximum entropy word segmenter of (Xue and Shen, 2003).</S>
			<S sid ="20" ssid = "4">Our word segmenter uses a maximum entropy framework and is trained on manually segmented sentences.</S>
			<S sid ="21" ssid = "5">It classifies each Chinese character given the features derived from its surrounding context.</S>
			<S sid ="22" ssid = "6">Each character can be assigned one of 4 possible boundary tags: “b” for a character that begins a word and is followed by another character, “m” for a character that occurs in the middle of a word, “e” for a character that ends a word, and “s” for a character that occurs as a single-character word.</S>
			<S sid ="23" ssid = "7">2.1 Word Segmenter Features.</S>
			<S sid ="24" ssid = "8">Besides implementing a subset of the features described in (Xue and Shen, 2003), we also came up with three additional types of features ((d) − (f) below) which improved the accuracy of word segmentation.</S>
			<S sid ="25" ssid = "9">The default feature, boundary tag feature of the previous character, and boundary tag feature of the character two before the current character used in (Xue and Shen, 2003) were dropped from our word segmenter, as they did not improve word segmentation accuracy in our experiments.</S>
			<S sid ="26" ssid = "10">In the following feature templates used in our word segmenter, C refers to a Chinese character while W refers to a Chinese word.</S>
			<S sid ="27" ssid = "11">Templates (a) − (c) refer to a context of five characters (the current character and two characters to its left C1C2 =记者 to be set to 1.</S>
			<S sid ="28" ssid = "12">2.2 Our Additional Features.</S>
			<S sid ="29" ssid = "13">W0 C 0 : This feature captures the word context in which the current character is found.</S>
			<S sid ="30" ssid = "14">For example, the character “社” within the word “新华社” will have the feature W0 C0 =新华社_社 set to 1.</S>
			<S sid ="31" ssid = "15">This feature helps in recognizing seen words.</S>
			<S sid ="32" ssid = "16">Pu( C 0 ) : A punctuation symbol is usually a good indication of a word boundary.</S>
			<S sid ="33" ssid = "17">This feature checks whether the current character is a punctuation symbol (such as “。”, “-”, “，”).</S>
			<S sid ="34" ssid = "18">T ( C − 2 )T ( C − 1 )T ( C 0 )T ( C 1 )T ( C 2 ) : This feature is especially helpful in predicting the word segmentation of dates and numbers, whose exact characters may not have been seen in the training text.</S>
			<S sid ="35" ssid = "19">Four type classes are defined: numbers represent class 1, dates (“日”, “月”, “年”, the Chinese character for “day”, “month”, “year”, respectively) represent class 2, English letters represent class 3, and other characters represent class 4.</S>
			<S sid ="36" ssid = "20">For example, when considering the character “年” in the character sequence “九〇年代R”, the feature and right).</S>
			<S sid ="37" ssid = "21">C 0 denotes the current character, Cn T (C−2 )KT (C2 ) =11243 will be set to 1 ( “九” ( C−n ) denotes the character n positions to the right (left) of the current character.</S>
			<S sid ="38" ssid = "22">(a) Cn (n = −2,−1,0,1,2 ) and “〇” are the Chinese characters for “9” and “0” respectively).</S>
			<S sid ="39" ssid = "23">2.3 Testing.</S>
			<S sid ="40" ssid = "24">During testing, the probability of a boundary tag (b) Cn Cn +1 ( n = −2,−1,0,1 ) (c) C −1C1 sequence assignment t1...tn given a character (d) W0 C0 sequence c1 ...cn is determine d by using the (e) Pu( C0 ) (f) T ( C −2 )T ( C −1 )T ( C0 )T ( C1 )T ( C 2 ) For example, given the character sequence “新华社 记者”, when considering the character “社”, template (a) results in the following features C-2 =新 C-1 =华 C0 =社 C1 =记 C 2 =者 to be set to 1, template (b) results in the features maximum entropy classifier to compute the probability that a boundary tag ti is assigned to each individual character ci.</S>
			<S sid ="41" ssid = "25">If we were to just assign each character the boundary tag with the highest probability, it is possible that the classifier produces a sequence of invalid tags (e.g., “m” followed by “s”).</S>
			<S sid ="42" ssid = "26">To eliminate such possibilities, we implemented a dynamic programming algorithm which considers only valid boundary tag sequences given an input character sequence.</S>
			<S sid ="43" ssid = "27">At each character position i, the algorithm considers each last word candidate ending at position i and consisting of K characters in length (K = 1, …, 20 in our experiments).</S>
			<S sid ="44" ssid = "28">To determine the boundary tag assignment to the last word W with K characters, the first character of W is assigned boundary tag “b”, the last character of W is assigned tag “e”, and the intervening characters are assigned tag “m”.</S>
			<S sid ="45" ssid = "29">(If W is a single-character word, then the single character is assigned “s”.)</S>
			<S sid ="46" ssid = "30">In this way, the dynamic programming algorithm only considers valid tag sequences, and we are also able to make use of the W0 C0 feature during testing.</S>
			<S sid ="47" ssid = "31">After word segmentation is done by the maximum entropy classifier, a post-processing step is applied to correct inconsistently segmented words made up of 3 or more characters.</S>
			<S sid ="48" ssid = "32">A word W is defined to be inconsistently segmented if the concatenation of 2 to 6 consecutive words elsewhere in the segmented output document matches W. In the post-processing step, the segmentation of the characters of these consecutive words is changed so that they are segmented as a single word.</S>
			<S sid ="49" ssid = "33">To illustrate, if the concatenation of 2 consecutive words “巴赛 罗纳” in the segmented output document matches another word “巴赛罗纳”, then “巴赛 罗纳” will be re-segmented as “巴赛罗纳”.</S>
			<S sid ="50" ssid = "34">2.4 Word Segmenter Experimental Results.</S>
			<S sid ="51" ssid = "35">To evaluate the accuracy of our word segmenter, we carried out 10-fold cross validation (CV) on the 250K-word Penn Chinese Treebank (CTB) (Xia et al., 2000) version 3.0.</S>
			<S sid ="52" ssid = "36">The Java opennlp maximum entropy package from sourceforge1 was used in our implementation, and training was done with a feature cutoff of 2 and 100 iterations.</S>
			<S sid ="53" ssid = "37">The accuracy of word segmentation is measured by recall (R), precision (P), and F- measure ( 2RP /(R + P) ).</S>
			<S sid ="54" ssid = "38">Recall is the proportion of correctly segmented words in the gold-standard segmentation, and precision is the proportion of correctly segmented words in word segmenter’s output.</S>
			<S sid ="55" ssid = "39">Figure 1 gives the word segmentation F- measure of our word segmenter based on 10-fold CV on the 250K-word CTB.</S>
			<S sid ="56" ssid = "40">Our word segmenter achieves an average F-measure of 95.1%.</S>
			<S sid ="57" ssid = "41">This accuracy compares favorably with 1 http://maxent.sourceforge.net (Luo, 2003), which reported 94.6% word segmentation F-measure using his full parser without additional lexical features, and about 94.9%2 word segmentation F-measure using only word boundaries information, no POS tags or constituent labels, but with lexical features derived from a 58K-entry word list.</S>
			<S sid ="58" ssid = "42">The average training time taken to train on 90% of the 250K-word CTB was 12 minutes, while testing on 10% of CTB took about 1 minute.</S>
			<S sid ="59" ssid = "43">The running times reported in this paper were all obtained on an Intel Xeon 2.4GHz computer with 2GB RAM.</S>
			<S sid ="60" ssid = "44">97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 1 2 3 4 5 6 7 8 9 10 Expe rim e nt Num be r Figure 1: CTB 10-fold CV word segmentation F- measure for our word segmenter As further evaluation, we tested our word segmenter on all the 4 test corpora (CTB, Academia Sinica (AS), Hong Kong CityU (HK), and Peking University (PK)) of the closed track of the 2003 ACLSIGHAN-sponsored First International Chinese Word Segmentation Bakeoff (Sproat and Emerson, 2003).</S>
			<S sid ="61" ssid = "45">For each of the 4 corpora, we trained our word segmenter on only the official released training data of that corpus.</S>
			<S sid ="62" ssid = "46">Training was conducted with feature cutoff of 2 and 100 iterations (these parameters were obtained by cross validation on the training set), except for the AS corpus where we used cutoff 3 since the AS training corpus was too big to train with cutoff 2.</S>
			<S sid ="63" ssid = "47">Figure 2 shows our word segmenter’s F- measure (based on the official word segmentation scorer of 2003 SIGHAN bakeoff) compared to those reported by all the 2003 SIGHAN participants in the four closed tracks (ASc, HKc, PKc, CTBc).</S>
			<S sid ="64" ssid = "48">Our word segmenter achieved higher F-measure than the best reported F-measure in the SIGHAN bakeoff on the ASc, HKc, and PKc corpus.</S>
			<S sid ="65" ssid = "49">For CTBc, due to the 2 Based on visual inspection of Figure 3 of (Luo,.</S>
			<S sid ="66" ssid = "50">2003) exceptionally high out-of-vocabulary (OOV) rate of the test data (18.1%), our word segmenter’s F- measure ranked in the third position.</S>
			<S sid ="67" ssid = "51">(Note that the top participant of CTBc (Zhang et al., 2003) used additional named entity knowledge/data in their word segmenter).</S>
			<S sid ="68" ssid = "52">Sighan Paticipants Our Word Segmenter official released CTB training data of SIGHAN, our word segmenter achieved an F-measure of 92.2%, higher than the best reported F-measure in the CTB open task.</S>
			<S sid ="69" ssid = "53">With sufficient training data, our word segmenter can perform very well.</S>
			<S sid ="70" ssid = "54">In our evaluation, we also found that the additional features we introduced in Section 2.2 and the post-processing step consistently improved average word segmentation F-measure, 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 82 A SC HKC PKC CTB C CTB o when evaluated on the 4 SIGHAN test corpora in the closed track.</S>
			<S sid ="71" ssid = "55">The additional features improved F measure by an average of about 0.4%, and the post-processing step added on top of the use of all features further improved F measure by 0.3% (i.e., for a cumulative total of 0 . 7 % i n c r e a s e i n F m e a s u r e ) .</S>
	</SECTION>
	<SECTION title="One-at-a-. " number = "3">
			<S sid ="72" ssid = "1">Time, Word Based POS Tagger Now that we have successfully built a state-of- the-art Chinese word segmenter, we are ready to explore issues of processing architecture and feature representation for Chinese POS tagging.</S>
			<S sid ="73" ssid = "2">An English POS tagger based on maximum entropy modeling was built by (Ratnaparkhi, 1996).</S>
			<S sid ="74" ssid = "3">As a first attempt, we investigated whether simply porting the method used by (Ratnaparkhi, 1996) for English POS tagging would work equally well for Chinese.</S>
			<S sid ="75" ssid = "4">Applying it in the context of Chinese POS tagging, Ratnaparkhi’s method assumes that words are pre-segmented, and it assigns POS tags on a word-by-word basis, making use of word features in the surrounding context.</S>
			<S sid ="76" ssid = "5">This gives rise to a one-at-a-time, word-based POS tagger.</S>
			<S sid ="77" ssid = "6">Note that in a one-at-a-time approach, the word-segmented input sentence given to the POS tagger may contain word segmentation errors, Figure 2: Comparison of word segmentation F- measure for SIGHAN bakeoff3 tasks We also compared the F-measure of our word segmenter on CTBO, the open category of the which can lower the POS tagging accuracy.</S>
			<S sid ="78" ssid = "7">3.1 Features.</S>
			<S sid ="79" ssid = "8">The following feature templates were chosen.</S>
			<S sid ="80" ssid = "9">W refers to a word while POS refers to the POS CTB corpus, where participants were free to use tag assigned.</S>
			<S sid ="81" ssid = "10">The feature Pu( W0 ) checks if all any available resources and were not restricted to only the official released training data of CTB.</S>
			<S sid ="82" ssid = "11">On this CTBO task, we used as additional training data the AS training corpus provided by SIGHAN, after converting the AS training corpus to GB encoding.</S>
			<S sid ="83" ssid = "12">We found that with this additional AS training data added to the original 3 Last ranked participant of SIGHAN CTB (closed).</S>
			<S sid ="84" ssid = "13">with F-measure 73.2% is not shown in Figure 2 due to space constraint.</S>
			<S sid ="85" ssid = "14">characters in the current word are punctuation characters.</S>
			<S sid ="86" ssid = "15">Feature (e) encodes the class of characters that constitute the surrounding words (similar to feature (f) of the word segmenter in Section 2.1).</S>
			<S sid ="87" ssid = "16">Four type classes are defined: a word is of class 1 if it is a number; class 2 if the word is made up of only numeric characters followed by “日”, “月”，or “年”; class 3 when the word is made up of only English characters and optionally punctuation characters; class 4 otherwise.</S>
			<S sid ="88" ssid = "17">(a) W n ( n = −2 ,−1,0 ,1,2 ) (b) WnWn +1 ( n = −2,−1,0,1 ) (c) W−1W1 (d) Pu( W0 ) (e) T ( W−2 )T ( W−1 )T ( W0 )T ( W1 )T ( W2 ) (f) POS( W−1 ) (g) POS( W−2 )POS( W−1 ) 3.2 Testing.</S>
			<S sid ="89" ssid = "18">The testing procedure is similar to the beam search algorithm of (Ratnaparkhi, 1996), which tags each word one by one and maintains, as it sees a new word, the N most probable POS tag sequence candidates up to that point in the sentence.</S>
			<S sid ="90" ssid = "19">For our experiment, we have chosen N to be 3.</S>
			<S sid ="91" ssid = "20">3.3 Experimental Results.</S>
			<S sid ="92" ssid = "21">The 250K-word CTB corpus, tagged with 32 different POS tags (such as “NR”, “PU”, etc) was employed in our evaluation of POS taggers in this study.</S>
			<S sid ="93" ssid = "22">We ran 10-fold CV on the CTB corpus, using our word segmenter’s output for each of the 10 runs as the input sentences to the POS tagger.</S>
			<S sid ="94" ssid = "23">POS tagging accuracy is simply calculated as (number of characters assigned correct POS tag) / (total number of characters).</S>
			<S sid ="95" ssid = "24">89 88 87 86 85 84 83 we also conducted POS tagging using only the features (a), (f), and (g) in Section 3.1, similar to (Ratnaparkhi, 1996), and we obtained an average POS tagging accuracy of 83.1% for that set of features.</S>
			<S sid ="96" ssid = "25">The features that worked well for English POS tagging did not seem to apply to Chinese in the maximum entropy framework.</S>
			<S sid ="97" ssid = "26">Language differences between Chinese and English have no doubt made the direct porting of an English POS tagging method to Chinese ineffective.</S>
	</SECTION>
	<SECTION title="One-at-a-Time,	Character-Based 	POS. " number = "4">
			<S sid ="98" ssid = "1">Tagger Since one-at-a-time, word-based POS tagging did not yield good accuracy, we proceeded to investigate other combinations of processing architecture and feature representation.</S>
			<S sid ="99" ssid = "2">We observed that character features were successfully used to build our word segmenter and that of (Xue and Shen, 2003).</S>
			<S sid ="100" ssid = "3">Similarly, character features were used to build a maximum entropy Chinese parser by (Luo, 2003), where his parser could perform word segmentation, POS tagging, and parsing in an integrated, unified approach.</S>
			<S sid ="101" ssid = "4">We hypothesized that assigning POS tags on a character-by-character basis, making use of character features in the surrounding context may yield good accuracy.</S>
			<S sid ="102" ssid = "5">So we next investigate such a one-at-a-time, character-based POS tagger.</S>
			<S sid ="103" ssid = "6">4.1 Features.</S>
			<S sid ="104" ssid = "7">The features that were used for our word segmenter ((a) − (f)) in Section 2.1 were yet again applied, with two additional features (g) and (h) to aid POS tag prediction.</S>
			<S sid ="105" ssid = "8">82 (a) C 81 (n = −2,−1,0,1,2 ) 80 79 1 2 3 4 5 6 7 8 9 10 Expe rim e nt Num be r (b) Cn Cn +1 ( n = −2,−1,0,1 ) (c) C−1C1 (d) W0 C0 (e) Pu( C0 )Figure 3: POS tagging accuracy using one-at-a (f) T ( C−2 )T ( C−1 )T ( C0 )T ( C1 )T ( C2 ) time, word-based POS tagger (g) POS( C −1W0 ) The POS tagging accuracy is plotted in Figure 3.</S>
			<S sid ="106" ssid = "9">The average POS tagging accuracy achieved.</S>
			<S sid ="107" ssid = "10">(h) POS( C −2W )POS( C −1W ) for the 10 experiments was only 84.1%, far lower than the 96% achievable by English POS taggers POS( C − 1W ) : This feature refers to the on the English Penn Treebank tag set.</S>
			<S sid ="108" ssid = "11">The average training time was 25 minutes, while testing took about 20 seconds.</S>
			<S sid ="109" ssid = "12">As an experiment, POS tag of the previous character before the current word.</S>
			<S sid ="110" ssid = "13">For example, in the character sequence “对 此 意见”, when considering the character “见”, the feature POS( C −1W0 ) =PN is was found to be significantly better than the word-based approach, at the level of significance set to 1 (assuming “此” was tagged as PN).</S>
			<S sid ="111" ssid = "14">0.01.</S>
			<S sid ="112" ssid = "15">POS( C − 2W0 )POS( C − 1W0 ) : For the same Assuming a one-at-a-time processing example given above, when considering the character “见”, the feature architecture, Chinese POS tagging using a character-based approach gives higher accuracy compared to a word-based approach.</S>
			<S sid ="113" ssid = "16">POS( C −2W0 )POS( C −1W0 ) =P_PN is set to 1 (assuming “对” was tagged as P and “此” was tagged as PN).</S>
			<S sid ="114" ssid = "17">4.2 Testing.</S>
			<S sid ="115" ssid = "18">The testing algorithm is similar to that described in Section 3.2, except that the probability of a word being assigned a POS tag t is estimated by the product of the probability of its individual characters being assigned the same POS tag t. For example, when estimating the probability of “新华社” being tagged NR, we find the product of the probability of “新” being tagged NR, “华” being tagged NR, and “社” being tagged NR.</S>
			<S sid ="116" ssid = "19">That is, we enforce the constraint that all characters within a segmented word in the pre- segmented input sentence must have the same POS tag.</S>
			<S sid ="117" ssid = "20">4.3 Experimental Results.</S>
			<S sid ="118" ssid = "21">10-fold CV for CTB is repeated for this POS tagger.</S>
			<S sid ="119" ssid = "22">Figure 4 shows the detailed POS tagging accuracy.</S>
			<S sid ="120" ssid = "23">With a one-at-a-time, character-based POS tagger, the average POS tagging accuracy improved to 91.7%, 7.6% higher than that achieved by the one-at-a-time, word-based POS tagger.</S>
			<S sid ="121" ssid = "24">The average training timing was 55 minutes, while testing took about 50 seconds.</S>
			<S sid ="122" ssid = "25">95</S>
	</SECTION>
	<SECTION title="All-at-Once,       Character-Based       POS. " number = "5">
			<S sid ="123" ssid = "1">Tagger and Segmenter Encouraged by the success of character features, we next explored whether a change in processing architecture, from one-at-a-time to all-at-once, while still retaining the use of character features, could give further improvement to POS tagging accuracy.</S>
			<S sid ="124" ssid = "2">In this approach, both word segmentation and POS tagging will be performed in a combined, single step simultaneously.</S>
			<S sid ="125" ssid = "3">Each character is assigned both a boundary tag and a POS tag, for example “b_NN” (i.e., the first character in a word with POS tag NN).</S>
			<S sid ="126" ssid = "4">Thus, given 4 possible boundary tags and 32 unique POS tags present in the training corpus, each character can potentially be assigned one of (4 × 32) classes.</S>
			<S sid ="127" ssid = "5">5.1 Features.</S>
			<S sid ="128" ssid = "6">The features we used are identical to those employed in the character-based POS tagger described in section 4.1, except that features (g) and (h) are replaced with those listed below.</S>
			<S sid ="129" ssid = "7">In the following templates, B refers to the boundary tag assigned.</S>
			<S sid ="130" ssid = "8">For example, given the character sequence “对 此 意见”, when considering the character “见”, template (g) results in the feature B( C −1W )POS( C −1W ) =s_PN to be set to 1.</S>
			<S sid ="131" ssid = "9">(assuming “此” was tagged as PN).</S>
			<S sid ="132" ssid = "10">94 (g) B( C 93 92 (h) B( C −1W0 )POS( C )POS( C −1W0 ) )B( C )POS( C ) −2W0 91 −2W0 −1W0 −1W0 90 89 1 2 3 4 5 6 7 8 9 10 Expe rim e nt Num be r Figure 4: POS tagging accuracy using one-at-a- time, character-based POS tagger When a paired t-test was carried out to compare character-based and word-based one-at- a-time approaches, the character-based approach Note that this approach is essentially that used by (Luo, 2003), since his parser performs both word segmentation and POS tagging (as well as parsing) in one unified approach.</S>
			<S sid ="133" ssid = "11">The features we used are similar to his tag features, except that we did not use features with three consecutive characters, since we found that the use of these features did not improve accuracy.</S>
			<S sid ="134" ssid = "12">We also added additional features (d) − (f).</S>
			<S sid ="135" ssid = "13">5.2 Testing.</S>
			<S sid ="136" ssid = "14">Beam search algorithm is used with N = 3 during the testing phase.</S>
			<S sid ="137" ssid = "15">5.3 Experimental Results.</S>
			<S sid ="138" ssid = "16">10-fold CV on CTB was carried out again, using unsegmented test sentences as input to the program.</S>
			<S sid ="139" ssid = "17">Figure 5 shows the word segmentation F- measure, while Figure 6 shows the POS tagging accuracy achieved by this approach.</S>
			<S sid ="140" ssid = "18">With an all- at-once, character-based approach, an average word segmentation F-measure of 95.2% and an average POS tagging accuracy of 91.9% was achieved.</S>
			<S sid ="141" ssid = "19">The average training timing was 3 hours, while testing took about 20 minutes.</S>
			<S sid ="142" ssid = "20">There is a slight improvement in word segmentation and POS tagging accuracy using this approach, compared to the one-at-a-time, character-based approach.</S>
			<S sid ="143" ssid = "21">When a paired t-test was carried out at the level of significance 0.01, the all-at-once approach was found to be significantly better than the one-at-a-time approach for POS tagging accuracy, although the difference was insignificant for word segmentation.</S>
			<S sid ="144" ssid = "22">97.0 96.5 96.0 95.5 95.0 94.5 94.0 93.5 93.0 1 2 3 4 5 6 7 8 9 10 Expe rim e nt Num be r Figure 5: CTB 10-fold CV word segmentation F- measure using an all-at-once approach 95 94 93 92 91 90 89 1 2 3 4 5 6 7 8 9 10 Expe r im e nt Num be r Figure 6: CTB 10-fold CV POS tagging accuracy using an all-at-once approach However, the time required for training and testing is increased significantly for the all-at- once approach.</S>
			<S sid ="145" ssid = "23">When efficiency is a major consideration, or if high quality hand segmented text is available, the one-at-a-time, character- based approach could indeed be a worthwhile compromise, performing only slightly worse than the all-at-once approach.</S>
			<S sid ="146" ssid = "24">Table 1 summarizes the methods investigated in this paper.</S>
			<S sid ="147" ssid = "25">Total testing time includes both word segmentation and POS tagging on 10% of CTB data.</S>
			<S sid ="148" ssid = "26">Note that an all-at- once, word based approach is not applicable as word segmentation requires character features to determine the word boundaries.</S>
			<S sid ="149" ssid = "27">M e t h o d W or d Se g F me as ur e (% ) P O S Ac cu rac y (% ) T o t a l T e s ti n g T i m e O neata Ti m e W ord Ba se d 9 5 . 1 8 4 . 1 1 m i n 20 se cs O neata Ti m e Char Ba se d 9 5 . 1 9 1 . 7 1 m i n 50 se cs All At O nc e Char Ba se d 9 5 . 2 9 1 . 9 20 mi ns Table 1: Summary table on the various methods investigated for POS tagging 6 Discussions Word-based or character-based?</S>
			<S sid ="150" ssid = "28">The findings that a character-based approach is better than a word-based approach for Chinese POS tagging is not too surprising.</S>
			<S sid ="151" ssid = "29">Unlike in English where each English letter by itself does not possess any meaning, many Chinese characters have well defined meanings.</S>
			<S sid ="152" ssid = "30">For example, the single Chinese character “知” means “know”.</S>
			<S sid ="153" ssid = "31">And when a character appears as part of a word, the word derives part of its meaning from the component characters.</S>
			<S sid ="154" ssid = "32">For example, “知识” means “knowledge”， “无知” means “ignorant”, “知名” means “well-known”, etc. In addition, since the out-of-vocabulary (OOV) rate for Chinese words is much higher than the OOV rate for Chinese characters, in the presence of an unknown word, using the component characters in the word to help predict the correct POS tag is a good heuristic.One-at-a-time or all-at-once?</S>
			<S sid ="155" ssid = "33">The all-at once approach, which considers all aspects of available information in an integrated, unified framework, can make better informed decisions, but incurs a higher computational cost.</S>
			<S sid ="156" ssid = "34">7 Related Work.</S>
			<S sid ="157" ssid = "35">Much previous research on Chinese language processing focused on word segmentation (Sproat et al., 1996; Teahan et al., 2000; Sproat and Emerson, 2003).</S>
			<S sid ="158" ssid = "36">Relatively less work has been done on Chinese POS tagging.</S>
			<S sid ="159" ssid = "37">Kwong and Tsou (2003) discussed the implications of POS ambiguity in Chinese and the possible approaches to tackle this problem when tagging a corpus for NLP tasks.</S>
			<S sid ="160" ssid = "38">Zhou and Su (2003) investigated an approach to build a Chinese analyzer that integrated word segmentation, POS tagging and parsing, based on a hidden Markov model.</S>
			<S sid ="161" ssid = "39">Jing et al.</S>
			<S sid ="162" ssid = "40">(2003) focused on Chinese named entity recognition, considering issues like character-based versus word-based approaches.</S>
			<S sid ="163" ssid = "41">To our knowledge, our work is the first to systematically investigate issues of processing architecture and feature representation for Chinese POS tagging.</S>
			<S sid ="164" ssid = "42">Our maximum entropy word segmenter is similar to that of (Xue and Shen, 2003), but the additional features we used and the post- processing step gave improved word segmentation accuracy.</S>
			<S sid ="165" ssid = "43">The research most similar to ours is (Luo, 2003).</S>
			<S sid ="166" ssid = "44">Luo presented a maximum entropy character-based parser, which as a consequence of parsing also performed word segmentation and POS tagging.</S>
			<S sid ="167" ssid = "45">The all-at-once, character- based approach reported in this paper is essentially the approach proposed by Luo.</S>
			<S sid ="168" ssid = "46">While our investigation reveals that such an approach gives good accuracy, our findings however indicate that a one-at-a-time, character-based approach to POS tagging gave quite comparable accuracy, with the benefit of incurring much reduced computational cost.</S>
			<S sid ="169" ssid = "47">8 Conclusion.</S>
			<S sid ="170" ssid = "48">Language differences between English and Chinese have made direct porting of an English POS tagging method to Chinese ineffective.</S>
			<S sid ="171" ssid = "49">In Chinese, individual characters encode information that aids in POS tagging.</S>
			<S sid ="172" ssid = "50">Using a character-based approach for Chinese POS tagging is more effective than a word-based approach.</S>
			<S sid ="173" ssid = "51">Our study has also revealed that the one-at-a-time, character-based approach gives relatively good POS tagging accuracy with a much improved training and testing time, compared with the all-at-once, character-based approach previously proposed.</S>
			<S sid ="174" ssid = "52">9 Acknowledgements.</S>
			<S sid ="175" ssid = "53">This research is partially supported by a research grant R252000-125112 from National University of Singapore Academic Research Fund.</S>
	</SECTION>
</PAPER>
