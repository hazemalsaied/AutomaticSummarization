<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper we present a Two-Phase LMRRC Tagging scheme to perform Chinese word segmentation.</S>
		<S sid ="2" ssid = "2">In the Regular Tagging phase, Chinese sentences are processed similar to the original LMR Tagging.</S>
		<S sid ="3" ssid = "3">Tagged sentences are then passed to the Correctional Tagging phase, in which the sentences are re-tagged using extra information from the first round tagging results.</S>
		<S sid ="4" ssid = "4">Two training methods, Separated Mode and Integrated Mode, are proposed to construct the models.</S>
		<S sid ="5" ssid = "5">Experimental results show that our scheme in Integrated Mode performs the best in terms of accuracy, where Separated Mode is more suitable under limited computational resources.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The Chinese word segmentation is a nontrivial task because no explicit delimiters (like spaces in English) are used for word separation.</S>
			<S sid ="7" ssid = "7">As the task is an important precursor to many natural language processing systems, it receives a lot of attentions in the literature for the past decade (Wu and Tseng, 1993; Sproat et al., 1996).</S>
			<S sid ="8" ssid = "8">In this paper, we propose a statistical approach based on the works of (Xue and Shen, 2003), in which the Chinese word segmentation problem is first transformed into a tagging problem, then the Maximum Entropy classifier is applied to solve the problem.</S>
			<S sid ="9" ssid = "9">We further improve the scheme by introducing correctional treatments after first round tagging.</S>
			<S sid ="10" ssid = "10">Two different training methods are proposed to suit our scheme.</S>
			<S sid ="11" ssid = "11">The paper is organized as follows.</S>
			<S sid ="12" ssid = "12">In Section 2, we briefly discuss the scheme proposed by (Xue and Shen, 2003), followed by our additional works to improve the performance.</S>
			<S sid ="13" ssid = "13">Experimental and bakeoff results are presented in Section 3.</S>
			<S sid ="14" ssid = "14">Finally, We conclude the paper in Section 4.</S>
	</SECTION>
	<SECTION title="Our Proposed Approach. " number = "2">
			<S sid ="15" ssid = "1">2.1 Chinese Word Segmentation as Tagging.</S>
			<S sid ="16" ssid = "2">One of the difficulties in Chinese word segmentation is that, Chinese characters can appear in different positions within a word (Xue and Shen, 2003), and LMR Tagging was proposed to solve the problem.</S>
			<S sid ="17" ssid = "3">The basic idea of LMR Tagging is to assign to each character, based on its contextual information, a tag which represents its relative position within the word.</S>
			<S sid ="18" ssid = "4">Note that the original tag set used by (Xue and Shen, 2003) is simplified and improved by (Ng and Low, 2004) . We shall then adopt and illustrate the simplified case here.</S>
			<S sid ="19" ssid = "5">The tags and their meanings are summarized in Table 1.</S>
			<S sid ="20" ssid = "6">Tag L, M, and R correspond to the character at the beginning, in the middle, and at the end of the word respectively.</S>
			<S sid ="21" ssid = "7">Tag S means the character is a “single-character” word.</S>
			<S sid ="22" ssid = "8">Figure 1 illustrates a Chinese sentence segmented by spaces, and the corresponding tagging results.</S>
			<S sid ="23" ssid = "9">After transforming the Chinese segmentation problem to the tagging problem, various solutions can be applied.</S>
			<S sid ="24" ssid = "10">Maximum Entropy model (MaxEnt) (Berger, S. A. Della Pietra, and Original sentence: Օᓡ໛ᦟپՓڍഅරΖ After segmentation: Օᓡ ໛ᦟ پ Փڍഅර Ζ Tagging: ҨҮ ҨҮ ү ҨҩҩҮ ү assigned: y∗ = arg max y∈{L,M,R,S} p(y|x).</S>
			<S sid ="25" ssid = "11">Figure 1: Example of LMR Tagging.</S>
			<S sid ="26" ssid = "12">V. J. Della Pietra, 1996; Ratnaparkhi, 1996) was proposed in the original work to solve the LMR Tagging problem.</S>
			<S sid ="27" ssid = "13">In order to make MaxEnt success in LMR Tagging, feature templates used in capturing useful contextual information must be carefully designed.</S>
			<S sid ="28" ssid = "14">Furthermore, it is unavoidable that invalid tag sequences will occur if we just assign the tag with the highest probability.</S>
			<S sid ="29" ssid = "15">In the next subsection, we describe the feature templates and measures used to correct the tagging.</S>
			<S sid ="30" ssid = "16">Table 1: Tags used in LMR Tagging scheme.</S>
			<S sid ="31" ssid = "17">The features describing the characteristics of Chinese segmentation problem are instantiations of the feature templates listed in Table 2.</S>
			<S sid ="32" ssid = "18">Note that feature templates only describe the forms of features, but not the actual features.</S>
			<S sid ="33" ssid = "19">So the number of features used is much larger than the number of templates.</S>
			<S sid ="34" ssid = "20">Table 2: Feature templates used in R-phase.</S>
			<S sid ="35" ssid = "21">Example used is “32 @�*”.</S>
			<S sid ="36" ssid = "22">Tag Description L Character is at the beginning of the word (or the character is the leftmost character in the word) M Character is in the middle of the word RCharacter is at the end of the word (or the charac ter is the rightmost character in the word) S Character is a ”single-character” word 2.2 Two-Phase LMRRC Tagging.</S>
			<S sid ="37" ssid = "23">In this section, we introduce our Two-Phase LMRRC Tagging used to perform Chinese Text Segmentation.</S>
			<S sid ="38" ssid = "24">The first phase, R-phase, is called Regular Tagging, in which similar procedures as in the original LMR Tagging are performed.</S>
			<S sid ="39" ssid = "25">The difference in this phase as compared to the original one is that, we use extra feature templates to capture characteristics of Chinese word segmentation.</S>
			<S sid ="40" ssid = "26">The second phase, C-phase, is called Correctional Tagging, in which the sentences are re- tagged by incorporating the regular tagging results.</S>
			<S sid ="41" ssid = "27">We hope that tagging errors can be corrected under this way.</S>
			<S sid ="42" ssid = "28">The models used in both phases are trained using MaxEnt model.</S>
			<S sid ="43" ssid = "29">Regular Tagging Phase In this phase, each character is tagged similar to the original approach.</S>
			<S sid ="44" ssid = "30">In our scheme, given the contextual information (x) of current charac ter, the tag (y∗) with highest probability will be Additional feature templates as compared to (Xue and Shen, 2003) and (Ng and Low, 2004) are template 5 and 6.</S>
			<S sid ="45" ssid = "31">Template 5 is used to handle documents with ASCII characters.</S>
			<S sid ="46" ssid = "32">For template 6, as it is quite common that word boundary occurs in between two characters with different types, this template is used to capture such characteristics.</S>
			<S sid ="47" ssid = "33">Correctional Tagging Phase In this phase, the sequence of characters is re- tagged by using the additional information of tagging results after R-phase.</S>
			<S sid ="48" ssid = "34">The tagging procedure is similar to the previous phase, except extra features (listed in Table 3) are used to assist the tagging.</S>
			<S sid ="49" ssid = "35">Table 3: Additional feature templates used in C- phase.</S>
			<S sid ="50" ssid = "36">Example used is “32 @�*” with tagging results after R-phase as “SSLMR”.</S>
			<S sid ="51" ssid = "37">Fe atu re Ty pe Ex am ple – Fe atu res ext rac ted of cha rac ter “ @ ” 7 Ta gs of ch ar act ers wit hin wi nd ow of ±2 T− 2 =“ S”, T − 1 = “ S ” , T 0 = “ L ” , T 1 = “ M ” , T 2 = “ R ” 8 Tw o co ns ec utiv e t a g s w i t h i n w i n d o w o f ± 2 T − 2 T − 1 = “ S S ” , T − 1 T 0 = “ S L ” , T 0 T 1 = “ L M ” , T 1 T 2 = “ M R ” 9 Pre vio us an d ne xt tag s T− 1 T1 =“ S M” Training Method Two training methods are proposed to construct models used in Rand C-phase: (1) Separated Mode, and (2) Integrated Mode.</S>
			<S sid ="52" ssid = "38">Separated Mode means the models used in two phases are separated.</S>
			<S sid ="53" ssid = "39">Model for R-phase is called R-model, and model for C-phase is called C-model.</S>
			<S sid ="54" ssid = "40">Integrated Mode means only one model, I-model is used in both phases.</S>
			<S sid ="55" ssid = "41">The training methods are illustrated now.</S>
			<S sid ="56" ssid = "42">First of all, training data are divided into three parts, (1) Regular Training, (2) Correctional Training, and (3) Evaluation.</S>
			<S sid ="57" ssid = "43">Our method first trains using observations extracted from Part 1 (observation is simply the pair (context, tag) of each character).</S>
			<S sid ="58" ssid = "44">The created model is used to process Part 2.</S>
			<S sid ="59" ssid = "45">After that, observations extracted from Part 2 (which include previous tagging results) are used to create the final model.</S>
			<S sid ="60" ssid = "46">The performance is then evaluated by processing Part 3.</S>
			<S sid ="61" ssid = "47">Let O be the set of observations, with subscripts R or C indicating the sources of them.</S>
			<S sid ="62" ssid = "48">Let T rainM odel : O → P , where P is the set of models, be the “model generating” function.</S>
			<S sid ="63" ssid = "49">The two proposed training methods can be illustrated as follow: 1.</S>
			<S sid ="64" ssid = "50">Separated Mode.</S>
			<S sid ="65" ssid = "51">R − model = T rainM odel(OR), C − model = T rainM odel(OC ).</S>
			<S sid ="66" ssid = "52">2.</S>
			<S sid ="67" ssid = "53">Integrated Mode.</S>
			<S sid ="68" ssid = "54">I − model = T rainM odel(OR ∪ OC ).</S>
			<S sid ="69" ssid = "55">The advantage of Separated Mode is that, it is easy to aggregate different sets of training data.</S>
			<S sid ="70" ssid = "56">It also provides a mean to handle large training data under limited resources, as we can divide the training data into several parts, and then use the similar idea to train each part.</S>
			<S sid ="71" ssid = "57">The drawback of this mode is that, it may lose the features’ characteristics captured from Part 1 of training data, and Integrated Mode is proposed to address the problem, in which all the features’ characteristics in both Part 1 and Part 2 are used to train the model.</S>
	</SECTION>
	<SECTION title="Experimental Results and Discussion. " number = "3">
			<S sid ="72" ssid = "1">We conducted closed track experiments on the Hong Kong City University (CityU) corpus in The Second International Chinese Word Segmentation Bakeoff to evaluate the proposed training and tagging methods.</S>
			<S sid ="73" ssid = "2">The training data were split into three portions.</S>
			<S sid ="74" ssid = "3">Part 1: 60% of the data is trained for R-phase; Part 2: 30% for C-phase training; and Part 3: the remaining 10% for evaluation.</S>
			<S sid ="75" ssid = "4">The evaluation part was further divided into six parts to simulate actual size of test document.</S>
			<S sid ="76" ssid = "5">The MaxEnt classifier was implemented using Java opennlp maximum entropy package from (Baldridge, Morton, and Bierner, 2004), and training was done with feature cutoff of 2 and 160 iterations.</S>
			<S sid ="77" ssid = "6">The experiments were run on an Intel Pentium4 3.0GHz machine with 3.0GB memory.</S>
			<S sid ="78" ssid = "7">To evaluate our proposed scheme, we carried out four experiments for each evaluation data.</S>
			<S sid ="79" ssid = "8">For Experiment 1, data were processed with R-phase only.</S>
			<S sid ="80" ssid = "9">For Experiment 2, data were processed with both Rand C-phase, using Separated Mode as training method.</S>
			<S sid ="81" ssid = "10">For Experiment 3, data were processed similar to Experiment 2, except Integrated Mode was used.</S>
			<S sid ="82" ssid = "11">Finally for Experiment 4, data were processed similar to Experiment 1, with both Part 1 and Part 2 data were used for R- model training.</S>
			<S sid ="83" ssid = "12">The purpose of Experiment 4 is to determine whether the proposed scheme can perform better than just the single Regular Tagging under the same amount of training data.</S>
			<S sid ="84" ssid = "13">Table 4 summarizes the experimental results measured in F-measure (the harmonic mean of precision and recall).</S>
			<S sid ="85" ssid = "14">From the results, we obtain the following observations.</S>
			<S sid ="86" ssid = "15">1.</S>
			<S sid ="87" ssid = "16">Both Integrated and Separated Training modes.</S>
			<S sid ="88" ssid = "17">Table 4: Experimental results of CityU corpus measured in F-measure.</S>
			<S sid ="89" ssid = "18">in Two-Phase Tagging (Exp 2 and Exp 3) outperform single Regular Tagging (Exp 1).</S>
			<S sid ="90" ssid = "19">It is reasonable as more data are used in training.</S>
			<S sid ="91" ssid = "20">2.</S>
			<S sid ="92" ssid = "21">Integrated Mode (Exp 3) still performs better.</S>
			<S sid ="93" ssid = "22">than Exp 4, in which same amount of training data are used.</S>
			<S sid ="94" ssid = "23">This reflects that extra tagging information after R-phase helps in the scheme.</S>
			<S sid ="95" ssid = "24">3.</S>
			<S sid ="96" ssid = "25">Separated Mode (Exp 2) performs worse than.</S>
			<S sid ="97" ssid = "26">both Exp 3 and Exp 4.</S>
			<S sid ="98" ssid = "27">The reason is that the C- model cannot capture enough features’ characteristics used for basic tagging.</S>
			<S sid ="99" ssid = "28">We believe that by adjusting the proportion of Part 1 and Part 2 of training data, performance can be increased.</S>
	</SECTION>
	<SECTION title="Under limited  computational resources, in. " number = "4">
			<S sid ="100" ssid = "1">which constructing single-model using all available data (as in Exp 3 and Exp 4) is not possible, Separated Mode shows its advantage in constructing and aggregating multi-models by dividing the training data into different portions.</S>
			<S sid ="101" ssid = "2">The official BakeOff2005 results are summarized in Table 5.</S>
			<S sid ="102" ssid = "3">We have submitted multiple results for CityU, MSR and PKU corpora by applying different tagging methods described in the paper.</S>
			<S sid ="103" ssid = "4">Table 5: Official BakeOff2005 results.</S>
			<S sid ="104" ssid = "5">Keys: F - Regular Tagging only, all training data are used P1 - Regular Tagging only, 90% of training data are used P2 - Regular Tagging only, 70% of training data are used S - Regular and Correctional Tagging, Separated Mode I - Regular and Correctional Tagging, Integrated Mode 4 Conclusion.</S>
			<S sid ="105" ssid = "6">We present a Two-Phase LMRRC Tagging scheme to perform Chinese word segmentation.</S>
			<S sid ="106" ssid = "7">Correctional Tagging phase is introduced in addition to the original LMR Tagging technique, in which the Chinese sentences are re-tagged using extra information of first round tagging results.</S>
			<S sid ="107" ssid = "8">Two training methods, Separated Mode and Integrated Mode, are introduced to suit our scheme.</S>
			<S sid ="108" ssid = "9">Experimental results show that Integrated Mode achieve the highest accuracy in terms of F- measure, where Separated Mode shows its advantages in constructing and aggregating multi- models under limited resources.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="109" ssid = "10">The work described in this paper was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No.</S>
			<S sid ="110" ssid = "11">CUHK4235/04E).</S>
	</SECTION>
</PAPER>
