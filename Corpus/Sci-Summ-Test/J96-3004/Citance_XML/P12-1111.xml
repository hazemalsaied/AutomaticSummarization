<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We show for both English POS tagging and Chinese word segmentation that with proper representation, large number of deterministic constraints can be learned from training examples, and these are useful in constraining probabilistic inference.</S>
		<S sid ="2" ssid = "2">For tagging, learned constraints are directly used to constrain Viterbi decoding.</S>
		<S sid ="3" ssid = "3">For segmentation, character-based tagging constraints can be learned with the same templates.</S>
		<S sid ="4" ssid = "4">However, they are better applied to a word-based model, thus an integer linear programming (ILP) formulation is proposed.</S>
		<S sid ="5" ssid = "5">For both problems, the corresponding constrained solutions have advantages in both efficiency and accuracy.</S>
	</ABSTRACT>
	<SECTION title="introduction" number = "1">
			<S sid ="6" ssid = "6">In recent work, interesting results are reported for applications of integer linear programming (ILP) such as semantic role labeling (SRL) (Roth and Yih, 2005), dependency parsing (Martins et al., 2009) and so on.</S>
			<S sid ="7" ssid = "7">In an ILP formulation, ’non-local’ deterministic constraints on output structures can be naturally incorporated, such as ”a verb cannot take two subject arguments” for SRL, and the projectivity constraint for dependency parsing.</S>
			<S sid ="8" ssid = "8">In contrast to probabilistic constraints that are estimated from training examples, this type of constraint is usually handwritten reflecting one’s linguistic knowledge.</S>
			<S sid ="9" ssid = "9">Dynamic programming techniques based on Markov assumptions, such as Viterbi decoding, cannot handle those ’non-local’ constraints as discussed above.</S>
			<S sid ="10" ssid = "10">However, it is possible to constrain Viterbi decoding by ’local’ constraints, e.g. ”assign label t to word w” for POS tagging.</S>
			<S sid ="11" ssid = "11">This type of constraint may come from human input solicited in interactive inference procedure (Kristjansson et al., 2004).</S>
			<S sid ="12" ssid = "12">In this work, we explore deterministic constraints for two fundamental NLP problems, English POS tagging and Chinese word segmentation.</S>
			<S sid ="13" ssid = "13">We show by experiments that, with proper representation, large number of deterministic constraints can be learned automatically from training data, which can then be used to constrain probabilistic inference.</S>
			<S sid ="14" ssid = "14">For POS tagging, the learned constraints are directly used to constrain Viterbi decoding.</S>
			<S sid ="15" ssid = "15">The corresponding constrained tagger is 10 times faster than searching in a raw space pruned with beam-width 5.</S>
			<S sid ="16" ssid = "16">Tagging accuracy is moderately improved as well.</S>
			<S sid ="17" ssid = "17">For Chinese word segmentation (CWS), which can be formulated as character tagging, analogous constraints can be learned with the same templates as English POS tagging.</S>
			<S sid ="18" ssid = "18">High-quality constraints can be learned with respect to a special tagset, however, with this tagset, the best segmentation accuracy is hard to achieve.</S>
			<S sid ="19" ssid = "19">Therefore, these character-based constraints are not directly used for determining predictions as in English POS tagging.</S>
			<S sid ="20" ssid = "20">We propose an ILP formulation of the CWS problem.</S>
			<S sid ="21" ssid = "21">By adopting this ILP formulation, segmentation F-measure is increased from 0.968 to 0.974, as compared to Viterbi decoding with the same feature set.</S>
			<S sid ="22" ssid = "22">Moreover, the learned constraints can be applied to reduce the number of possible words over a character sequence, i.e. to reduce the number of variables to set.</S>
			<S sid ="23" ssid = "23">This reduction of problem size immediately speeds up an ILP solver by more than 100 times.</S>
			<S sid ="24" ssid = "24">1054 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 1054–1062, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="25" ssid = "25">Qc 2012 Association for Computational Linguistics</S>
	</SECTION>
	<SECTION title="English POS tagging. " number = "2">
			<S sid ="26" ssid = "1">2.1 Explore deterministic constraints.</S>
			<S sid ="27" ssid = "2">Suppose that, following (Chomsky, 1970), we distinguish major lexical categories (Noun, Verb, Adjective and Preposition) by two binary features: +|− N and +|− V. Let (+N, −V)=Noun, (−N, +V)=Verb, (+N, +V)=Adjective, and (−N, −V)=preposition.</S>
			<S sid ="28" ssid = "3">A word occurring in between a preceding word the and a following word of always bears the feature +N. On the other hand, consider the annotation guideline of English Treebank (Marcus et al., 1993) instead.</S>
			<S sid ="29" ssid = "4">Part-of-speech (POS) tags are used to categorize words, for example, the POS tag VBG tags verbal gerunds, NNS tags nominal plurals, DT tags determiners and so on.</S>
			<S sid ="30" ssid = "5">Following this POS representation, there are as many as 10 possible POS tags that may occur in between the–of, as estimated from the WSJ corpus of Penn Treebank.</S>
			<S sid ="31" ssid = "6">2.1.1 Templates of deterministic constraints To explore determinacy in the distribution of POS tags in Penn Treebank, we need to consider that a POS tag marks the basic syntactic category of a word as well as its morphological inflection.</S>
			<S sid ="32" ssid = "7">A constraint that may determine the POS category should reflect both the context and the morphological feature of the corresponding word.</S>
			<S sid ="33" ssid = "8">The practical difficulty in representing such deterministic constraints is that we do not have a perfect mechanism to analyze morphological features of a word.</S>
			<S sid ="34" ssid = "9">Endings or prefixes of English words do not deterministically mark their morphological inflections.</S>
			<S sid ="35" ssid = "10">We propose to compute the morph feature of a word as the set of all of its possible tags, i.e. all tag types that are assigned to the word in training data.</S>
			<S sid ="36" ssid = "11">Furthermore, we approximate unknown words in testing data by rare words in training data.</S>
			<S sid ="37" ssid = "12">For a word that occurs less than 5 times in the training corpus, we compute its morph feature as its last two characters, which is also conjoined with binary features indicating whether the rare word contains digits, hyphens or uppercase characters respectively.</S>
			<S sid ="38" ssid = "13">See examples of morph features in Table 1.</S>
			<S sid ="39" ssid = "14">We consider bigram and trigram templates for generating potentially deterministic constraints.</S>
			<S sid ="40" ssid = "15">Let wi denote the ith word relative to the current word w0; and mi denote the morph feature of wi.</S>
			<S sid ="41" ssid = "16">A Table 1: Morph features of frequent words and rare words as computed from the WSJ Corpus of Penn Treebank.bi -gram w−1 w0 , w0 w1 , m−1 w0 , w0 m1 w−1 m0 , m0 w1 , m−1 m0 , m0 m1tri -gram w−1 w0 w1 , m−1 w0 w1 , w−1 m0 w1 , m−1 m0 w1 w−1 w0 m1 , m−1 w0 m1 , w−1 m0 m1 , m−1 m0 m1 Table 2: The templates for generating potentially deterministic constraints of English POS tagging.</S>
			<S sid ="42" ssid = "17">bigram constraint includes one contextual word (w−1|w1) or the corresponding morph feature; and a trigram constraint includes both contextual words or their morph features.</S>
			<S sid ="43" ssid = "18">Each constraint is also conjoined with w0 or m0, as described in Table 2.</S>
			<S sid ="44" ssid = "19">2.1.2 Learning of deterministic constraints In the above section, we explore templates for potentially deterministic constraints that may determine POS category.</S>
			<S sid ="45" ssid = "20">With respect to a training corpus, if a constraint C relative to w0 ’always’ assigns a certain POS category t∗ to w0 in its context, i.e. count(C ) &gt; thr, and this constraint occurs more than a cutoff number, we consider it as a deterministic constraint.</S>
			<S sid ="46" ssid = "21">The threshold thr is a real number just under 1.0 and the cutoff number is empirically set to 5 in our experiments.</S>
			<S sid ="47" ssid = "22">2.1.3 Decoding of deterministic constraints By the above definition, the constraint of w−1 =the, m0 = {NNS, VBZ} and w1 = of is determinis tic.</S>
			<S sid ="48" ssid = "23">It determines the POS category of w0 to be NNS.</S>
			<S sid ="49" ssid = "24">There are at least two ways of decoding these constraints during POS tagging.</S>
			<S sid ="50" ssid = "25">Take the word trades for example, whose morph feature is {NNS, VBZ}.One alternative is that as long as trades occurs be tween the-of, it is tagged with NNS.</S>
			<S sid ="51" ssid = "26">The second alternative is that the tag decision is made only if all deterministic constraints relative to this occurrence of trades agree on the same tag.</S>
			<S sid ="52" ssid = "27">Both ways of decoding are purely rule-based and involve no probabilistic inference.</S>
			<S sid ="53" ssid = "28">In favor of a higher precision, we adopt the latter one in our experiments.</S>
			<S sid ="54" ssid = "29">raw input O(nT 2 ) n = 23 The complex financing plan in the S&amp;L bailout law includes... to the current word w0.</S>
			<S sid ="55" ssid = "30">As discussed in (Shen et al., 2007), categorical information of neighbouring constrained input O(m1 T + m2 T 2 ) m1 = 2, m2 = 1 words on both sides of w0help resolve POS ambi The/DT complex/– financing/– plan/NN in/IN the/DT S&amp;L/– bailout/NN law/NN includes/VBZ ...</S>
			<S sid ="56" ssid = "31">Table 3: Comparison of raw input and constrained input.</S>
			<S sid ="57" ssid = "32">2.2 Search in a constrained space.</S>
			<S sid ="58" ssid = "33">Following most previous work, we consider POS tagging as a sequence classification problem and decompose the overall sequence score over the linear n guity of w0.</S>
			<S sid ="59" ssid = "34">In (Shen et al., 2007), lookahead features may be available for use during decoding since searching is bidirectional instead of left-to-right as in Viterbi decoding.</S>
			<S sid ="60" ssid = "35">In this work, deterministic constraints are decoded before the application of probabilistic models, therefore lookahead features are made available during Viterbi decoding.</S>
	</SECTION>
	<SECTION title="Chinese Word Segmentation (CWS). " number = "3">
			<S sid ="61" ssid = "1">3.1 Word segmentation as character tagging.</S>
			<S sid ="62" ssid = "2">structure, i.e. ˆt = arg max ) score(ti) where t∈tagGEN(w) i=1 function tagGEN maps input sentence w = w1...wn to the set of all tag sequences that are of length n. If a POS tagger takes raw input only, i.e. for every word, the number of possible tags is a constant T , the space of tagGEN is as large as T n. On the other hand, if we decode deterministic constraints first before a probabilistic search, i.e. for some words, the number of possible tags is reduced to 1, the search Considering the ambiguity problem that a Chinese character may appear in any relative position in a word and the out-of-vocabulary (OOV) problem that it is impossible to observe all words in training data, CWS is widely formulated as a character tagging problem (Xue, 2003).</S>
			<S sid ="63" ssid = "3">A character-based CWS decoder is to find the highest scoring tag sequence ˆt over the input character sequence c, i.e. n space is reduced to T m, where m is the number of ˆt = arg max ) score(ti) .(unconstrained) words that are not subject to any de terministic constraints.</S>
			<S sid ="64" ssid = "4">Viterbi algorithm is widely used for tagging, and runs in O(nT 2) when searching in an unconstrained space.</S>
			<S sid ="65" ssid = "5">On the other hand, consider searching in a constrained space.</S>
			<S sid ="66" ssid = "6">Suppose that among the m un- constrained words, m1 of them follow a word that has been tagged by deterministic constraints and m2 (=m-m1) of them follow another unconstrained word.</S>
			<S sid ="67" ssid = "7">Viterbi decoder runs in O(m1T + m2T 2) while searching in such a constrained space.</S>
			<S sid ="68" ssid = "8">The example in Table 3 shows raw and constrained input with respect to a typical input sentence.</S>
			<S sid ="69" ssid = "9">Lookahead features The score of tag predictions are usually computed in a high-dimensional feature space.</S>
			<S sid ="70" ssid = "10">We adopt the basic feature set used in (Ratnaparkhi, 1996) and (Collins, 2002).</S>
			<S sid ="71" ssid = "11">Moreover, when deterministic constraints have applied to contextual words of w0, it is also possible to include some lookahead feature templates, such as: t0&amp;t1, t0&amp;t1&amp;t2, and t−1&amp;t0&amp;t1 where ti represents the tag of the ith word relative t∈tagGEN(c) i=1 This is the same formulation as POS tagging.</S>
			<S sid ="72" ssid = "12">The Viterbi algorithm is also widely used for decoding.</S>
			<S sid ="73" ssid = "13">The tag of each character represents its relative position in a word.</S>
			<S sid ="74" ssid = "14">Two popular tagsets include 1) IB: where B tags the beginning of a word and I all other positions; and 2) BMES: where B, M and E represent the beginning, middle and end of a multi- character word respectively, and S tags a single- character word.</S>
			<S sid ="75" ssid = "15">For example, after decoding with BMES, 4 consecutive characters associated with the tag sequence BMME compose a word.</S>
			<S sid ="76" ssid = "16">However, after decoding with IB, characters associated with BIII may compose a word if the following tag is B or only form part of a word if the following tag is I. Even though character tagging accuracy is higher with tagset IB, tagset BMES is more popular in use since better performance of the original problem CWS can be achieved by this tagset.</S>
			<S sid ="77" ssid = "17">Character-based feature templates We adopt the ’non-lexical-target’ feature templates in (Jiang et al., 2008a).</S>
			<S sid ="78" ssid = "18">Let ci denote the ith character relative to the current character c0 and t0 denote the tag assigned to c0.</S>
			<S sid ="79" ssid = "19">The following templates are used: ci&amp;t0 (i=-2...2), cici+1&amp;t0 (i=-2...1) and c−1c1&amp;t0.</S>
			<S sid ="80" ssid = "20">Character-based deterministic constraints We can use the same templates as described in Table 2 to generate potentially deterministic constraints for CWS character tagging, except that there are no morph features computed for Chinese characters.</S>
			<S sid ="81" ssid = "21">As we will show with experimental results in Section 5.2, useful deterministic constraints for CWS can be learned with tagset IB but not with Table 4: Comparison of raw input and constrained input.</S>
			<S sid ="82" ssid = "22">an optimal solution x = x1...xs that maximizes s ) score(wi) · xi, subject to i=1 tagset BMES.</S>
			<S sid ="83" ssid = "23">It is interesting but not surprising to notice, again, that the determinacy of a problem is sen (1) ) i:c∈wi xi = 1, ∀c ∈ c; sitive to its representation.</S>
			<S sid ="84" ssid = "24">Since it is hard to achieve the best segmentations with tagset IB, we propose an indirect way to use these constraints in the following section, instead of applying these constraints as straightforwardly as in English POS tagging.</S>
			<S sid ="85" ssid = "25">3.2 Word-based word segmentation.</S>
			<S sid ="86" ssid = "26">A word-based CWS decoder finds the highest scor (2) xi ∈ {0, 1}, 1 ≤ i ≤ s The boolean value of xi, as guaranteed by constraint (2), indicates whether wi is selected in the segmentation solution or not.</S>
			<S sid ="87" ssid = "27">Constraint (1) requires every character to be included in exactly one selected word, thus guarantees a proper segmentation of the whole sequence.</S>
			<S sid ="88" ssid = "28">This resembles the ILP formula ing segmentation sequence wˆ that is composed by tion of the set cover problem, though the first con the input character sequence c, i.e. |w| straint is different.</S>
			<S sid ="89" ssid = "29">Take n = 2 for example, i.e. c = c1c2, the set of possible words is {c1, c2, c1c2}, i.e. s = |x| = 3.</S>
			<S sid ="90" ssid = "30">There are only two possible so wˆ = arg max ) score(wi) . lutions subject to constraints (1) and (2), x = 110 w∈segGEN(c) i=1 where function segGEN maps character sequence c to the set of all possible segmentations of c. For example, w = (c1..cl1 )...(cn−lk +1...cn) represents a segmentation of k words and the lengths of the first and last word are l1 and lk respectively.</S>
			<S sid ="91" ssid = "31">In early work, rule-based models find words one by one based on heuristics such as forward maximum match (Sproat et al., 1996).</S>
			<S sid ="92" ssid = "32">Exact search is possible with a Viterbi-style algorithm, but beam- search decoding is more popular as used in (Zhang and Clark, 2007) and (Jiang et al., 2008a).</S>
			<S sid ="93" ssid = "33">We propose an Integer Linear Programming (ILP) formulation of word segmentation, which is naturally viewed as a word-based model for CWS.</S>
			<S sid ="94" ssid = "34">Character-based deterministic constraints, as discussed in Section 3.1, can be easily applied.</S>
			<S sid ="95" ssid = "35">3.3 ILP formulation of CWS.</S>
			<S sid ="96" ssid = "36">Given a character sequence c=c1...cn, there are s(= n(n + 1)/2) possible words that are contiguous subsets of c, i.e. w1, ..., ws ⊆ c. Our goal is to find giving an output set {c1, c2}, or x = 001 giving an output set {c1c2}.</S>
			<S sid ="97" ssid = "37">The efficiency of solving this problem depends on the number of possible words (contiguous subsets) over a character sequence, i.e. the number of vari ables in x. So as to reduce |x|, we apply determin istic constraints predicting IB tags first, which are learned as described in Section 3.1.</S>
			<S sid ="98" ssid = "38">Possible words are generated with respect to the partially tagged character sequence.</S>
			<S sid ="99" ssid = "39">A character tagged with B always occurs at the beginning of a possible word.</S>
			<S sid ="100" ssid = "40">Table 4 illustrates the constrained and raw input with respect to a typical character sequence.</S>
			<S sid ="101" ssid = "41">3.4 Character- and word-based.</S>
			<S sid ="102" ssid = "42">features As studied in previous work, word-based feature templates usually include the word itself, sub-words contained in the word, contextual characters/words and so on.</S>
			<S sid ="103" ssid = "43">It has been shown that combining the use of character- and word based features helps improve performance.</S>
			<S sid ="104" ssid = "44">However, in the character tagging formulation, word-based features are non-local.</S>
			<S sid ="105" ssid = "45">To incorporate these non-local features and make the search tractable, various efforts have been made.</S>
			<S sid ="106" ssid = "46">For example, Jiang et al.</S>
			<S sid ="107" ssid = "47">(2008a) combine different levels of knowledge in an outside linear model of a two- layer cascaded model; Jiang et al.</S>
			<S sid ="108" ssid = "48">(2008b) uses the forest re-ranking technique (Huang, 2008); and in (Kruengkrai et al., 2009), only known words in vocabulary are included in the hybrid lattice consisting of both character- and word-level nodes.</S>
			<S sid ="109" ssid = "49">We propose to incorporate character-based features in word-based models.</S>
			<S sid ="110" ssid = "50">Consider a character- based feature function φ(c, t, c) that maps a character-tag pair to a high-dimensional feature space, with respect to an input character sequence c. For a possible word over c of length l , wi = ci0 ...ci0 +l−1, tag each character cij in this word with a character-based tag tij . Character-based features of wi can be computed as {φ(cij , tij , c)|0 ≤ j &lt; l}.</S>
			<S sid ="111" ssid = "51">The first row of Table 5 illustrates character-based features of a word of length 3, which is tagged with tagset BMES.</S>
			<S sid ="112" ssid = "52">From this view, the character-based feature templates defined in Section 3.1 are naturally used in a word-based model.</S>
			<S sid ="113" ssid = "53">When character-based features are incorporated into word-based CWS models, some word-based features are no longer of interest, such as the starting character of a word, sub-words contained in the word, contextual characters and so on.</S>
			<S sid ="114" ssid = "54">We consider word counting features as a complementary to character-based features, following the idea of using web-scale features in previous work, e.g.</S>
			<S sid ="115" ssid = "55">(Bansal and Klein, 2011).</S>
			<S sid ="116" ssid = "56">For a possible word w, let count(w) return the count of times that w occurs as a legal word in training data.</S>
			<S sid ="117" ssid = "57">The word count number is further processed following (Bansal and Klein, 2011), wc(w) = f loor(log(count(w)) ∗ 5)/5.</S>
			<S sid ="118" ssid = "58">In addition to wc(wi), we also use corresponding word count features of possible words that are composed of the boundary and contextual characters of wi.</S>
			<S sid ="119" ssid = "59">The specific word-based feature templates are illustrated in the second row of Table 5.</S>
	</SECTION>
	<SECTION title="Training. " number = "4">
			<S sid ="120" ssid = "1">We use the following linear model for scoring predictions: score(y)=θT φ(x, y), where φ(y) is a high- dimensional binary feature representation of y over input x and θ contains weights of these features.</S>
			<S sid ="121" ssid = "2">For ch ar ac ter b a s e d φ( ci0 , B, c), φ( ci1 , M, c), φ( ci2 , E, c) w o r d b a s e d wc (ci 0 ci1 ci2 ), wc (cl ci0 ), wc (ci 2 cr ) Table 5: Character- and word-based features of a possible word wi over the input character sequence c. Suppose that wi = ci0 ci1 ci2 , and its preceding and following characters are cl and cr respectively.</S>
			<S sid ="122" ssid = "3">parameter estimation of θ, we use the averaged per- ceptron as described in (Collins, 2002).</S>
			<S sid ="123" ssid = "4">This training algorithm relies on the choice of decoding algorithm.</S>
			<S sid ="124" ssid = "5">When we experiment with different decoders, by default, the parameter weights in use are trained with the corresponding decoding algorithm.</S>
			<S sid ="125" ssid = "6">Especially, for experiments with lookahead features of English POS tagging, we prepare training data with the stacked learning technique, in order to alleviate overfitting.</S>
			<S sid ="126" ssid = "7">More specifically, we divide the training data into k folds, and tag each fold with the deterministic model learned over the other k-1 folds.</S>
			<S sid ="127" ssid = "8">The predicted tags of all folds are then merged into the gold training data and used (only) as lookahead features.</S>
			<S sid ="128" ssid = "9">Sun (2011) uses this technique to merge different levels of predictors for word segmentation.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="129" ssid = "1">5.1 Data set.</S>
			<S sid ="130" ssid = "2">We run experiments on English POS tagging on the WSJ corpus in the Penn Treebank.</S>
			<S sid ="131" ssid = "3">Following most previous work, e.g.</S>
			<S sid ="132" ssid = "4">(Collins, 2002) and (Shen et al., 2007), we divide this corpus into training set (sections 018), development set (sections 1921) and the final test set (sections 2224).</S>
			<S sid ="133" ssid = "5">We run experiments on Chinese word segmentation on the Penn Chinese Treebank 5.0.</S>
			<S sid ="134" ssid = "6">Following (Jiang et al., 2008a), we divide this corpus into training set (chapters 1260), development set (chapters 271300) and the final test set (chapters 301325).</S>
			<S sid ="135" ssid = "7">5.2 Deterministic constraints.</S>
			<S sid ="136" ssid = "8">Experiments in this section are carried out on the development set.</S>
			<S sid ="137" ssid = "9">The cutoff number and threshold as defined in 2.1.2, are fixed as 5 and 0.99 respectively.</S>
			<S sid ="138" ssid = "10">pr ec isi on re ca ll F 1 b i g r a m 0 . 9 9 3 0.</S>
			<S sid ="139" ssid = "11">84 1 0.</S>
			<S sid ="140" ssid = "12">91 1 t r i g r a m 0 . 9 9 6 0.</S>
			<S sid ="141" ssid = "13">60 8 0.</S>
			<S sid ="142" ssid = "14">75 5 bi +t rig ra m 0 . 9 9 2 0.</S>
			<S sid ="143" ssid = "15">85 7 0.</S>
			<S sid ="144" ssid = "16">92 0 Table 6: POS tagging with deterministic constraints.</S>
			<S sid ="145" ssid = "17">The maximum in each column is bold.</S>
			<S sid ="146" ssid = "18">m0 ={VBN, VBZ} &amp; m1 ={JJ, VBD, VBN} → VBN w0 =also &amp; m1 ={VBD, VBN} → RB m0 =−es &amp; m−1 ={IN, RB, RP} → NNS w0 =last &amp; w−1 = the → JJ Table 7: Deterministic constraints for POS tagging.</S>
			<S sid ="147" ssid = "19">Deterministic constraints for POS tagging For English POS tagging, we evaluate the deterministic constraints generated by the templates described in Section 2.1.1.</S>
			<S sid ="148" ssid = "20">Since these deterministic constraints are only applied to words that occur in a constrained context, we report F-measure as the accuracy measure.</S>
			<S sid ="149" ssid = "21">Precision p is defined as the percentage of correct predictions out of all predictions, and recall r is defined as the percentage of gold predictions that are correctly predicted.</S>
			<S sid ="150" ssid = "22">F-measure F1 is computed by 2pr/(p + r).</S>
			<S sid ="151" ssid = "23">As shown in Table 6, deterministic constraints learned with both bigram and trigram templates are all very accurate in predicting POS tags of words in their context.</S>
			<S sid ="152" ssid = "24">Constraints generated by bigram template alone can already cover 84.1% of the input words with a high precision of 0.993.</S>
			<S sid ="153" ssid = "25">By adding the constraints generated by trigram template, recall is increased to 0.857 with little loss in precision.</S>
			<S sid ="154" ssid = "26">Since these deterministic constraints are applied before the decoding of probabilistic models, reliably high precision of their predictions is crucial.</S>
			<S sid ="155" ssid = "27">There are 114589 bigram deterministic constraints and 130647 trigram constraints learned from the training data.</S>
			<S sid ="156" ssid = "28">We show a couple of examples of bigram deterministic constraints in Table 7.</S>
			<S sid ="157" ssid = "29">As defined in Section 2.2, we use the set of all possible POS tags for a word, e.g. {VBN, VBZ}, as its morph feature if the word is frequent (occurring more than 5 times in training data).</S>
			<S sid ="158" ssid = "30">For a rare word, the last two characters are used as its morph feature, e.g. −es.</S>
			<S sid ="159" ssid = "31">A constraint is composed of w−1, w0 and w1, as well as the morph features m−1, m0 and m1.</S>
			<S sid ="160" ssid = "32">For ex Table 8: Character tagging with deterministic constraints.</S>
			<S sid ="161" ssid = "33">ample, the first constraint in Table 7 determines the tag VBN of w0.</S>
			<S sid ="162" ssid = "34">A deterministic constraint is aware of neither the likelihood of each possible tag or the relative rank of their likelihoods.</S>
			<S sid ="163" ssid = "35">Deterministic constraints for character tagging For the character tagging formulation of Chinese word segmentation, we discussed two tagsets IB and BMES in Section 3.1.</S>
			<S sid ="164" ssid = "36">With respect to either tagset, we use both bigram and trigram templates to generate deterministic constraints for the corresponding tagging problem.</S>
			<S sid ="165" ssid = "37">These constraints are also evaluated by F-measure as defined above.</S>
			<S sid ="166" ssid = "38">As shown in Table 8, when tagset IB is used for character tagging, high precision predictions can be made by the deterministic constraints that are learned with respect to this tagset.</S>
			<S sid ="167" ssid = "39">However, when tagset BMES is used, the learned constraints don’t always make reliable predictions, and the overall precision is not high enough to constrain a probabilistic model.</S>
			<S sid ="168" ssid = "40">Therefore, we will only use the deterministic constraints that predict IB tags in following CWS experiments.</S>
			<S sid ="169" ssid = "41">5.3 English POS tagging.</S>
			<S sid ="170" ssid = "42">For English POS tagging, as well as the CWS problem that will be discussed in the next section, we use the development set to choose training iterations (= 5), set beam width etc. The following experiments are done on the final test set.</S>
			<S sid ="171" ssid = "43">As introduced in Section 2.2, we adopt a very compact feature set used in (Ratnaparkhi, 1996)1.</S>
			<S sid ="172" ssid = "44">While searching in a constrained space, we can also extend this feature set with some basic lookahead features as defined in Section 2.2.</S>
			<S sid ="173" ssid = "45">This replicates the feature set B used in (Shen et al., 2007).</S>
			<S sid ="174" ssid = "46">In this work, our main interest in the POS tagging problem is on its efficiency.</S>
			<S sid ="175" ssid = "47">A well-known technique to speed up Viterbi decoding is to conduct beam search.</S>
			<S sid ="176" ssid = "48">Based on experiments carried out 1 Our implementation of this feature set is basically the same as the version used in (Collins, 2002).</S>
			<S sid ="177" ssid = "49">R at n a p ar k hi (1 9 9 6) ’s fe at ur e B e a m = 1 B e a m = 5 r a w 9 6.</S>
			<S sid ="178" ssid = "50">4 6 % / 3 × 9 7.</S>
			<S sid ="179" ssid = "51">1 6 / 1 × c o n s t r a i n e d 96 .8 0 % /1 4 × 97 .2 0/ 10 × Fe atu re B in (Sh en et al., 20 07 ) (S he n et al., 20 07 ) 97 .1 5 % (B e a m = 3) c o n s t r a i n e d 97 .0 3 % /1 1 × 9 7.</S>
			<S sid ="180" ssid = "52">2 0 / 8 × Table 9: POS tagging accuracy and speed.</S>
			<S sid ="181" ssid = "53">The maximum in each column is bold.</S>
			<S sid ="182" ssid = "54">The baseline for speed in all cases is the unconstrained tagger using (Ratnaparkhi, 1996)’s feature and conducting a beam (=5) search.</S>
			<S sid ="183" ssid = "55">on the development set, we set beam-width of our baseline model as 5.</S>
			<S sid ="184" ssid = "56">Our baseline model, which uses Ratnaparkhi (1996)’s feature set and conducts a beam (=5) search in the unconstrained space, achieves a tagging accuracy of 97.16%.</S>
			<S sid ="185" ssid = "57">Tagging accuracy is measured by the percentage of correct predictions out of all gold predictions.</S>
			<S sid ="186" ssid = "58">We consider the speed of our baseline model as 1×, and compareother taggers with this one.</S>
			<S sid ="187" ssid = "59">The speed of a POS tag ger is measured by the number of input words processed per second.</S>
			<S sid ="188" ssid = "60">As shown in Table 9, when the beam-width is reduced from 5 to 1, the tagger (beam=1) is 3 times faster but tagging accuracy is badly hurt.</S>
			<S sid ="189" ssid = "61">In contrast, when searching in a constrained space rather than the raw space, the constrained tagger (beam=5) is 10 times fast as the baseline and the tagging accuracy is even moderately improved, increasing to 97.20%.</S>
			<S sid ="190" ssid = "62">When we evaluate the speed of a constrained tag- ger, the time of decoding deterministic constraints is included.</S>
			<S sid ="191" ssid = "63">These constraints make more accurate predictions than probabilistic models, thus besides improving the overall tagging speed as we expect, tagging accuracy also improves by a little.</S>
			<S sid ="192" ssid = "64">In Viterbi decoding, all possible transitions between two neighbour states are evaluated, so the addition of locally lookahead features may have NO impact on performance.</S>
			<S sid ="193" ssid = "65">When beam-width is set to 5, tagging accuracy is not improved by the use of Feature B in (Shen et al., 2007); and because the size of the feature model grows, efficiency is hurt.</S>
			<S sid ="194" ssid = "66">On the other hand, when lookahead features are used, Viterbi-style decoding is less affected by the reduction of beam-width.</S>
			<S sid ="195" ssid = "67">As compared to the con strained greedy tagger using Ratnaparkhi (1996)’s feature set, with the additional use of three locally lookahead feature templates, tagging accuracy is increased from 96.80% to 97.02%.</S>
			<S sid ="196" ssid = "68">When no further data is used other than training data, the bidirectional tagger described in (Shen et al., 2007) achives an accuracy of 97.33%, using a much richer feature set (E) than feature set B, the one we compare with here.</S>
			<S sid ="197" ssid = "69">As noted above, the addition of three feature templates already has a notable negative impact on efficiency, thus the use of feature set E will hurt tagging efficiency much worse.</S>
			<S sid ="198" ssid = "70">Rich feature sets are also widely used in other work that pursue state-of-art tagging accuracy, e.g.</S>
			<S sid ="199" ssid = "71">(Toutanova et al., 2003).</S>
			<S sid ="200" ssid = "72">In this work, we focus on the most compact feature sets, since tagging efficiency is our main consideration in our work on POS taging.</S>
			<S sid ="201" ssid = "73">The proposed constrained taggers as described above can achieve near state-of-art POS tagging accuracy in a much more efficient manner.</S>
			<S sid ="202" ssid = "74">5.4 Chinese word segmentation.</S>
			<S sid ="203" ssid = "75">Like other tagging problems, Viterbi-style decoding is widely used for character tagging for CWS.</S>
			<S sid ="204" ssid = "76">We transform tagged character sequences to word segmentations first, and then evaluate word segmentations by F-measure, as defined in Section 5.2.</S>
			<S sid ="205" ssid = "77">We proposed an ILP formulation of the CWS problem in Section 3.3, where we present a word- based model.</S>
			<S sid ="206" ssid = "78">In Section 3.4, we describe a way of mapping words to a character-based feature space.</S>
			<S sid ="207" ssid = "79">From this view, the highest scoring tagging sequence is computed subject to structural constraints, giving us an inference alternative to Viterbi decoding.</S>
			<S sid ="208" ssid = "80">For example, recall the example of input character sequence c = c1c2 discussed in Section 3.3.</S>
			<S sid ="209" ssid = "81">The two possible ILP solutions give two possible segmentations {c1, c2} and {c1c2}, thus there are 2 tag se quences evaluated by ILP, BB and BI.</S>
			<S sid ="210" ssid = "82">On the other hand, there are 4 tag sequences evaluated by Viterbi decoding: BI, BB, IB and II.</S>
			<S sid ="211" ssid = "83">With the same feature templates as described in Section 3.1, we now compare these two decoding methods.</S>
			<S sid ="212" ssid = "84">Tagset BMES is used for character tagging as well as for mapping words to character-based feature space.</S>
			<S sid ="213" ssid = "85">We use the same Viterbi decoder as implemented for English POS tagging and use a noncommercial ILP solver included in GNU Linear ProF m ea su re av g. |x| #c h ar p er s e c. r a w 0 . 9 7 4 1 2 9 0.</S>
			<S sid ="214" ssid = "86">4 1 1 3 ( 1 × ) co nst rai ne d 0 . 9 7 4 8 3 . 7 5 12 19 0 (1 07 ×) Table 11: ILP problem size and segmentation speed.</S>
			<S sid ="215" ssid = "87">Table 10: F-measure on Chinese word segmentation.</S>
			<S sid ="216" ssid = "88">Only character-based features are used.</S>
			<S sid ="217" ssid = "89">POS-/+: perceptron trained without/with POS.</S>
			<S sid ="218" ssid = "90">gramming Kit (GLPK), version 4.3.</S>
			<S sid ="219" ssid = "91">2 As shown in Table 10, optimal solutions returned by an ILP solver are more accurate than optimal solutions returned by a Viterbi decoder.</S>
			<S sid ="220" ssid = "92">The F-measure is improved by a relative error reduction of 18.8%, from 0.968 to 0.974.</S>
			<S sid ="221" ssid = "93">These results are compared to the core perceptron trained without POS in (Jiang et al., 2008a).</S>
			<S sid ="222" ssid = "94">They only report results with ’lexical-target’ features, a richer feature set than the one we use here.</S>
			<S sid ="223" ssid = "95">As shown in Table 10, we achieve higher performance even with more compact features.</S>
			<S sid ="224" ssid = "96">Joint inference of CWS and Chinese POS tagging is popularly studied in recent work, e.g.</S>
			<S sid ="225" ssid = "97">(Ng and Low, 2004), (Jiang et al., 2008a), and (Kruengkrai et al., 2009).</S>
			<S sid ="226" ssid = "98">It has been shown that better performance can be achieved with joint inference, e.g. F-measure 0.978 by the cascaded model in (Jiang et al., 2008a).</S>
			<S sid ="227" ssid = "99">We focus on the task of word segmentation only in this work and show that a comparable F-measure is achievable in a much more efficient manner.</S>
			<S sid ="228" ssid = "100">Sun (2011) uses the stacked learning technique to merge different levels of predictors, obtaining a combined system that beats individual ones.</S>
			<S sid ="229" ssid = "101">Word-based features can be easily incorporated, since the ILP formulation is more naturally viewed as a word-based model.</S>
			<S sid ="230" ssid = "102">We extend character-based features with the word count features as described in Section 3.4.</S>
			<S sid ="231" ssid = "103">Currently, we only use word counts computed from training data, i.e. still a closed test.</S>
			<S sid ="232" ssid = "104">The addition of these features makes a moderate improvement on the F-measure, from 0.974 to 0.975.</S>
			<S sid ="233" ssid = "105">As discussed in Section 3.3, if we are able to determine that some characters always start new words, the number of possible words is reduced, i.e. the number of variables in an ILP solution is reduced.</S>
			<S sid ="234" ssid = "106">As shown in Table 11, when character se 2 http://www.gnu.org/software/glpk quences are partially tagged by deterministic constraints, the number of possible words per sentence, i.e. avg.</S>
			<S sid ="235" ssid = "107">|x|, is reduced from 1290.4 to 83.7.</S>
			<S sid ="236" ssid = "108">This re duction of ILP problem size has a very important im pact on the efficiency.</S>
			<S sid ="237" ssid = "109">As shown in Table 11, when taking constrained input, the segmentation speed is increased by 107 times over taking raw input, from 113 characters per second to 12,190 characters per second on a dual-core 3.0HZ CPU.</S>
			<S sid ="238" ssid = "110">Deterministic constraints predicting IB tags are only used here for constraining possible words.</S>
			<S sid ="239" ssid = "111">They are very accurate as shown in Section 5.2.</S>
			<S sid ="240" ssid = "112">Few gold predictions are missed from the.</S>
			<S sid ="241" ssid = "113">constrained set of possible words.</S>
			<S sid ="242" ssid = "114">As shown in Table 11, F-measure is not affected by applying these constraints, while the efficiency is significantly improved.</S>
	</SECTION>
	<SECTION title="Conclusion and future. " number = "6">
			<S sid ="243" ssid = "1">work We have shown by experiments that large number of deterministic constraints can be learned from training examples, as long as the proper representation is used.</S>
			<S sid ="244" ssid = "2">These deterministic constraints are very useful in constraining probabilistic search, for example, they may be directly used for determining predictions as in English POS tagging, or used for reducing the number of variables in an ILP solution as in Chinese word segmentation.</S>
			<S sid ="245" ssid = "3">The most notable advantage in using these constraints is the increased efficiency.</S>
			<S sid ="246" ssid = "4">The two applications are both well-studied; there isn’t much space for improving accuracy.</S>
			<S sid ="247" ssid = "5">Even so, we have shown that as tested with the same feature set for CWS, the proposed ILP formulation significantly improves the F-measure as compared to Viterbi decoding.</S>
			<S sid ="248" ssid = "6">These two simple applications suggest that it is of interest to explore data-driven deterministic constraints learnt from training examples.</S>
			<S sid ="249" ssid = "7">There are more interesting ways in applying these constraints, which we are going to study in future work.</S>
	</SECTION>
</PAPER>
