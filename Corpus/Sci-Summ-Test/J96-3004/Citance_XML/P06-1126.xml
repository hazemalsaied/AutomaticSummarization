<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a discriminative pruning method of n-gram language model for Chinese word segmentation.</S>
		<S sid ="2" ssid = "2">To reduce the size of the language model that is used in a Chinese word segmentation system, importance of each bigram is computed in terms of discriminative pruning criterion that is related to the performance loss caused by pruning the bi- gram.</S>
		<S sid ="3" ssid = "3">Then we propose a step-by-step growing algorithm to build the language model of desired size.</S>
		<S sid ="4" ssid = "4">Experimental results show that the discriminative pruning method leads to a much smaller model compared with the model pruned using the state-of-the-art method.</S>
		<S sid ="5" ssid = "5">At the same Chinese word segmentation F-measure, the number of bigrams in the model can be reduced by up to 90%.</S>
		<S sid ="6" ssid = "6">Correlation between language model perplexity and word segmentation performance is also discussed.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">Chinese word segmentation is the initial stage of many Chinese language processing tasks, and has received a lot of attention in the literature (Sproat et al., 1996; Sun and Tsou, 2001; Zhang et al., 2003; Peng et al., 2004).</S>
			<S sid ="8" ssid = "8">In Gao et al.</S>
			<S sid ="9" ssid = "9">(2003), an approach based on source-channel model for Chinese word segmentation was proposed.</S>
			<S sid ="10" ssid = "10">Gao et al.</S>
			<S sid ="11" ssid = "11">(2005) further developed it to a linear mixture model.</S>
			<S sid ="12" ssid = "12">In these statistical models, language models are essential for word segmentation disambiguation.</S>
			<S sid ="13" ssid = "13">However, an uncom pressed language model is usually too large for practical use since all realistic applications have memory constraints.</S>
			<S sid ="14" ssid = "14">Therefore, language model pruning techniques are used to produce smaller models.</S>
			<S sid ="15" ssid = "15">Pruning a language model is to eliminate a number of parameters explicitly stored in it, according to some pruning criteria.</S>
			<S sid ="16" ssid = "16">The goal of research for language model pruning is to find criteria or methods, using which the model size could be reduced effectively, while the performance loss is kept as small as possible.</S>
			<S sid ="17" ssid = "17">A few criteria have been presented for language model pruning, including count cutoff (Jelinek, 1990), weighted difference factor (Seymore and Rosenfeld, 1996), KullbackLeibler distance (Stolcke, 1998), rank and entropy (Gao and Zhang, 2002).</S>
			<S sid ="18" ssid = "18">These criteria are general for language model pruning, and are not optimized according to the performance of language model in specific tasks.</S>
			<S sid ="19" ssid = "19">In recent years, discriminative training has been introduced to natural language processing applications such as parsing (Collins, 2000), machine translation (Och and Ney, 2002) and language model building (Kuo et al., 2002; Roark et al., 2004).</S>
			<S sid ="20" ssid = "20">To the best of our knowledge, it has not been applied to language model pruning.</S>
			<S sid ="21" ssid = "21">In this paper, we propose a discriminative pruning method of n-gram language model for Chinese word segmentation.</S>
			<S sid ="22" ssid = "22">It differentiates from the previous pruning approaches in two respects.</S>
			<S sid ="23" ssid = "23">First, the pruning criterion is based on performance variation of word segmentation.</S>
			<S sid ="24" ssid = "24">Second, the model of desired size is achieved by adding valuable bigrams to a base model, instead of by pruning bigrams from an unpruned model.</S>
			<S sid ="25" ssid = "25">We define a misclassification function that approximately represents the likelihood that a sentence will be incorrectly segmented.</S>
			<S sid ="26" ssid = "26">The 1001 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1001–1008, Sydney, July 2006.</S>
			<S sid ="27" ssid = "27">Qc 2006 Association for Computational Linguistics variation value of the misclassification function caused by adding a parameter to the base model is used as the criterion for model pruning.</S>
			<S sid ="28" ssid = "28">We called relative entropy or KullbackLeibler distance.</S>
			<S sid ="29" ssid = "29">It is computed as: also suggest a step-by-step growing algorithm − ∑ P( wi , h j )[log P ′( wi | h j ) − log P ( wi | h j )] w ,h (2) that can generate models of any reasonably de- i j sired size.</S>
			<S sid ="30" ssid = "30">We take the pruning method based on KullbackLeibler distance as the baseline.</S>
			<S sid ="31" ssid = "31">Experimental results show that our method outperforms the baseline significantly with small model size.</S>
			<S sid ="32" ssid = "32">With the F-Measure of 96.33%, number of bigrams decreases by up to 90%.</S>
			<S sid ="33" ssid = "33">In addition, by combining the discriminative pruning method with the baseline method, we obtain models that achieve better performance for any model size.</S>
			<S sid ="34" ssid = "34">Correlation between language model perplexity and system performance is also discussed.</S>
			<S sid ="35" ssid = "35">The remainder of the paper is organized as follows.</S>
			<S sid ="36" ssid = "36">Section 2 briefly discusses the related work on language model pruning.</S>
			<S sid ="37" ssid = "37">Section 3 proposes our discriminative pruning method for Chinese word segmentation.</S>
			<S sid ="38" ssid = "38">Section 4 describes the experimental settings and results.</S>
			<S sid ="39" ssid = "39">Result analysis and discussions are also presented in this section.</S>
			<S sid ="40" ssid = "40">We draw the conclusions in section 5.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="41" ssid = "1">A simple way to reduce the size of an n-gram language model is to exclude those n-grams occurring infrequently in training corpus.</S>
			<S sid ="42" ssid = "2">It is named as count cutoff method (Jelinek, 1990).</S>
			<S sid ="43" ssid = "3">Because counts are always integers, the size of the model can only be reduced to discrete values.Gao and Lee (2000) proposed a distribution The sum is over all words wi and histories hj.</S>
			<S sid ="44" ssid = "4">This criterion removes some of the approximations employed in Seymore and Rosenfeld (1996).</S>
			<S sid ="45" ssid = "5">In addition, Stolcke (1998) presented a method for efficient computation of the Kull- back-Leibler distance of each n-gram.</S>
			<S sid ="46" ssid = "6">In Gao and Zhang (2002), three measures are studied for the purpose of language model pruning.</S>
			<S sid ="47" ssid = "7">They are probability, rank, and entropy.</S>
			<S sid ="48" ssid = "8">Among them, probability is very similar to that proposed by Seymore and Rosenfeld (1996).</S>
			<S sid ="49" ssid = "9">Gao and Zhang (2002) also presented a method of combining two criteria, and showed the combination of rank and entropy achieved the smallest models.</S>
	</SECTION>
	<SECTION title="Discriminative  Pruning  for. " number = "3">
			<S sid ="50" ssid = "1">Chinese Word Segmentation 3.1 Problem.</S>
			<S sid ="51" ssid = "2">Definition In this paper, discussions are restricted to bigram language model P(wy|wx).</S>
			<S sid ="52" ssid = "3">In a bigram model, three kinds of parameters are involved: bigram probability Pm(wy|wx) for seen bigram wxwy in training corpus, unigram probability Pm(w) and backoff coefficient αm(w) for any word w. For any wx and wy in the vocabulary, bigram probability P(wy|wx) is computed as: based pruning.</S>
			<S sid ="53" ssid = "4">Instead of pruning n-grams that ⎧ Pm (wy | wx ) if c(wx , wy ) &gt; 0 are infrequent in training data, they prune n- grams that are likely to be infrequent in a new P(wy | wx ) = ⎨ ⎩ m (w x ) × Pm ( wy ) if c(wx , wy ) = 0 (3) document.</S>
			<S sid ="54" ssid = "5">Experimental results show that it is better than traditional count cutoff method.</S>
			<S sid ="55" ssid = "6">Seymore and Rosenfeld (1996) proposed a method to measure the difference of the models before and after pruning each n-gram, and the difference is computed as: As equation (3) shows, the probability of an unseen bigram is computed by the product of the unigram probability and the corresponding back- off coefficient.</S>
			<S sid ="56" ssid = "7">If we remove a seen bigram from the model, we can still yield a bigram probability for it, by regarding it as an unseen bigram.</S>
			<S sid ="57" ssid = "8">Thus, − N (h j , wi ) × [log P ′( wi | h j ) − log P( wi | h j )] (1) we can reduce the number of bigram probabilities explicitly stored in the model.</S>
			<S sid ="58" ssid = "9">By doing this, Where P(wi|hj) denotes the conditional probabilities assigned by the original model, and P′(wi|hj) denotes the probabilities in the pruned model.</S>
			<S sid ="59" ssid = "10">N(hj, wi) is the discounted frequency of n- gram event hjwi.</S>
			<S sid ="60" ssid = "11">Seymore and Rosenfeld (1996) showed that this method is more effective than the traditional cutoff method.</S>
			<S sid ="61" ssid = "12">Stolcke (1998) presented a more sound criterion for computing the difference of models before and after pruning each n-gram, which is model size decreases.</S>
			<S sid ="62" ssid = "13">This is the foundation for bigram model pruning.</S>
			<S sid ="63" ssid = "14">The research issue is to find an effective criterion to compute &quot;importance&quot; of each bigram.</S>
			<S sid ="64" ssid = "15">Here, &quot;importance&quot; indicates the performance loss caused by pruning the bigram.</S>
			<S sid ="65" ssid = "16">Generally,given a target model size, the method for lan guage model pruning is described in Figure 1.</S>
			<S sid ="66" ssid = "17">In fact, deciding which bigrams should be excluded from the model is equivalent to deciding which bigrams should be included in the model.</S>
			<S sid ="67" ssid = "18">Hence, we suggest a growing algorithm through g(S,W ; Λ, Γ) = log P(W ) + log P(S | W ) (5) which a model of desired size can also be achieved.</S>
			<S sid ="68" ssid = "19">It is illustrated in Figure 2.</S>
			<S sid ="69" ssid = "20">Here, two terms are introduced.</S>
			<S sid ="70" ssid = "21">Full-bigram model is the unpruned model containing all seen bigrams in training corpus.</S>
			<S sid ="71" ssid = "22">And base model is currently the unigram model.For the discriminative pruning method sug Where Г denotes a language model that is used to compute P(W), and Λ denotes a generative model that is used to compute P(S|W).</S>
			<S sid ="72" ssid = "23">In language model pruning, Λ is an invariable.</S>
			<S sid ="73" ssid = "24">The discriminative pruning criterion is inspired by the comparison of segmented sentences using full-bigram model ГF and using base model gested in this paper, growing algorithm instead ГB.B Given a sentence S, full-bigram model of pruning algorithm is applied to generate the chooses W * as the segmentation result, and base model of desired size.</S>
			<S sid ="74" ssid = "25">In addition, &quot;importance&quot; of each bigram indicates the performance improvement caused by adding a bigram into the model chooses * satisfying: as the segmentation result, base model.</S>
			<S sid ="75" ssid = "26">W * = arg max g (S ,W ; Λ, Γ ) (6) F F W 1.</S>
			<S sid ="76" ssid = "27">Given the desired model size, compute.</S>
			<S sid ="77" ssid = "28">W * = arg max g(S ,W ; Λ, Γ ) (7) the number of bigrams that should be B W B pruned.</S>
			<S sid ="78" ssid = "29">The number is denoted as m; 2.</S>
			<S sid ="79" ssid = "30">Compute &quot;importance&quot; of each bigram;.</S>
			<S sid ="80" ssid = "31">3.</S>
			<S sid ="81" ssid = "32">Sort all bigrams in the language model,.</S>
			<S sid ="82" ssid = "33">according to their &quot;importance&quot;; Here, given a language model Г, we define a misclassification function representing the differ ence between discriminant functions of W * and</S>
	</SECTION>
	<SECTION title="Remove m most &quot;unimportant&quot; bigrams. " number = "4">
			<S sid ="83" ssid = "1">from the model;</S>
	</SECTION>
	<SECTION title="Re-compute backoff coefficients in the. " number = "5">
			<S sid ="84" ssid = "1">W * : d(S;Λ,Γ) = g(S,W*;Λ,Γ) − g(S,W*;Λ,Γ) (8) B F model.</S>
			<S sid ="85" ssid = "2">The misclassification function reflects which Figure 1.</S>
			<S sid ="86" ssid = "3">Language Model Pruning Algorithm one of W * and W * is inclined to be chosen as the segmentation result.</S>
			<S sid ="87" ssid = "4">If W * ≠ W * , we may 1.</S>
			<S sid ="88" ssid = "5">Given the desired model size, compute F B. the number of bigrams that should be added into the base model.</S>
			<S sid ="89" ssid = "6">The number is denoted as n; 2.</S>
			<S sid ="90" ssid = "7">Compute &quot;importance&quot; of each bigram.</S>
			<S sid ="91" ssid = "8">included in the full-bigram model but excluded from the base model; 3.</S>
			<S sid ="92" ssid = "9">Sort the bigrams according to their &quot;im-.</S>
			<S sid ="93" ssid = "10">portance&quot;; extract some hints from the comparison of them, and select a few valuable bigrams.</S>
			<S sid ="94" ssid = "11">By adding these bigrams to base model, we should make the model choose the correct answer between W * and W * . If W * = W * , no hints can be extracted.</S>
			<S sid ="95" ssid = "12">B F B Let W0 be the known correct word sequence.</S>
			<S sid ="96" ssid = "13">Under the precondition W * ≠ W * , we describe F B 4.</S>
			<S sid ="97" ssid = "14">Add n most &quot;important&quot; bigrams into.</S>
			<S sid ="98" ssid = "15">our method in the following three cases.</S>
			<S sid ="99" ssid = "16">the base model; W * = W W * ≠ W 5.</S>
			<S sid ="100" ssid = "17">Re-compute backoff coefficients in the.</S>
			<S sid ="101" ssid = "18">Case 1: F 0 and B 0 base model.</S>
			<S sid ="102" ssid = "19">Figure 2.</S>
			<S sid ="103" ssid = "20">Growing Algorithm for Language Model Pruning 3.2 Discriminative Pruning Criterion.</S>
			<S sid ="104" ssid = "21">Given a Chinese character string S, a word segmentation system chooses a sequence of words W* as the segmentation result, satisfying: Here, full-bigram model chooses the correct answer, while base model does not.</S>
			<S sid ="105" ssid = "22">Based on equation (6), (7) and (8), we know that d(S;Λ,ГB)&gt; 0 and d(S;Λ,ГF) &lt; 0.</S>
			<S sid ="106" ssid = "23">It implies that adding bigrams into base model may lead the misclassifi cation function from positive to negative.</S>
			<S sid ="107" ssid = "24">Which bigram should be added depends on the variation of misclassification function caused by adding it.</S>
			<S sid ="108" ssid = "25">If adding a bigram makes the misclassification W * = arg max(log P(W ) + log P(S |W )) W (4) function become smaller, it should be added with higher priority.</S>
			<S sid ="109" ssid = "26">The sum of the two logarithm probabilities in We add each bigram individually to ГB,B and equation (4) is called discriminant function: then compute the variation of the misclassifica tion function.</S>
			<S sid ="110" ssid = "27">Let Г′ denotes the model after add ing bigram wxwy into ГBB.</S>
			<S sid ="111" ssid = "28">According to equation (5) and (8), we can write the misclassification provement caused by adding wxwy.</S>
			<S sid ="112" ssid = "29">Thus, &quot;importance&quot; of bigram wxwy on S is computed as: function using ГBB and Г′ separately: imp(wx wy ; S ) = Δd (S; wx wy ) (14) d ( S; Λ, Γ ) = log P (W * ) + log P (S | W * ) B B B Λ B (9) Case 2: W * ≠ W and W * = W − log P (W * ) − log P (S | W * ) F 0 B 0 B F Λ F d (S; Λ, Γ′) = log P′(W * ) + log P (S | W * ) Here, it is just contrary to case 1.</S>
			<S sid ="113" ssid = "30">In this way, we have: B Λ B (10) − log P′(W * ) − log P (S |W * ) F Λ F imp(wx w y ; S ) = −Δd (S; wx wy ) (15) Where PB(.), P′(.), PΛ(.)</S>
			<S sid ="114" ssid = "31">represent probabilities * * in base model, model Г′ and model Λ separately.</S>
			<S sid ="115" ssid = "32">The variation of the misclassification function is computed as: Δd (S; wx wy ) = d (S; Λ, ΓB ) − d (S; Λ, Γ′) Case 3: WF ≠ W0 ≠ WB In case 1 and 2, bigrams are added so that discriminant function of correct word sequence becomes bigger, and that of incorrect word sequence becomes smaller.</S>
			<S sid ="116" ssid = "33">In case 3, both W * and = [log P′(W * ) − log P (W * )] (11) * F B F − [log P′(W * ) − log P (W * )] B B B Because the only difference between base model and model Г′ is that model Г′ involves the bigram probability P′(wy|wx), we have: logP′(W* ) − logP (W * ) WB are incorrect.</S>
			<S sid ="117" ssid = "34">Thus, the misclassification function in equation (8) does not represent the likelihood that S will be incorrectly segmented.</S>
			<S sid ="118" ssid = "35">Therefore, variation of the misclassification function in equation (13) can not be used to measure the &quot;importance&quot; of a bigram.</S>
			<S sid ="119" ssid = "36">Here, sentence S is ignored, and the &quot;importance&quot; of all F B ∑ F (i) F F (i−1) B F (i) F (i−1) bigrams on S are zero.</S>
			<S sid ="120" ssid = "37">= [logP′(w* i | w* ) − logP (w* | w* ] (12) The above three cases are designed for one = n(W* , w w )[logP′(w | w ) − logP (w ) sentence.</S>
			<S sid ="121" ssid = "38">The &quot;importance&quot; of each bigram on F x y y x B y the whole training corpus is the sum of its &quot;im − logαB (wx )] portance&quot; on each single sentence, as equation (16) shows.</S>
			<S sid ="122" ssid = "39">Where n(W * , w w ) denotes the number of F x y times the bigram wxwy appears in sequence W * . x y ∑ x y imp(w w ) = F imp(w w ; S) S (16) Note that in equation (12), base model is treated as a bigram model instead of a unigram model.</S>
			<S sid ="123" ssid = "40">The reason lies in two respects.</S>
			<S sid ="124" ssid = "41">First, the uni- gram model can be regarded as a particular bi- gram model by setting all backoff coefficients to 1.</S>
			<S sid ="125" ssid = "42">Second, the base model is not always a uni-.</S>
			<S sid ="126" ssid = "43">To sum up, the &quot;importance&quot; of each bigram is computed as Figure 3 shows.</S>
			<S sid ="127" ssid = "44">1.</S>
			<S sid ="128" ssid = "45">For each wxwy, set imp(wxwy) = 0;.</S>
			<S sid ="129" ssid = "46">2.</S>
			<S sid ="130" ssid = "47">For each sentence in training corpus:.</S>
			<S sid ="131" ssid = "48">For each wxwy: * = W * ≠ Wgram model during the step-by-step growing al if WF 0 and WB 0 :gorithm, which will be discussed in the next sub section.</S>
			<S sid ="132" ssid = "49">imp(wxwy) += Δd(S;wxwy); * W * WIn fact, bigram probability P′(wy|wx) is ex else if WF ≠ 0 and WB = 0 : tracted from full-bigram model, so P′(wy|wx) = PF(wy|wx).</S>
			<S sid ="133" ssid = "50">In addition, similar deductions can be conducted to the second bracket in equation (11).</S>
			<S sid ="134" ssid = "51">Thus, we have: imp(wxwy) −= Δd(S;wxwy); Figure 3.</S>
			<S sid ="135" ssid = "52">Calculation of &quot;Importance&quot; of Bigrams Δd (S; w w ) = [n(W * , w w ) − n(W * , w w )] We illustrate the process of computing &quot;im x y F x y B x y (13) portance&quot; of bigrams with a simple example.</S>
			<S sid ="136" ssid = "53">× [log PF (wy | wx ) − log PB (wy ) − logα B (wx )] Note that d(S;Λ,Г) approximately indicates the likelihood that S will be incorrectly segmented, so Δd(S;wxwy) represents the performance im Suppose S is &quot; 这 (zhe4) 样 (yang4) 才 (cai2) 能 (neng2) 更 (geng4) 方 (fang1) 便 (bian4)&quot;.</S>
			<S sid ="137" ssid = "54">The segmented result using full-bigram model is &quot;这 样(zhe4yang4)/才(cai2)/能(neng2)/更(geng4)/方 便(fang1bian4)&quot;, which is the correct word se quence.</S>
			<S sid ="138" ssid = "55">The segmented result using base model is &quot; 这样 (zhe4yang4)/ 才能 (cai2neng2)/ 更 (geng4)/ 方 便 (fang1bian4)&quot;.</S>
			<S sid ="139" ssid = "56">Obviously, it matches case 1.</S>
			<S sid ="140" ssid = "57">For bigram &quot;这样(zhe4yang4)才 (cai2)&quot;, it occurs in W * once, and does not occur in W * . According to equation (13), its &quot;importance&quot; on sentence S is: imp(这样(zhe4yang4)才(cai2);S) = logPF(才(cai2)|这样(zhe4yang4)) − [logPB(B 才(cai2)) + logαBB(这样(zhe4yang4))] For bigram &quot; 更 (geng4) 方便 (fang1bian4)&quot;, 4 Experiments.</S>
			<S sid ="141" ssid = "58">4.1 Experiment Settings.</S>
			<S sid ="142" ssid = "59">The training corpus comes from People&apos;s daily 2000, containing about 25 million Chinese characters.</S>
			<S sid ="143" ssid = "60">It is manually segmented into word sequences, according to the word segmentation specification of Peking University (Yu et al., 2003).</S>
			<S sid ="144" ssid = "61">The testing text that is provided by Peking University comes from the second international Chinese word segmentation bakeoff organized by SIGHAN.</S>
			<S sid ="145" ssid = "62">The testing text is a part of People&apos;s daily 2001, consisting of about 170K Chi since it occurs once both in W * &quot;importance&quot; on S is zero.</S>
			<S sid ="146" ssid = "63">3.3 Step-by-step Growing.</S>
			<S sid ="147" ssid = "64">and W * , its nese characters.</S>
			<S sid ="148" ssid = "65">The vocabulary is automatically extracted from the training corpus, and the words occurring only once are removed.</S>
			<S sid ="149" ssid = "66">Finally, about 67K words are included in the vocabulary.</S>
			<S sid ="150" ssid = "67">The full Given the target model size, we can add exact number of bigrams to the base model at one time by using the growing algorithm illustrated in Figure 2.</S>
			<S sid ="151" ssid = "68">But it is more suitable to adopt a step- by-step growing algorithm illustrated in Figure 4.</S>
			<S sid ="152" ssid = "69">As shown in equation (13), the &quot;importance&quot; of each bigram depends on the base model.</S>
			<S sid ="153" ssid = "70">Initially, the base model is set to the unigram model.</S>
			<S sid ="154" ssid = "71">With bigrams added in, it becomes a growing bigram model and the unigram model are trained by CMU language model toolkit (Clarkson and Rosenfeld, 1997).</S>
			<S sid ="155" ssid = "72">Without any count cutoff, the full-bigram model contains about 2 million bi- grams.</S>
			<S sid ="156" ssid = "73">The word segmentation system is developed based on a source-channel model similar to that described in (Gao et al., 2003).</S>
			<S sid ="157" ssid = "74">Viterbi algorithm is applied to find the best word segmentation bigram model.</S>
			<S sid ="158" ssid = "75">Thus, * and log α B (wx ) will path.</S>
			<S sid ="159" ssid = "76">change.</S>
			<S sid ="160" ssid = "77">So, the added bigrams will affect the calculation of &quot;importance&quot; of bigrams to be added.</S>
			<S sid ="161" ssid = "78">Generally, adding more bigrams at one time will lead to more negative impacts.</S>
			<S sid ="162" ssid = "79">Thus, it is expected that models produced by step-by-step growing algorithm may achieve better performance than growing algorithm, and smaller step size will lead to even better performance.</S>
			<S sid ="163" ssid = "80">1.</S>
			<S sid ="164" ssid = "81">Given step size s;.</S>
			<S sid ="165" ssid = "82">2.</S>
			<S sid ="166" ssid = "83">Set the base model to be the unigram.</S>
			<S sid ="167" ssid = "84">model; 3.</S>
			<S sid ="168" ssid = "85">Segment corpus with full-bigram model;.</S>
			<S sid ="169" ssid = "86">4.2 Evaluation Metrics.</S>
			<S sid ="170" ssid = "87">The language models built in our experiments are evaluated by two metrics.</S>
			<S sid ="171" ssid = "88">One is F-Measure of the word segmentation result; the other is language model perplexity.</S>
			<S sid ="172" ssid = "89">For F-Measure evaluation, we firstly segment the raw testing text using the model to be evaluated.</S>
			<S sid ="173" ssid = "90">Then, the segmented result is evaluated by comparing with the gold standard set.</S>
			<S sid ="174" ssid = "91">The evaluation tool is also from the word segmentation bakeoff.</S>
			<S sid ="175" ssid = "92">F-Measure is calculated as: 4.</S>
			<S sid ="176" ssid = "93">Segment corpus with base model;.</S>
			<S sid ="177" ssid = "94">5.</S>
			<S sid ="178" ssid = "95">Compute &quot;importance&quot; of each bigram.</S>
			<S sid ="179" ssid = "96">F-Measure = 2 × Precision × Recall Precision + Recall (17) included in the full-bigram model but excluded from the base model;</S>
	</SECTION>
	<SECTION title="Sort the bigrams according to their &quot;im-. " number = "6">
			<S sid ="180" ssid = "1">portance&quot;;</S>
	</SECTION>
	<SECTION title="Add s bigrams with the biggest &quot;impor-. " number = "7">
			<S sid ="181" ssid = "1">tance&quot; to the base model; For perplexity evaluation, the language model to be evaluated is used to provide the bigram probabilities for each word in the testing text.</S>
			<S sid ="182" ssid = "2">The perplexity is the mean logarithm probability as shown in equation (18): 1 N</S>
	</SECTION>
	<SECTION title="Re-compute backoff coefficients in the. " number = "8">
			<S sid ="183" ssid = "1">base model;</S>
	</SECTION>
	<SECTION title="If the base model is still smaller than the. " number = "9">
			<S sid ="184" ssid = "1">desired size, go to step 4; otherwise, stop.</S>
			<S sid ="185" ssid = "2">− ∑ log2 P ( wi |wi −1 ) PP( M ) = 2 N i =1 4.3 Comparison of Pruning Methods.</S>
			<S sid ="186" ssid = "3">(18) Figure 4.</S>
			<S sid ="187" ssid = "4">Step-by-step Growing Algorithm The KullbackLeibler Distance (KLD) based method is the state-of-the-art method, and is taken as the baseline 1 . Pruning algorithm illustrated in Figure 1 is used for KLD based pruning.</S>
			<S sid ="188" ssid = "5">Growing algorithms illustrated in Figure 2 and Figure 4 are used for discriminative pruning method.</S>
			<S sid ="189" ssid = "6">Growing algorithms are not applied to KLD based pruning, because the computation of KLD is independent of the base model.</S>
			<S sid ="190" ssid = "7">At step 1 for KLD based pruning, m is set to produce ten models containing 10K, 20K, …, 100K bigrams.</S>
			<S sid ="191" ssid = "8">We apply each of the models to the word segmentation system, and evaluate the segmented results with the evaluation tool.</S>
			<S sid ="192" ssid = "9">The F-Measures of the ten models are illustrated in Figure 5, denoted by &quot;KLD&quot;.</S>
			<S sid ="193" ssid = "10">For the discriminative pruning criterion, the growing algorithm illustrated in Figure 2 is firstly used.</S>
			<S sid ="194" ssid = "11">Unigram model acts as the base model.</S>
			<S sid ="195" ssid = "12">At step 1, n is set to 10K, 20K, …, 100K separately.</S>
			<S sid ="196" ssid = "13">At step 2, &quot;importance&quot; of each bi- gram is computed following Figure 3.</S>
			<S sid ="197" ssid = "14">Ten models are produced and evaluated.</S>
			<S sid ="198" ssid = "15">The F-Measures are also illustrated in Figure 5, denoted by &quot;Dis- crim&quot;.</S>
			<S sid ="199" ssid = "16">By adding bigrams step by step as illustrated in Figure 4, and setting step size to 10K, 5K, and 2K separately, we obtain other three series of models, denoted by &quot;Step-10K&quot;, &quot;Step-5K&quot; and &quot;Step-2K&quot; in Figure 5.</S>
			<S sid ="200" ssid = "17">We also include in Figure 5 the performance of the count cutoff method.</S>
			<S sid ="201" ssid = "18">Obviously, it is inferior to other methods.</S>
			<S sid ="202" ssid = "19">96.6 such as those models containing less than 70K bigrams, the performance of &quot;Discrim&quot; is better than &quot;KLD&quot;.</S>
			<S sid ="203" ssid = "20">For the models containing more than 70K bigrams, &quot;KLD&quot; gets better performance than &quot;Discrim&quot;.</S>
			<S sid ="204" ssid = "21">The reason is that the added bigrams affect the calculation of &quot;importance&quot; of bigrams to be added, which has been discussed in section 3.3.</S>
			<S sid ="205" ssid = "22">If we add the bigrams step by step, better performance is achieved.</S>
			<S sid ="206" ssid = "23">From Figure 5, it can be seen that all of the models generated by step-by- step growing algorithm outperform &quot;KLD&quot; and &quot;Discrim&quot; consistently.</S>
			<S sid ="207" ssid = "24">Compared with the baseline KLD based method, step-by-step growing methods result in at least 0.2 percent improvement for each model size.Comparing &quot;Step-10K&quot;, &quot;Step-5K&quot; and &quot;Step 2K&quot;, they perform differently before the 60Kbigram point, and perform almost the same after that.</S>
			<S sid ="208" ssid = "25">The reason is that they are approaching their saturation states, which will be discussed in section 4.5.</S>
			<S sid ="209" ssid = "26">Before 60Kbigram point, smaller step size yields better performance.</S>
			<S sid ="210" ssid = "27">An example of detailed comparison result is shown in Table 1, where the F-Measure is 96.33%.</S>
			<S sid ="211" ssid = "28">The last column shows the relative model sizes with respect to the KLD pruned model.</S>
			<S sid ="212" ssid = "29">It shows that with the F-Measure of 96.33%, number of bigrams decreases by up to 90%.</S>
			<S sid ="213" ssid = "30">96.5 96.4 96.3 96.2 96.1 96.0 K L D D i s c r i m S t e p 1 0 K S t e p 5 K S t e p 2 K C u t o f f 1 2 3 4 5 6 7 8 9 10 B i g r a m N u m ( 1 0 K ) T a b l e 1 . C o m p a r i s o n o f N u m b e r o f B i g r a m s a t F M e a s u r e 9 6 . 3 3 % 4.4 C o r r e l a t i o n b e t w e e n P e r p l e x i t y a n d F - M e a s u r e Perple xities of the models built above are evaluated over the gold standar d set.</S>
			<S sid ="214" ssid = "31">Figure 6 shows how the perple xities vary with the bigram numbers in models . Here,.</S>
			<S sid ="215" ssid = "32">we notice that the KLD models achiev e the lowest perple xities.</S>
			<S sid ="216" ssid = "33">It is not a Figure 5.</S>
			<S sid ="217" ssid = "34">Performance Comparison of Different Pruning Methods First, we compare the performance of &quot;KLD&quot; and &quot;Discrim&quot;.</S>
			<S sid ="218" ssid = "35">When the model size is small, 1 Our pilot study shows that the method based on Kullback-.</S>
			<S sid ="219" ssid = "36">Leibler distance outperforms methods based on other criteria introduced in section 2.</S>
			<S sid ="220" ssid = "37">surprising result, because the goal of KLD based pruning is to minimize the KullbackLeibler distance that can be interpreted as a relative change of perplexity (Stolcke, 1998).</S>
			<S sid ="221" ssid = "38">Now we compare Figure 5 and Figure 6.</S>
			<S sid ="222" ssid = "39">Perplexities of KLD models are much lower than that of the other models, but their F-Measures are much worse than that of step-by-step growing models.</S>
			<S sid ="223" ssid = "40">It implies that lower perplexity does not always lead to higher F-Measure.</S>
			<S sid ="224" ssid = "41">the mis-aligned part of the segmented corpus, where W * ≠ W * . It is likely that not all bigrams F B However, when the comparison is restricted in a single pruning method, the case is different.</S>
			<S sid ="225" ssid = "42">For each pruning method, as more bigrams are included in the model, the perplexity curve falls, and the F-Measure curve rises.</S>
			<S sid ="226" ssid = "43">It implies there are correlations between them.</S>
			<S sid ="227" ssid = "44">We compute the Pearson product-moment correlation coefficient for each pruning method, as listed in Table 2.</S>
			<S sid ="228" ssid = "45">It shows that the correlation between perplexity and F-Measure is very strong.</S>
			<S sid ="229" ssid = "46">To sum up, the correlation between language model perplexity and system performance (here represented by F-Measure) depends on whether the models come from the same pruning method.</S>
			<S sid ="230" ssid = "47">If so, the correlation is strong.</S>
			<S sid ="231" ssid = "48">Otherwise, the correlation is weak.</S>
			<S sid ="232" ssid = "49">700 650 600 550 have the opportunity.</S>
			<S sid ="233" ssid = "50">As more and more bigrams are added into the base model, the segmented training corpus using the current base model approaches to that using the full-bigram model.</S>
			<S sid ="234" ssid = "51">Gradually, none bigram can be added into the current base model.</S>
			<S sid ="235" ssid = "52">At that time, the model stops growing, and reaches its saturation state.</S>
			<S sid ="236" ssid = "53">The model that reaches its saturation state is named as saturated model.</S>
			<S sid ="237" ssid = "54">In our experiments, three step-by-step growing models reach their saturation states when about 100K bigrams are added in.</S>
			<S sid ="238" ssid = "55">By combining with the baseline KLD based method, we obtain models that outperform the baseline for any model size.</S>
			<S sid ="239" ssid = "56">We combine them as follows.</S>
			<S sid ="240" ssid = "57">If the desired model size is smaller than that of the saturated model, step-by-step growing is applied.</S>
			<S sid ="241" ssid = "58">Otherwise, KullbackLeibler distance is used for further growing over the saturated model.</S>
			<S sid ="242" ssid = "59">For instance, by growing over the saturated model of &quot;Step-2K&quot;, we obtain combined models containing from 100K to 2 500 450 400 350 300 K L D D i s c r i m S t e p 1 0 K S t e p 5 K S t e p 2 K C u t o f f 1 2 3 4 5 6 7 8 9 10 B i g r a m N u m ( 1 0 K ) millio n bigra ms. The perfor manc e of the combined model s and that of the baseli ne KLD models are illustr ated in Figur e 7.</S>
			<S sid ="243" ssid = "60">It shows that the combi ned model perfor ms consis tently better than KLD model over all of bigra m numb ers.</S>
			<S sid ="244" ssid = "61">Finall y, the two curve s conve rge at the perform a n c e o f t h e f u l l b i g r a m m o d e l . 97.0 Figure 6.</S>
			<S sid ="245" ssid = "62">Perplexity Comparison of Different Pruning Methods 96.9 96.8 96.7 96.6 96.5 96.4 96.3 KLD Combined Model Table 2.</S>
			<S sid ="246" ssid = "63">Correlation between Perplexity and F-Measure 4.5 Combination of Saturated Model and.</S>
			<S sid ="247" ssid = "64">KLD The above experimental results show that step- by-step growing models achieve the best performance when less than 100K bigrams are added in.</S>
			<S sid ="248" ssid = "65">Unfortunately, they can not grow up into any desired size.</S>
			<S sid ="249" ssid = "66">A bigram has no chance to be added into the base model, unless it appears in Bigram Num(10K) Figure 7.</S>
			<S sid ="250" ssid = "67">Performance Comparison of Combined Model and KLD Model 5 Conclusions and Future Work.</S>
			<S sid ="251" ssid = "68">A discriminative pruning criterion of n-gram language model for Chinese word segmentation was proposed in this paper, and a step-by-step growing algorithm was suggested to generate the model of desired size based on a full-bigram model and a base model.</S>
			<S sid ="252" ssid = "69">Experimental results showed that the discriminative pruning method achieves significant improvements over the baseline KLD based method.</S>
			<S sid ="253" ssid = "70">At the same F-measure, the number of bigrams can be reduced by up to 90%.</S>
			<S sid ="254" ssid = "71">By combining the saturated model and the baseline KLD based method, we achieved better performance for any model size.</S>
			<S sid ="255" ssid = "72">Analysis shows that, if the models come from the same pruning method, the correlation between perplexity and performance is strong.</S>
			<S sid ="256" ssid = "73">Otherwise, the correlation is weak.</S>
			<S sid ="257" ssid = "74">The pruning methods discussed in this paper focus on bigram pruning, keeping unigram probabilities unchanged.</S>
			<S sid ="258" ssid = "75">The future work will attempt to prune bigrams and unigrams simultaneously, according to a same discriminative pruning criterion.</S>
			<S sid ="259" ssid = "76">And we will try to improve the efficiency of the step-by-step growing algorithm.</S>
			<S sid ="260" ssid = "77">In addition, the method described in this paper can be extended to other applications, such as IME and speech recognition, where language models are applied in a similar way.</S>
	</SECTION>
</PAPER>
