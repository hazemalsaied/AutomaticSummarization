<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper presents a technique for word segmentation for the Urdu OCR system.</S>
		<S sid ="2" ssid = "2">Word segmentation or word tokenization is a preliminary task for Urdu language processing.</S>
		<S sid ="3" ssid = "3">Several techniques are available for word segmentation in other languages.</S>
		<S sid ="4" ssid = "4">A methodology is proposed for word segmentation in this paper which determines the boundaries of words given a sequence of ligatures, based on collocation of ligatures and words in the corpus.</S>
		<S sid ="5" ssid = "5">Using this technique, word identification rate of 96.10% is achieved, using trigram probabilities normalized over the number of ligatures and words in the sequence.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Urdu uses Nastalique style of Arabic script for writing, which is cursive in nature.</S>
			<S sid ="7" ssid = "7">Characters join together to form ligatures, which end either with a space or with a non-joining character.</S>
			<S sid ="8" ssid = "8">A word may be composed of one of more ligatures.</S>
			<S sid ="9" ssid = "9">In Urdu, space is not used to separate two consecutive words in a sentence; instead readers themselves identify the boundaries of words, as the sequence of ligatures, as they read along the text.</S>
			<S sid ="10" ssid = "10">Space is used to get appropriate character shapes and thus it may even be used within a word to break the word into constituent ligatures (Naseem 2007, Durrani 2008).</S>
			<S sid ="11" ssid = "11">Therefore, like other languages (Theeramunkong &amp; Usanavasin, 2001; Wan and Liu, 2007; Khankasikam &amp; Muansuwan, 2005; Haruechaiyasak et al., 2008; Haizhou &amp; Baosheng, 1998), word segmentation or word tokenization is a prelimi nary task for Urdu language processing.</S>
			<S sid ="12" ssid = "12">It has applications in many areas like spell checking, POS tagging, speech synthesis, information retrieval etc. This paper focuses on the word segmentation problem from the point of view of Optical Character Recognition (OCR) System.</S>
			<S sid ="13" ssid = "13">As space is not visible in typed and scanned text, spacing cues are not available to the OCR for word separation and therefore segmentation has to be done more explicitly.</S>
			<S sid ="14" ssid = "14">This word segmentation model for Urdu OCR system takes input in the form of a sequence of ligatures recognized by an OCR to construct a sequence of words from them.</S>
	</SECTION>
	<SECTION title="Literature Review. " number = "2">
			<S sid ="15" ssid = "1">Many languages, e.g., English, French, Hindi, Nepali, Sinhala, Bengali, Greek, Russian, etc. segment text into a sequence of words using delimiters such as space, comma and semi colon etc., but on the other hand many Asian languages like Urdu, Persian, Arabic, Chinese, Dzongkha, Lao and Thai have no explicit word boundaries.</S>
			<S sid ="16" ssid = "2">In such languages, words are segmented using more advanced techniques, which can be categorized into three methods: (i) Dictionary/lexicon based approaches (ii) Linguistic knowledge based approaches (iii) Machine learning based approaches/statistical approaches (Haruechaiyasak et al., 2008) Longest matching (Poowarawan, 1986; Richard Sproat, 1996) and maximum matching (Sproat et al., 1996; Haizhou &amp; Baosheng, 1998) are examples of lexicon based approaches.</S>
			<S sid ="17" ssid = "3">These techniques segment text using the lexicon.</S>
			<S sid ="18" ssid = "4">Their 88 Proceedings of the 8th Workshop on Asian Language Resources, pages 88–94, Beijing, China, 2122 August 2010.</S>
			<S sid ="19" ssid = "5">Qc 2010 Asian Federation for Natural Language Processing accuracy depends on the quality and size of the dictionary.</S>
			<S sid ="20" ssid = "6">N-Grams (Chang et al., 1992; Li Haizhou et al., 1997; Richard Sproat, 1996; Dai &amp; Lee, 1994; Aroonmanakun, 2002) and Maximumcollocation (Aroonmanakun, 2002) are Linguis tic knowledge based approaches, which alsorely very much on the lexicon.</S>
			<S sid ="21" ssid = "7">These approach es select most likely segmentation from the set of possible segmentations using a probabilistic or cost-based scoring mechanism.</S>
			<S sid ="22" ssid = "8">Word segmentation using decision trees (Sornlertlamvanich et al., 2000; Theeramunkong &amp; Usanavasin, 2001) and similar other techniques fall in the third category of word segmentation techniques.</S>
			<S sid ="23" ssid = "9">These approaches use a corpus in which word boundaries are explicitly marked.</S>
			<S sid ="24" ssid = "10">These approaches do not require dictionaries.</S>
			<S sid ="25" ssid = "11">In these approaches ambiguity problems are handled by providing a sufficiently large set of training examples to enable accurate classification.</S>
			<S sid ="26" ssid = "12">A knowledge based approach has been adopted for earlier work on Urdu word segmen tation (Durrani 2007; also see Durrani and Hussain 2010).</S>
			<S sid ="27" ssid = "13">In this technique word segmentation of Urdu text is achieved by employing know ledge based on the Urdu linguistics and script.</S>
			<S sid ="28" ssid = "14">The initial segmentations are ranked using min- word, unigram and bigram techniques.</S>
			<S sid ="29" ssid = "15">It reports 95.8 % overall accuracy for word segmentation of Urdu text.</S>
			<S sid ="30" ssid = "16">Mukund et al.</S>
			<S sid ="31" ssid = "17">(2009) propose using character model along with linguistic rules and report 83% precision.</S>
			<S sid ="32" ssid = "18">Lehal (2009) propos es a two stage process, which first uses Urdu linguistic knowledge, and then uses statistical information of Urdu and Hindi (also using transliteration into Hindi) in the second stage for words not addressed in the first stage, reporting an accuracy of 98.57%.</S>
			<S sid ="33" ssid = "19">These techniques use characters or words in the input, whereas an OCR outputs a series of ligatures.</S>
			<S sid ="34" ssid = "20">The current paper presents work done using statistical methods as an alternative, which works with ligatures as input.</S>
	</SECTION>
	<SECTION title="Methodology. " number = "3">
			<S sid ="35" ssid = "1">Current work uses the co-occurrence information of ligatures and words to construct a statistical model, based on manually cleaned and segmented training corpora.</S>
			<S sid ="36" ssid = "2">Ligature and word statistics are derived from these corpora.</S>
			<S sid ="37" ssid = "3">In the decoding phase, first all sequences of words are generated from input set of ligatures and ranking of these sequences is done based on lexical lookup.</S>
			<S sid ="38" ssid = "4">Top k sequences are selected for further processing, based on the number of valid words.</S>
			<S sid ="39" ssid = "5">Finally, the probability of each of the k sequences is calculated for the final decision.</S>
			<S sid ="40" ssid = "6">Details are described in the subsequent sections.</S>
			<S sid ="41" ssid = "7">3.1 Data collection and preparation.</S>
			<S sid ="42" ssid = "8">An existing lexicon of 49630 unique words is used (derived from Ijaz et al. 2007).</S>
			<S sid ="43" ssid = "9">The corpus used for building ligature grams consists of half a million words.</S>
			<S sid ="44" ssid = "10">Of these, 300,000 words are taken from the Sports, Consumer Information and Culture/Entertainment domains of the 18 million word corpus (Ijaz et al. 2007),100,000 words are obtained from UrduNepali English Parallel Corpus (available at www.PANL10n.net), and another 100,000 words are taken from a previously POS tagged corpus (Sajjad, 2007; tags of this corpus are removed before further processing).</S>
			<S sid ="45" ssid = "11">This corpus is manually cleaned for word segmentation errors, by adding missing spaces between words and replacing spaces with Zero Width Non- Joiner (ZWNJ) within words.</S>
			<S sid ="46" ssid = "12">For the computa tion of word grams, the 18 million word corpus of Urdu is used (Ijaz et al. 2007).</S>
			<S sid ="47" ssid = "13">3.2 Count and probability calculations.</S>
			<S sid ="48" ssid = "14">Table 1 and Table 2 below give the counts for unigram, bigrams and trigram of the ligatures and the words derived from the corpora respectively.</S>
			<S sid ="49" ssid = "15">Ligature Tokens Ligature Unigram Ligature Bigrams Ligature Trigrams 1508078 10215 35202 65962 Table 1.</S>
			<S sid ="50" ssid = "16">Unigram, bigram and trigram counts of the ligature corpus Word Tokens Word Unigrams Word Bigrams Word Trigrams 17352476 157379 1120524 8143982 Table 2.</S>
			<S sid ="51" ssid = "17">Unigram, bigram and trigram counts of the word corpus After deriving word unigrams, bigrams, and trigrams, the following cleaning of corpus is performed.</S>
			<S sid ="52" ssid = "18">In the 18 million word corpus, certain words are combined due to missing space, but are separate words.</S>
			<S sid ="53" ssid = "19">Some of these words occur with very high frequency in the corpus.</S>
			<S sid ="54" ssid = "20">For example “� J7” (ho ga, “will be”) exists as single word rather than two words due to missing space.</S>
			<S sid ="55" ssid = "21">To solve this space insertion problem, a list of about 700 words with frequency greater than 50 is obtained from the word unigrams.</S>
			<S sid ="56" ssid = "22">Each word of the list is manually reviewed and space is inserted, where required.</S>
			<S sid ="57" ssid = "23">Then these error words are removed from the word unigram and added to the word unigram frequency list as two or three individual words incrementing respective counts.</S>
			<S sid ="58" ssid = "24">For the space insertion problem in word bigrams, each error word in joined-word list (700-word list) is checked.</S>
			<S sid ="59" ssid = "25">Where these error words occurs in a bigram word frequency list, for example “� J7 �y.S” (kiya ho ga “will have done”) exists in the bigram list and contains &quot;� J7&quot; error word, then this bigram entry “� J7 �y.S” is removed from the bigram list and counts of “� J7” and “J7 �y.S” are increased by the count of “� J7 �y.S”.</S>
			<S sid ="60" ssid = "26">If these words do not exist in the word bigram list then they are added as a new bi- grams with the count of “� J7 �y.S”.</S>
			<S sid ="61" ssid = "27">Same procedure is performed for the word trigrams.</S>
			<S sid ="62" ssid = "28">The second main issue is with word-affixes, which are sometimes separated by spaces from the words.</S>
			<S sid ="63" ssid = "29">Therefore, in calculations, these are treated as separate words and exist as bigram entries in the list rather than a unigram entry.</S>
			<S sid ="64" ssid = "30">For example &quot;-&quot;• &quot;&quot;&apos;-�&quot; (sehat+mand, “healthy”) exists as a bigram entry but in Urdu it is a single word.</S>
			<S sid ="65" ssid = "31">To cope with this problem, a list of word-affixes is used.</S>
			<S sid ="66" ssid = "32">If any entry of word bi- gram matches with an affix, then this word is combined by removing spurious space from it (and inserting ZWNJ, if required to maintain its glyph shape).</S>
			<S sid ="67" ssid = "33">Then this word is inserted in the 3.3 Word sequences generation from input.</S>
			<S sid ="68" ssid = "34">The input, in the form of sequence of ligatures is used to generate all possible words.</S>
			<S sid ="69" ssid = "35">These sequences are then ranked based on real words.</S>
			<S sid ="70" ssid = "36">For this purpose, a tree of these sequences is incrementally built.</S>
			<S sid ="71" ssid = "37">The first ligature is added as a root of tree, and at each level two to three additional nodes are added.</S>
			<S sid ="72" ssid = "38">For example the second level of the tree contains the following tree nodes.</S>
			<S sid ="73" ssid = "39">• Current ligature forms a separate word, separated with space, from the sequence at its parent, l1 l2 • Current ligature concatenates, without a space, with the sequence at its parent, l1l2 • Current ligature concatenates, without a space, with the sequence at its parent but with an additional, l1ZWNJl2 For each node, at each level of the tree, a numeric value is assigned, which is the sum of squares of the number of ligatures in each word which is in the dictionary.</S>
			<S sid ="74" ssid = "40">If a word does not exist in dictionary then it does not contribute to the total sum.</S>
			<S sid ="75" ssid = "41">If a node-string has only one word and this word does not occur in the dictionary as a valid word then it is checked that this word may occur at the start of any dictionary entry.</S>
			<S sid ="76" ssid = "42">In this case numeric value is also assigned.</S>
			<S sid ="77" ssid = "43">After assignment, nodes are ranked according to these values and best k (beam value) nodes are selected.</S>
			<S sid ="78" ssid = "44">These selected nodes are further ranked using statistical methods discussed below.</S>
			<S sid ="79" ssid = "45">3.4 Best word segmentation selection.</S>
			<S sid ="80" ssid = "46">For selection of the most probable word segmentation sequence word and ligature models are used.</S>
			<S sid ="81" ssid = "47">For word probabilities the following is used.</S>
			<S sid ="82" ssid = "48">unigram list with its original bigram count and unigram list updated accordingly.</S>
			<S sid ="83" ssid = "49">Same proce P(W) = argmaxwn ∈ S P(wn) dure is performed if a trigram word matches with an affix.After cleaning, unigram, bigram and tri gram counts for both words and ligatures areTo reduce the complexity of computing, Mar kov assumption are taken to give bigram and trigram approximations (e.g., see Jurafsky &amp; Martin 2006) as given below.</S>
			<S sid ="84" ssid = "50">calculated.</S>
			<S sid ="85" ssid = "51">To avoid data sparseness One Count P(W) = argmaxwl ∏1 P( w |w ) Smoothing (Chen &amp; Goodman, 1996) is applied.</S>
			<S sid ="86" ssid = "52">P W) = argmaxw n ∈ S ∈ S n k=1 i i-1 k k-1 k-2 ( n (∏n l P(w |w w ) Similarly the ligature models are built by taking the assumption that sentences are made up of sequences of ligatures rather than words and space is also a valid ligature.</S>
			<S sid ="87" ssid = "53">By taking the Markov bigram and trigram assumption for ligature grams we get the following.</S>
			<S sid ="88" ssid = "54">This gives the maximum probable word sequence among all the alternative word sequences.</S>
			<S sid ="89" ssid = "55">The precision of the equation can be taken at bigram or trigram level for both n ∈ S m i i-1 ligature and word, giving the following pos P(L) = argmaxwl (∏1 (P (l |l )) sibilities.</S>
			<S sid ="90" ssid = "56">Additionally, normalization is also n ∈ S m i i-1 i-2 P(L) = argmaxwl (∏1 (P (l |l l )) done to better compare different sequences, Given the ligatures, e.g. as input from and OCR, we can formulate the decoding problem as the following equation.</S>
			<S sid ="91" ssid = "57">as each sequences has different number of words and ligatures per word.</S>
			<S sid ="92" ssid = "58">• Ligature trigram and word bigram based P(W|L) = argmaxwl P(wn | lm ) technique n ∈ S 1 1 P(W) = argmaxwl (∏m(P (l |l l )) ∗ where wn = w w w w w and m n k=1 P(wk |wk1 )) n ∈ S 1 i i-1 i-2 l1 = l1, l2, l3, l4,… lm ; n represents number of • Ligature bigram and word trigram basedwords and m represents the number of liga tures.</S>
			<S sid ="93" ssid = "59">This equation also represents that m technique n ∈ S (∏m(P (li |li1 )) ∗ number of ligatures can be assigned to n P(W) = argmaxwl 1 number of words.</S>
			<S sid ="94" ssid = "60">By applying the Bayesian n k=1 P(wk |wk1 wk2 )) theorem we get the following derivation.</S>
			<S sid ="95" ssid = "61">P(Il lwl ).P(wl ) • Ligature trigram and word trigram based technique P(W|L) = argmaxwl m n n P(Im ) P(W) = argmaxwn ∈ S (∏m(P (li |li1 li2 )) ∗ m n l 1As P (l1 ) is same for all w1 , so the denominator does not change the equation, simplify n k=1 P(wk |wk1 wk2 )) ing to the following expression.</S>
			<S sid ="96" ssid = "62">m n n • Normalized ligature bigram and word bi gram based technique P(W|L) = argmaxwl P(l1 |w1 ).</S>
			<S sid ="97" ssid = "63">P(w1 ) P(W) = argmaxwl (∏m(P (l |l ))1/NL ∗ where m n n n k=1 P(wk |w k-1 n ∈ S 1 ) /NW i i-1 P(l1 |w1 ) =P (l1,l2, l3, … lm |w1 ) =P( l1 |wn ) ∗ P (l |wnl ) ∗ P(l |wn l l ) ∗ 1 2 1 1 3 1 1 2 n n• Normalized ligature trigram and word bi gram based technique P(l4 |w1 l1 l2 l3) ∗ … P(lm |w1 l1 l2 l3 … lm1 ) Assuming that a ligature li depends only on P(W) = argmaxwl (∏m(P (l |l l ) 1/NL ∗ n ∈ S 1 n 1/ i i-1 i-2the word sequence w1 and its previous liga ture li1, and not the ligature history, the n k=1 P(wk |wk1 )) NW above equation can be simplifed as follows.</S>
			<S sid ="98" ssid = "64">• Normalized ligature bigram and word tri m n n n n gram based technique P(l1 |w1 ) = P (l1 |w1 ) ∗ P (l2 |w1 l1) ∗ P(l3 |w1 l2 ) n n 1 ∗ P(l4 |w1 l3 ) ∗ … P(lm |w1 lm1 ) P(W) = argmaxwn ∈ S (∏m(P (li |li1 )) /NL ∗ l 1 m n 1/NW = ∏1 P (li |w1 li1 ) n k=1 P(wk |wk1 wk2 ) Further, if it is assumed that li depends on the word in which it appears, not whole wordsequence, the equation can be further simpli• Normalized ligature trigram and word tri gram based technique 1 n (∏m(P (l |l l )) /NL ∗fied to the following (as probability of li with P(W) = argmaxwl ∈ S 1 1/ i i-1 i-2 in a word is 1).</S>
			<S sid ="99" ssid = "65">n k=1 P(wk |wk1 wk2 ) NW m n m P(l1 |w1 ) = ∏1 P (li |li1 ) Thus, considering bigrams, P(W|L) = In the current work, all the above tech niques are used and the best sequence from each argmaxwl m (P (l |l n ) ( P(w |w )) one is shortlisted.</S>
			<S sid ="100" ssid = "66">Then the word sequence which occurs the most times in this shortlist is finally selected.</S>
			<S sid ="101" ssid = "67">n ∈ S 1 i i-1 k=1 k k-1 NL represents the number of ligature bi- grams or trigrams and NW represents the number of word bigram or trigrams that exist in the given sentence.</S>
	</SECTION>
	<SECTION title="Results and Discussion. " number = "4">
			<S sid ="102" ssid = "1">The model is tested on a corpus of 150 sentences composed of 2156 words and 6075 ligatures.</S>
			<S sid ="103" ssid = "2">In these sentences, 62 words are unknown, i.e. the words that do not exist in our dictionary.</S>
			<S sid ="104" ssid = "3">The average length of the sentence is 14 words and 40.5 ligatures.</S>
			<S sid ="105" ssid = "4">The average length of word is 2.81 ligatures.</S>
			<S sid ="106" ssid = "5">All the techniques are tested with a beam value, k, of 10, 20, 30, 40, and 50.</S>
			<S sid ="107" ssid = "6">The results can be viewed from two perspectives: sentence identification rate, and word identification rate.</S>
			<S sid ="108" ssid = "7">A sentence is considered incorrect even if one word of the sentence is identified wrongly.</S>
			<S sid ="109" ssid = "8">The technique gives the sentence identification rate of 76% at the beam value of 30.</S>
			<S sid ="110" ssid = "9">At word level, Normalized Ligature Trigram Word Trigram Technique outperforms other techniques and gives a 96.10% word identification rate at the beam value of 50.</S>
			<S sid ="111" ssid = "10">The normalized data gives much better prediction compared to the un-normalized data.</S>
			<S sid ="112" ssid = "11">Sentence identification errors depend heavily on the unknown words.</S>
			<S sid ="113" ssid = "12">For example, at the beam value of 30 we predict 38 incorrect sentences, of which 25 sentence level errors are due to unknown-words and 13 errors are due to known word identification errors.</S>
			<S sid ="114" ssid = "13">Thus improving system vocabulary will have significant impact on accuracy.</S>
			<S sid ="115" ssid = "14">Many of the word errors are caused due to insufficient cleaning of word the larger corpus.</S>
			<S sid ="116" ssid = "15">Though the words with frequency greater than 50 from the 18 million word corpus have been cleaned, the lower frequency words cause these errors.</S>
			<S sid ="117" ssid = "16">For example word list still contains &quot;� د�y&quot;�&quot;(bunyad per, “depends on”), &quot; y &quot; (se taqseem, “divided by”) with frequency of 40 and 5 respectively, and each should be two words with a space between them.</S>
			<S sid ="118" ssid = "17">If low fre quency words are also cleaned results will further improve, though it would take a lot of manual effort.</S>
			<S sid ="119" ssid = "18">Be a m Va lue To t a l S e n t e n c e s i d e n t i f i e d %a ge To tal W or ds I d e n ti fi e d %a ge T ot al k n o w n w o r d s i d e n ti fi e d %a ge T o t a l u n k n o w n w o r d s i d e n t i f i e d %a ge 1 0 1 1 0 / 1 5 0 73.3 3% 2 0 6 0 / 2 1 5 6 95.5 5% 2 0 2 4 / 2 0 9 2 96.</S>
			<S sid ="120" ssid = "19">75 % 3 6 / 6 4 56.2 5% 2 0 1 1 2 / 1 5 0 74.6 7% 2 0 6 6 / 2 1 5 6 95.8 3% 2 0 2 7 / 2 0 9 2 96.</S>
			<S sid ="121" ssid = "20">89 % 3 9 / 6 4 60.9 4% 3 0 1 1 4 / 1 5 0 76 % 2 0 6 2 / 2 1 5 6 95.6 4% 2 0 1 9 / 2 0 8 3 96.</S>
			<S sid ="122" ssid = "21">93 % 4 3 / 7 3 58.9 0% 4 0 1 0 5 / 1 5 0 70 % 2 0 3 7 / 2 1 5 6 94.4 8% 2 0 0 0 / 2 0 9 2 95.</S>
			<S sid ="123" ssid = "22">60 % 3 7 / 6 4 57.8 1% 5 0 1 0 6 / 1 5 0 70.6 7% 2 0 4 0 / 2 1 5 6 94.6 2% 2 0 0 0 / 2 0 9 2 95.</S>
			<S sid ="124" ssid = "23">60 % 4 0 / 6 4 62.5 0% Table 3.</S>
			<S sid ="125" ssid = "24">Results changing beam width k of the tree T e c h n i q u e Total sent ence s i d e n t i f i e d %ag e Tot al wo rds id en tif ie d %a ge T o t a l k n o w n wor ds Ide ntif ied %ag e Tot al un kn ow n wo rds ide ntif ied %a ge L i g a t u r e B i g r a m 5 0 / 1 5 0 33.33 % 18 35 /2 15 6 85.11 % 1 8 0 6 / 2 0 9 2 86.33 % 2 9 / 6 4 45.31 % Li gat ure Bi gr a m W or d B i g r a m 6 8 / 1 5 0 45.33 % 19 00 /2 15 6 88.13 % 1 8 6 5 / 2 0 9 2 89.15 % 3 5 / 6 4 54.69 % Li gat ure Bi gr a m W or d T r i g r a m 8 3 / 1 5 0 55.33 % 19 60 /2 15 6 90.91 % 1 9 2 4 / 2 0 9 2 91.97 % 3 6 / 6 4 56.25 % L i g a t u r e T r i g r a m 1 6 / 1 5 0 10.67 % 16 37 /2 15 6 75.93 % 1 6 1 0 / 2 0 9 2 76.96 % 2 7 / 6 4 42.19 % Lig atur e Trig ra m Wo rd B i g r a m 4 2 / 1 5 0 28 % 17 76 /2 15 6 82.38 % 1 7 4 6 / 2 0 9 2 83.46 % 3 0 / 6 4 46.88 % Lig atur e Trig ra m Wo rd T r i g r a m 6 2 / 1 5 0 41.33 % 18 68 /2 15 6 86.64 % 1 8 3 5 / 2 0 9 2 87.72 % 3 3 / 6 4 51.56 % No rm ali ze d Li gat ur e Big ra m Wo rd Big ra m 9 0 / 1 5 0 60 % 20 67 /2 15 6 95.87 % 2 0 2 4 / 2 0 9 2 96.75 % 4 3 / 6 4 67.19 % No rm ali ze d Li gat ur e Big ra m Wo rd Tri gra m 1 0 0 / 1 5 0 66.67 % 20 70 /2 15 6 96.01 % 2 0 2 8 / 2 0 9 2 96.94 % 4 2 / 6 4 65.63 % No rm ali ze d Li gat ur e Trig ra m Wo rd Big ra m 9 3 / 1 5 0 62 % 20 71 /2 15 6 96.06 % 2 0 3 0 / 2 0 9 2 97.04 % 4 1 / 6 4 64.06 % No rm ali ze d Li gat ur e Trig ram Wo rd Trig ram 1 0 1 / 1 5 0 67.33 % 20 72 /2 15 6 96.10 % 2 0 3 0 / 2 0 9 2 97.04 % 4 2 / 6 4 65.63 % W o r d B i g r a m 4 7 / 1 5 0 31.33 % 18 27 /2 15 6 84.74 % 1 7 9 6 / 2 0 9 2 85.85 % 3 1 / 6 4 48.44 % W o r d T r i g r a m 7 4 / 1 5 0 49.33 % 19 37 /2 15 6 89.84 % 1 9 0 3 / 2 0 9 2 90.97 % 3 4 / 6 4 53.13 % Table 4.</S>
			<S sid ="126" ssid = "25">Results for all techniques for the beam value of 50 Errors are also caused if an alternate ligature sequence exists.</S>
			<S sid ="127" ssid = "26">For example the proper noun &quot;...</S>
			<S sid ="128" ssid = "27">ر�.S&quot; (kartak) is not identifiec as it does not exist in dictionary, but the alternate two word sequence &quot;...</S>
			<S sid ="129" ssid = "28">ر�.S&quot; (kar tak, “till the car”) is valid.</S>
			<S sid ="130" ssid = "29">This work uses the knowledge of ligature grams and word grams.</S>
			<S sid ="131" ssid = "30">It can be further enhanced by using the character grams.</S>
			<S sid ="132" ssid = "31">We have tried to clean the corpus.</S>
			<S sid ="133" ssid = "32">Further cleaning and additional corpus will improve the results as well.</S>
			<S sid ="134" ssid = "33">Improvement can also be achieved by handling abbreviations and English words transliterated in the text.</S>
			<S sid ="135" ssid = "34">The unknown word detection rate can be increased by applying POS tagging to further help rank the multiple possible sentences.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "5">
			<S sid ="136" ssid = "1">This work presents an initial effort on statistical solution of word segmentation, especially for Urdu OCR systems.</S>
			<S sid ="137" ssid = "2">This work develops a cleaned corpus of half a million Urdu words for statistical training of ligature based data, which is now available for the research community.</S>
			<S sid ="138" ssid = "3">In addition, the work develops a statistical model for word segmentation using ligature and word statistics.</S>
			<S sid ="139" ssid = "4">Using ligature statistics improves upon using just the word statistics.</S>
			<S sid ="140" ssid = "5">Further normalization has significant impact on accuracy.</S>
	</SECTION>
</PAPER>
