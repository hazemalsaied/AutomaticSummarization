<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">ust li e other NLP applications a serious problem with Chinese word segmentation lies in the ambiguities involved Disambiguation methods fall into different categories e g rule-based statistical-based and example-based approaches each of which may involve a variety of machine learning techniques In this paper we report our current progress within the example-based approach including its frame- wor example representation and collection example matching and application Experimental results show that this effective approach resolves more than 90% of ambiguities found Hence if it is integrated effectivelywith a segmentation method of the precision P &gt; 95% the resulting segmentation accuracy can reach theoretically beyond 99 5%</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="2" ssid = "2">It has been nearly two decades since the early work of Chinese word segmentation (Liang 1984 Liang and Liu 1985 Liu and Liang 1986.</S>
			<S sid ="3" ssid = "3">Liang 1986) Tokenization has been recognized as a widespread problem rather than being unique to Chinese and other oriental languages It is an initial or prerequisite phase of NLP for all languages although the obscurity of the problem varies from language to language (Webster and Kit 1992a Palmer 2 ) Recent work on tokenization for European languages such as English is reported in (Grefenstette and Tapanainen 1994 Grefenstette 1999 Grefenstette et al 2 ) adopt ing a finite-state approach However identification of multi-word units such as proper names and technical terms in these languages is highlyof literature on both linguistic and compu tational sides as listed in (Liu et al 1994 Guo 1997) among many others In general these strategies can be divided into two camps namely dictionary-based and statistical-based approaches Nevertheless the former can be understood as a restricted instance of the latter with an equi-probability for each word in a given dictionaryl Most if not all dictionary-based strategies are built upon a few basic &quot;mechanical&quot; segmentation methods based on string matching (Kit et al 1989) among which the most applicable thus widely used since the very beginning are the two max mum ma ch ng methods (MMs) one scanning forward (FMM) and the other backward (BMM) Interestingly their performance frequently used as the baseline for evaluation is never too far away from the state- of-the-art approaches in terms of segmentation accuracy Although performing little statistical computation the MMs comply in general with the essential principle of the statistical-based approaches select a segmentation as probable as possible among all choices This ad hoc way of choosing the segmentation with fewest words usually leads to by coincidence a more probable output than most other choices with more words2 A dictionary is actually a restricted form of language model, in this sense.</S>
			<S sid ="4" ssid = "4">The coincidence of fewer words with a greater probability can be illustrated as follows: given a string s, the probability of its most probable segmentation seg(s) in terms of a given language model isII comparable to that of multi-character Chinese prob(seg(s)) = max s== w prob(w ) words there are no delimiters available So far a great variety of segmentation strategies for Chinese with various linguistic resources have been explored yielding a large volume wlw2 n where prob(w ) is some conditional probability in the model.</S>
			<S sid ="5" ssid = "5">Since all prob(w ) &lt; 1.0, this probability becomes smaller for a greater n. Clearly, it looks more straightforward in an equi-probability setting.Statistical approaches involve language mod els mostly finite-state ones trained on some large-scale corpora as showed in Fan and Tsai (1988) Chang et al (1991) Chiang et al (1992) Sproat et al (1996) Pont and Croft (1996) and Ng and Lua (forthcoming) These approaches do not provide any explicit strategy for disambiguation but they get more ambiguous chunks correctly segmented than MMs by virtue of probability Other linguistic resources or computational processes can also be integrated for further improvement e g Lai et al (1991) attempts to integrate POS tagging with word segmentation for the enhancement of accuracy and Gan et al (1997) integrates word boundary disambiguation into sentence processing within a probabilistic emergent model There are also other approaches that incorporate various techniques of statistical NLP and machine learning e g transformation-based error-driven learning (Palmer 1997 Hockenmaier and Brew 1998) and compression-based algorithm (Teahan et al 2 ) Recent research shifts its focus onto the following aspects resorting to a variety of resources and techniques in particular machine learning techniques 1 Lexical resource acquisition including.</S>
			<S sid ="6" ssid = "6">compilation and automatic detection of high-tech terms and unknown words like names to complement a never-big-enough dictionary (Chang et al 1995 Pont and Croft 1996 Chang and Su 1997)</S>
	</SECTION>
	<SECTION title="Investigation into the nature  and statistics. " number = "2">
			<S sid ="7" ssid = "1">of ambiguities (Sun and Zhou 1998)</S>
	</SECTION>
	<SECTION title="Unsupervised  learning  of words (Ge et al. " number = "3">
			<S sid ="8" ssid = "1">1999 Peng and Schuurmans 2 1)3.</S>
	</SECTION>
	<SECTION title="Disambiguation with  different  approaches. " number = "4">
			<S sid ="9" ssid = "1">(Liang 1989 Jin 1994 Sun and T sou 1995) The work reported in this paper belongs to the last category taking an instance-based learning Recent research in this direction appears to beapproach aimed to examine its prospects of dis ambiguation The rest of the paper is organized as follows Section 2 briefly introduces the ambiguity problem and existing ambiguity detection strategies Section 3 defines the notion and representation of examples and formulates a similarity measure between an ambiguous input and an example We present our disambiguation algorithm in Section 4 and experimental results and evaluation in Section 5 together with some discussion on the remaining errors before concluding the paper in Section 6 2 Ambiguity.</S>
			<S sid ="10" ssid = "2">Conceptually there are two essential types of ambiguity in Chinese word segmentation which are conventionally termed as o rrapp ng and comb na onar ambiguities They can be formally defined as follows given a dictionary D Overlapping ambiguity A given string a(3y involves an overlapping ambiguity if the set of sub-strings {a(3 (3y}cD Combinational ambiguity A given string a(3 involves a combinational ambiguity if the set of sub-strings {a (3 a(3}cD In practice the first type commonly co-occurs with the second because almost all Chinese characters can be mono-character words For the same reason almost every multi-character word involves a combinational ambiguity Fortunately however most of them are &quot;resolved&quot; characteristically in a sense by a MM strategy Therefore the focus of disambiguation is unsurprisingly put on the unresolved ones as well as the overlapping ambiguities 2.1 Ambiguity detection.</S>
			<S sid ="11" ssid = "3">Conventionally a straightforward strategy is exploited to detect ambiguities with the aid of FMM and BMM the discrepancies of their outputs signal ambiguous strings It appears adequately eÆcient because only a forward and a backward scanning of the input will do However its reliability remains a question 3 closely related to the studies on computational lexical acquisition of other languages such as English (de Mar- cken, 1996; Brent, 1999; Kit and Wilks, 1999; Kit, 2000; Venkataraman, 2001) and to language modeling technology (Jelinek, 199 ), typically involving a version of the EM algorithm (Dempster et al., 19 ).</S>
			<S sid ="12" ssid = "4">although it has been taken for granted for a long time that there would be few ambiguities left out which is at odds with our observation that there are ambiguous strings for which both MMs output an identical segmentation E g given a string abcde with {a ab bcd c de e}E D it is conceivable that both MMs output the ambiguous string a is defined as {l,r}··· ab c de ··· and consequently the embed ded ambiguity is unseen So far we haven t seen any report on the incompleteness of ambiguity .6(A E) = Æ(a e)(1 + Æ (Ca C )) (1) detection via this strategy A more comprehensive strategy would be that we first locate the boundaries of all poss br words in terms of a given dictionary4 are first lo where Æ(· ·) indicates the identity of two ambiguous strings defined as 1 if a = e cated and then the common sub-strings among these words are detected any common sub Æ(a e) = otherwise (2) string indicates an ambiguity Since our current work is intended to examine the effectiveness of an example-based learning approach to resolve found ambiguities its mer and Æ (· ·) (for i E {l r}) is the similarity of the corresponding contexts measured in terms of the length of their common prefix (for the right contexts) or suÆx (for the left contexts) in num 5 its do not rely on the completeness of ambiguity ber of words For two given strings if we denote detection The conventional strategy would suffice for the purpose of identifying an adequate their common suÆx (i e aÆx from the r gh ) and prefix (i e aÆx from the r f y) respectively r l number of ambiguities for our experiments as f (· ·) and f (· ·) we have Æ (· ·) = /f (· ·)/ 3 Examples and similarity measure.</S>
			<S sid ="13" ssid = "5">Thus we can rewrite (1) into (3) below {l, r} We intend to disambiguate Chinese word segmentation ambiguities within the framework .6(A E) = Æ(a e)(1 + /f (Ca C )/) (3) of cas -based learning This supervised learning approach also labeled as m mory -based ns anc -based or xampr -based learning has been popular for various NLP applications in recent years e g the TiMBL learner (Daelemans et al 2 1) TiMBL is developed as a general memory-based learning environment to in tegrate a set of learning algorithms It has been Actually the idea behind this equation is more straightforward than it looks Basically we measure the similarity of a given triple (i e an ambiguous string and its contexts) and an example in terms of the similarity of their contexts However this similarity is meaningful if and only if the strings in question are identical This is why we define Æ(a e) widely applied to disambiguating a variety of Given a triple A =&lt;C la C r &gt; and a col NLP tasks including PP attachment (Zavrel et al 1998) shallow parsing (Daelemans et al 1999) and WSD (Veenstra et al 2 Stevenson and Wilks 2 1) In this paper the general principle of case-based learning is followed but lection E of examples known as xampr bas (EB) the strategy we undertake to determine a segmentation for the ambiguous string a can be formulated as follows for .6(A E) 2 1 the formulation below is nevertheless specific to our problem An xampr here is defined as a quadruple seg(a E A) = seg arg max .6(A E) (4) &lt;C l e C r S &gt; where the strings C l and C r are the left and right con x s within which the ambiguous string e appears and S is the correct segmentation of e within the particular context If denoting the quadruple as E we also refer to S as seg(E) or seg(e) interchangeablyThe d s anc or s m rar y between an exam where seg(·) denotes the segmentation of a given string or example Straightforwardly Equation (4) can be read off as the following segment a in the same way as its most similar example in the example base Obviously, measuring the length in number of characters is an alternative to explore in our future research.</S>
			<S sid ="14" ssid = "6">6 ple E and a given triple A =&lt;C l a C r &gt; with For example, ( ) = ( ) = Notice that ambiguities are dictionary-dependent.</S>
			<S sid ="15" ssid = "7">( ) = ( ) = 4 Algorithms.</S>
			<S sid ="16" ssid = "8">In order to test the effectiveness of the disambiguation strategy formulated above we need to collect examples from a large-scale unrestricted corpus via a sound ambiguity detection program and apply the examples to ambiguous strings in a test corpus via an example application program In this section we present the where q(·) gives a probability-like score for a segmentation by which we hope to get a better result than a random or brute-force choice between the FMM and BMM outputs (that we could have made) We refer to q(·) as a sor d n ss function that is defined as the following mainly for the simplicity of IIimplementation algorithms for these purposes 4.1 Ambiguity detection.</S>
			<S sid ="17" ssid = "9">q(wl w2 ··· wn ) = Pw (w ) (5) We take a conventional approach to ambiguity detection by detecting the discrepancies of the outputs from the FMM and BMM segmenta where Pw (·) is the probability of a given string being a word It is defined astions Given an input corpus C it can be real fw Pw (w ) = (w ) ized plainly by the following algorithm Ambiguity detection algorithm arbd(C) 1 F = FMM(C) and B = BMM(C) 2 Return diff(F B).</S>
			<S sid ="18" ssid = "10">where FMM(·) and FMM(·) return the FMM and BMM segmentations of C and diff(· ·) returns the discrepancies of the two segmentations The dictionary used to support the MMs is a merger of the word lists from Liu et al (1994) and Yu (1998) consisting of 53K entries It is a medium-sized dictionary With regards to the dictionary size and the weakness of the ambiguity detection algorithm we keep ourselves alert of the fact that there are a certain number of ambiguities that are not detected by our program And the resolutions for the ambiguous strings so detected are manually prepared by selecting an answer from the outputs of the MMs in use 4.2 Disambiguation.</S>
			<S sid ="19" ssid = "11">Given an example base E and a text corpus C as testing data the disambiguation algorithm works along the following steps Disambiguation algorithm disarb(C E ) 1 Ambiguity detection A = arbd(C).</S>
			<S sid ="20" ssid = "12">2 For every aa(3 E C such that a E A. let A =&lt;a a (3&gt; 2 1 Search for E = arg max .6(A e) e 2 2 If .6(A E) &gt; 1 seg(a) = seg(E) f (w ) where fw (·) and f (·) are respectively the frequencies of a given item occurring as a word and as a string in the training corpus Since it is an approximation we can count the word frequencies based on the FMM output</S>
	</SECTION>
	<SECTION title="Experiment  and evaluation. " number = "5">
			<S sid ="21" ssid = "1">A number of experiments were conducted on unrestricted texts for the purpose of testing the effectiveness of the above disambiguation approach In this section we present the data for training (i e example collection) and testing experimental results and evaluation 5.1 Data.</S>
			<S sid ="22" ssid = "2">The data we used for the experiments are news texts collected from mainland China Hong Kong and Taiwan The corpus size is of 778K words and 1 5M characters in total in 1534 text files About 3 4 of the data of 1 16M characters in 1 1K files are used for training and the remaining 1 4 of 36 K characters in about 4K files for testing The statistics about the ambiguous strings found in the training and testing data is given in Table 1 From the ambiguity- word (EW) ratio we can see that the ambiguity distribution among the two data sets is approximately even 5.2 Results and evaluation.</S>
			<S sid ="23" ssid = "3">Theoretically disambiguation accuracy on the training data should be 1 % because all found ambiguities are manually resolved In contrast 2 3 Else seg(a) = arg max s {FMM(a), BMM(a)} q(s) the accuracy on the test set is more indicative of the effectiveness of the disambiguation strategy Training Data EW Ratio Number Total 54 1 91% of cases Unique 3 18 51% Testing Data EW Ratio Number Total 1648 9 % of cases Unique 995 54% Table 1 Ambiguities in training &amp; testing data Our experimental results show that among 1648 ambiguities found in the test set 1488 are properly resolved in terms of our manual checking of the disambiguation outputs Accordingly the disambiguation accuracy is 9 29% We do not report the overall segmentation accuracy here for a number of reasons Firstly almost every paper in recent years reports a segmentation accuracy that nearly reaches the ceiling This fact suggests that such figures seem to have carried less and less academic significance in the sense that they do not measure any significant advance in tackling the major remaining problems in Chinese word segmentation such as unknown words and segmentation ambiguities Instead all these figures seem to indicate a similar performance which is more interestingly even similar to the performance reported a decade ago Secondly we have not had much ground to compare different systems performance not only because they were tested with different sets of data but also because the ways of calculating the segmentation accuracy are observed to be different from one another On the contrary the disambiguation accuracy is more specific revealing exactly the capacity of a disambiguation strategy to resolve particular ambiguities found It is reasonable to assume that everyone can get the unambiguous part correct in word segmentation so we need not bother taking this part into account for the evaluation of disambiguation performance Instead we choose to concentrate on the problematic part reporting only the disambiguation accuracy for the purpose of evaluation 5.</S>
			<S sid ="24" ssid = "4">Discussions.</S>
			<S sid ="25" ssid = "5">As pointed out before the conventional strategy for ambiguity detection that we have adopted is known to be incomplete Many remaining ambiguities in the data are still to be brought to light It is certainly a research direction that deserves more effort Discovering more such missing cases can no doubt enlarge the example base significantly and consequently enhance the strength of this case-based learning approach to disambiguation This problem is also related to the intrinsic disambiguation ability of the rudimental MMs they segment many ambiguous strings correctly because of their own characteristics rather than by chance Thus it is worth digging out these uncovered ambiguities as examples so that they can be correctly handled when they show up elsewhere that would puzzle the MMs A more detailed analysis of experimental results is also expected e g how many cases are resolved by existing examples and how many others by chance i e by the q(·) function which was designed to alleviate rather than resolve the problem Also a careful analysis of unseen cases in the testing data is also critical for a more thorough evaluation of the merits of the case-based learning approach It will reveal the co rag of the EB and severity of the sparse data problem A conceivable solution for the moment is that we construct all possible ambiguities based on a given dictionary and assign to them proper resolutions so as to produce an EB with greater coverage</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "6">
			<S sid ="26" ssid = "1">In this paper we have presented a case-based learning approach to resolving Chinese word segmentation ambiguities We adopted a simple representation for the examples each consisting of an ambiguous string and its contexts and also formulated a similarity measure for matching an ambiguity and an example from the example base The effectiveness of this learning approach was tested on a set of unrestricted news texts of 1 5M characters and a disambiguation accuracy of 9 % was achieved With this promising result what we can expect is that if this approach could be effectively integrated with a segmentation algorithm that has a segmentation performance of the accuracy P the overall segmentation accuracy one can expect would be P I = P + (1 - P )9 % = (9 + 1 P )% From this formula we can see that if P &gt; 9 % then P I &gt; 99% and if P &gt; 95% then P I &gt; 9995% Therefore a bright future seems to be promised because most Chinese word segmenters were re ported to have achieved an accuracy over 95% according to the literature However the problems we still have with this case-based learning approach include mainly the incompleteness of ambiguity detection and the unknown coverage of the example base collected from unrestricted texts All these remaining problems that we will tackle in our future research would have certain effect on the effectiveness of integrating it into any Chinese word segmenter</S>
	</SECTION>
</PAPER>
