<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">VerbNet (VN) is a major large-scale English verb lexicon.</S>
		<S sid ="2" ssid = "2">Mapping verb instances to their VN classes has been proven useful for several NLP tasks.</S>
		<S sid ="3" ssid = "3">However, verbs are polysemous with respect to their VN classes.</S>
		<S sid ="4" ssid = "4">We introduce a novel supervised learning model for mapping verb instances to VN classes, using rich syntactic features and class membership constraints.</S>
		<S sid ="5" ssid = "5">We evaluate the algorithm in both in-domain and corpus adaptation scenarios.</S>
		<S sid ="6" ssid = "6">In both cases, we use the manually tagged Sem- link WSJ corpus as training data.</S>
		<S sid ="7" ssid = "7">For in- domain (testing on Semlink WSJ data), we achieve 95.9% accuracy, 35.1% error reduction (ER) over a strong baseline.</S>
		<S sid ="8" ssid = "8">For adaptation, we test on the GENIA corpus and achieve 72.4% accuracy with 10.7% ER.</S>
		<S sid ="9" ssid = "9">This is the first large-scale experimentation with automatic algorithms for this task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="10" ssid = "10">The organization of verbs into classes whose members exhibit similar syntactic and semantic behavior has been discussed extensively in the linguistics literature (see e.g.</S>
			<S sid ="11" ssid = "11">(Levin and Rappaport Hovav, 2005; Levin, 1993)).</S>
			<S sid ="12" ssid = "12">Such an organization helps in avoiding lexicon representation redundancy and enables generalizations across similar verbs.</S>
			<S sid ="13" ssid = "13">It can also be of great practical use, e.g. in compensating NLP statistical models for data sparseness.</S>
			<S sid ="14" ssid = "14">Indeed, Levin’s seminal work had motivated Qc 2008.</S>
			<S sid ="15" ssid = "15">Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).</S>
			<S sid ="16" ssid = "16">Some rights reserved.</S>
			<S sid ="17" ssid = "17">much research aimed at automatic discovery of verb classes (see Section 2).</S>
			<S sid ="18" ssid = "18">VerbNet (VN) (Kipper et al., 2000; Kipper- Schuler, 2005) is a large scale, publicly available domain independent verb lexicon that builds on Levin classes and extends them with new verbs, new classes, and additional information such as semantic roles and selectional restrictions.</S>
			<S sid ="19" ssid = "19">VN classes were proven beneficial for Semantic Role Labeling (SRL) (Swier and Stevenson, 2005), Semantic Parsing (Shi and Mihalcea, 2005) and building conceptual graphs (Hensman and Dun- nion, 2004).</S>
			<S sid ="20" ssid = "20">Levin-inspired classes have been used in several NLP tasks, such as Machine Translation (Dorr, 1997) and Document Classification (Klavans and Kan, 1998).</S>
			<S sid ="21" ssid = "21">Many applications that use VN need to map verb instances onto their VN classes.</S>
			<S sid ="22" ssid = "22">However, verbs are polysemous with respect to VN classes.</S>
			<S sid ="23" ssid = "23">Sem- link (Loper et al., 2007) is a dataset that maps each verb instance in the WSJ Penn Treebank to its VN class.</S>
			<S sid ="24" ssid = "24">The mapping has been created using a combination of automatic and manual methods.</S>
			<S sid ="25" ssid = "25">Yi et al.</S>
			<S sid ="26" ssid = "26">(2007) have used Semlink to improve SRL.</S>
			<S sid ="27" ssid = "27">In this paper we present the first large-scale experimentation with a supervised machine learning classification algorithm for disambiguating verb instances to their VN classes.</S>
			<S sid ="28" ssid = "28">We use rich syntactic features extracted from a treebank-style parse tree, and utilize a learning algorithm capable of imposing class membership constraints, thus taking advantage of the nature of our task.</S>
			<S sid ="29" ssid = "29">We use Semlink as the training set.</S>
			<S sid ="30" ssid = "30">We evaluate our algorithm in both in-domain and corpus adaptation scenarios.</S>
			<S sid ="31" ssid = "31">In the former, we test on the WSJ (using Semlink again), obtaining 95.9% accuracy with 35.1% error reduction (ER) over a strong baseline (most frequent 9 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 9–16 Manchester, August 2008 class) when using a modern statistical parser.</S>
			<S sid ="32" ssid = "32">In the corpus adaptation scenario, we disambiguate verbs in sentences taken from outside the training domain.</S>
			<S sid ="33" ssid = "33">Since the manual annotation of new corpora is costly, and since VN is designed to be a domain independent resource, adaptation results are important to the usability in NLP in practice.</S>
			<S sid ="34" ssid = "34">We manually annotated 400 sentences from GENIA (Kim et al., 2003), a medical domain corpus1 . Testing on these, we achieved 72.4% accuracy with 10.7% ER.</S>
			<S sid ="35" ssid = "35">Our adaptation scenario is complete in the sense that the parser we use was also trained on a different corpus (WSJ).</S>
			<S sid ="36" ssid = "36">We also report experiments done using gold-standard (manually created) parses.</S>
			<S sid ="37" ssid = "37">The most relevant previous works addressing verb instance class classification are (Lapata and Brew, 2004; Li and Brew, 2007; Girju et al., 2005).</S>
			<S sid ="38" ssid = "38">The former two do not use VerbNet and their experiments were narrower than ours, so we cannot compare to their results.</S>
			<S sid ="39" ssid = "39">The latter mapped to VN, but used a preliminary highly restricted setup where most instances were monosemous.</S>
			<S sid ="40" ssid = "40">For completeness, we compared our method to theirs2 , achieving similar results.</S>
			<S sid ="41" ssid = "41">We review related work in Section 2, and discuss the task in Section 3.</S>
			<S sid ="42" ssid = "42">Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="43" ssid = "1">VerbNet.</S>
			<S sid ="44" ssid = "2">VN is a major electronic English verb lexicon.</S>
			<S sid ="45" ssid = "3">It is organized in a hierarchical structure of classes and subclasses, each subclass inheriting the full characterization of its super-class.</S>
			<S sid ="46" ssid = "4">VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically.</S>
			<S sid ="47" ssid = "5">VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004).</S>
			<S sid ="48" ssid = "6">VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type).</S>
			<S sid ="49" ssid = "7">Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class).</S>
			<S sid ="50" ssid = "8">VN’s significant coverage of 75.5% coverage of VN classes over PropBank3 instances (Loper et al., 2007).</S>
			<S sid ="51" ssid = "9">Each class contains rich semantic information, including semantic roles of the arguments augmented with selectional restrictions, and possible subcategorization frames consisting of a syntactic description and semantic predicates with temporal information.</S>
			<S sid ="52" ssid = "10">VN thematic roles are relatively coarse, vs. the situation-specific FrameNet role system or the verb-specific PropBank role system, enabling generalizations across a wide semantic scope.</S>
			<S sid ="53" ssid = "11">Swier and Stevenson (2005) and Yi et al.</S>
			<S sid ="54" ssid = "12">(2007) used VN for SRL.</S>
			<S sid ="55" ssid = "13">Verb type classification.</S>
			<S sid ="56" ssid = "14">Quite a few works have addressed the issue of verb type classification and in particular classification to ‘Levin inspired’ classes (e.g., (Schulte im Walde, 2000; Merlo and Stevenson, 2001)).</S>
			<S sid ="57" ssid = "15">Such work is not comparable to ours, as it deals with verb type (sense) rather than verb token (instance) classification.</S>
			<S sid ="58" ssid = "16">Verb token classification.</S>
			<S sid ="59" ssid = "17">Lapata and Brew (2004) dealt with classification to Levin classes of polysemous verbs.</S>
			<S sid ="60" ssid = "18">They established a prior from the BNC in an unsupervised manner.</S>
			<S sid ="61" ssid = "19">They also showed that this prior helps in the training of a naive Bayes classifier employed to distinguish between possible verb classes of a given verb in a given frame (when the ambiguity is not solved by knowing the frame alone).</S>
			<S sid ="62" ssid = "20">Li and Brew (2007) extended this model by proposing a method to train the class disambiguator without using hand-tagged data.</S>
			<S sid ="63" ssid = "21">While these papers have good results, their experimental setup was rather narrow and used only at most 67 polysemous verbs (in 4 frames).</S>
			<S sid ="64" ssid = "22">VN includes 912 polysemous verbs, of which 695 appeared in our in-domain experiments.</S>
			<S sid ="65" ssid = "23">Girju et al.</S>
			<S sid ="66" ssid = "24">(2005) performed the only previous work we are aware of that addresses the problem of token level verb disambiguation into VN classes.</S>
			<S sid ="67" ssid = "25">They treated the task as a supervised learning problem, proposing features based on a POS tagger, a Chunker and a named entity classifier.</S>
			<S sid ="68" ssid = "26">In order to create the data4 , they used a mapping between Propbank rolesets and VN classes, and took the instances in WSJ sections 1518,20,21 that were annotated by Propbank and for which the roleset determines the VN class uniquely.</S>
			<S sid ="69" ssid = "27">This resulted in most instances being in fact monosemous.</S>
			<S sid ="70" ssid = "28">Their the English verb lexicon is demonstrated by the</S>
	</SECTION>
	<SECTION title="Propbank (Palmer et al., 2005) is a corpus annotation of. " number = "3">
			<S sid ="71" ssid = "1">1 Our annotations will be made available to the community..</S>
			<S sid ="72" ssid = "2">2 Using the same sentences and instances, obtained from.</S>
			<S sid ="73" ssid = "3">the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition.</S>
			<S sid ="74" ssid = "4">experiment was conducted in a WSJ in-domain scenario, and in a much narrower scope than in this paper.</S>
			<S sid ="75" ssid = "5">They had 870 (39 polysemous) unique verb lemmas, compared to 2091 (695 polysemous) in our in-domain scenario.</S>
			<S sid ="76" ssid = "6">They did not test their model in an adaptation scenario.</S>
			<S sid ="77" ssid = "7">The scope and difficulty contrast between our setup and theirs are demonstrated by the large differences in the number of instances and in the percentage of polysemous instances: 972/12431 (7.8%) in theirs, compared to 49571/84749 (58.5%) in our in-domain scenario (training+test).</S>
			<S sid ="78" ssid = "8">We compared our method to theirs for completeness and achieved similar results.</S>
			<S sid ="79" ssid = "9">Semlink.</S>
			<S sid ="80" ssid = "10">The Semlink project (Yi et al., 2007; Loper et al., 2007) aims to create a mapping of PropBank, FrameNet (Baker et al., 1998), Word- Net (henceforth WN) and VN to one another, thus allowing these resources to synergize.</S>
			<S sid ="81" ssid = "11">In addition, the project includes the most extensive token mapping of verbs to their VN classes available today.</S>
			<S sid ="82" ssid = "12">It covers all verbs in the WSJ sections of the Penn Treebank within VN coverage (out of 113K verb instances, 97K have lemmas present in VN).</S>
			<S sid ="83" ssid = "13">3 Nature of the Task.</S>
			<S sid ="84" ssid = "14">Polysemy is a major issue in NLP.</S>
			<S sid ="85" ssid = "15">Verbs are not an exception, resulting in a single verb form (lemma) appearing in more than a single class.</S>
			<S sid ="86" ssid = "16">This pol- ysemy is also present in the original Levin classification, where polysemous classes account for more than 48% of the BNC verb instances (Lapata and Brew, 2004).</S>
			<S sid ="87" ssid = "17">Given a verb instance whose lemma is within the coverage of VN, given the sentence in which it appears, given a parse tree of this sentence (see below), and given the VN resource, our task is to classify the verb instance to its correct VN class.</S>
			<S sid ="88" ssid = "18">There are currently 237 possible classes5 . Each verb has only a few possible classes (no more than 10, but only about 2.5 on the average over the polysemous verbs).</S>
			<S sid ="89" ssid = "19">Depending on the application, the parse tree for the sentence may be either a gold standard parse or a parse tree generated by a parser.</S>
			<S sid ="90" ssid = "20">We have experimented with both options.</S>
			<S sid ="91" ssid = "21">The task can be viewed in two complementary ways: per-class and per-verb type.</S>
			<S sid ="92" ssid = "22">The per- class perspective takes into consideration the small 5 We ignore subclass distinctions.</S>
			<S sid ="93" ssid = "23">This is justified since in. 98.2% of the in-coverage instances in Semlink, knowing the number of classes relative to the number of types6 . A classifier may gather valuable information for all members of a certain VN class, without seeing all of its members in the training data.</S>
			<S sid ="94" ssid = "24">From this perspective the task resembles POS tagging.</S>
			<S sid ="95" ssid = "25">In both tasks there are many dozens (or more) of possible labels, while each word has only a small subset of possible labels.</S>
			<S sid ="96" ssid = "26">Different words may receive the same label.</S>
			<S sid ="97" ssid = "27">The per-verb perspective takes into consideration the special properties of every verb type.</S>
			<S sid ="98" ssid = "28">Even the best lexicons necessarily ignore certain idiosyncratic characteristics of the verb when assigning it to a certain class.</S>
			<S sid ="99" ssid = "29">If a verb appears many times in the corpus, it is possible to estimate its parameters to a reasonable reliability, and thus to use its specific distributional properties for disambiguation.</S>
			<S sid ="100" ssid = "30">Viewed in this manner, the task resembles a word sense disambiguation (WSD) task: each verb has a small distinct set of senses (types), and no two different verbs have the same sense.</S>
			<S sid ="101" ssid = "31">The similarity to WSD suggests that our task might be solved by WN sense disambiguation followed by a mapping from WN to VN.</S>
			<S sid ="102" ssid = "32">However, good results are not to be expected, due to the medium quality of today’s WSD algorithms and because the mapping between WN and VN is both incomplete and many-to-many7 . Even for a perfect WN WSD algorithm, the resulting WN synset may not be mapped to VN at all or may be mapped onto multiple VN classes.</S>
			<S sid ="103" ssid = "33">We experimented with this method and obtained results below the MF baseline we used8 . The above discussion does not rule out the possibility of obtaining reasonable results through applying a high quality WSD engine followed by a WN to VN mapping.</S>
			<S sid ="104" ssid = "34">However, there are much fewer VN classes than WN classes per verb.</S>
			<S sid ="105" ssid = "35">This may result in the WSD engine learning many distinctions that are not useful in this context, which may in turn jeopardize its performance with respect to our task.</S>
			<S sid ="106" ssid = "36">Moreover, a word sense may belong to a single verb only while a VN class contains many verbs.</S>
			<S sid ="107" ssid = "37">Consequently, the performance 6 237 classes vs. 4991 types.</S>
			<S sid ="108" ssid = "38">7 In the WN to VN mapping built into VN, 14.69% of the covered WN synsets were mapped to more than a single VN class.</S>
			<S sid ="109" ssid = "39">8 We used the publicly available SenseLearner 2.0, the VB- Collocations model.</S>
			<S sid ="110" ssid = "40">We chose VN classes containing the lemma in random when a single mapping is not specified.</S>
			<S sid ="111" ssid = "41">We obtained 67.74% accuracy on section 00 of the WSJ, which is on a certain lemma may benefit from training instances of other lemmas.</S>
			<S sid ="112" ssid = "42">Note that our task is not reducible to VN frame identification (a nontrivial task given the richness of the information used to define a frame in VN).</S>
			<S sid ="113" ssid = "43">Although the categorizing criterion for Levin’s classification is the subset of frames the verb may appear in (equivalently, the diathesis alternations the verbal proposition may perform), knowing only the frame in which an instance appears does not suffice, as frames are shared among classes.</S>
	</SECTION>
	<SECTION title="The Learning Model. " number = "4">
			<S sid ="114" ssid = "1">As common in supervised learning models, we encode the verb instances into feature vectors and then apply a learning algorithm to induce a classifier.</S>
			<S sid ="115" ssid = "2">We first discuss the feature set and then the learning algorithm.</S>
			<S sid ="116" ssid = "3">Features.</S>
			<S sid ="117" ssid = "4">Our feature set heavily relies on syntactic annotation.</S>
			<S sid ="118" ssid = "5">Dorr and Jones (1996) showed that perfect knowledge of the allowable syntactic frames for a verb allows 98% accuracy in type assignment to Levin classes.</S>
			<S sid ="119" ssid = "6">This motivates the encoding of the syntactic structure of the sentence as features, since we have no access to all frames, only to the one appearing in the sentence.</S>
			<S sid ="120" ssid = "7">Since some verbs may appear in the same syntactic frame in different VN classes, a model relying on the syntactic frame alone would not be able to disambiguate instances of these verbs when appearing in those frames.</S>
			<S sid ="121" ssid = "8">Hence our features include lexical context words.</S>
			<S sid ="122" ssid = "9">The parse tree enables us to use words that appear in specific syntactic slots rather than in a linear window around the verb.</S>
			<S sid ="123" ssid = "10">To this end, we use the head words of the neighboring constituents.</S>
			<S sid ="124" ssid = "11">The definition of the head of a constituent is given in (Collins, 1999).</S>
			<S sid ="125" ssid = "12">Our feature set is comprised of two parallel sets of features.</S>
			<S sid ="126" ssid = "13">The first contains features extracted from the parse tree and the verb’s lemma as a standalone feature.</S>
			<S sid ="127" ssid = "14">In the second set, each feature is a conjunction of a feature from the first set with the verb’s lemma.</S>
			<S sid ="128" ssid = "15">By doing so we created a general feature space shared by all verbs, and replications of it for each and every verb.</S>
			<S sid ="129" ssid = "16">This feature selection strategy was chosen in view of the two perspectives on the task (per-class and per-verb) discussed in Section 3.</S>
			<S sid ="130" ssid = "17">Our first set of features encodes the verb’s context as inferred from the sentence’s parse tree (Fig Fi rst Fe at ur e Se t T h e st e m m e d h e a d w or d s, P O S, p ar s e tr e e la b el s, fu n ct io n ta g s, a n d or di n al s o f th e v er b’ s ri g ht k r si bl in g s ( k r is th e m a xi m u m n u m b er o f ri g ht si b - li n g s in th e c or p u s. T h es e ar e at m o st 5 k r di ff ere nt fe at ur es ).</S>
			<S sid ="131" ssid = "18">Th e ste m m ed he ad w or ds, P O S, la be ls, fu nc tio n ta g s a n d or di n al s o f th e v er b’ s le ft kl si bl in g s, a s a b o v e. Th e ste m m ed he ad w or d &amp; P O S of th e ‘se co nd h e a d w o r d ’ n o d e s o n th e le ft a n d ri g ht (s e e te xt f o r p re ci s e d ef in it io n ).</S>
			<S sid ="132" ssid = "19">Al l of th e ab ov e fe at ur es e m pl oy ed on th e sib li n g s o f th e p ar e nt o f th e v er b ( o nl y if th e v er b’ s p ar e nt is th e h e a d c o n st it u e nt o f it s g ra n d p ar e nt ) Th e nu m be r of rig ht/ lef t sib lin gs of th e ve rb.</S>
			<S sid ="133" ssid = "20">Th e nu m be r of rig ht/ lef t sib lin gs of th e ve rb’ s pa re nt.</S>
			<S sid ="134" ssid = "21">Th e pa rse tre e la be l of th e ve rb’ s pa re nt.</S>
			<S sid ="135" ssid = "22">Th e ve rb’ s vo ice (a cti ve or pa ssi ve ).</S>
			<S sid ="136" ssid = "23">Th e ve rb’ s le m m a. Figure 1: The first set of features in our model.</S>
			<S sid ="137" ssid = "24">All of them are binary.</S>
			<S sid ="138" ssid = "25">The final feature set includes two sets: the set here, and a set obtained by its conjunction with the verb’s lemma.</S>
			<S sid ="139" ssid = "26">ure 1).</S>
			<S sid ="140" ssid = "27">We attempt to encode both the syntactic frame, by encoding the tree structure, and the argument preferences, by encoding the head words of the arguments and their POS.</S>
			<S sid ="141" ssid = "28">The restriction on the verb’s parent being the head constituent of its grandparent is done in order to focus on the correct verb in verb series such as ‘intend to run’.</S>
			<S sid ="142" ssid = "29">The 3rd cell in the table makes use of a ‘second head word’ node, defined as follows.</S>
			<S sid ="143" ssid = "30">Consider a left sibling (right siblings are addressed analogously) M of the verb’s node.</S>
			<S sid ="144" ssid = "31">Take the node H in the subtree of M where M ’s head appears.</S>
			<S sid ="145" ssid = "32">H is a descendent of a node J which is a child of M . The ‘second head word’ node is J ’s sibling on the right.</S>
			<S sid ="146" ssid = "33">For example, in the sentence We went to school (see Figure 2) the head word of the PP ‘to school’ is ‘to’, and the ‘second head word’ node is ‘school’.</S>
			<S sid ="147" ssid = "34">The rationale is that ‘school’ could be a useful feature for ‘went’, in addition to ‘to’, which is highly polysemous (note that it is also a feature for ‘went’, in the 1st and 2nd cells of the table).</S>
			<S sid ="148" ssid = "35">The voice feature was computed using a simple heuristic based on the verb’s POS tag (past participle) and presence of auxiliary verbs to its left.</S>
			<S sid ="149" ssid = "36">NP PRP We S VBD went VP PP TO NP to NN school parser (Charniak and Johnson, 2005) (Note that this parser does not output function tags).</S>
			<S sid ="150" ssid = "37">The parser was also trained on sections 0221 and tuned on section 0010 . Consequently, our adaptation scenario is a full adaptation situation in which both the parser and the VerbNet training data are not in the test domain.</S>
			<S sid ="151" ssid = "38">Note that generative parser adaptation Figure 2: An example parse tree for the ‘second head word’ feature.</S>
			<S sid ="152" ssid = "39">The current set of features does not detect verb particle constructions.</S>
			<S sid ="153" ssid = "40">We leave this for future research.</S>
			<S sid ="154" ssid = "41">Learning Algorithm.</S>
			<S sid ="155" ssid = "42">Our learning task can be formulated as follows.</S>
			<S sid ="156" ssid = "43">Let xi denote the feature vector of an instance i, and let X denote the space of all such feature vectors.</S>
			<S sid ="157" ssid = "44">The subset of possible labels for xi is denoted by Ci, and the correct label by ci ∈ Ci.</S>
			<S sid ="158" ssid = "45">We denote the label space by S. Let T be the training set of instances T = {&lt; x1, C1 , c1 &gt;, &lt; x2, C2 , c2 &gt;, ..., &lt; xn, Cn, cn &gt; } ⊆ (X × 2S × S)n, where n is the size of the training set.</S>
			<S sid ="159" ssid = "46">Let &lt; xn+1, Cn+1 &gt;∈ (X × 2S ) be a new instance.</S>
			<S sid ="160" ssid = "47">Our task is to select which of the labels in Cn+1 is its correct label cn+1 (xn+1 does not have to be a previously observed lemma, but its lemma must appear in a VN class).</S>
			<S sid ="161" ssid = "48">The structure of the task lets us apply a learning algorithm that is especially appropriate for it.</S>
			<S sid ="162" ssid = "49">What we need is an algorithm that allows us to restrict the possible labels of each instance, both in training and in testing.</S>
			<S sid ="163" ssid = "50">The sequential model algorithm presented by Even-Zohar and Roth (2001) directly supports this requirement.</S>
			<S sid ="164" ssid = "51">We use the SNOW learning architecture for multi-class classification (Roth, 1998), which contains an implementation of that algorithm 9 .</S>
	</SECTION>
	<SECTION title="Experimental Setup. " number = "5">
			<S sid ="165" ssid = "1">We used SemLink VN annotations and parse trees on sections 0221 of the WSJ Penn Treebank for training, and section 00 as a development set, as is common in the parsing community.</S>
			<S sid ="166" ssid = "2">We performed two parallel sets of experiments, one using manually created gold standard parse trees and one using parse trees created by a state-of-the-art 9 Experiments on development data revealed that for verbs for which almost all of the training instances are mapped to the same VN class, it is most beneficial to select that class.</S>
			<S sid ="167" ssid = "3">Thus, where more than 90% of the training instances of a verb are mapped to the same class, our algorithm mapped the instances of the verb to that class regardless of the context.</S>
			<S sid ="168" ssid = "4">results are known to be of much lower quality than in-domain results (Lease and Charniak, 2005).</S>
			<S sid ="169" ssid = "5">The quality of the discriminative parser we used did indeed decrease in our adaptation scenario (Section 7).</S>
			<S sid ="170" ssid = "6">The training data included 71209 VN in-scope instances (of them 41753 polysemous) and the development 3624 instances (2203 polysemous).</S>
			<S sid ="171" ssid = "7">An ‘in-scope’ instance is one that appears in VN and is tagged with a verb POS.</S>
			<S sid ="172" ssid = "8">The same trained model was used in both the in-domain and adaptation scenarios, which only differ in their test sets.</S>
			<S sid ="173" ssid = "9">In-Domain.</S>
			<S sid ="174" ssid = "10">Tests were held on sections 01,22,23,24 of WSJ PTB.</S>
			<S sid ="175" ssid = "11">Test data includes all in- scope instances for which there is a SemLink annotation, yielding 13540 instances, 7798 (i.e., 57.6%) of them polysemous.</S>
			<S sid ="176" ssid = "12">Adaptation.</S>
			<S sid ="177" ssid = "13">For the testing we annotated sentences from GENIA (Kim et al., 2003) (version 3.0.2).</S>
			<S sid ="178" ssid = "14">The GENIA corpus is composed of MED- LINE abstracts related to transcription factors in human blood cells.</S>
			<S sid ="179" ssid = "15">We annotated 400 sentences from the corpus, each including at least one in- scope verb instance.</S>
			<S sid ="180" ssid = "16">We took the first 400 sentences from the corpus that met that criterion11 . After cleaning some GENIA POS inconsistencies, this amounts to 690 in-scope instances (380 of them polysemous).</S>
			<S sid ="181" ssid = "17">The tagging was done by two annotators with an inter-annotator agreement rate of 80.35% and Kappa 67.66%.</S>
			<S sid ="182" ssid = "18">Baselines.</S>
			<S sid ="183" ssid = "19">We used two baselines, random and most frequent (MF).</S>
			<S sid ="184" ssid = "20">The random baseline selects uniformly and independently one of the possible classes of the verb.</S>
			<S sid ="185" ssid = "21">The most frequent (MF) baseline selects the most frequent class of the verb in the training data for verbs seen while training, and selects in random for the unseen ones.</S>
			<S sid ="186" ssid = "22">Consequently, it obtains a perfect score over the monosemous verbs.</S>
			<S sid ="187" ssid = "23">This baseline is a strong one and is common in disambiguation tasks.We repeated all of the setup above in two sce 10 For the very few sentences out of coverage for the parser, we used the MF baseline (see below).</S>
			<S sid ="188" ssid = "24">11 Discarding the first 120 sentences, which were used to design the annotator guidelines.</S>
			<S sid ="189" ssid = "25">narios.</S>
			<S sid ="190" ssid = "26">In the first (main) scenario, in-scope instances were always mapped to VN classes, while in the second (‘other is possible’ (OIP)) scenario, in-scope instances were allowed to be tagged (during training) and classified (during test) as not belonging to any existing VN class12 . In all cases, out-of-scope instances (verbs whose lemmas do not appear in VN) were ignored.</S>
			<S sid ="191" ssid = "27">For the OIP scenario, we used a different ‘other’ label for each of the lemmas, not a single label shared by them all.</S>
	</SECTION>
	<SECTION title="Results. " number = "6">
			<S sid ="192" ssid = "1">Table 1 shows our results.</S>
			<S sid ="193" ssid = "2">In addition to the overall results, we also show results for the polysemous ones alone, since the task is trivial for the monosemous ones.</S>
			<S sid ="194" ssid = "3">The results using gold standard parses effectively set an upper bound on our model’s performance, while those using statistical parser output demonstrate its current usability.</S>
			<S sid ="195" ssid = "4">In-Domain.</S>
			<S sid ="196" ssid = "5">Results are shown in the WSJ → WSJ columns of Table 1.</S>
			<S sid ="197" ssid = "6">Using gold standard parses (top), we achieve 96.42% accuracy overall.</S>
			<S sid ="198" ssid = "7">Over the polysemous verbs, the accuracy is 93.68%.</S>
			<S sid ="199" ssid = "8">This translates to an error reduction over the MF baseline of 43.35% overall and 43.22% for the polysemous verbs.</S>
			<S sid ="200" ssid = "9">In the ‘other is possible’ scenario (right), we obtained 36.67% error reduction.</S>
			<S sid ="201" ssid = "10">Using a state-of-the-art parser (Charniak and Johnson, 2005) (bottom), we experienced some degradation of the results (as expected), but they remained significantly above baseline.</S>
			<S sid ="202" ssid = "11">We achieve 95.9% accuracy overall and 92.77% for the polysemous verbs, which translates to about 35.13% and 35.04% error reduction respectively.</S>
			<S sid ="203" ssid = "12">In the OIP scenario, we obtained 28.95% error reduction.</S>
			<S sid ="204" ssid = "13">The results of the random baseline for the in- domain scenario are substantially worse than the MF baseline.</S>
			<S sid ="205" ssid = "14">On the WSJ the random baseline scored 66.97% (37.51%) accuracy in the main (OIP) scenarios.</S>
			<S sid ="206" ssid = "15">Adaptation.</S>
			<S sid ="207" ssid = "16">Here we test our model’s ability to generalize across domains.</S>
			<S sid ="208" ssid = "17">Since VN is supposed to be a domain independent resource, we hope to acquire statistics that are relevant across domains as well and so to enable us to automatically map verbs in domains of various genres.</S>
			<S sid ="209" ssid = "18">The results are shown in the WSJ → GENIA columns of Table 1.</S>
			<S sid ="210" ssid = "19">When using gold standard parses, our model scored 73.16%.</S>
			<S sid ="211" ssid = "20">This translates to about13.17% ER on GENIA.</S>
			<S sid ="212" ssid = "21">We interestingly experi 12i.e., including instances tagged by SemLink as ‘none’.</S>
			<S sid ="213" ssid = "22">enced very little degradation in the results when moving to parser output, achieving 72.4% accuracy which translates to 10.71% error reduction over the MF baseline.</S>
			<S sid ="214" ssid = "23">The random baseline on GENIA was again worse than MF, obtaining 66.04% accuracy as compared to 69.09% of MF (in the OIP scenario, 39.12% compared to 46.41%).</S>
			<S sid ="215" ssid = "24">Run-time performance.</S>
			<S sid ="216" ssid = "25">Given a parsed corpus, our main model trains and runs in no morethan a few minutes for a training set of ∼60K in stances and a test set of ∼11K instances, using aPentium 4 CPU 2.40GHz with 1GB main mem ory.</S>
			<S sid ="217" ssid = "26">The bottleneck in tagging large corpora using our model is thus most likely the running time of current parsers.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "7">
			<S sid ="218" ssid = "1">In this paper we introduced a new statistical model for automatically mapping verb instances into VerbNet classes, and presented the first large-scale experiments for this task, for both in-domain and corpus adaptation scenarios.</S>
			<S sid ="219" ssid = "2">Using gold standard parse trees, we achieved 96.42% accuracy on WSJ test data, showing 43.35% error reduction over a strong baseline.</S>
			<S sid ="220" ssid = "3">For adaptation to the GENIA corpus, we showed 13.1% error reduction over the baseline.</S>
			<S sid ="221" ssid = "4">A surprising result in the context of adaptation is the little influence of using gold standard parses versus using parser output, especially given the relatively low performance of today’s parsers in the adaptation task (91.4% F-score for the WSJ in-domain scenario compared to 81.24% F-score when parsing our GENIA test set).</S>
			<S sid ="222" ssid = "5">This is an interesting direction for future work.</S>
			<S sid ="223" ssid = "6">In addition, we conducted some additional preliminary experiments in order to shed light on some aspects of the task.</S>
			<S sid ="224" ssid = "7">The experiments reported below were conducted on the development data, given gold standard parse trees.</S>
			<S sid ="225" ssid = "8">First, motivated by the close connection between WSD and our task (see Section 3), we conducted an experiment to test the applicability of using a WSD engine.</S>
			<S sid ="226" ssid = "9">In addition to the experiments listed above, we also attempted to encode the output of a modern WSD engine (the VBCollo- cations Model of SenseLearner 2.0 (Mihalcea and Csomai, 2005)), both by encoding the synset (if exists) of the verb instance as a feature, and by encoding each possible mapped class of the WSD engine output synset as a feature.</S>
			<S sid ="227" ssid = "10">There are k Main Scenario ‘Other is Possible’ (OIP) Scenario WSJ→ WSJ WSJ→ GENIA WSJ→ WSJ WSJ→ GENIA M F Mo del MF Mo del M F Mo del MF Mo del Go ld Std Tot al ER 93.</S>
			<S sid ="228" ssid = "11">68 96.</S>
			<S sid ="229" ssid = "12">42 43.</S>
			<S sid ="230" ssid = "13">35 69.</S>
			<S sid ="231" ssid = "14">09 73.</S>
			<S sid ="232" ssid = "15">16 13.</S>
			<S sid ="233" ssid = "16">17 88.</S>
			<S sid ="234" ssid = "17">6 92.</S>
			<S sid ="235" ssid = "18">78 36.</S>
			<S sid ="236" ssid = "19">67 46.</S>
			<S sid ="237" ssid = "20">41 52.</S>
			<S sid ="238" ssid = "21">46 11.</S>
			<S sid ="239" ssid = "22">29 Pol y. ER 88.</S>
			<S sid ="240" ssid = "23">87 93.</S>
			<S sid ="241" ssid = "24">68 43.</S>
			<S sid ="242" ssid = "25">22 48.</S>
			<S sid ="243" ssid = "26">58 55.</S>
			<S sid ="244" ssid = "27">35 13.</S>
			<S sid ="245" ssid = "28">17 – – – – – – Par ser Tot al ER 93.</S>
			<S sid ="246" ssid = "29">68 95.</S>
			<S sid ="247" ssid = "30">9 35.</S>
			<S sid ="248" ssid = "31">13 69.</S>
			<S sid ="249" ssid = "32">09 72.</S>
			<S sid ="250" ssid = "33">4 10.</S>
			<S sid ="251" ssid = "34">71 88.</S>
			<S sid ="252" ssid = "35">6 91.</S>
			<S sid ="253" ssid = "36">9 28.</S>
			<S sid ="254" ssid = "37">95 46.</S>
			<S sid ="255" ssid = "38">41 52.</S>
			<S sid ="256" ssid = "39">46 11.</S>
			<S sid ="257" ssid = "40">29 Pol y. ER 88.</S>
			<S sid ="258" ssid = "41">87 92.</S>
			<S sid ="259" ssid = "42">77 35.</S>
			<S sid ="260" ssid = "43">04 48.</S>
			<S sid ="261" ssid = "44">58 55.</S>
			<S sid ="262" ssid = "45">35 10.</S>
			<S sid ="263" ssid = "46">72 – – – – – – Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline.</S>
			<S sid ="264" ssid = "47">Error reduction is computed as M ODEL− M F . Results are given for the WSJ and GENIA corpora test 100−M F sets.</S>
			<S sid ="265" ssid = "48">The top table is for a model receiving gold standard parses of the test data.</S>
			<S sid ="266" ssid = "49">The bottom is for a model using (Charniak and Johnson, 2005) state-of-the-art parses of the test data.</S>
			<S sid ="267" ssid = "50">In the main scenario (left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (during both training and test) to map instances as not belonging to any existing class.</S>
			<S sid ="268" ssid = "51">For the latter, no results are displayed for polysemous verbs, since each verb can be mapped both to ‘other’ and to at least one class.</S>
			<S sid ="269" ssid = "52">features if there are k possible classes13 . There was no improvement over the previous model.</S>
			<S sid ="270" ssid = "53">A possible reason for this is the performance of the WSD engine (e.g. 56.1% precision on the verbs in Senseval3 all-words task data).</S>
			<S sid ="271" ssid = "54">Naturally, more research is needed to establish better methods of incorporating WSD information to assist in this task.</S>
			<S sid ="272" ssid = "55">Second, we studied the relative usability of class information as opposed to verb idiosyncratic information in the VN disambiguation task.</S>
			<S sid ="273" ssid = "56">By measuring the accuracy of our model, first given the per-class features (the first set of features excluding the verb’s lemma feature) and second given the per-verb features (the conjunction of the first set with the verb’s lemma), we tried to address this question.</S>
			<S sid ="274" ssid = "57">We obtained 94.82% accuracy for the per-class experiment, and 95.51% for the per-verb experiment, compared to 95.95% when using both in the in-domain gold standard scenario.</S>
			<S sid ="275" ssid = "58">The MF baseline scored 92.45% on this development set.</S>
			<S sid ="276" ssid = "59">These results, which are close in the per-class experiment to those of the MF baseline, indicate that combining both approaches in the construction of the classifier is justified.</S>
			<S sid ="277" ssid = "60">Third, we studied the importance of having a learning algorithm utilizing the task’s structure (mapping into a large label space where each in 13The mapping is many-to-many and partial.</S>
			<S sid ="278" ssid = "61">To overcome the first issue, given a WN sense of the verb, we encoded all possible VN classes that correspond to it.</S>
			<S sid ="279" ssid = "62">To overcome the second, we treated a verb in a certain VN class, for which the mapping to WN was available, as one that can be mapped to all WN senses of the verb.</S>
			<S sid ="280" ssid = "63">stance can be mapped to only a small subspace).</S>
			<S sid ="281" ssid = "64">Our choice of the algorithm in (Even-Zohar and Roth, 2001) was done in light of this requirement.</S>
			<S sid ="282" ssid = "65">We conducted an experiment in which we omitted these per-instance restrictions on the label space, effectively allowing each verb to take every label in the label space.</S>
			<S sid ="283" ssid = "66">We obtained 94.54% accuracy, which translates to 27.68% error reduction, compared to 95.95% accuracy (46.36% error reduction) when using the restrictions.</S>
			<S sid ="284" ssid = "67">These results indicate that although our feature set keeps us substantially above baseline even without the above algorithm, using it boosts our results even further.</S>
			<S sid ="285" ssid = "68">This result is different from the results obtained in (Girju et al., 2005), where the results of the un- constrained (flat) model were significantly below baseline.</S>
			<S sid ="286" ssid = "69">As noted earlier, the field of instance level verb classification into Levin-inspired classes is far from being exhaustively explored.</S>
			<S sid ="287" ssid = "70">We intend to make our implementation of the model available to the community, to enable others to engage in further research on this task.</S>
			<S sid ="288" ssid = "71">Acknowledgements.</S>
			<S sid ="289" ssid = "72">We would like to thank Dan Roth, Mark Sammons and Ran Luria for their help.</S>
	</SECTION>
</PAPER>
