Using Subcategorization to Resolve Verb Class Ambiguity



Maria Lapata
School of Cognitive Science 
Division of Informatics 
University of Edinburgh
2 Buccleuch Place 
Edinburgh EH8 9LW, UK 
mlap@cogsci.ed.ac.uk


Abstract
Levin's (1993) taxonomy of verbs and their classes 
is a widely used resource for lexical semantics. In 
her framework, some verbs, such as give exhibit no 
class ambiguity. But other verbs, such as write, can 
inhabit more than one class. In some of these am­


Chris Brew
HCRC  Language Technology Group 
Division of Informatics University of 
Edinburgh
2 Buccleuch Place
Edinburgh EH8  9LW, UK
chrisbr@cogsci.ed.ac.uk


(cf.  (3a))  and  the  prepositional  frame  NP-V-NP­
PP10, (cf. (3b)).

(l)  a. 	Janet broke the cup. 
b.	The cup broke.
(2)   a. 	Bill sold a car to Tom. 
b.	Bill sold Tom a car.


biguous cases the appropriate class for a particular
token of a verb is immediately obvious from inspec­
tion of the surrounding context. In others it is not,


(3)   a.
b.


Martha carved the baby a toy.
Martha carved a toy for the baby.


and an application which wants to recover this infor­ 
mation will be forced to rely on some more or Jess 
elaborate process of inference. We present a simple 
statistical model of verb class ambiguity and show 
how it can be used to carry out such inference.

1   Introduction
The relation between the syntactic realization of a 
verb's arguments and its meaning has been exten­ 
sively  studied  in  Levin  (1993).  Levin's  work  re­ 
lies on the hypothesis that "the behavior of a verb, 
particularly  with respect to the expression and in­ 
terpretation of its arguments, is to a large extent 
determined  by  its  meaning"  (Levin,  1993,  p.  I). 
Verbs which display the same diathesis alterna­ 
tions-alternations   in the realization  of their argu­ 
ment structure-are assumed to share certain mean­ 
ing components  and are organized into a semanti­ 
cally coherent class.
  As   an   example   consider   sentences   (1)-(3) 
taken from Levin. Example (l) iiiustrates the 
causative/inchoative  alternation.  Verbs undergoing 
this alternation can be manifested either as transi­ 
tive with a causative reading (cf. (I a)) or as intransi­ 
tive with an inchoative reading (cf. (lb)).  Examples 
(2) and (3) illustrate the dative and benefactive al­ 
ternations respectively. Verbs which license the for­ 
mer alternate between the prepositional frame NP­ 
V-NP-PPro (cf. (2a))  and the double object frame 
V-NP-NP (cf. (2b)), whereas verbs which undergo 
the latter alternate between the double object frame


Verbs like crack and chip pattern with break in li­
censing the causative/inchoative alternation and are 
associated with the semantic class of BREAK verbs. 
Verbs make and build behave similar  to carve in 
licensing the benefactive alternation and are mem­ 
bers of the class of BUILD verbs, whereas sell and 
give undergo the dative alternation and participate 
in the GIVE class. By grouping together verbs which 
pattern together  with respect  to diathesis  alterna­ 
tions Levin defines approximately 200 verb classes, 
which she argues reflect important semantic regu­ 
larities.

2   Motivation
Levin provides an index of 3,024  verbs for which 
she lists the semantic classes and diathesis alterna­ 
tions. The mapping  between  verbs and classes  is 
not one-to-one. Of the 3,024 verbs which she cov­ 
ers, 784 are listed as having more than one class. 
Even though Levin's monosemous verbs outnumber 
her polysemous verbs by a factor of nearly four to 
one, the total frequency of the former (4,252,715) 
is comparable to the total frequency of the latter 
(3,986,014).  This  means that close  to half of the 
cases processed by a hypothetical semantic  tagger 
would manifest some degree of ambiguity. The fre­ 
quencies are detailed in table  I   and were compiled 
from a lemmatized version of the British National 
Corpus  (BNC),  a  widely  distributed  100  million 
word collection of samples of written and spoken 
English (Burnard, 1995).



Classes
Verbs 	BNC frequency
I
2
3
4
5
6
7
lO
2,239
536
173
43
23
7
2
I
4,252,715
2,325,982
738.854
395,212
222,747
272,669
26,123
4,427

Table I: Polysemous verbs according to Levin


100

90

80

70
-•e   60



and FULFILLING. Each of these classes can in 
tum license four distinct syntactic frames. As 
shown in the examples 1      below, in (4a) serve 
appears ditran­ sitively and belongs to the 
semantic class of GIVE verbs, in (4b) it occurs 
transitively and is a mem­ ber of the class of 
FIT verbs, in ( 4c) it takes the predicative 
complement as minister  of  the interior and is a 
member of MASQUERADE verbs. Finally, in 
sentence (4d) serve is a FULFILLING  verb and 
takes two complements, a noun phrase (an 
appren­ ticeship) and a prepositional phrase 
headed by to. In the case of verbs like serve  
we can guess their semantic class solely on the 
basis of the frame with which they appear.


0
" so

40
0.


30

20

10

0
I	2     3   4     S     6     7     8   9    10   II    12
Number of alternations


Figure 1: Relation  between number of classes and 
alternations


  Furthermore,  as shown in figure 1, the number 
of alternations  licensed by a given verb increases 
with  the  number  of  classes  it  inhabits.  Consider 
for example verbs participating in one alternation 
only: of these, 90.4% have one semantic class, 8.6% 
have two classes, 0.7% have three classes and 0.3% 
have four classes. In contrast, of the verbs licensing 
six different alternations, 14% have one class, I 7% 
have two classes, 12.4% have three classes, 53.6% 
have four classes, 2% have six classes and 1% has 
seven classes.
  Palmer (1999) and Dang et a!. (1998) argue that 
the use of syntactic frames and verb classes can sim­ 
plify the definition of different verb senses. Beyond 
this, we claim that information about the argument 
structure of a polysemous  verb can often help dis­ 
ambiguating it.
  Consider  for instance the verb serve  which is a 
member of four classes: GIVE, FIT, MASQUERADE









But sometimes we do not have the syntactic infor­ 
mation that would provide cues for semantic disam­ 
biguation. Consider sentence (5a). The verb  write 
is a member of three Levin classes, two of which 
(MESSAGE TRANSFER, PERFORMANCE) take the 
ditransitive  frame  NP-V-NP-NP. In  this  case  we 
have the choice between the "message transfer" 
reading (cf. (5a))  and the "performance"  reading 
(cf. (5b)). This is an instance of the common prob­ 
lem of inferring the value of a hidden variable (in 
this case the "true  class" of a particular  instance 
of  write). The same situation arises with the verb 
phone which is listed as a GET verb and an INSTRU­ 
MENT OF COMMUNICATION verb and in both cases 
can take the frame NP-V-NP-NP. In sentence (5c) 
the preferred reading is that of "get" instead of "in­ 
strument of communication" (cf. sentence (5d)).

(5)
a.
A solicitor wrote him a letter at the air­


b.
port.
I want you to write me a screenplay called


"The Trip".
    1 Unless stated otherwise the example sentences were taken 
from the BNC and simplified for clarification purposes.


c. 	I'II phone you a taxi.
d. 	As I entered the room I wished I'd thought 
of phoning a desperate SOS to James.

The  objective   of  this  paper  is  to  address   the 
verb class  disambiguation  problem by developing 
a probabilistic framework which combines linguis­


By	substituting 	(7) 	and	(8) 	into 	(6),
P (verb, class ,frame) can be written as:

(9)   P(verb,frame, class),
P (verb) P (frame Iverb) P (frame Iclass) P (class)
P(jrame)


tic knowledge (i.e., Levin's classification) and frame 
frequencies acquired from the BNC. Our initial ex­ 
periments focus on the syntactic frames characteris­
tic for the dative and benefactive alternations (cf. ex­


We	estimate
P(jramelverb),
as follows:


the	probabilities
P(jramelclass) 	and


f(verb)


P(verb),
P(class)


amples (2) and (3)). These frames are licensed by 
a fairly large number of classes: 19 classes license 
the double object frame, 22 license the NP-V-NP­ 
PP10  frame  and  14  classes  license  the  NP-V-NP
PP1," frame. The semantic and syntactic properties 
of these alternations  have been extensively studied
and are well understood  (see Levin (1993) and the 
references therein). Furthermore, they are fairly pro­ 
ductive and one would expect them to be well rep­
resented in a large corpus.


(10)  P(verb)""'  "
L.. f(verb;)


f (verb.jrame)
(I I) P(jramelverb)""'  "
L.. j(verb.frame;)


f(class,frame)
(12)  P(jramelclass) ""' "
L..  j(c/ass.frame;)


  In section 3 we describe the statistical model and 
the estimation of the various model parameters, sec­ 
tion 4 presents some  preliminary  results and sec­ 
tion 5 contains some discussion and concluding re­



(13)  P(class) ""'


j(class)
"
L.. f(class;)


f(jrame)


marks.

3    The Model

We view the choice  of a class  for a polysemous 
verb in a given frame as the joint probability 
P(verb,Jrame, class)   which  we rewrite using the 
chain rule in (6).

(6)    P(verb,frame, class)=  P(verb)
P (frame I verb) P(classlverb,frame)

We also make the following independence assump­
tion:

(7)    P(classlverb,frame) P(classlframe)

The independence assumption reflects Levin's hy­ 
pothesis that the argument structure of a given verb 
is a direct reflection of its meaning. Accordingly we 
assume that the semantic class determines the argu­ 
ment structure of its members without making ref­ 
erence to the individual  verbs. By applying Bayes 
Law we write P(classlframe) as:

.	P(jramelclass)P(class)


(14)  P (frame)  ""'  "
L.. f(jrame;)


It is easy  to obtain  f (verb)  from  the lemmatized 
BNC. For the experiments  reported here, syntactic 
frames for the dative and benefactive alternations 
were automatically  extracted from the BNC using 
Gsearch (Keller et al., 1999), a tool which facilitates 
search of arbitrary POS-tagged corpora for shallow 
syntactic patterns based on a user-specified context­ 
free grammar and a syntacti-c query. The acquisition 
and filtering process is detailed  in Lapata (1999). 
We rely on Gsearch  to provide  moderately  accu­ 
rate information about verb frames in the same way 
that Hindle and Rooth (1993) relied on Fidditch to 
provide moderately accurate information about syn­ 
tactic structure,  and Ratnaparkhi  ( 1998) relied on 
simple  heuristics defined over part-of-speech  tags 
to deliver information nearly as useful as that pro­ 
vided by Fidditch. We estimated  f(verb,frame)  as 
the number of times a verb co-occurred with a par­ 
ticular frame in the corpus.
We cannot read off P (jramelclass) from the cor­
pus, because it is not annotated  with verb classes. 
Nevertheless we can use the information  listed in
Levin with respect to the syntactic  frames exhib­


(8)


P(c 
lass  
rame)  
= ---
-''--- 
--'----
-'-- 	.:_
P
(
j
r
a
m
e
)


ited 
by 
the 
verbs 
of a 
given 
class. 
For 
each 
class


I   Class                     Frames








NP-V-NP-PP10 , NP-V-NP-PPJltr•





Table 2: Sample of verb classes and their syntactic 
frames



we recorded the syntactic frames it licenses (cf. ta­ 
ble 2).  Levin's  description  of the argument struc­ 
ture of various verbs goes beyond the simple list­ 
ing of their subcategorization.  Useful information 
is provided about the thematic roles of verbal argu­ 
ments and their interpretation. Consider the exam­ 
ples in (15): in (15a) the verb present is a member of 
the FULFILLING class and its theme is expressed by 
the prepositional phrase with an award, in (15b) the 
PP headed by with receives a locative interpretation 
and the verb load inhabits the SPRAY/LOAD class, 
whereas in (15c) the prepositional phrase is instru­ 
mental and hit inhabits the HIT class. None of the 
information concerning thematic roles was retained. 
All three classes (FULFILLING, SPRAY/LOAD and 
HIT) were assigned the frame NP-V-NP-PPwiri.'.


I   Class    si-e(class)  p(c/asslamb_c/ass) f(verb  class) 
I
THROW        27                    0.40                    
7783.6
SEND	20 	0.27 
	5253.9
GIVE	15	0.20 
	3891.8
MARRY	10 	0.13 
	2529.6

Table 3: Estimation of .f(verb. class)  for the 
verb
pas
s


The  estimate  of   f(verb, class)   for  monosemous 
verbs reduces to the count of the verb in the cor­ 
pus. Once again we cannot estimate .f(verb, 
class) for polysemous verbs directly. All we 
have is the overall frequency of a given verb in 
the BNC and the number of classes it is a member 
of according to Levin. We rewrite f(verb, class) 
as:

(17)  .f(verb, class)= 
.f(verb)p(classlverb)

We approximate p(classlverb) by collapsing 
across all verbs that have the appropriate pattern 
of ambi­ guity:

(18) .f(verb, class) "'='  
.f(verb)p(c/asslamb_class)

Here amb_c/ass,  the ambiguity class of a verb, is 
the set of classes that it might inhabit 2 We 
collapse verbs into ambiguity classes in order to 
reduce the number of parameters which must be 
estimated: we certainly lose information,  but the 
approximation makes it easier to get reliable 
estimates from limited data. In future work we 
plan to use the EM algo­ rithm (Dempster et al., 
1977) to uncover the hidden class, but for the 
present study, we simply approxi­
mate p(classlamb_class) using a heuristic based 
on


(15)  a.
b.
c.


John 
presented 
the 
student 
with an 
award.
J
o
h
n 
l
o
a
d
e
d 
t
h
e 
tr
u
c
k 
w
it
h 
b
ri
c
k
s. 
J
o
h
n 
h
it 
t
h
e 
w
al
l 
w
it
h 
a 
h
a
m
m
e
r.


class 
size:

(19)  
p(classla
mb..class
) "'='




s
i
z
e
(
 
c
l
a
s
s
)
-
-
=
L
=
-
-
-
.,i
-
ze
-
(-
.,.
c)
5


Because  we  didn't   have  corpus  counts  for  the
quantity  .f(class,frame) we simply  assumed  that 
all  frames  for  a  given  class  are  equally  likely. 
This  means,  for  instance,  that  the  estimate  for 
P(NP-V-NP-NP10 IGIVE) is and similarly the es­ 
timate for  P(NP-VIPERFORMANCE)  is (cf. ta­
ble 2). This  is clearly  a simplification,  since one
would expect .f(class,.frame) to be different for dif­ 
ferent corpora, and to vary with respect to class size 
and the frequency of class members.
In order  to estimate  P(class) we first estimate
.f (class)  which we rewrite as follows:
(16)  .f(class) = L.f(verb;, class)


r.:  E amb...class

For each class we recorded the number of its mem­ 
bers after discarding  verbs whose frequency  was 
less than 1 per 1M in the BNC. This gave us a first 
approximation of the size of each class. We then 
computed, for each polysemous verb, the total size 
of the classes of which it was a member. We calcu­ 
lated p(classlamb_class) by dividing the former by 
the latter (cf. equation (19)). We obtained an esti­ 
mate for the class frequency .f(class) by multiply­ 
ing  p(classlamb_c/ass) by the observed frequency 
of the verb in the BNC (cf. equation (18)).

2our use of ambiguity classes is inspired by a similar use in
HMM based part-of-speech tagging (Kupiec. 1992).


6e+05





., 	•=


100

80


•=..

J!
<.>


4e+05




2e+0
5



•  
60
.E
- 
40

2
0
0 
	..	=









...



..  ..1


..	..	..	..



..	t..


..;


..I


i ., 	..	• 	.l!	.


z	'.i.	s	"'


I	I 	s	! !


'.i.


I	. 1	. 1
I	..	..	..	..


..	..


z	d.


I	I
I 	z	z


I 	z	I 	I	I


·c	! • 	J! 	=	.g


·;o


I	I	I


.!<	·	E	c


z	I 	z	'i


f	...


.•..	• 	E	.!l


.I.	z


.I.


d.	z	'.i.


I 	z	dz.


.•..


e 	.a
	1  i


dz.	z 	z 	z	z


'6	e
Figure 2: The ten most frequent classes



  As an example  consider the verb  pass which has 
the classes  THROW, SEND, GIVE and MARRY. The
respective   p(classiamb_c/ass) for these classes  are



Figure 3: Ten most frequent  frames  in Levin



(e.g.,  Thameslink  presently carries  20,000  passen­ 
gers daily) is larger than the CARRY class, it will be 
given a higher probability  (0.45  versus 0.4). This is


7272• 7202• 7125 and  w . By muIu.pIym.


g these by the fre-


clearly  wrong,  but it is an 
empirical  question  how


quency   of  pass  in  the  BNC  (19,559)   we  obtain 
the  estimates for   j(verb, class)  given  in  table  3. 
Note  that simply  relying  on class  size,  without  re­ 
gard to frequency,  would give quite different  results. 
For example  the class  of MANNER OF SPEAKING 
verbs  has 76 members, of which  30 have frequen­ 
cies  which  are less than  1 per 1M,  and is the sev­ 
enth  largest class  in Levin's classification. Accord­ 
ing to our estimation  scheme  MANNER OF SPEAK­ 
ING verbs are the !16th largest class. The estimates 
for the ten most  frequent  classes  are shown  in fig­ 
ure 2.
  The   estimation   process    described    above   in­ 
volves  at least  one  gross  simplification, since 
p(classiamb_cfass) is calculated   without  reference 
to  the  identity   of  the  verb  in  question.   For  any 
two verbs which fall into the same set of classes 
p(classlamb_cfass) will  be the same,  even  though 
one or both may be atypical  in its distribution  across 
the  classes.   Furthermore, the  estimation   tends  to 
favour large classes, again irrespectively  of the iden­ 
tity of  the verb  in question.  For example  the  verb 
carry  has three classes,  CARRY, FIT and COST. In­ 
tuitively  speaking,  the CARRY class is the most fre­ 
quent  (e.g.,  Smoking  can impair the blood  which 
carries  oxygen  to the brain, I carry sugar lumps 
around  with  me).  However,  since  the  FIT  class


much it matters.
  Finally,  we  wanted  to  estimate   the  probability 
of a given  frame,   P(jrame). We could  have done 
this by acquiring  Levin  compatible subcategoriza­ 
tion frames from the BNC. Techniques  for the auto­ 
matic acquisition  of subcategorization dictionaries 
have been developed  by Manning  (1993),  Briscoe 
and Carroll  (1997)  and  Carroll  and  Rooth  (1998). 
But the present  study  was less ambitious, and  nar­ 
rowly  focused  on  the frames  representing  the da­ 
tive and the benefactive alternation. In default of the 
more ambitious  study, which we plan for the future, 
the estimation  of P(jrame) was carried out on types 
and not on tokens. The mapping  of Levin's  linguis­ 
tic specifications  into surface  syntactic  information 
resulted in 79 different frame types. By counting the 
number of times a given frame is licensed by several 
semantic classes we get a distribution  of frames,  a 
sample of which is shown in figure 3.
  The  probabilities   P(jrameiclass)   and 
P(jrameiverb)  will  be  unreliable   when  the 
frequency estimates for f(verb,frame)  and 
f(class.frame)  are  small,    and   ill-defined   when 
the    frequency    estimates     are    zero.    Following 
Hindle   and   Rooth   (1993)   we   smooth   the   ob­
served   frequencies   in  the  following   way,  where
f(V.frame) 	=	Li .f(verb;.frame), f(V) 	=


:L1 f(verh,), f(C,frame) =  :L, f(class;,frame)
and   f(C) 	=	Li f(c/ass,).   We  redefine  the
probability estimates as follows:

f(verh,Jrame) + fWt ';"<l



data. Given the restriction that these verbs are se­ 
mantically ambiguous in a specific syntactic frame 
we could not simply sample from the entire 
BNC, since this would decrease the chances of 
finding the
verb in the frame we are interested in. Instead, 
for


(20)


P(ji<t
melve
rh)""'


L 
f(verh
,fram
e
i


J<
1 ) +I


31 
clas
s 
am
big
uou
s 
ver
bs 
we 
ran
do
mly 
sele
cted 
ap­ 
pro
xim
atel
y 
100 
tok
ens 
fro
m 
the 
data 
use
d 
for 
the 
acq
uisi
tion 
of 
fra
me 
freq
uen
cies 
for 
the 
dati
ve 
and


f(class,frame) + f"<Cf mtel
(21) P(jramelclass)""' L              J<   1
f(class,frame 1 ) +I


When   f(verh,frame)  is  zero,  the  estimate  used 
is proportional  to the average  /( :1 ';"•1   across all
verbs. Similarly, when  f(class,frame) is zero, our 
estimate  is  proportional  to  the  average   f(C.tr"me)
f(C)
across all classes. We don't claim that this scheme is
perfect, but any deficiencies it may have are almost 
certainly masked by the effects of approximations 
and simplifications elsewhere in the system.

4    Results
We evaluated the performance of the model on all 
verbs listed in Levin which are polysemous and take 
frames characteristic for the dative and benefactive 
alternations. This resulted in 154 verbs which take 
the NP-V-NP-NP frame, 135 verbs which take the 
NP-V-NP-PP111   frame and 84 verbs which take the 
NP-V-NP-PPp, frame. The verbs were all polyse­ 
mous and had an average of 3.8 classes. Each class 
had an average of 3.4 frames. Furthermore, we di­ 
vided these verbs in two categories: verbs which can 
be disambiguated solely on the basis of their frame 
(e.g., serve; category A) and verbs which are gen­ 
uinely ambiguous, i.e., they inhabit a single frame 
and yet can be members of more than one semantic 
class (e.g., write; category B).
  The task was the following: given that we know 
the frame of a given verb can we predict its se­ 
mantic class? In other  words by varying the class 
in the term  P(verh,frame, class)  we are trying to 
see whether the class which maximizes it is the one 
predicted by the lexical semantics and the argument 
structure of the verb in question.
  For the verbs belonging to category A (306 in 
total) we used Levin's own classification in eval­ 
uation. The model's  performance was considered 
correct if it agreed with Levin in assigning a verb 
the appropriate class given a particular frame. For 
class ambiguous  verbs (category B) we compared 
the model's predictions against manually annotated


benefactive alternation. Verbs with frame frequency 
less than I 00 were not used in the evaluation.
  The selected tokens were annotated with class in­ 
formation by two judges. The judges were given an­ 
notation guidelines but no prior training. We mea­ 
sured the judges' agreement on the annotation task 
using the Kappa coefficient (Siegel and Castellan,
1988) which is the ratio of the proportion of times, 
P(A),  that k raters agree to the proportion of times, 
P(E),  that we would expect the raters to agree by 
chance (cf. (22)). If there is a complete agreement
among the raters, then K = I, whereas if there is no
agreement among the raters (other than the agree­
ment which would be expected to occur by chance), 
then K = 0.
) K  =  P(A)- P(E)
22               1 - P(E)
We counted the performance of our model as cor­ 
rect if it agreed with the "most preferred", i.e., most 
frequent verb class as determined in the manually 
annotated corpus sample by taking the average of 
the responses of both judges.
We also compared the results for both categories
to a naive baseline which relies only on class in­ 
formation and does not take subcategorization into 
account. For a given polysemous verb, the baseline 
was computed by defaulting to its most frequent 
class, where class frequency was determined by the 
estimation procedure described in the previous sec­ 
tion.
As shown in table 4, in all cases our model out­
performs the baseline. It achieves a combined pre­ 
cision of 91.8% for category  A verbs. One  might 
expect a precision of 100% since these verbs can 
be disambiguated solely on the basis of the frame. 
However, the performance of our model is Jess, 
mainly because of the way we estimated the terms 
P(class)   and  P(jramelclass):   we  overemphasize 
the importance of frequent classes  without taking 
into account how individual verbs distribute across 
classes.
The  model  achieves  a  combined  precision  of
83.9% for category B verbs (cf. table 4). Further-



I
I	Category A	Category B
Fr
a
m
e
V
er
bs
Ba
sel
in
e
M
od
el
Ve
rb
s
Ba
sel
in
e
M
od
el
N
P-
V-
N
P-
N
P
1
2
3
61
.8
%
87
.8
%
14
42
.8
%
85
.7
%
N
P-
V-
N
P-
PP
,0
1
1
3
67
.2
%
92
%
15
73
.4
%
86
.6
%
N
P-
V-
N
P-
P
Pr
or
7
0
70
%
98
.5
%
2
0
%
50
%


[COmbmed	11     306 	1         65.7% 	1         91.8%  1      31


I  61.3%


83.9%



Table 4: Model accuracy against baseline


I Verb	Frame 	Preferences 
save 	NP-V-NP-NP 	GET, BILL 
call 	NP-V-NP-NP 	GET, DUB
write 	NP-V-NP-NP 	MESSAGE TRANSFER, PERFORMANCE
make 	NP-V-NP-NP 	DUB, BUILD
extend 	NP-V-NP-PP,o	FUTURE HAVING, CONTRIBUTE
present 	NP-V-NP-PPw	FULFILLING, REFLEXIVE APPEARANCE
take 	NP-V-NP-PPJi r	STEAL, PERFORMANCE
produce 	NP-V-NP-PP1'"	PERFORMANCE, CREATE
     Table 5: Random sample of eight verbs and their semantic preferences as ranked by the model 
more, our model makes interesting predictions with
respect to the semantic preferences of a given verb. 
In table 5 we show the class preferences the model 
came  up  with  for  eight  randomly  selected  verbs 
(class preferences are ranked from left to right, with 
the leftmost class being the most preferred one). Ta­ 
ble 6 summarizes  the average class frequencies for 
the same eight verbs as assigned to corpus tokens 
by the two judges together with inter-judge agree­ 
ment ( K). The category OTHER is reserved for cor­ 
pus tokens which either  have the wrong frame or 
for which the classes in question are not applicable. 
In general agreement  on the class annotation  task 
was good with Kappa values ranging from 0.68 to 
I. As shown in table 6, with the exceptions of call 
and produce  the model's  predictions are borne out 
in corpus data.



5    Discussion

This papen explores  the degree to which syntactic 
frame information can be used to disambiguate verb 
semantic classes. In doing so, we cast the task of 
verb class disambiguation  in a probabilistic frame­ 
work which exploits Levin's semantic classification 
and frame frequencies acquired from the BNC. The 
approach is promising in that it achieves high preci­ 
sion with a simple model and can be easily extended 
to incorporate  other sources of information  which


Table 6: Random sample of eight  verbs and  their
semantic preferences as ranked by two judges



can influence the class selection process (i.e., selec­
tional restrictions).
  The semantic preferences which we generate can 
be thought of as default semantic knowledge, to be 
used in the absence of any explicit contextual or 
lexico-semantic information to the contrary (cf. ta­ 
ble 5). Consider  the verb  write for example.  The


model comes up with an intuitively reasonable rank­ 
ing: we more often write things to people ("message 
transfer" reading) than for them ("performance" 
reading). However, faced with a sentence like Max 
wrote Elisabeth a book pragmatic knowledge forces 
us to prefer the "performance"  reading versus the 
the "message transfer" reading. In other cases the 
model comes up with a counterintuitive ranking. For 
the verb call, for instance, the "get" reading (e.g., I 
will call you a cab) is preferred over the more natu­ 
ral "dub" reading (e.g., John called me a fool).
  We still rely heavily on the verb class informa­ 
tion  provided  by Levin.  But  part of original aim 
was to infer class information for verbs not listed 
by Levin. For such a verb, P(class), and hence 
P(verb,frame, class)  will  be  zero,  which  is  not 
what we want. Recent work in computational lin­ 
guistics (e.g., Schiitze (1993)) and cognitive psy­ 
chology (e.g., Landauer and Dumais (1997)) has 
shown that large corpora implicitly contain seman­ 
tic information, which can be extracted and manipu­ 
lated in the form of co-occurrence vectors. The idea 
would be to compute the centroid (geometric mean) 
of the vectors of all members of a semantic class. 
Given an unknown verb (i.e., a verb not listed in 
Levin) we can decide its semantic class by compar­ 
ing its semantic vector to the centroids of all seman­ 
tic classes. We could (for example) determine class 
membership on the basis of the closest distance to 
the centroid representing a semantic class (cf. Patel 
et a!. (1998) for a proposal similar in spirit). Once 
we have chosen a class for an unknown verb, we are 
entitled to assume that it will share the broad syn­ 
tactic and semantic properties of that class.
  We also  intend  to experiment  with a full scale 
subcategorization  dictionary  acquired from the 
BNC. We believe this will address issues such as: 
(a) relations between frames and classes (what are 
the frames for which the semantic class is predicted 
most  accurately)  and  (b)  relations  between  verbs 
and classes (what are the verbs for which the seman­ 
tic class is predicted most accurately). We also plan 
to experiment  with different classification schemes 
for verb semantics such as WordNet (Miller et al.,
1990) and intersective Levin classes (Dang et al.,
1998).

References

Ted Briscoe and John Carroll.  1997. Automatic 
extraction of subcategorization  from corpora. In 
Proceedings of the 5th ACL Conference on Ap-


plied Natural Language Processing, pages 356-
363, Washinton, DC.
Lou Burnard. 1995. Users Guide for the British Na­ 
tional Corpus.  British National Corpus Consor­ 
tium, Oxford University Computing Service.
Glenn Carroll and Mats Rooth.  1998.  Valence 
induction with a head-lexicalized PCFG.  In 
Proceedings  of the  3rd  Conference  on  Empir­ 
ical Methods in Natural Language Processing, 
Granada.
Hoa Trang Dang, Karin Kipper. Martha Palmer, and 
Joseph  Rosenzweig.   1998.  Investigating  regu­ 
lar sense extensions based on intersective Levin 
classes. In Proceedings of the 17th International 
Conference  on  Computational  Linguistics  and
36th Annual Meeting of the Association for Com­
  putational Linguistics, pages 293-299,  Montreal. 
A. P. Dempster,  N.  M.  Laird,  and  D.  B.  Rubin.
1977.  Maximum likelihood for incomplete data
via the EM algorithm.  Journal of the Royal Sta­
tistical Society, 39(2):1-38.
Donald Hindle and Mats Roath.  1993. Structural 
ambiguity and lexical relations.  Computational 
Linguistics, 19(1):103-120.
Frank  Keller, Martin  Corley,  Steffan  Corley, 
Matthew W. Crocker, and Shari Trewin.  1999. 
Gsearch: A tool for syntactic investigation of 
unparsed corpora.  In Proceedings of the EACL 
Workshop on Linguistically Interpreted Corpora, 
Bergen.
Julian  Kupiec.   1992.  Robust oart-of-speech  tag­ 
ging using a hidden Markov model.  Computer 
Speech and Language, 6(3):225-242.
Thomas K. Landauer and Susan T. Dumais.  1997.
A solution to Plato's problem: The latent seman­ 
tic analysis theory of acquisition, induction and 
representation of knowledge.  Psychological Re­ 
view, 104(2):211-240.
Maria Lapata.  1999.  Acquiring lexical generaliza­ 
tions from corpora: A case study for diathesis 
alternations.  In Proceedings of the 37th Annual 
Meeting of the Association for Computational 
Linguistics, College Park, MA.
Beth Levin. 1993. English Verb Classes and Alter­ 
nations: A Preliminary Investigation. University 
of Chicago Press, Chicago.
Christopher D. Manning. 1993. Automatic acquisi­ 
tion of a large subcategorization dictionary from 
corpora. In Proceedings of the 31st Annual Meet­ 
ing of the Association for Computational Linguis­ 
tics, pages 235-242,  Columbus, OH.


George  A.  Miller,  Richard  Beckwith,  Christiane
Fellbaum, Derek Gross, and Katherine J. Miller.
1990.  Introduction to WordNet: An on-line lexi­ 
cal database.  International Journal of Lexicogra­ 
phy, 3(4):235-244.
Martha Palmer.  1999. Consistent criteria for sense 
distinctions.  Computers and the Humanities, to 
appear.
Malti Patel, John A. Bullinaria, and Joseph P. Levy.
1998. Extracting semantic representations from 
large text corpora.  In John A. Bullinaria, D. W. 
Glasspool, and G. Houghton, editors, In Proceed­ 
ings of the 4th Workshop on  Neural Computa­ 
tion and Psychology, pages 199-212.  Springer, 
Berlin.
Adwait Ratnaparkhi.   1998.  Unsupervised statisti­ 
cal models for prepositional  phrase attachment. 
In Proceedings of the 7th International Confer­ 
ence on Computational  Linguistics, pages 1079-
1085.
Hinrich 	Schiitze.       1993.      Word   space.      In 
Stephen.  Jose  Hanson,   Jack   D.  Cowan,  and 
C. Lee Giles, editors, Advances in Neural In­ 
formation Processing Systems, pages 895-902. 
Morgan 'Kaufmann, San Mateo, CA.
Sidney Siegel and N. John Castellan.  1988.  Non 
Parametric Statistics for the Behavioral Sciences. 
McGraw,Hill,  New York.





