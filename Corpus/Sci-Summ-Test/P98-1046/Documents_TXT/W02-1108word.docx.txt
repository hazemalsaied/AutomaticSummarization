Assigning Verbs to Semantic Classes via WordNet
 
 
Anna Korhonen
 
 
University of Cambridge, Computer Laboratory
 
 
15 JJ Thomson Avenue, Cambridge CB3 OFD, UK
Anna.Korhonen©cl.cam.ac.uk
 
 
Abstract
 
 
We propose a method for semi-automatic classi-
fication of verbs to Levin classes via the seman-
tic network of WordNet. The method involves
first classifying entire WordNet senses to seman-
tic classes and then classifying individual verbs
on the basis of their WordNet senses. We report
evaluation which shows that the method can
be used to build a verb classification accurate
enough for practical NLP use. The WordNet-
Levin mapping produced as a by-product, may,
in turn, be used to supplement WordNet with
novel information.
 
 
1 Introduction
 
 
Linguistic research has shown that verbs fall
into classes distinctive in terms of their syntac-
tic and semantic properties (Jackendoff, 1990;
Hale and Keyser, 1993; Levin, 1993; Pinker,
1989). For example, verbs which share the
meaning component of &apos;motion&apos; (e.g. fly and
walk) tend to behave similarly also in terms of
subcategorization and can thus be grouped to a
linguistically coherent class.
While the correspondence between the syntax
and semantics of verbs is arguably not perfect
and while the whole notion of a verb class is
somewhat elusive&apos;, verb classifications can be
constructed which provide a generalization over
a range of syntactic and semantic properties of
verbs.
Such classifications are particularly useful
from a practical NLP point of view. They can
be used as a means of reducing redundancy in
the lexicon and for filling gaps in lexical knowl-
edge. To a certain extent, they enable inferring
1-For example, as most verbs can be characterized by
several meaning components, there is potential for cross-
classification. Therefore different, equally viable classifi-
cation schemes can be constructed.
the semantics of a word on the basis of its syn-
tactic behaviour, and the syntax of a word on
the basis of its semantic behaviour.
Verb classifications have, in fact, been used
to support various NLP tasks, including ma-
chine translation, language generation (Dorr,
1997), document classification (Klavans and
Kan, 1998), lexicography (Sanfilippo, 1994) and
lexical acquisition, such as word sense disam-
biguation (Dorr and Jones, 1996) and subcate-
gorization acquisition (Korhonen, 2002b).
The verb classification employed most widely
in NLP is Levin&apos;s taxonomy of verbs and their
classes (Levin, 1993). Levin classes are based on
the ability of a verb to occur in specific diathe-
sis alternations, i.e. specific pairs of syntactic
frames which are assumed to be meaning re-
tentive. The classification covers a substantial
number of diathesis alternations occurring in
English. It is not, however, exhaustive. More
work is required on extending and refining it
until a comprehensive resource can be obtained
suitable for large-scale NLP use (Dorr and Jones,
1996; Dorr, 1997; Korhonen, 2002b).
Perhaps the most challenging task is to ex-
tend the classification with new participants.
Manual classification of verbs to semantic
classes yields accurate results but is time con-
suming (Levin, 1993; Dang et al., 1998). Fully
automatic classification, on the other hand, is
fast but suffers from low accuracy (Dorr, 1997;
Stevenson and Merlo, 1999). In this paper,
we propose combining the strengths of these
approaches. We present a method for semi-
automatic semantic classification of verbs which
exploits automatic techniques but also allows
for some manual intervention.
The method involves classifying verbs seman-
tically via the semantic network of WordNet
(Miller, 1990). Unlike Levin&apos;s source, Wordnet
is a comprehensive lexical database. Although
it classifies verbs on a purely semantic basis,
the syntactic regularities studied by Levin are to
some extent reflected by semantic relatedness as
it is represented by WordNet&apos;s particular struc-
ture (Dorr, 1997; Fellbaum, 1999). Our method
makes use of this partial overlap between Word-
Net and Levin classes. It involves first classify-
ing entire WordNet senses to semantic classes
and then classifying individual verbs on the ba-
sis of their WordNet senses.
We use this method to classify verbs in a num-
ber of WordNet files and report experimental
evaluation which shows that the method is fairly
accurate and can also be used to build a clas-
sification suitable for practical NLP use. The
classification built using our method can also
be used to benefit linguistic research, as knowl-
edge about novel verb and verb class associa-
tions can be used to test and enrich linguistic
theory (e.g. (Levin, 1993)).
As a by-product, the method produces a map-
ping between WordNet senses and Levin classes.
This mapping — once comprehensive — can act
as a useful supplement to WordNet which in-
corporates information about diathesis alterna-
tions and semantic verb classes. Currently this
information is absent in WordNet.
We discuss the background for our work in
section 2. In section 3, we describe the method
for classifying verbs semantically via WordNet.
The details of the experimental evaluation of
our method are supplied in section 4. Section 5
concludes with directions for future work.
 
 
2 Background
 
 
In Levin (1993), verbs which display the same or
similar set of diathesis alternations in the real-
ization of their argument structure are assumed
to share certain meaning components and are
organized into a semantically coherent class.
For instance, the Levin class of &amp;quot;Break Verbs&amp;quot;
(class 45.1) refers to actions that bring about a
change in the material integrity of some entity.
It is characterized by its participation (1-3) or
non-participation (4-6) in the following alterna-
tions:
 
 
1. Causative/inchoative alternation:
 
 
Tony broke the window ---&gt; The window
broke
 
 
2. Middle alternation:
 
 
Tony broke the window ---&gt; The window
broke easily
 
 
3. Instrument subject alternation:
 
 
Tony broke the window with the hammer ---&gt;
The hammer broke the window
 
 
4. * With/against alternation:
 
 
Tony broke the cup against the wall ---&gt;
*Tony broke the wall with the cup
 
 
5. *Conative alternation:
 
 
Tony broke the window ---&gt; *Tony broke at
the window
 
 
6. *Body-Part possessor ascension alter-
nation:
 
 
*Tony broke herself on the arm ---&gt; Tony
broke her arm
Levin&apos;s classification is attractive in provid-
ing a summary of the variety of theoretical re-
search done on semantic verb classes and a ref-
erence work extensive enough for NLP research.
It is not, however, comprehensive in breadth or
depth of coverage. It provides a classification
of around 3200 verbs into 48 classes (some of
which are split further into more distinctive sub-
classes, making the total number of 191 classes)
according to their participation in 79 alterna-
tions involving NP and PP complements only.
Work on refining the existent semantic classes
and composing novel ones is under way. Dang et
al. (1998), for example, have refined the current
classification by creating intersective classes for
those verbs which share membership of more
than one Levin class. Dorr (1997) has created
new classes for verbs whose syntactic behaviour
differs from the syntactic description of existing
Levin classes. Korhonen (2002b) has proposed
novel alternations not covered by Levin — par-
ticularly those involving sentential complements
— with the aim to create new classes for further
verb types.
Attempts have also been made to extend the
classification with new participants. Levin, for
instance, associates only 13 verbs with the class
of &amp;quot;Break Verbs&amp;quot; (break, chip, crack, crash,
crush, fracture, rip, shatter, smash, snap, splin-
ter, split, and tear), while it is possible that
other English verbs fall into this class as well.
Stevenson and Merlo (1999) and Dorr (1997)
have proposed methods for automatically infer-
ring participation in verb classes. Stevenson
and Merlo have applied machine learning tech-
niques to automatically classify a set of verbs,
based on distributional features extracted from
a corpus. They achieve 70% accuracy on the
task, but their method is applicable to three
verb classes only. Dorr has proposed a method
applicable to all Levin classes which relies heav-
ily on lexical resources. It uses grammatical
information in the LDOCE dictionary (Procter,
1978) alongside semantic information in Word-
Net. The method yields 61% accuracy in as-
signing verbs to (one of their) correct classes.
Although fully automatic classification would
be an ideal solution, the methods proposed so
far have not led to a resource accurate enough
for all-purpose practical NLP use (e.g. (Korho-
nen, 2002b)). On the other hand, manual classi-
fication is not an ideal solution as it is time con-
suming and thus costly. Here, we address this
problem by proposing a semi-automatic method
which uses automatic techniques to speed up
the classification task but allows for some man-
ual intervention to maximise accuracy of classi-
fication.
 
 
3 Classifying Verbs Semantically via
WordNet
 
 
Our method involves assigning verbs to seman-
tic classes via WordNet. Section 3.1 provides a
brief introduction to this semantic network and
discusses its suitability for the task. Section 3.2
gives details of the classification algorithm.
 
 
3.1 WordNet
 
 
In contrast to Levin&apos;s classification, WordNet
organizes words on a purely semantic basis with-
out regard to their syntactic properties. Its or-
ganization is that of a network of interlinked
nodes representing word meanings. The nodes
are sets of synonym sets (`synsets&apos;), which con-
sist of all the word forms that can express
a given concept (e.g. {snooze, drowse, doze}).
While synonymy links individual words within
synsets, the super-/subordinate relation links
entire synsets, resulting in hierarchical struc-
tures.
WordNet has several qualities which make it
suitable for our purposes. Unlike Levin&apos;s source,
it is a comprehensive lexical database. It also
includes useful information about semantic re-
lations and the frequency of word senses which
can be exploited in the classification task. Al-
though WordNet&apos;s semantic organization does
not always go hand in hand with syntactic infor-
mation, Dorr and Jones (1996) and Dorr (1997)
have demonstrated that synonymous verbs in
WordNet exhibit syntactic behaviour similar to
that characterised in the classification system
of Levin. This enables association of verbs with
semantic classes on the basis of their WordNet
synonyms.
We utilized the verb hierarchy of WordNet
version 1.6. which contains 10,319 word forms
whose 22,066 senses spread over 12,127 synsets.
Most synsets in this hierarchy are interrelated
by a pointer standing for a manner relation tro-
ponymy2. They divide into 15 subhierarchies
which represent different, mostly semantically-
driven domains (e.g. &amp;quot;verbs of motion&amp;quot;, &amp;quot;verbs
of perception&amp;quot;).
 
 
3.2 Classification Algorithm
 
 
In addition to WordNet, our classification al-
gorithm employs the following set of lexical re-
sources:
 
 
• Levin&apos;s semantic classification of 3200
verbs
• The LDOCE dictionary
• Dorr&apos;s (1997) source of LDOCE grammati-
cal codes for Levin classes. This source was
obtained by extracting automatically basic
syntactic patterns from all the sentences in
Levin&apos;s book, mapping these onto LDOCE
grammatical codes and grouping them into
canonical and prohibited codes for each
class.
 
 
Dorr (1997) employed the same set of lexi-
cal resources in her fully automatic classifica-
tion approach. In Dorr&apos;s approach, each un-
known verb was assigned to one of its semantic
classes by examining the verb&apos;s synonyms from
WordNet and selecting those whose Levin class
is associated with syntactic information match-
ing that of the unknown verb. The notion of
 
 
2For example, the synset {snooze, drowse, doze}
is represented as one of the troponyms (subordinates)
of the hypernym (superordinate) synset {rest, repose},
since snooze, drowse or doze mean to rest or repose in a
particular manner.
 
 
&amp;quot;match&amp;quot; was based on the degree of intersec-
tion between the verb&apos;s LDOCE codes and the
LDOCE codes for a candidate class.
Our semi-automatic approach differs, how-
ever, essentially from Dorr&apos;s. In our approach,
entire WordNet senses (i.e. synsets) are first
assigned to semantic classes, using informa-
tion in the lexical resources merely as a guid-
ance. After synsets are classified, individual
verbs receive the classification of their respec-
tive synsets. By classifying entire synsets, our
objective is to build a more static source where
WordNet senses are associated with different
Levin classes. Although static, the source will
allow for updating and adding new verbs to
WordNet.
Our algorithm makes use of the hierarchical
structure of WordNet. It classifies synsets sub-
hierarchy by subhierarchy, starting from the top
level synsets, and going further down in the tax-
onomy only when required. This makes sense,
as many of the top level synsets intersect di-
rectly with Levin classes. For example, &amp;quot;Send-
ing and Carrying&amp;quot; and &amp;quot;Force Exerting&amp;quot; verbs
are all found under the same top level synset
{move, displace}.
Each synset is classified by first assigning the
majority of its member verbs to a semantic class
and then choosing the Levin class supported by
the highest number of verbs. &apos;Member verbs&apos;
refer here to those which are members of the
synset in question and of its hyponym (sub-
ordinate) synsets. Thus if a classified synset
has hyponym synsets, the latter are classified
according to their classified hypernym synset.
At the present state of development, our al-
gorithm classifies verbs according to their pre-
dominant (i.e. most frequent) sense in Word-
Net only3. Accordingly, synsets are classified by
considering only those member verbs whose pre-
dominant sense belongs to the synset in ques-
tion. The method is, however, generally ap-
plicable (i.e. fully suitable for classifying non-
predominant senses of verbs as well).
The algorithm proceeds as follows:
Step 1: If the majority of member verbs of a
given synset S are Levin verbs from the
same class, classify S directly.
 
 
3The WordNet frequency data is derived from the
SemCor corpus.
 
 
Step 2: Otherwise, classify more member
verbs (according to Step 4a-d) until the ma-
jority are classified, and then go back to
Step 1.
 
 
Step 3: Otherwise, if the classified verbs point
to different Levin classes, examine whether
S consists of hyponym synsets. If not, as-
sign S to the Levin class supported by the
highest number of classified verbs. If yes,
go one level down in the hierarchy and clas-
sify the hyponym synsets separately, start-
ing again from Step 1.
Step 4: If S includes no Levin verbs, proceed
as follows to classify the majority of mem-
ber verbs of S:
 
 
(a) Extract the predominant sense of a
given verb V from WordNet
(b) Extract the syntactic codes from
LDOCE relevant to this sense
(c) Examine whether V could be assigned
to a Levin class already associated
with the other verbs in the (i) same
synset, (i) possible hypernym synset or
(iii) possible sister synsets by compar-
ing the LDOCE codes of the sense and
Dorr&apos;s LDOCE codes of the respective
Levin class(es). Given the hypothe-
sised classes, make the final class as-
signment manually.
(d) If no suitable class is found, re-
examine the case after more verbs have
been analysed. If the classification re-
mains unsolved, set V aside for later
examination.
 
 
The above algorithm is for the most part au-
tomatic, however, Step 4b and part of Step
4c (the final class assignment) are done man-
ually to ensure accuracy of classification. The
approach of classifying &amp;quot;only&amp;quot; the majority of
verbs in synsets (as opposed to all verbs) aims,
however, to minimise manual effort.
The following examples illustrate the use of
this algorithm to assign hyponym synsets of
the top level synset {move, displace} to Levin
classes4:
 
 
4As we only consider verbs whose predominant senses
belong to the synsets in question, for clarity, we refer to
synsets in these examples as WordNet synset identifier
 
 
Example 1: The synset no. 01328437 has 5
member verbs, 3 of which are Levin verbs from
the same verb class. It is assigned directly to the
Levin class of &amp;quot;Verbs of Sending and Carrying&amp;quot;
(Step 1).
 
 
2. [X9 esp. OFF, AWAY] to remove
b. by taking suddenly:
&amp;quot;She whisked the cups away
/ whisked him (off) home&amp;quot;
ship =&gt; Verbs of Sending and Carrying
despatch
dispatch =&gt; Verbs of Sending and Carrying
route
forward =&gt; Verbs of Sending and Carrying
 
 
Example 2: The synset no. 01278717 in-
cludes Levin verbs which point to different
classes. Since it consists of hyponym synsets (as
indicated by the synset identifiers below), we go
one level down in the taxonomy and classify the
hyponym synsets separately (Step 3).
 
 
push =&gt; Verbs of Exerting Force
jab poke 01296169 =&gt; Poke Verbs
nudge prod 00838894 =&gt; Verbs of Contact
repel 01034588
shove 01278320 =&gt; Verbs of Exerting Force
ram 01296169
obtrude 01279473
thrust 01296169 =&gt; Verbs of Exerting Force
elbow shoulder 01278320
 
 
Example 3: The synset no. 00994853 in-
cludes 13 member verbs, 4 of which are Levin
&amp;quot;Verbs of Sending and Carrying&amp;quot;. We need to
classify more verbs to determine class assign-
ment (Step 2). We choose whisk and extract its
predominant sense from WordNet (Step 4a):
 
 
whisk -- (move somewhere quickly;
&amp;quot;The president was whisked away
in his limo&amp;quot;)
In LDOCE the verb has three senses. That cor-
responding to the predominant WordNet sense
is identified as (Step 4b):
codes, rather than their actual names. To simplify the
examples somewhat, we also refer to broad rather than
fine-grained Levin classes.
11 Levin classes are already matched with the
verbs in the same, hypernym and sister synsets.
Those whose syntactic description includes the
LDOCE code X9 are:
 
 
Verbs of putting
Verbs of removing
Verbs of sending and carrying
Verbs of exerting force
Verbs of motion
 
 
After verifying these options manually, whisk
is assigned to &amp;quot;Verbs of Sending and Carrying&amp;quot;
(Step 4c).
Example 4: The synset no. 01527059 in-
cludes around 90 member verbs related to the
transfer of messages. These spread over nearly
60 hyponym synsets. Seven of the verbs are
Levin verbs from various classes which include
verbs taking sentential complements. Two of
them are listed by Dorr (1997) as members of
her new semantic classes. The synset is set aside
for future work (Step 4d).
 
 
4 Experimental Evaluation
 
 
We applied the verb classification algorithm to
three WordNet subhierarchies: contact, posses-
sion and motion verbs. 1581 synsets in these
files were assigned to one of 32 Levin classes
and 148 were left unclassified. A small number
of synsets (35) from other verb files were classi-
fied as well.
To evaluate the classification algorithm, we
chose 60 synsets from among the 1616 classi-
fied. The synsets were chosen at random so
that 20 were taken from each of the three Word-
Net verb files (contact, possession and motion
verbs). From the total of 501 verbs in these
synsets, we chose for evaluation those 224 which
were neither Levin verbs nor classified manu-
ally when linking synsets with Levin classes.
For these verbs, the semantic classification (to a
broad or — where applicable — to a fine-grained
Levin class) was compared against a manually
obtained gold standard classification.
The algorithm classified correctly 181 verbs
and incorrectly 43 verbs. Accuracy of the class
assignment was thus 81%.5 All the misclas-
sifications corresponded to cases where there
is a semantic mismatch between WordNet and
Levin, i.e. the majority, but not all verbs in a
synset corresponded to the same Levin class.
21% of the misclassifications concerned synsets
which divide further into hyponym synsets. It
is possible that better results could be obtained
for these synsets if the classification algorithm
was refined further (e.g. to classify more than
&amp;quot;just&amp;quot; the majority of the member verbs in a
synset, for example 70%, before classifying the
entire synset).
79% of the misclassifications concerned, how-
ever, synsets which do not divide further into
hyponym synsets and whose analysis cannot be
improved, if we maintain the idea of linking
entire synsets with Levin classes. In the light
of these results, it thus seems that the upper
bound of our method is around 85%.
The correspondence between WordNet
synsets and Levin classes varies, however,
considerably across WordNet and this affects
the accuracy of our method. Further evaluation
is therefore needed with a larger set of WordNet
files to get a better idea of the performance of
the algorithm.
Our results are not directly comparable with
those of Dorr (1997), as Dorr does not restrict
her evaluation to a certain sense. She reports
61% accuracy in assigning a verb to any of its
correct classes, while we report 81% accuracy
in assigning a verb to the class corresponding
to its predominant sense.
Task-based evaluation of our classification al-
gorithm was reported in (Korhonen, 2002a). In
this evaluation, a classification obtained using
our algorithm was employed in the task of auto-
matic subcategorization acquisition. The algo-
rithm was adapted to classify verbs to (mainly)
broad Levin classes as in subcategorization ac-
quisition it was adequate to assume a broad no-
5Korhonen (2002a) reported a smaller evaluation of
this classification algorithm with 151 verbs which yielded
93% accuracy. However, in this evaluation a broad no-
tion of Levin class was assumed which explains the higher
accuracy.
tion of a semantic class.
The method for subcategorization acquisition
involved first using the system of Briscoe and
Carroll (1997) to acquire a putative subcate-
gorization frame (scF) distribution for a verb
from corpus data. The system used a shallow
parser to obtain the subcategorization informa-
tion and employed a classification of 163 ver-
bal scFs. The putative distribution was then
smoothed with the back-off distribution corre-
sponding to the semantic class of (the predom-
inant sense of) a verb. The semantic class was
determined by our classification algorithm.
This task-based evaluation is fairly sensitive
to the accuracy of the class assigments. Where
the predominant sense is assigned correctly, sig-
nificant improvement can be reported in SCF ac-
quisition, as accurate back-off estimates help to
correct the acquired SCF distribution and deal
with sparse data. Incorrect class assignments
can, however, degrade performance.
On a test set of 91 verbs from 16 Levin classes,
the method yielded 81% type precision (the per-
centage of SCF types that the method proposes
which are correct) and 73% type recall (the per-
centage of SCF types in the gold standard that
the method proposes). The method was com-
pared against a baseline method which assumed
no semantic class and involved no smoothing.
F measure6 was 76 for the method that as-
sumed a semantic class and 61 for the baseline.
This result shows that our algorithm is accu-
rate enough to yield a classification which can
benefit a practical NLP task.
 
 
5 Conclusion
 
 
We have proposed a method for semi-automatic
semantic classification of verbs which combines
the benefits of manual classification and auto-
matic techniques. The method assigns verbs to
semantic classes via WordNet, by first classi-
fying entire WordNet senses and then classify-
ing individual verbs according to their Word-
Net senses. We have reported evaluation which
shows that method is fairly accurate and can be
used to build a classification suitable for practi-
cal NLP use.
In the future, we will apply the method
to other Wordnet files, perform more com-
prehensive evaluation and further refine the
 
 
6 F 2.precision•recall
 
 
precision+recall
classification algorithm to consider also non-
predominant senses of verbs. Our long term
goal is to build a comprehensive semantic clas-
sification of verbs which can be useful for NLP
as well as for linguistic research and — as a by-
product — a mapping between WordNet senses
and Levin classes which can act as a useful sup-
plement to WordNet.
 
 
6 Acknowledgements
 
 
We thank Bonnie Dorr for the use of her source
of LDOCE grammatical codes for Levin classes.
This work was partly supported by UK EPSRC
project GR/N36462/93: &apos;Robust Accurate Sta-
tistical Parsing (RASP)&apos;.
 
 
References
 
 
E. J. Briscoe and J. Carroll. 1997. Automatic
extraction of subcategorization from corpora.
In 5th ACL Conference on Applied Natural
Language Processing, pages 356-363, Wash-
ington DC.
H. T. Dang, K. Kipper, M. Palmer, and
J. Rosenzweig. 1998. Investigating regular
sense extensions based on intersective Levin
classes. In 36th Annual Meeting of the As-
sociation for Computational Linguistics and
17th International Conference on Computa-
tional Linguistics, pages 293-299, Montreal,
Canada.
B. Dorr and D. Jones. 1996. Role of word
sense disambiguation in lexical acquisition:
predicting semantics from syntactic cues. In
16th International Conference on Computa-
tional Linguistics, pages 322-333, Copen-
hagen, Denmark.
B. Dorr. 1997. Large-scale dictionary construc-
tion for foreign language tutoring and inter-
lingual machine translation. Machine Trans-
lation, 12(4) :271-325.
C. Fellbaum. 1999. The organization of verbs
and verb concepts in a semantic net. In
P. Saint-Dizier, editor, Predicative Forms in
Natural Language and in Lexical Knowledge
Bases, pages 93-110. Kluwer Academic Pub-
lishers, Netherlands.
K. Hale and S. J. Keyser. 1993. On Argument
Structure and Lexical Expression of Syntac-
tic Relations. MIT Press, Cambridge, Mas-
sachusetts.
R. Jackendoff. 1990. Semantic Structures. MIT
Press, Cambridge, Massachusetts.
J. L. Klavans and M. Kan. 1998. Role of verbs
in document analysis. In 36th Annual Meet-
ing of the Association for Computational Lin-
guistics and 17th International Conference on
Computational Linguistics, pages 680-686,
Montreal, Canada.
A. Korhonen. 2002a. Semantically motivated
subcategorization acquisition. In ACL Work-
shop on Unsupervised Lexical Acquisition,
Philadelphia. To appear.
A. Korhonen. 2002b. Subcategorization Acqui-
sition. Ph.D. thesis, University of Cambridge,
UK.
B. Levin. 1993. English Verb Classes and Alter-
nations. Chicago University Press, Chicago.
G. A. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicogra-
phy, 3(4):235-312.
S. Pinker. 1989. Learnability and Cognition:
The Acquisition of Argument Structure. MIT
Press, Cambridge, Massachusetts.
P. Procter. 1978. Longman Dictionary of Con-
temporary English. Longman, England.
A. Sanfilippo. 1994. Word knowledge acqui-
sition, lexicon construction and dictionary
compilation. In International Conference on
Computational Linguistics, pages 273-277,
Kyoto, Japan.
S. Siegel and N. J. Castellan, editors.
1988. Non-Parametric Statistics for the Be-
havioural Sciences. McGraw-Hill, New York.
S. Stevenson and P. Merlo. 1999. Auto-
matic verb classification using distributions
of grammatical features. In 9th Conference
of the European Chapter of the Association
for Computational Linguistics, pages 45-52,
Bergen, Norway.