 
 
with low frequency scFs. According to one ac-
count (Briscoe and Carroll, 1997) the majority
of errors arise in SCF acquisition because of the
statistical filtering process.
Korhonen et al. (2000) investigated reasons
for the poor performance of the statistical filters
and reported better accuracy with a simple filter
which sets a threshold on the relative frequency
of SCF entries, selecting only those scFs which
are high in frequency. Korhonen (2000) pro-
posed combining this filter with a method which
deals better with sparse data. The method
exploits the knowledge that semantically sim-
ilar verbs show similar subcategorization be-
haviour. It involves identifying the semantic
class of a predicate, using a hypothesis generator
to acquire a SCF distribution for the predicate,
smoothing this distribution with the back-off es-
timates of the respective semantic verb class,
and setting an empirically defined threshold on
the probability estimates from smoothing to fil-
ter out unreliable scFs.
The small scale experiment reported in
Korhonen (2000) demonstrates that this
semantically-driven approach can significantly
improve the accuracy of hypothesis selection,
for both high and low frequency scFs. In
this paper, we describe refining this approach
further so that it is suitable for larger scale SCF
acquisition. Essentially, we propose methods for
construction of semantic classes and automatic
semantic classification of verbs. We report an
experiment which shows that the method can
be used to improve large-scale SCF acquisition.
We introduce the baseline SCF acquisition
framework in section 2 and describe our exten-
sions to it in section 3. Section 4 gives details of
the experimental evaluation and section 5 con-
cludes with directions for future work.
 
 
2 Framework for SCF Acquisition
 
 
2.1 Hypothesis Generation
 
 
We employ for hypothesis generation the SCF ac-
quisition system of Briscoe and Carroll (1997).
This system is capable of distinguishing 163 ver-
bal scFs a superset of those found in the ANLT
(Boguraev et al., 1987) and COMLEX Syntax dic-
tionaries (Grishman et al., 1994) and return-
ing relative frequencies for each SCF found for a
verb.
It works by first tagging, lemmatizing and
parsing corpus data using a robust statistical
parser (Carroll and Briscoe, 1996) which em-
ploys a grammar written in a feature-based uni-
fication grammar formalism. This yields com-
plete though intermediate parses. Local syntac-
tic frames including the syntactic categories and
head lemmas of constituents are then extracted
from parses, from sentence subanalyses which
begin/end at the boudaries of predicates.
The resulting patterns are assigned to scFs on
the basis of the feature values of syntactic cate-
gories and head lemmas in each pattern. Finally,
sets of scFs are gathered for verbs and putative
lexical entries are constructed.
 
 
2.2 Hypothesis Selection
 
 
The method for hypothesis selection exploits the
knowledge that semantically similar verbs are
frequently similar also in terms of subcatego-
rization. The motion verbs fly and move, for
example, take similar SCF distributions, which
differ essentially from the ones taken e.g. by
communication verbs tell and say.
Although no perfect correspondence exists
between the semantic and syntactic properties
of verbs, useful generalizations can be made:
Levin (1993) has demonstrated that verb senses
can be divided into semantic classes distinctive
in terms of subcategorization. Korhonen (2000)
shows that verb forms can also be divided into
such classes, according to their predominant (i.e.
the most frequent) sense.
For instance, the verb form specific SCF dis-
tributions for the highly polysemic verbs fly and
move correlate quite well because the predom-
inant senses of these verbs (according to the
WordNet (Miller, 1990) frequency data2) are
similar. They both belong to the Levin &amp;quot;Mo-
tion verbs&amp;quot;. Good correlation is observed be-
cause the distribution of verb senses tends to be
zipfian: the predominant sense typically covers
most of the total frequency mass (Preiss et al.,
 
 
2111 this paper, we refer to the WordNet version 1.6.
 
 
2002).
The method for hypothesis selection involves
first (manually) identifying, for a single verb,
the broad Levin class3 corresponding to the pre-
dominant sense of this verb in WordNet. A
SCF distribution is then acquired for the verb
from corpus data, using the hypothesis genera-
tor described in the previous section. This dis-
tribution is smoothed - using linear interpolation
(Chen and Goodman, 1996). - with the &amp;quot;back-
off&amp;quot; estimates of the Levin class.
Back-off estimates are obtained by (i) choos-
ing 4-5 representative Levin verbs from a class,
(ii) building SCF distributions for these verbs by
manually analysing c. 300 occurrences of each
verb in the BNC corpus (Leech, 1992) and (iii)
merging the resulting set of SCF distributions.
For example, the back-off estimates for the &amp;quot;Mo-
tion verbs&amp;quot; are constructed by merging the SCF
distributions for fly, walk, march and travel.
A simple method is finally used for filtering,
which sets an empirically defined threshold on
the probability estimates from smoothing.
Essentially, this method involves using a pri-
ori knowledge about generalizations of verb se-
mantics to guide subcategorization acquisition.
Back-off estimates are used to correct the ac-
quired SCF distribution and deal with sparse
data. However, the parameters used in smooth-
ing are obtained by optimising SCF acquisition
performance on held-out training data so that
most of the smoothed probability is determined
by the maximum likelihood estimate (mLE) from
the hypothesis generator.
Korhonen (2000) evaluated this method with
60 test verbs from 10 Levin classes. It yielded
88% type precision (the percentage of SCF types
that the method proposes which are correct) and
69% type recall (the percentage of SCF types in
the gold standard that the method proposes).
The method was compared against a baseline
method which sets threshold on MLES of SCFS
from the hypothesis generator. F measure4 was
77.1 for the novel method and 70.1 for the base-
 
 
3Korhonen (2000) employs broad Levin classes only
(e.g. 51. &amp;quot;Motion verbs&amp;quot;), not subclasses these may di-
vide into (e.g. 51.2 &amp;quot;Leave verbs&amp;quot;).
4F = 2.precision•recall
precision±recall
 
 
line.
This preliminary experiment shows that the
proposed method provides an effective way of
dealing with low frequency associations and a
means of predicting unseen associations in cor-
pus data.
In this paper, we describe how this
semantically-driven method for hypothesis selec-
tion was refined further and integrated as part
of large-scale SCF acquisition.
 
 
3 Extensions to the Framework
 
 
Applying the method on a large scale requires
us to (a) define a comprehensive set of seman-
tic verb classes, (b) to obtain back-off estimates
for each class, and (c) to implement a method
capable of automatically assigning verbs to se-
mantic classes. We propose methods for (a) and
(c) in sections 3.1 and 3.2, respectively. Section
3.3 discusses the work completed on (a), (b) and
(c).
 
 
3.1 Semantic Classes
 
 
Korhonen (2000) classified verbs into Levin
classes. These provide us with a good start-
ing point for large-scale SCF acquisition as well.
Although not comprehensive, they cover a sub-
stantial number of diathesis alternations occur-
ring in English and work on refining and extend-
ing this classification is under way (Dang et al.,
1998; Dorr, 1997)5.
As it is important to minimise the cost in-
volved in constructing back-off estimates, we do
not adopt the 191 Levin classes as they stand, al-
though this would allow maximal accuracy. We
also do not adopt broad Levin classes as they
stand, as some proved inaccurate in the prelim-
inary experiment. Rather, a method was de-
vised for determining the specificity of the Levin
class(es) required for adequate distinctiveness in
terms of subcategorization. The method pro-
ceeds in two steps, by examining the:
 
 
Step 1: Syntactic similarity between Levin
classes. In this, we use Dorr&apos;s (1997) source
of LDOCE grammatical codes (Procter, 1978) for
5In the work reported in this paper, we concentrate
on Levin classes only, leaving the construction of novel
verb classes for future work.
 
 
Levin classes°. Dorr constructed this source by
extracting basic syntactic patterns from all the
sentences in Levin (1993) and mapping these onto
LDOCE codes. To determine syntactic similarity
between a set of Levin (sub)classes, we consider
the degree of intersection between the LDOCE codes
for these classes.
Step 2: Subcategorization similarity between
verbs in Levin classes. We choose a few
individual verbs whose predominant sense belongs
to the Levin (sub)classes under investigation and
examine (1) the intersection of SCFS between the
verbs, (2) the dissimilarity of SCF distributions for
these verbs according to Kullback-Leibler distance
(KL) and (3) the similarity in ranking of SCFS in
the distributions according to the Spearman rank
correlation coefficient (Ftc)7.
 
 
Step 2 complements Step 1, as the syntac-
tic information included in Levin (1993) is not
conclusive and does not provide any informa-
tion about the relative frequency of scFs. In
addition, it allows us to examine the degree of
SCF correlation between all the verb form spe-
cific SCF distributions we are actually concerned
with.
For example, this method can be used to
decide whether the Levin class of &amp;quot;Verbs of
Sending and Carrying&amp;quot; is distinctive enough
in terms of subcategorization, or whether it
should be divided into subclasses, i.e. &amp;quot;Send&amp;quot;,
&amp;quot;Slide&amp;quot;, &amp;quot;Bring and Take&amp;quot;, &amp;quot;Carry&amp;quot; and &amp;quot;Drive&amp;quot;
verbs. Step 1 determines that the intersection of
LDOCE codes between the five subclasses is fairly
large. All classes share 2 LDOCE codes, four
share 5, and three share 1. Step 2 concludes that
the SCF distributions for five individual verbs
(one from each subclass: send, float, bring, carry
and ferry) are fairly similar in terms of KL, RC
and the intersection of scFs. The broad class
seems thus syntactically coherent enough to pro-
vide an adequate basis for back-off estimates8.
 
 
3.2 Automatic Verb Classification
 
 
In Korhonen (2000), verbs were manually as-
signed to semantic classes. For large-scale SCF
acquisition, a method is needed for automatic
 
 
6We are indebted to Bonnie Dorr for the use of these
codes.
7We use SCF distributions obtained via manual analy-
sis of corpus data. See e.g. (Manning and Schfitze, 1999)
for details of Kt, and RC.
8See Korhonen (2002) for details of this method.
 
 
classification of verbs. We propose one which
involves assigning verbs to semantic classes via
WordNet. Although WordNet&apos;s semantic or-
ganization does not always go hand in hand
with syntactic information, synonymous verbs
in WordNet exhibit syntactic behaviour similar
to that characterised in the classification system
of Levin (Dorr, 1997).
Dorr (1997) has proposed a fully automatic
verb classification algorithm which classifies
verbs semantically on the basis of their Word-
Net synonyms. This algorithm relies solely on
lexical resources but is not accurate enough for
our purpose. As inaccurate assignments can
actually degrade SCF acquisition performance,
some allowance for manual intervention is nec-
essary. We therefore propose a semi-automatic
algorithm. This assigns entire WordNet synsets
to semantic classes9, making use of Levin&apos;s verb
index, the LDOCE dictionary and Dorr&apos;s LDOCE
codes for Levin classes.
The algorithm classifies synsets subhierarchy
by subhierarchy, starting from the top level
synsets, and going further down in the taxon-
omy when required. Each synset is classified by
first assigning the majority of its member verbs
to a semantic class and then choosing the Levin
class supported by the highest number of verbs.
&apos;Member verbs&apos; refer here to those which, ac-
cording to their predominant sense, are mem-
bers of the synset in question and of its hyponym
synsets. The algorithm proceeds as follows:
 
 
Step 1: If the majority of member verbs of a given
synset S are Levin verbs19 from the same class, clas-
sify S directly.
Step 2: Otherwise, classify more member verbs (accord-
ing to Step 4a-d) until the majority are classified,
and then go back to Step 1.
Step 3: Otherwise, if the classified verbs point to dif-
ferent Levin classes, examine whether S consists of
hyponym synsets. If not, assign S to the Levin class
supported by the highest number of classified verbs.
If yes, go one level down in the hierarchy and clas-
sify the hyponym synsets separately, starting again
from Step 1.
 
 
9Our objective is to build a static source where Word-
Net synsets are associated with different Levin classes.
Although static, the source will allow for updating and
adding new verbs to WordNet.
&apos;9`Levin verbs&apos; refer to verbs whose predominant sense
belongs to the Levin class in question.
 
 
Step 4: If S includes no Levin verbs, proceed as follows
to classify the majority of member verbs of S:
 
 
(a) Extract the predominant sense of a given verb
V from WordNet
(b) Extract the syntactic codes from LDOCE rele-
vant to this sense
 
 
(c) Examine whether V could be assigned to a
Levin class already associated with the other
verbs in the (i) same synset, (i) possible hyper-
nym synset or (iii) possible sister synsets by
comparing the LDOCE codes of the sense and
Dorr&apos;s LDOCE codes of the respective Levin
 
 
class(es). Given the hypothesised classes,
make the final class assignment manually.
(d) If no suitable class is found, re-examine the
case after more verbs have been analysed.
If the classification remains unsolved, set V
aside for later examination.
The above algorithm is for the most part au-
tomatic, however, Step 4b and part of Step 4c
(the final class assignment) are done manually
to ensure accuracy of classification.
For example, the synset no. 00994853 includes
13 member verbs, 4 of which are Levin &amp;quot;Verbs
of Sending and Carrying&amp;quot;. We need to classify
more verbs to determine class assignment. We
choose whisk and extract its predominant sense
from WordNet:
whisk -- (move somewhere quickly; &amp;quot;The president
was whisked away in his limo&amp;quot;)
In LDOCE the verb has three senses. That cor-
responding to the predominant WordNet sense
is identified as:
2. [X9 esp. OFF, AWAY] to remove
b. by taking suddenly:
&amp;quot;She whisked the cups away
/ whisked him (off) home&amp;quot;
11 Levin classes are already matched with the
verbs in the same, hypernym and sister synsets.
Those whose syntactic description includes the
LDOCE code X9 are:
Verbs of putting
Verbs of removing
Verbs of sending and carrying
Verbs of exerting force
Verbs of motion
After verifying these options manually, whisk
is assigned to &amp;quot;Verbs of Sending and Carrying&amp;quot;.
 
 
3.3 Completed Work
 
 
The verb classification algorithm was applied to
3 WordNet verb files (contact, possession and
motion verbs). 1581 synsets in these files were
assigned to semantic classes and 148 were left
unclassified. A small number of synsets (35)
from other verb files were classified as well.
From the total of 32 broad Levin classes ex-
emplified among the classified WordNet synsets,
22 of the most frequent were chosen for fur-
ther work. These were re-grouped into semantic
classes by using the method described in sec-
tion 3.1. This led to the combination of 5 pairs
of broad Levin classes and the division of 3 into
subclasses. The resulting 20 semantic classes are
shown in table 1, labelled by class codes shown
in the first column. Back-off estimates for these
classes were built using the method described in
section 2.2.
 
 
Levin Verbs of
A 9. Putting
B 10. Removing: 10.1-3, 10.5-9
C 10. Removing: 10.4
D 11. Sending and Carrying
12. Exerting Force
E 13. Change of Possession
F 15. Hold and Keep
16. Concealment
G 17. Throwing
H 18. Contact by Impact
19. Poke Verbs
I 20. Contact
J 21. Cutting
K 22. Combining and Attaching:
22.1-4
L 22. Combining and Attaching:
22.5
M 23. Separating and Disassembling:
21.1-3
N 23. Separating and Disassembling:
23.4
0 34. Assessment
35. Searching
P 36. Social Interaction
Q 42. Killing
R 44. Destroy
S 47. Existence: Verbs of Spatial
Configuration
50. Assuming Position
T 51. Motion
 
 
Table 1: Semantic verb classes
 
 
4 Experimental Evaluation
 
 
4.1 Classification Algorithm
 
 
To evaluate the classification algorithm, we
chose 30 synsets from among the 1616 classi-
fied. The synsets were chosen at random so
that 10 were taken from each of the three Word-
Net verb files (contact, possession and motion
verbs). From the total of 378 verbs in these
synsets, we chose for evaluation those 151 which
were neither Levin verbs nor classified manu-
ally when linking synsets with Levin classes. For
these verbs, the semantic classification was com-
pared against manually obtained gold standard
classification. The algorithm classified correctly
140 verbs. The accuracy of the class assignment
was thus 93%.
This result is encouraging. However, the cor-
respondence between synsets and Levin classes
varies largely across WordNet. Further evalu-
ation is therefore needed in the future with a
larger set of WordNet files.
 
 
4.2 Subcategorization Acquisition
4.2.1 Test Data and Method
 
 
To evaluate the revised approach to hypothe-
sis selection, we took a sample of 20M words of
BNC. We extracted all sentences containing an
occurrence of one of 91 verbs. The verbs were
chosen at random, subject to the constraint that
they were classified by the algorithm as members
of one of the 20 semantic classes constructed (ta-
ble 1). After the extraction process, we retained
c. 1000 citations for each verb.
The sentences containing these verbs were
processed by the SCF acquisition system. The
hypothesis generator was held constant, the ex-
ception being that the data for these experi-
ments were parsed using a probabilistic chart
parser (Chitrao and Grishman, 1990). For hy-
pothesis selection, we employed the method of
Korhonen (2000) with the extensions described.
We also obtained results for the baseline MLE
thresholding method without any smoothing.
The results were evaluated against a manual
analysis of the corpus data. This was obtained
by analysing a maximum of 300 occurrences for
each test verb in the BNC corpora. We calculated
type precision, type recall and F measure. In
addition to the system results, we calculated KL
and RC between the acquired unfiltered SCF dis-
tributions and the gold standard distributions.
We also recorded the total number of unseen
scFs in the acquired unfiltered SCF distributions
which occurred in the gold standard distribu-
tions. This was to investigate how well the ap-
proach deals with sparse data.
 
 
4.2.2 Results
 
 
Table 2 gives average results for all the 91 test
verbs. The semantically-driven method outper-
forms the baseline on most measures. The im-
proved Tun indicates that the method improves
the overall accuracy of SCF distributions. The
results with RC show that it helps to correct the
ranking of scFs. Precision worsens slighly from
the baseline (1.4%), however, recall improves
significantly (24%). That both precision and
recall are high demonstrates that the method
deals well with both highly ranked scFs and
those low in frequency. The baseline method
simply ignores low frequency data and therefore
yields poor recall. While a total of 114 gold
standard scFs were unseen in the data from the
hypothesis generator, only 24 were unseen after
smoothing with back-off estimates. This shows
further that the method deals effectively with
sparse data.
Verb class specific results in table 3 allow us to
examine the accuracy of the back-off estimates.
The first column shows a semantic verb class and
the second indicates the number of verbs tested
for the class.
According to the F measure, the semantically-
driven method (sEm) outperforms the baseline
method (BL) in all 16 verb classes. KL and RC
show improvement in 12 classes. Four classes
show worse performance: B, E, F, and 0. Several
reasons were identified for the poor performance
(or small improvement) with some verbs/classes:
Firstly, the class specific back-off estimates are
not accurate enough for highly polysemic verbs.
&amp;quot;Note that KT, &gt; 0, with KT, near to 0 denoting strong
association, and —1 &lt; RC &lt;1, with RC near to 0 denot-
ing a low degree of association and RC near to -1 and 1
denoting strong association.
 
 
System results Unseen
Method KL RC Precision (%) Recall (%) F SCES
Baseline 0.42 0.62 82.2 48.5 61.0 114
Semantic 0.26 0.74 80.8 72.5 76.4 24
 
 
Table 2: Average results for 91 verbs
 
 
Sem. Verbs KL RC F Measure Unseen
Class Tested SCES
BL SEM BL SEM BL SEM BL SEM
A 6 0.39 0.23 0.63 0.83 62.9 83.3 10 2
B 7 0.11 0.21 0.90 0.87 65.7 73.7 8 1
C 3 0.54 0.16 0.33 0.72 54.6 84.0 5 0
D 10 0.63 0.39 0.46 0.62 55.6 74.5 20 2
E 9 0.13 0.16 0.87 0.87 58.6 60.4 17 3
F 5 0.21 0.26 0.78 0.88 50.7 60.2 11 6
G 3 0.52 0.27 0.48 0.88 71.4 89.8 6 1
H 6 0.45 0.27 0.64 0.64 64.9 84.2 4 1
I 3 0.15 0.14 0.59 0.66 57.9 73.7 7 1
J 5 0.40 0.11 0.66 0.84 63.9 84.8 1 1
K 5 0.52 0.45 0.66 0.77 63.3 70.3 3 0
0 4 0.35 0.44 0.59 0.59 54.6 57.7 3 2
P 7 0.55 0.39 0.62 0.63 61.9 65.3 10 3
R 3 0.10 0.00 1.00 1.00 63.2 66.7 0 0
S 4 0.71 0.22 0.37 0.65 66.7 87.9 2 0
T 11 0.59 0.28 0.55 0.71 64.2 89.1 7 1
 
 
Table 3: Results for semantic classes
 
 
For example, 6 scFs are reported unseen with
class F and these all involve senses not taken
by the verbs used for constructing the back-
off estimates. Secondly, the back-off estimates
are not accurate for verbs whose distribution
of senses is not particularly zipfian, i.e. verbs
whose predominant sense is not very frequent
(e.g. keep in class F). Thirdly, the empiri-
cally set (verb class specific) filtering thresh-
olds appeared too high/low for some individual
verbs. Finally, although most semantic classes
proved fine-grained enough, one class appeared
too broad: class F, which was obtained by merg-
ing two broad Levin classes (&amp;quot;Hold and Keep&amp;quot;
and &amp;quot;Concealment&amp;quot; verbs).
In sum, the method proposed for construc-
tion of semantic classes proved fairly accurate.
Semantic classes can easily be evaluated in the
context of SCF acquisition and refined further
where required. Korhonen (2000) predicted that
the total number of semantic classes across the
whole lexicon is unlikely to exceed 50. This
seems still a valid prediction, as promising re-
sults were generally obtained assuming a broad
notion of a semantic class.
 
 
5 Conclusion
 
We adopted the semantically motivated ap-
proach to hypothesis selection proposed by
Korhonen (2000) and modified it to be suitable
for large-scale SCF acquisition. A method was
proposed for construction of semantic classes
and a classification algorithm was designed
which automatically assigns verbs to semantic
classes via WordNet. Experimental evaluation
was provided which showed that the classifica-
tion algorithm is highly accurate and that the
revised method for semantically-driven hypoth-
esis selection can be used to succesfully guide,
structure and improve the large-scale acquisition
of scFs from corpus data.
Future work will include: investigating re-
ducing manual effort in the classification algo-
rithm and construction of back-off estimates,
constructing semantic classes and back-off es-
timates across the entire lexicon, applying the
classification algorithm to the remaining Word-
Net synsets and verb files, addressing the prob-
lem of polysemy, and working on improving the A.
accuracy of the filtering method.
 
 
References

B. Boguraev, E. J. Briscoe, J. Carroll, D. Carter,
and C. Grover. 1987. The derivation of a
grammatically-indexed lexicon from the longman
dictionary of contemporary english. In 25th An-
nual Meeting of the Association for Computational
M. Brent. 1993. From grammar to lexicon: unsu-
pervised learning of lexical syntax. Computational
Linguistics, 19(3):243-262.
E. J. Briscoe and J. Carroll. 1997. Automatic ex-
traction of subcategorization from corpora. In
5th ACL Conference on Applied Natural Language
Processing, pages 356-363.
J. Carroll and E. J. Briscoe. 1996. Apportioning
development effort in a probabilistic lr parsing
system through evaluation. In 1st ACL/SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 92-100.
G. Carroll and M. Rooth. 1998. Valence induction
with a head-lexicalized pcfg. In 3rd Conference on
Empirical Methods in Natural Language Process-
ing.
Stanley F. Chen and Joshua Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In ACL-96, pages 310-318. ACL.
M. Chitrao and R. Grishman. 1990. Statistical pars-
ing of messages. In Darpa Speech and Natural Lan-
guage Workshop, pages 263-266.
H. T. Dang, K. Kipper, M. Palmer, and
J. Rosensweig. 1998. Investigating regular sense
extensions based on intersective levin classes.
In 36th Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics, pages
293-299.
B. Dorr. 1997. Large-scale dictionary construc-
tion for foreign language tutoring and interlin-
gual machine translation. Machine Translation,
12(4):271-325.
R. Grishman, C. Macleod, and A. Meyers. 1994.
Comlex syntax: building a computational lexi-
con. In International Conference on Computa-
tional Linguistics, pages 268-272.
Korhonen, G. Gorrell, and D. McCarthy. 2000.
Statistical filtering and subcategorization frame
acquisition. In Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing
and Very Large Corpora, pages 199-205.
Korhonen. 2000. Using semantically motivated
estimates to help subcategorization acquisition. In
Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large
Corpora, pages 216-223.
Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
G. Leech. 1992. 100 million words of english:
the british national corpus. Language Research,
28(1):1-13.
B. Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
C. Manning and H. Schiitze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, Massachusetts.
C. Manning. 1993. Automatic acquisition of a large
subcategorization dictionary from corpora. In 31st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 235-242.
G. A. Miller. 1990. Wordnet: An on-line lexical
database. International Journal of Lexicography,
3(4):235-312.
J. Preiss, A. Korhonen, and E. J. Briscoe. 2002. Sub-
categorization acquisition as an evaluation method
for wsd. In Language Resources and Evaluation
Conference. To appear.
P. Procter. 1978. Longman Dictionary of Contem-
porary English. Longman, England.
A. Sarkar and D. Zeman. 2000. Automatic extrac-
tion of subcategorization frames for czech. In 19th
International Conference on Computational Lin-
guistics, pages 691-697.
A. Ushioda, D. Evans, T. Gibson, and A. Waibel.
1993. The automatic acquisition of frequencies
of verb subcategorization frames from tagged cor-
pora. In B. Boguraev and J. Pustejovsky, edi-
tors, SIGLEX ACL Workshop on the Acquisition
of Lexical Knowledge from Text, pages 95-106.
Columbus, Ohio.
 
Linguistics, pages 193-200. A.