More than Words: Syntactic Packaging and Implicit Sentiment





Stephan Greene∗
ATG, Inc.
1111 19th St, NW Suite 600
Washington, DC 20036
sgreene@atg.com


Philip 
Resnik
Linguistics / UMIACS CLIP 
Laboratory
University of Maryland 
College Park, MD 20742 
resnik@umiacs.umd
.edu









Abstract

Work on sentiment analysis often focuses on 
the words and phrases that people use in 
overtly opinionated text. In this paper, we in- 
troduce a new approach to the problem that 
focuses not on lexical indicators, but on the 
syntactic “packaging” of ideas, which is well 
suited to investigating the identification of im- 
plicit sentiment, or perspective. We establish a 
strong predictive connection between linguis- 
tically well motivated features and implicit 
sentiment, and then show how computational 
approximations of these features can be used 
to improve on existing state-of-the-art senti- 
ment classification results.


1   Introduction

As Pang and Lee (2008) observe, the last several 
years have seen a “land rush” in research on senti- 
ment analysis and opinion mining, with a frequent 
emphasis on the identification of opinions in evalua- 
tive text such as movie or product reviews.  How- 
ever, sentiment also may be carried implicitly by 
statements that are not only non-evaluative, but not 
even visibly subjective. Consider, for example, the 
following two descriptions of the same (invented) 
event:

1(a) On November 25, a soldier veered his jeep into 
a crowded market and killed three civilians.
(b) On November 25, a soldier’s jeep veered into a 
crowded market, causing three civilian deaths.


Both descriptions appear on the surface to be objec- 
tive statements, and they use nearly the same words. 
Lexically, the sentences’ first clauses differ only in 
the difference between ’s and his to express the rela- 
tionship between the soldier and the jeep, and in the 
second clauses both kill and death are terms with 
negative connotations, at least according to the Gen- 
eral Inquirer lexicon (Stone, 1966). Yet the descrip- 
tions clearly differ in the feelings they evoke: if the 
soldier were being tried for his role in what hap- 
pened on November 25, surely the prosecutor would 
be more likely to say (1a) to the jury, and the defense 
attorney (1b), rather than the reverse.1
  Why, then, should a description like (1a) be per- 
ceived as less sympathetic to the soldier than (1b)? 
If the difference is not in the words, it must be in 
the way they are put together; that is, the structure 
of the sentence. In Section 2, we offer a specific hy- 
pothesis about the connection between structure and 
implicit sentiment: we suggest that the relationship 
is mediated by a set of “grammatically relevant” se- 
mantic properties well known to be important cross- 
linguistically in characterizing the interface between 
syntax and lexical semantics. In Section 3, we val- 
idate this hypothesis by means of a human ratings 
study, showing that these properties are highly pre- 
dictive of human sentiment ratings. In Section 4, we 
introduce observable proxies for underlying seman- 
tics (OPUS), a practical way to approximate the rele- 
vant semantic properties automatically as features in 
a supervised learning setting. In Section 5, we show 
that these features improve on the existing state of
the art in automatic sentiment classification.  Sec-


∗This work was done while the first author was a student in	 	


the Department of Linguistics, University of Maryland.


1 We refer readers not sharing this intuition to Section 3.



503
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 503–511, 
Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics


tions 6 and 7 discuss related work and summarize.

2   Linguistic Motivation

Verbal descriptions of an event often carry along 
with them an underlying attitude toward what is be- 
ing described. By framing the same event in differ- 
ent ways, speakers or authors “select some aspects 
of a perceived reality and make them more salient 
in a communicating text, in such a way as to pro- 
mote a particular problem definition, causal inter- 
pretation, moral evaluation, and/or treatment recom- 
mendation” (Entman, 1993, p.  52).  Clearly lexi- 
cal choices can accomplish this kind of selection, 
e.g.  choosing to describe a person as a terrorist 
rather than a freedom fighter, or referencing killer 
whales rather than orcas.2   Syntactic choices can 
also have framing effects. For example, Ronald Rea- 
gan’s famous use of the passive construction, “Mis- 
takes were made” (in the context of the Iran-Contra 
scandal), is a classic example of framing or spin: 
used without a by-phrase, the passive avoids iden- 
tifying a causal agent and therefore sidesteps the is- 
sue of responsibility (Broder, 2007). A toddler who 
says “My toy broke” instead of “I broke my toy” is 
employing the same linguistic strategy.
  Linguists have long studied syntactic variation 
in descriptions of the same event, often under the 
general heading of syntactic diathesis alternations 
(Levin, 1993; Levin and Hovav, 2005).  This line 
of research has established a set of semantic prop- 
erties that are widely viewed as “grammatically rel- 
evant” in the sense that they enable generalizations 
about syntactic “packaging” of meaning within (and 
across) the world’s languages.  For example, the 
verb break in English participates in the causative- 
inchoative alternation (causative event X broke Y 
can also be expressed without overt causation as Y 
broke), but the verb climb does not (X also causes 
the event in X climbed Y, but that event cannot be 
expressed as Y climbed).  These facts about partic- 
ipation in the alternation turn out to be connected 
with the fact that a breaking event entails a change of 
state in Y but a climbing event does not. Grammati- 
cally relevant semantic properties of events and their

  2 Supporters of an endangered species listing in Puget Sound 
generally referred to the animals as orcas, while opponents gen- 
erally said killer whales (Harden, 2006).


participants — causation, change of state, and others
— are central not only in theoretical work on lex- 
ical semantics, but in computational approaches to 
the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr,
1993; Wu and Palmer, 1994; Dang et al., 1998)).
  The approach we propose draws on two influ- 
ential discussions about grammatically relevant se- 
mantic properties in theoretical work on lexical se- 
mantics.  First, Dowty (1991) characterizes gram- 
matically relevant properties of a verb’s arguments 
(e.g.  subject and object) via inferences that follow 
from the meaning of the verb. For example, expres- 
sions like X murders Y or X interrogates Y entail 
that subject X caused the event.3   Second, Hopper 
and Thompson (1980) characterize “semantic transi- 
tivity” using similar properties, connecting semantic 
features to morphosyntactic behavior across a wide 
variety of languages.
  Bringing together Dowty with Hopper and 
Thompson, we find 13 semantic properties or- 
ganized  into  three  groups,  corresponding  to  the 
three components of a canonical transitive clause, 
expressed  as  X  verb  Y  in  English.4        Proper- 
ties associated with X involve volitional involve- 
ment in the event or state, causation of the event, 
sentience/awareness and/or perception, causing a 
change of state in Y , kinesis or movement, and ex- 
istence independent of the event.  Properties asso- 
ciated with the event or state conveyed by the verb 
include aspectual features of telicity (a defined end- 
point) and punctuality (the latter of which may be 
inversely related to a property known as incremen- 
tal theme).  Properties associated with Y include 
affectedness, change of state, (lack of) kinesis or 
movement, and (lack of) existence independent of 
the event.
  Now, observe that this set of semantic proper- 
ties involves many of the questions that would nat- 
urally help to shape one’s opinion about the event 
described by veer in (1).  Was anyone or anything 
affected by what took place, and to what degree? 
Did the event just happen or was it caused? Did the 
event reach a defined endpoint? Did participation in

  3 Kako (2006) has verified that people make these inferences 
based on X’s syntactic position even when a semantically empty 
nonsense verb is used.
4 We are deliberately sidestepping the choice of terminology
for X and Y, e.g. proto-Patient, theme, etc.


the event involve conscious thought or intent?  Our 
hypothesis is that the syntactic aspects of “framing”, 
as characterized by Entman, involve manipulation of 
these semantic properties, even when overt opinions 
are not being expressed. That is, we propose a con- 
nection between syntactic choices and implicit senti- 
ment mediated by the very same semantic properties 
that linguists have already identified as central when 
connecting surface expression to underlying mean- 
ing more generally.

3   Empirical Validation

We validated the hypothesized connection between 
implicit sentiment and grammatically relevant se- 
mantic properties using psycholinguistic methods, 
by varying the syntactic form of event descriptions, 
and showing that the semantic properties of descrip- 
tions do indeed predict perceived sentiment.5

3.1   Semantic property ratings
Materials.   Stimuli  were  constructed  using  11 
verbs of killing, which are widely viewed as proto- 
typical for the semantic properties of interest here 
(Lemmens, 1998):  X killed Y normally involves 
conscious, intentional causation by X  of a kinetic 
event that causes a (rather decisive and clearly ter- 
minated!) change of state in Y . The verbs comprise 
two classes:  the “transitive” class, involving ex- 
ternally caused change-of-state verbs (kill, slaugh- 
ter, assassinate, shoot, poison), and the “ergative” 
class (strangle, smother, choke, drown, suffocate, 
starve), within which verbs are internally caused 
(McKoon and MacFarland, 2000) or otherwise em- 
phasize properties of the object. Variation of syntac- 
tic description involved two forms: a transitive syn- 
tactic frame with a human agent as subject (“transi- 
tive form”, 2a), and a nominalization of the verb as


properties as well as Hopper and Thompson’s se- 
mantic transitivity components, responding via rat- 
ings on a 1-to-7 scale.  For example, the questions 
probing volition were:  “In this event, how likely
is it that (subject) chose to be involved?”, where
(subject) was the gunmen and the shooting, for 2(a-
b), respectively.6

3.2   Sentiment ratings

Materials.   We used the materials above to con- 
struct short, newspaper-like paragraphs, each one 
accompanied by a “headline” version of the same 
syntactic descriptions used above.   For example, 
given this paragraph:


A man has been charged for the suffocation of a 
woman early Tuesday morning.   City police say 
the man suffocated the 24-year-old woman using 
a plastic garbage bag. The woman, who police say 
had a previous relationship with her attacker, was 
on her way to work when the incident happened. 
Based on information provided by neighbors, po- 
lice were able to identify the suspect, who was ar- 
rested at gunpoint later the same day.

the three alternative headlines would be:

3(a) Man suffocates 24-year old woman 
(b) Suffocation kills 24-year-old woman 
(c) 24-year-old woman is suffocated

Some paragraphs were based on actual news sto- 
ries.7   In all paragraphs, there is an obvious nomi- 
nal referent for both the perpetrator and the victim, 
it is clear that the victim dies, and the perpetrator 
in the scenario is responsible for the resulting death 
directly rather than indirectly (e.g.  through negli-


subject and the verb kill as the predicate (“nominal-	 	


ized form”, 2b).

2(a) The gunmen shot the opposition leader
(b) The shooting killed the opposition leader

Participants and procedure.  A set of 18 vol- 
unteer participants, all native speakers of English, 
were presented with event descriptions and asked to 
answer questions probing both Dowty’s proto-role

5 Full details and materials in Greene (2007).
  

6 Standard experimental design methods were followed with 
respect to counterbalancing, block design, and distractor stim- 
uli; for example, no participant saw more than one of 2(a) or
2(b), and all participants saw equal numbers of transitive and
nominalized descriptions. The phrase In this event was repeated 
in each question and emphasized visually in order to encourage 
participants to focus on the particular event described in the sen- 
tence, rather than on the entities or events denoted in general.
7 In those cases no proper names were used, to avoid any
inadvertent emotional reactions or legal issues, although the de- 
scriptions retained emotional impact because we wanted readers 
to have some emotional basis with which to judge the headlines.


gence).8  The stem of the nominalization always ap- 
peared in the event description in either verbal or 
nominal form.

Participants and procedure.   A set of 31 volun- 
teers, all native speakers of English, were presented 
with the paragraph-length descriptions and accom- 
panying headlines. As a measure of sentiment, par- 
ticipants were asked to rate headlines on a 1-to-7 
scale with respect to how sympathetic they perceive 
the headline to be toward the perpetrator. For exam- 
ple, given the paragraph and one of the associated 
headlines in (3), a participant would be asked to rate 
“How sympathetic or unsympathetic is this headline 
to the man?”9

3.3   Analysis and discussion

Unsurprisingly, but reassuringly, an analysis of the 
sentiment ratings yields a significant effect of syn- 
tactic  form  on  sympathy  toward  the  perpetrator 
(F (2, 369) = 33.902, p  < .001), using a mixed 
model ANOVA run with the headline form as fixed 
effect.  The transitive form of the headline yielded 
significantly lower sympathy ratings than the nom- 
inalized or passive forms in pairwise comparisons 
(both p < .001). We have thus confirmed empir- 
ically that Reagan’s “Mistakes were made” was a 
wise choice of phrasing on his part.
  More important, we are now in a position to ex- 
amine the relationship between syntactic forms and 
perceived sentiment in more detail.  We performed 
regression analyses treating the 13 semantic prop- 
erty ratings plus the identity of the verb as indepen- 
dent variables to predict sympathy rating as a de- 
pendent variable, using the 24 stimulus sentences 
that bridged both collections of ratings.10   Consid-

  8 An alert reader may observe that headlines with nominal- 
ized subjects using the verb kill require some other nominaliza- 
tion, so they don’t say “Killing kills victim”.  For these cases 
in the data, an appropriate nominalization drawn from the event 
description was used (e.g., explosion).
9 Again, standard experimental design methods were used
with respect to block design, distractor stimuli, etc. The phrase 
this headline was emphasized to stress that it is the headline 
being rated, not the story. A second question rating sympathy 
toward the victim was also asked in each case, as an additional 
distractor.



ering semantic properties individually, we find that 
volition has the strongest correlation with sympathy
(a negative correlation, with r = −.776), followed
by sentience (r =  −.764) and kinesis/movement
(r = −.751). Although performing a multiple re-
gression with all variables for this size dataset is im-
possible, owing to overfitting (as a rule of thumb,
5 to 10 observed items are necessary per each in- 
dependent variable), a multiple regression involving 
verb, volition, and telicity as independent variables 
yields R = .88, R2 = .78 (p < .001). The value for 
adjusted R2, which explicitly takes into account the 
small number of observations, is 74.1.
  In summary, then, this ratings study confirms the 
influence of syntactic choices on perceptions of im- 
plicit sentiment.  Furthermore, it provides support 
for the idea that this influence is mediated by “gram- 
matically relevant” semantic properties, demonstrat- 
ing that these accounted for approximately 75% of 
the variance in implicit sentiment expressed by al- 
ternative headlines describing the same event.

4   Observable Approximation

Thus far, we have established a predictive connec- 
tion between syntactic choices and underlying or im- 
plicit sentiment, mediated by grammatically relevant 
semantic properties. In an ideal world, we could har- 
ness the predictive power of those properties by us- 
ing volition, causation, telicity, etc.  as features for 
regression or classification in sentiment prediction 
tasks. Unfortunately, the properties are not directly 
observable, and neither automatic annotators nor la- 
beled training data currently exist.
  We therefore pursue a different strategy, which we 
refer to as observable proxies for underlying seman- 
tics (OPUS). It can be viewed as a middle ground 
between relying on construction-level syntactic dis- 
tinctions (such as the 3-way transitive, nominalized 
subject, passive distinction in Section 3) and an- 
notation of fine-grained semantic properties.   The 
key idea is to use observable grammatical relations, 
drawn from the usages of terms determined to be 
relevant to a domain, as proxies for the underlying 
semantic properties that gave rise to their syntactic
realization using those relations. Automatically cre-


10These involved only the transitive and nominalized forms,	 	


because many of the questions were inapplicable to the passive 
form.  Since the two ratings studies involved different subject


pools, regression models were run over the mean values of each 
observation in the experimental data.


ated features based on those observable proxies are 
then used in classification as described in Section 5.
  In order to identify the set T of terms relevant 
to a particular document collection, we adopt the 
relative frequency ratio (Damerau, 1993), R(t) =
t
  

The N O O B J features can capture a habitual read- 
ing, or in some cases a detransitivizing effect as- 
sociated with omission of the direct object (Olsen 
and Resnik,  1997).    The bold text in (5) yields 
N O O B J:kill as a feature.


Rt 	t


t	fc


domain/Rreference ,  where Rc =


Nc   is the ratio of


5(a) At the same time, we should 
never ignore the


term t’s frequency in corpus c to the size Nc of that
corpus.  R(t) is a simple but effective comparison 
of a term’s prevalence in a particular collection as 
compared to a general reference corpus.  We used 
the British National Corpus as the reference because 
it is both very large and representative of text from a 
wide variety of domains and genres. The threshold 
of R(t) permitting membership in T is an experi- 
mental parameter.
  OPUS features are defined in terms of syntactic 
dependency relations involving terms in T . Given a 
set D of syntactic dependency relations, features are
of the form t : d or d : t, with d ∈ D, t ∈ T . That
is, they are term-dependency pairs extracted from
term-dependency-term dependency tuples, preserv- 
ing whether the term is the head or the dependent 
in the dependency relation. In addition, we add two 
construction-specific features: TR A N S:v, which rep- 
resents verb v in a canonical, syntactically transitive 
usage, and N O O B J:v, present when verb v is used 
without a direct object.11
Example 4 shows source text (bolded clause in
4a),  an illustrative subset of parser dependencies
(4b), and corresponding OPUS features (4c):

4(a) Life Without Parole does not eliminate the risk 
that the prisoner will murder a guard, a visi- 
tor, or another inmate.
(b) nsubj(murder,  prisoner);   aux(murder,  will);
dobj(murder, guard)
(c) TR A N S:murder,  murder:nsubj,  nsubj:prisoner, 
murder:aux, aux:will, murder:dobj, dobj:guard

Intuitively the presence of TR A N S:murder suggests 
the entire complex of semantic properties discussed 
in Section 2, bringing together the impliciation of 
volition,  causation,  etc.   on the part of prisoner 
(as does nsubj:prisoner), affectedness and change of 
state on the part of guard (as does dobj:guard), and 
so forth.

11We parsed English text using the Stanford parser.


risks of allowing the inmate to kill again.

In this case, omitting the direct object decreases the 
extent to which the killing event is interpreted as 
telic, and it eliminates the possibility of attributing 
change-of-state to a specific affected object (much 
like “Mistakes were made” avoids attributing cause 
to a specified subject),  placing the phrasing at a 
less “semantically transitive” point on the transi- 
tivity continuum (Hopper and Thompson, 1980). 
Some informants find a perceptible increase in neg- 
ative sentiment toward inmate when the sentence is 
phrased as in 5(b):

5(b) At the same time, we should never ignore the 
risks of allowing the inmate to kill someone 
again.

5   Computational Application

Having discussed linguistic motivation, empirical 
validation, and practical approximation of seman- 
tically relevant features, we now present two stud- 
ies demonstrating their value in sentiment classifica- 
tion. For the first study, we have constructed a new 
data set particularly well suited for testing our ap- 
proach, based on writing about the death penalty. In 
our second study, we make a direct comparison with 
prior state-of-the-art classification using the Bitter 
Lemons corpus of Lin et al. (2006).

5.1   Predicting Opinions of the Death Penalty
Corpus.   We constructed a new corpus for exper- 
imentation on implicit sentiment by downloading 
the  contents  of  pro-  and  anti-death-penalty  Web 
sites  and  manually checking,  for  a large  subset, 
that the viewpoints expressed in documents were as 
expected.   The collection, which we will refer to 
as the DP corpus, comprises documents from five 
pro-death-penalty sites and three anti-death-penalty 
sites, and the corpus was engineered to have an even 
balance, 596 documents per side.12

12Details in Greene (2007).



Frequent bigram baseline.   We adopted a super- 
vised classification approach based on word n-gram 
features, using SVM classification in the WEKA 
machine learning package. In initial exploration us- 
ing both unigrams and bigrams, and using both word 
forms and stems, we found that performance did not 
differ significantly, and chose stemmed bigrams for 
our baseline comparisons. In order to control for the 
difference in the number of features available to the 
classifier in our comparisons, we use the N most fre- 
quent stemmed bigrams as the baseline feature set 
where N is matched to number of OPUS features 
used in the comparison condition.

OPUS-kill verbs:  OPUS features for manually 
selected verbs.   We created OPUS features for 14 
verbs — those used in Section 3, plus murder, exe- 
cute, and stab and their nominalizations (including 
both event and -er nominals, e.g.  both killing and 
killer) — generating N = 1016 distinct features.

OPUS-domain:  OPUS features for domain- 
relevant verbs.   We created OPUS features for the
117 verbs for which the relative frequency ratio 
was greater than 1.  This list includes many of the 
kill verbs we used in Section 3, and introduces, 
among others, many transitive verbs describing acts 
of physical force (e.g. rape, rob, steal, beat, strike, 
force, fight) as well as domain-relevant verbs such 
as testify, convict, and sentence. Included verbs near 
the borderline included, for example, hold, watch, 
allow, and try. Extracting OPUS features for these 
verbs yielded N = 7552 features.

Evaluation.   Cross-validation   at   the   document 
level does not test what we are interested in, since 
a classifier might well learn to bucket documents ac- 
cording to Web site, not according to pro- or anti- 
death-penalty sentiment. To avoid this difficulty, we 
performed site-wise cross-validation. We restricted 
our attention to the two sites from each perspec- 
tive with the most documents, which we refer to as 
pro1, pro2, anti1, and anti2, yielding 4-fold cross- 
validation.  Each fold ftrain,test  is defined as con- 
taining all documents from one pro and one anti site 
for training, using all documents from the remain- 
ing pro and anti sites for testing.   So, for exam- 
ple, fold f11,22  uses all documents from pro1 and 
anti1 in training, and all documents from pro2 and



Co
nd
iti
on
N 
fea
tur
es
SV
M 
ac
cu
ra
cy
Ba
sel
ine
OP
US
-
kil
l 
ver
bs
10
16
10
16
68.
37
82.
09
Ba
sel
ine
OP
US
-
do
ma
in
75
52
75
52
71.
96
88.
10

Table 1: Results for 4-fold site-wise cross-validation us- 
ing the DP corpus

Co
nd
iti
on
N 
fea
tur
es
SV
M 
ac
cu
ra
cy
Ba
sel
ine
OP
US
-
fre
qu
ent 
ver
bs
OP
US
-
kil
l 
ver
bs
15
18
15
18
10
62
55.
95
55.
95
66.
67

Table 2: DP corpus comparison for OPUS features based 
on frequent vs. domain-relevant verbs


anti2 for testing.13   As Table 1 shows, OPUS fea- 
tures provide substantial and statistically significant 
gains (p < .001).
  As a reality check to verify that it is domain- 
relevant verb usages and the encoding of events they 
embody that truly drives improved classification, we 
extracted OPUS features for the 14 most frequent 
verbs found in the DP Corpus that were not in our 
manually created list of kill verbs, along with their 
nominalizations. Table 2 shows the results of a clas- 
sification experiment using a single train-test split, 
training on 1062 documents from pro1, pro2, anti1, 
anti2 and testing on 84 test documents from the sig- 
nificantly smaller remaining sites.     Using OPUS 
features for the most frequent non-kill verbs fails 
to beat the baseline, establishing that it is not sim- 
ply term frequency, the presence of particular gram- 
matical relations, or a larger feature set that the kill- 
verb OPUS model was able to exploit, but rather the 
properties of event encodings involving the kill verbs 
themselves.

5.2   Predicting Points of View in the
Israeli-Palestinian Conflict

In order to make a direct comparison here with prior 
state-of-the-art work on sentiment analysis, we re- 
port on sentiment classification using OPUS features 
in experiments using a publicly available corpus in- 
volving opposing perspectives, the Bitter Lemons

  13Site (# of documents): pro1= clarkprosecutor.org (437), 
pro2= prodeathpenalty.com (117), anti1= deathpenaltyinfo.org 
(319), anti2= nodeathpenalty.org (212)


(hence BL) corpus introduced by Lin et al. (2006).



Corpus.   The Bitter Lemons corpus comprises es- 
says posted at www.bitterlemons.org, which, 
in the words of the site, “present Israeli and Pales- 
tinian viewpoints on prominent issues of concern”. 
As a corpus, it has a number of interesting proper- 
ties. First, its topic area is one of significant interest 
and considerable controversy, yet the general tenor 
of the web site is one that eschews an overly shrill 
or extreme style of writing. Second, the site is orga- 
nized in terms of issue-focused weekly editions that 
include essays with contrasting viewpoints from the 
site’s two editors, plus two essays, also contrasting, 
from guest editors. This creates a natural balance be- 
tween the two sides and across the subtopics being 
discussed. The BL corpus as prepared by Lin et al. 
contains 297 documents from each of the Israeli and



C lassification Accuracy, B L C orpus
Test S cenario 1 (GeneralFilter)

12
98

10
96

8	94

92
6
90
4
88

2
86

0	84
Individua l Ex pe rim e nt (ρ va lue s a nd a ccura cy)


C lassification Accuracy, B L C orpus
Test Scenario 2 (GeneralFilter)

12

85
10

8	80

6








ρ (Verb)
ρ (Noun)
 	OPUS
Lin 2006 NB-B
            Lin 2006 SVM











ρ (Verb) ρ (Noun) OPUS


Palestinian viewpoints, averaging 700-800 words in 
length.

Lin et al.  classifiers.	Lin et al.  report results on



4

2

0
Individua l Ex pe rim e nt (ρ va lue s a nd a ccura cy)


75	            Lin 2006 NB-B Lin 2006 SVM

70


65


distinguishing Israeli vs.  Palestinian perspectives 
using an SVM classifier, a naive Bayes classifier 
NB-M using maximum a posteriori estimation, and a 
naive Bayes classifier NB-B using full Bayesian in- 
ference. (Document perspectives are labeled clearly 
on the site.)  We continue to use the WEKA SVM 
classifier, but compare our results to both their SVM 
and NB-B, since the latter achieved their best results.

OPUS features.   As in Section 5.1, we experi- 
mented with OPUS features driven by automati- 
cally extracted lists of domain-relevant verbs.  For 
these experiments, we included domain-relevant 
nouns,  and we varied a threshold ρ for the rela- 
tive frequency ratio, including only terms for which 
log(R(t)) > ρ.  In addition, we introduced a gen- 
eral filter on OPUS features, eliminating syntactic 
dependency types that do not usefully reflect seman- 
tically relevant properties: det, predet, preconj, prt, 
aux, auxpas, cc, punct, complm, mark, rel, ref, expl.

Evaluation.   Lin et al.  describe two test scenar- 
ios. In the first, referred to as Test Scenario 1, they 
trained on documents written by the site’s guests, 
and tested on documents from the site’s editors. Test 
Scenario 2 represents the reverse, training on docu- 
ments from the site editors and testing on documents


Figure 1: Results on the Bitter Lemons corpus



from guest authors.  As in our site-wise cross vali- 
dation for the DP corpus, this strategy ensures that 
what is being tested is classification according to the 
viewpoint, not author or topic.
  Figure 1 (top) summarizes a large set of experi- 
ments for Test Scenario 1, in which we varied the 
values of ρ for verbs and nouns.  Each experiment,
using a particular (ρ(verbs), ρ(nouns)), corresponds
to a vertical strip on the x-axis.  The points on that
strip include the ρ values for verbs and nouns, mea- 
sured by the scale on the y-axis at the left of the 
figure; the accuracy of Lin et al.’s SVM (88.22% ac- 
curacy, constant across all our variations); the accu- 
racy of Lin et al.’s NB-B classifier (93.46% accu- 
racy, constant across all our variations), and the ac- 
curacy of our SVM classifier using OPUS features, 
which varies depending on the ρ values. Across 423 
experiments, our average accuracy is 95.41%, with 
the best accuracy achieved being 97.64%. Our clas- 
sifier underperformed NB-B slightly, with accura- 
cies from 92.93% to 93.27%, in just 8 of the 423 
experiments.
Figure 1 (bottom) provides a similar summary for


experiments in Test Scenario 2. The first thing to no- 
tice is that accuracy for all methods is lower than for 
Test Scenario 1. This is not terribly surprising: it is 
likely that training a classifier on the more uniform 
authorship of the editor documents builds a model 
that generalizes less well to the more diverse au- 
thorship of the guest documents (though accuracy 
is still quite high).  In addition, the editor-authored 
documents comprise a smaller training set, consist- 
ing of 7,899 sentences, while the guest documents 
have a total of 11,033 sentences, a 28% difference. 
In scenario 2, we obtain average accuracy across ex- 
periments of 83.12%, with a maximum of 85.86%, 
in this case outperforming the 81.48% obtained by 
Lin’s SVM fairly consistently, and in some cases ap- 
proaching or matching NB-B at 85.85%.

6   Related Work

Pang and Lee’s (2008) excellent monograph pro- 
vides a thorough, well organized, and relatively re- 
cent description of computational work on senti- 
ment, opinion, and subjectivity analysis.
  The problem of classifying underlying sentiment 
in statements that are not overtly subjective is less 
studied within the NLP literature, but it has received 
some attention in other fields. These include, for ex- 
ample, research on content analysis in journalism, 
media studies, and political economy (Gentzkow 
and Shapiro, 2006a; Gentzkow and Shapiro, 2006b; 
Groseclose and Milyo, 2005; Fader et al., 2007); au- 
tomatic identification of customer attitudes for busi- 
ness e-mail routing (Durbin et al., 2003).  And, of 
course, the study of perceptions in politics and me- 
dia bears a strong family resemblance to real-world 
marketing problems involving reputation manage- 
ment and business intelligence (Glance et al., 2005).
  Within computational linguistics, what we call 
implicit sentiment was introduced as a topic of study 
by Lin et al. (2006) under the rubric of identifying 
perspective, though similar work had begun earlier 
in the realm of political science (e.g.  (Laver et al.,
2003)). Other recent work focusing on the notion of 
perspective or ideology has been reported by Martin 
and Vanberg (2008) and Mullen and Malouf (2008).
  Among prior authors, Gamon’s (2004) research is 
perhaps closest to the work described here, in that 
he uses some features based on a sentence’s logical


form, generated using a proprietary system.  How- 
ever, his features are templatic in nature in that they 
do not couple specific lexical entries with their logi- 
cal form. Hearst (1992) and Mulder et al. (2004) de- 
scribe systems that make use of argument structure 
features coupled with lexical information, though 
neither provides implementation details or experi- 
mental results.
  In terms of computational experimentation, work 
by Thomas et al.  (2006), predicting yes and no 
votes in corpus of United States Congressional floor 
debate speeches, is quite relevant.  They combined 
SVM classification with a min-cut model on graphs 
in order to exploit both direct textual evidence and 
constraints suggested by the structure of Congres- 
sional debates, e.g.  the fact that the same individ- 
ual rarely gives one speech in favor of a bill and an- 
other opposing it.  We have extend their method to 
use OPUS features in the SVM and obtained signifi- 
cant improvements over their classification accuracy 
(Greene, 2007; Greene and Resnik, in preparation).

7   Conclusions

In this paper we have introduced an approach to 
implicit sentiment motivated by theoretical work in 
lexical semantics, presenting evidence for the role of 
semantic properties in human sentiment judgments. 
This research is, to our knowledge, the first to draw 
an explicit and empirically supported connection be- 
tween theoretically motivated work in lexical se- 
mantics and readers’ perception of sentiment. In ad- 
dition, we have reported positive sentiment classifi- 
cation results within a standard supervised learning 
setting, employing a practical first approximation to 
those semantic properties, including positive results 
in a direct comparison with the previous state of the 
art.
  Because we computed OPUS features for opin- 
ionated as well as non-evaluative language in our 
corpora, obtaining overall positive results, we be- 
lieve these features may also improve conventional 
opinion labeling for subjective text. This will be in- 
vestigated in future work.

Acknowledgments

The authors gratefully acknowledge useful discus- 
sions with Don Hindle and Chip Denman.


References

John Broder. 2007. Familiar fallback for officials: ’mis- 
takes were made’. New York Times. March 14.
F. J. Damerau. 1993. Generating and evaluating domain- 
oriented multi-word terms from texts.   Information 
Processing and Management, 29:433–447.
Hoa Trang Dang, Karin Kipper, Martha Palmer, and 
Joseph  Rosenzweig.     1998.     Investigating Regu- 
lar Sense Extensions Based on Intersective Levin 
Classes.  In ACL/COLING 98, pages 293–299, Mon- 
treal, Canada, August 10–14.
Bonnie J. Dorr. 1993. Machine Translation: A View from 
the Lexicon. The MIT Press, Cambridge, MA.
David Dowty.  1991.  Thematic Proto-Roles and Argu- 
ment Selection. Language, 67:547–619.
S. D. Durbin, J. N. Richter, and D. Warner. 2003. A sys- 
tem for affective rating of texts. In Proc. 3rd Workshop 
on Operational Text Classification, KDD-2003.
Robert M. Entman. 1993. Framing: Toward clarification 
of a fractured paradigm.  Journal of Communication,
43(4):51–58.
Anthony Fader, Dragomir R. Radev, Michael H. Crespin, 
Burt L. Monroe, Kevin M. Quinn, and Michael Co- 
laresi.   2007.   MavenRank:  Identifying influential 
members of the US Senate using lexical centrality. In 
Proceedings of the Conference on Empirical Methods 
in Natural Language Processing (EMNLP).
Michael Gamon. 2004. Sentiment classification on cus- 
tomer feedback data: noisy data, large feature vectors, 
and the role of linguistic analysis. In Proc. COLING.
M. Gentzkow and J. Shapiro.  2006a.  Media bias and 
reputation.  Journal of Political Economy, 114:280–
316.
M. Gentzkow and J. Shapiro.    2006b.    What drives 
media slant?     Evidence from U.S. newspapers. 
http://ssrn.com/abstract=947640.
Natalie Glance, Matthew Hurst, Kamal Nigam, Matthew
Siegler,  Robert  Stockton,  and  Takashi  Tomokiyo.
2005.   Deriving marketing intelligence from online 
discussion.  In Proc. KDD’05, pages 419–428, New 
York, NY, USA. ACM.
Stephan Greene.  2007.  Spin: Lexical Semantics, Tran- 
sitivity, and the Identification of Implicit Sentiment. 
Ph.D. thesis, University of Maryland.
T. Groseclose and J. Milyo. 2005. A measure of media 
bias. The Quarterly Journal of Economics, 120:1191–
1237.
B. Harden. 2006. On Puget Sound, It’s Orca vs. Inc. The
Washington Post. July 26, page A3.
Marti Hearst.  1992.  Direction-based text interpretation 
as an information access refinement. In Paul Jacobs, 
editor, Text-Based Intelligent Systems, pages 257–274. 
Lawrence Erlbaum Associates.



Paul Hopper and Sandra Thompson. 1980.  Transitivity 
in Grammar and Discourse. Language, 56:251–295.
E. Kako. 2006. Thematic role properties of subjects and 
objects. Cognition, 101(1):1–42, August.
Michael Laver, Kenneth Benoit, and John Garry.  2003.
Extracting policy positions from political texts using 
words as data.   American Political Science Review,
97(2):311–331.
M. Lemmens. 1998. Lexical perspectives on transitivity 
and ergativity. John Benjamins.
Beth Levin and Malka Rappaport Hovav.  2005.  Argu- 
ment Realization.   Research Surveys in Linguistics. 
Cambridge University Press, New York.
Beth Levin.   1993.   English Verb Classes and Alter- 
nations:  A Preliminary Investigation.  University of 
Chicago Press, Chicago, IL.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and 
Alexander Hauptmann. 2006. Which side are you on? 
identifying perspectives at the document and sentence 
levels.  In Proceedings of the Conference on Natural 
Language Learning (CoNLL).
Lanny W. Martin and Georg Vanberg.   2008.   A ro- 
bust transformation procedure for interpreting political 
text. Political Analysis, 16(1):93–100.
G. McKoon and T. MacFarland.  2000.  Externally and 
internally caused change of state verbs.   Language, 
pages 833–858.
M. Mulder, A. Nijholt, M. den Uyl,  and P. Terpstra.
2004. A lexical grammatical implementation of affect. 
In Proc. TSD-04, Lecture notes in computer science
3206, pages 171–178). Springer-Verlag.
Tony Mullen and Robert Malouf.  2008.  Taking sides: 
User classification for informal online political dis- 
course. Internet Research, 18:177–190.
Mari Broman Olsen and Philip Resnik.  1997.  Implicit 
Object Constructions and the (In)transitivity Contin- 
uum.  In 33rd Proceedings of the Chicago Linguistic 
Society, pages 327–336.
Bo Pang and Lillian Lee.   2008.   Opinion mining and 
sentiment analysis.  Foundations and Trends in Infor- 
mation Retrieval, 2(1-2):1–135.
James Pustejovsky.	1991.	The Generative Lexicon.
Computational Linguistics, 17(4):409–441.
Philip J. Stone.  1966.  The General Inquirer: A Com- 
puter Approach to Content Analysis. The MIT Press.
Matt Thomas, Bo Pang, and Lillian Lee.   2006.   Get 
out  the  vote:   Determining  support  or  opposition 
from Congressional floor-debate transcripts.  In Proc. 
EMNLP, pages 327–335.
Zhibao Wu and Martha Palmer.  1994.  Verb Semantics 
and Lexical Selection. In Proc. ACL, pages 133–138, 
Las Cruces, New Mexico.

