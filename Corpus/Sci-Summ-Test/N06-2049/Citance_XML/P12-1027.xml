<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a joint model for Chinese word segmentation and new word detection.</S>
		<S sid ="2" ssid = "2">We present high dimensional new features, including word-based features and enriched edge (label-transition) features, for the joint modeling.</S>
		<S sid ="3" ssid = "3">As we know, training a word segmentation system on large-scale datasets is already costly.</S>
		<S sid ="4" ssid = "4">In our case, adding high dimensional new features will further slow down the training speed.</S>
		<S sid ="5" ssid = "5">To solve this problem, we propose a new training method, adaptive online gradient descent based on feature frequency information, for very fast online training of the parameters, even given large-scale datasets with high dimensional features.</S>
		<S sid ="6" ssid = "6">Compared with existing training methods, our training method is an order magnitude faster in terms of training time, and can achieve equal or even higher accuracies.</S>
		<S sid ="7" ssid = "7">The proposed fast training method is a general purpose optimization method, and it is not limited in the specific task discussed in this paper.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Since Chinese sentences are written as continuous sequences of characters, segmenting a character sequence into words is normally the first step in the pipeline of Chinese text processing.</S>
			<S sid ="9" ssid = "9">The major problem of Chinese word segmentation is the ambiguity.</S>
			<S sid ="10" ssid = "10">Chinese character sequences are normally ambiguous, and new words (out- of-vocabulary words) are a major source of the ambiguity.</S>
			<S sid ="11" ssid = "11">A typical category of new words is named entities, including organization names, person names, location names, and so on.</S>
			<S sid ="12" ssid = "12">In this paper, we present high dimensional new features, including word-based features and enriched edge (label-transition) features, for the joint modeling of Chinese word segmentation (CWS) and new word detection (NWD).</S>
			<S sid ="13" ssid = "13">While most of the state-of-the-art CWS systems used semi- Markov conditional random fields or latent variable conditional random fields, we simply use a single first-order conditional random fields (CRFs) for the joint modeling.</S>
			<S sid ="14" ssid = "14">The semi-Markov CRFs and latent variable CRFs relax the Markov assumption of CRFs to express more complicated dependencies, and therefore to achieve higher disambiguation power.</S>
			<S sid ="15" ssid = "15">Alternatively, our plan is not to relax Markov assumption of CRFs, but to exploit more complicated dependencies via using refined high- dimensional features.</S>
			<S sid ="16" ssid = "16">The advantage of our choice is the simplicity of our model.</S>
			<S sid ="17" ssid = "17">As a result, our CWS model can be more efficient compared with the heavier systems, and with similar or even higher accuracy because of using refined features.</S>
			<S sid ="18" ssid = "18">As we know, training a word segmentation system on large-scale datasets is already costly.</S>
			<S sid ="19" ssid = "19">In our case, adding high dimensional new features will further slow down the training speed.</S>
			<S sid ="20" ssid = "20">To solve this challenging problem, we propose a new training method, adaptive online gradient descent based on feature frequency information (ADF), for very fast word segmentation with new word detection, even given large-scale datasets with high dimensional features.</S>
			<S sid ="21" ssid = "21">In the proposed training method, we try to use more refined learning rates.</S>
			<S sid ="22" ssid = "22">Instead of using a single learning rate (a scalar) for all weights, we extend the learning rate scalar to a learning rate vector based on feature frequency information in the updating.</S>
			<S sid ="23" ssid = "23">By doing so, each weight has 253 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 253–262, Jeju, Republic of Korea, 814 July 2012.</S>
			<S sid ="24" ssid = "24">Qc 2012 Association for Computational Linguistics its own learning rate adapted on feature frequency information.</S>
			<S sid ="25" ssid = "25">We will show that this can significantly improve the convergence speed of online learning.</S>
			<S sid ="26" ssid = "26">We approximate the learning rate vector based on feature frequency information in the updating process.</S>
			<S sid ="27" ssid = "27">Our proposal is based on the intuition that a feature with higher frequency in the training process should be with a learning rate that is decayed faster.</S>
			<S sid ="28" ssid = "28">Based on this intuition, we will show the formalized training algorithm later.</S>
			<S sid ="29" ssid = "29">We will show in experiments that our solution is an order magnitude faster compared with exiting learning methods, and can achieve equal or even higher accuracies.</S>
			<S sid ="30" ssid = "30">The contribution of this work is as follows: • We propose a general purpose fast online training method, ADF.</S>
			<S sid ="31" ssid = "31">The proposed training method requires only a few passes to complete the training.</S>
			<S sid ="32" ssid = "32">• We propose a joint model for Chinese word segmentation and new word detection.</S>
			<S sid ="33" ssid = "33">• Compared with prior work, our system achieves better accuracies on both word segmentation and new word detection.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="34" ssid = "1">First, we review related work on word segmentation and new word detection.</S>
			<S sid ="35" ssid = "2">Then, we review popular online training methods, in particular stochastic gradient descent (SGD).</S>
			<S sid ="36" ssid = "3">2.1 Word Segmentation and New Word.</S>
			<S sid ="37" ssid = "4">Detection Conventional approaches to Chinese word segmentation treat the problem as a sequential labeling task (Xue, 2003; Peng et al., 2004; Tseng et al., 2005; Asahara et al., 2005; Zhao et al., 2010).</S>
			<S sid ="38" ssid = "5">To achieve high accuracy, most of the state- of-the-art systems are heavy probabilistic systems using semi-Markov assumptions or latent variables (Andrew, 2006; Sun et al., 2009b).</S>
			<S sid ="39" ssid = "6">For example, one of the state-of-the-art CWS system is the latent variable conditional random field (Sun et al., 2008; Sun and Tsujii, 2009) system presented in Sun et al.</S>
			<S sid ="40" ssid = "7">(2009b).</S>
			<S sid ="41" ssid = "8">It is a heavy probabilistic model and it is slow in training.</S>
			<S sid ="42" ssid = "9">A few other state-of-the-art CWS systems are using semi-Markov perceptron methods or voting systems based on multiple semi-Markov perceptron segmenters (Zhang and Clark, 2007; Sun, 2010).</S>
			<S sid ="43" ssid = "10">Those semi-Markov perceptron systems are moderately faster than the heavy probabilistic systems using semi-Markov conditional random fields or latent variable conditional random fields.</S>
			<S sid ="44" ssid = "11">However, a disadvantage of the perceptron style systems is that they can not provide probabilistic information.</S>
			<S sid ="45" ssid = "12">On the other hand, new word detection is also one of the important problems in Chinese information processing.</S>
			<S sid ="46" ssid = "13">Many statistical approaches have been proposed (J. Nie and Jin, 1995; Chen and Bai, 1998; Wu and Jiang, 2000; Peng et al., 2004; Chen and Ma, 2002; Zhou, 2005; Goh et al., 2003; Fu and Luke, 2004; Wu et al., 2011).</S>
			<S sid ="47" ssid = "14">New word detection is normally considered as a separate process from segmentation.</S>
			<S sid ="48" ssid = "15">There were studies trying to solve this problem jointly with CWS.</S>
			<S sid ="49" ssid = "16">However, the current studies are limited.</S>
			<S sid ="50" ssid = "17">Integrating the two tasks would benefit both segmentation and new word detection.</S>
			<S sid ="51" ssid = "18">Our method provides a convenient framework for doing this.</S>
			<S sid ="52" ssid = "19">Our new word detection is not a stand- alone process, but an integral part of segmentation.</S>
			<S sid ="53" ssid = "20">2.2 Online Training.</S>
			<S sid ="54" ssid = "21">The most representative online training method is the SGD method.</S>
			<S sid ="55" ssid = "22">The SGD uses a small randomly-selected subset of the training samples to approximate the gradient of an objective function.</S>
			<S sid ="56" ssid = "23">The number of training samples used for this approximation is called the batch size.</S>
			<S sid ="57" ssid = "24">By using a smaller batch size, one can update the parameters more frequently and speed up the convergence.</S>
			<S sid ="58" ssid = "25">The extreme case is a batch size of 1, and it gives the maximum frequency of updates, which we adopt in this work.</S>
			<S sid ="59" ssid = "26">Then, the model parameters are updated in such a way: wt+1 = wt + γt ∇wt Lstoch (zi , wt ), (1) where t is the update counter, γt is the learning rate, and Lstoch (zi , wt ) is the stochastic loss function based on a training sample zi . There were accelerated versions of SGD, including stochastic meta descent (Vishwanathan et al., 2006) and periodic step-size adaptation online learning (Hsu et al., 2009).</S>
			<S sid ="60" ssid = "27">Compared with those two methods, our proposal is fundamentally different.</S>
			<S sid ="61" ssid = "28">Those two methods are using 2nd-order gradient (Hessian) information for accelerated training, while our accelerated training method does not need such 2nd-order gradient information, which is costly and complicated.</S>
			<S sid ="62" ssid = "29">Our ADF training method is based on feature frequency adaptation, and there is no prior work on using feature frequency information for accelerating online training.</S>
			<S sid ="63" ssid = "30">Other online training methods includes averaged SGD with feedback (Sun et al., 2010; Sun et al., 2011), latent variable perceptron training (Sun et al., 2009a), and so on.</S>
			<S sid ="64" ssid = "31">Those methods are less related to this paper.</S>
	</SECTION>
	<SECTION title="System Architecture. " number = "3">
			<S sid ="65" ssid = "1">3.1 A Joint Model Based on CRFs First, we briefly review CRFs.</S>
			<S sid ="66" ssid = "2">CRFs are proposed as a method for structured classification by solving “the label bias problem” (Lafferty et al., 2001).</S>
			<S sid ="67" ssid = "3">Assuming a feature function that maps a pair of observation sequence x and label sequence y to a global feature vector f , the probability of a label sequence y conditioned on the observation sequence x is modeled as follows (Lafferty et al., 2001): exp {w⊤f (y, x)} Since no word list can be complete, new word identification is an important task in Chinese NLP.</S>
			<S sid ="68" ssid = "4">New words in input text are often incorrectly segmented into single-character or other very short words (Chen and Bai, 1998).</S>
			<S sid ="69" ssid = "5">This phenomenon will also undermine the performance of Chinese word segmentation.</S>
			<S sid ="70" ssid = "6">We consider here new word detection as an integral part of segmentation, aiming to improve both segmentation and new word detection: detected new words are added to the word list lexicon in order to improve segmentation.</S>
			<S sid ="71" ssid = "7">Based on our CRF word segmentation system, we can compute a probability for each segment.</S>
			<S sid ="72" ssid = "8">When we find some word segments are of reliable probabilities yet they are not in the existing word list, we then treat those “confident” word segments as new words and add them into the existing word list.</S>
			<S sid ="73" ssid = "9">Based on preliminary experiments, we treat a word segment as a new word if its probability is larger than 0.5.</S>
			<S sid ="74" ssid = "10">Newly detected words are re- incorporated into word segmentation for improving segmentation accuracies.</S>
			<S sid ="75" ssid = "11">3.2 New Features.</S>
			<S sid ="76" ssid = "12">Here, we will describe high dimensional new features for the system.</S>
			<S sid ="77" ssid = "13">3.2.1 Word-based Features P (y|x, w) = ∑ ∀y′ exp {w⊤f (y′, x) } , (2) There are two ideas in deriving the refined where w is a parameter vector.</S>
			<S sid ="78" ssid = "14">Given a training set consisting of n labeled sequences, zi = (xi , yi ), for i = 1 . . .</S>
			<S sid ="79" ssid = "15">n, parameter estimation is performed by maximizing the objective function, n L(w) = ∑ log P (yi |xi , w) − R(w).</S>
			<S sid ="80" ssid = "16">(3) i=1 The first term of this equation represents a conditional log-likelihood of a training data.</S>
			<S sid ="81" ssid = "17">The second term is a regularizer for reducing overfitting.</S>
			<S sid ="82" ssid = "18">We employed an L2 prior, R(w) = ||w|| . In what follows, we denote the conditional log-likelihood of each sample log P (yi |xi , w) as ℓ(zi , w).</S>
			<S sid ="83" ssid = "19">The final objective function is as follows: features.</S>
			<S sid ="84" ssid = "20">The first idea is to exploit word features for node features of CRFs.</S>
			<S sid ="85" ssid = "21">Note that, although our model is a Markov CRF model, we can still use word features to learn word information in the training data.</S>
			<S sid ="86" ssid = "22">To derive word features, first of all, our system automatically collect a list of word unigrams and bigrams from the training data.</S>
			<S sid ="87" ssid = "23">To avoid overfitting, we only collect the word unigrams and bigrams whose frequency is larger than 2 in the training set.</S>
			<S sid ="88" ssid = "24">This list of word unigrams and bigrams are then used as a unigram-dictionary and a bigram-dictionary to generate word-based unigram and bigram features.</S>
			<S sid ="89" ssid = "25">The word-based features are indicator functions that fire when the local character sequence matches a word unigram or bigram occurred in the training data.</S>
			<S sid ="90" ssid = "26">The word-based feature templates derived for the label yi are as follows: n L(w) = ∑ ℓ(zi , w) − || w||2 .</S>
			<S sid ="91" ssid = "27">(4) • unigram1(x, yi ) ← [xj,i , yi ], if the i=1 2σ2 character sequence xj,i matches a word w ∈ U, with the constraint i − 6 &lt; j &lt; i. The item xj,i represents the character sequence xj . . .</S>
			<S sid ="92" ssid = "28">xi . U represents the unigram-dictionary collected from the training data.</S>
			<S sid ="93" ssid = "29">• unigram2(x, yi ) ← [xi,k , yi ], if the character sequence xi,k matches a word w ∈ U, with the constraint i &lt; k &lt; i + 6.</S>
			<S sid ="94" ssid = "30">• bigram1(x, yi ) ← [xj,i−1 , xi,k , yi ], if the word bigram candidate [xj,i−1 , xi,k ] hits a word bigram [wi , wj ] ∈ B, and satisfies the aforementioned constraints on j and k. B represents the word bigram dictionary collected from the training data.</S>
			<S sid ="95" ssid = "31">• bigram2(x, yi ) ← [xj,i , xi+1,k , yi ], if the word bigram candidate [xj,i , xi+1,k ] hits a word bigram [wi , wj ] ∈ B, and satisfies the aforementioned constraints on j and k. We also employ the traditional character-based features.</S>
			<S sid ="96" ssid = "32">For each label yi , we use the feature templates as follows: • Character unigrams locating at positions i − 2, i − 1, i, i + 1 and i + 2 • Character bigrams locating at positions i − 2, i − 1, i and i + 1 • Whether xj and xj+1 are identical, for j = i − 2, . . .</S>
			<S sid ="97" ssid = "33">, i + 1 • Whether xj and xj+2 are identical, for j = i − 3, . . .</S>
			<S sid ="98" ssid = "34">, i + 1 The latter two feature templates are designed to detect character or word reduplication, a morphological phenomenon that can influence word segmentation in Chinese.</S>
			<S sid ="99" ssid = "35">3.2.2 High Dimensional Edge Features The node features discussed above are based on a single label yi . CRFs also have edge features the observation sequence (i.e., x).</S>
			<S sid ="100" ssid = "36">The major reason for this simple realization of edge features in traditional CRF implementation is for reducing the dimension of features.</S>
			<S sid ="101" ssid = "37">Otherwise, there can be an explosion of edge features in some tasks.</S>
			<S sid ="102" ssid = "38">For example, in part-of-speech tagging tasks, there can be more than 40 labels and more than 1,600 types of label transitions.</S>
			<S sid ="103" ssid = "39">Therefore, incorporating local observation information into the edge feature will result in an explosion of edge features, which is 1,600 times larger than the number of feature templates.</S>
			<S sid ="104" ssid = "40">Fortunately, for our task, the label set is quite small, Y = {B, I, E}1 . There are only nine possible label transitions: T = Y × Y and |T| = 9.2 As a result, the feature dimension will have nine times increase over the feature templates, if we incorporate local observation information of x into the edge features.</S>
			<S sid ="105" ssid = "41">In this way, we can effectively combine observation information of x with label transitions yi−1 yi . We simply used the same templates of node features for deriving the new edge features.</S>
			<S sid ="106" ssid = "42">We found adding new edge features significantly improves the disambiguation power of our model.</S>
	</SECTION>
	<SECTION title="Adaptive Online Gradient Descent based. " number = "4">
			<S sid ="107" ssid = "1">on Feature Frequency Information As we will show in experiments, the training of the CRF model with high-dimensional new features is quite expensive, and the existing training method is not good enough.</S>
			<S sid ="108" ssid = "2">To solve this issue, we propose a fast online training method: adaptive online gradient descent based on feature frequency information (ADF).</S>
			<S sid ="109" ssid = "3">The proposed method is easy to implement.</S>
			<S sid ="110" ssid = "4">For high convergence speed of online learning, we try to use more refined learning rates than the SGD training.</S>
			<S sid ="111" ssid = "5">Instead of using a single learning rate (a scalar) for all weights, we extend the learning rate scalar to a learning rate vector, which has the same dimension of the weight vector w. The learning rate vector is automatically adapted based on feature frequency information.</S>
			<S sid ="112" ssid = "6">By doing so, each weight that are based on label transitions.</S>
			<S sid ="113" ssid = "7">The second idea is to incorporate local observation information of x in edge features.</S>
			<S sid ="114" ssid = "8">For traditional implementation of CRF systems (e.g., the HCRF package), usually the edges features contain only the information of yi−1 and yi , and without the information of 1 B means beginning of a word, I means inside a word, and E means end of a word.</S>
			<S sid ="115" ssid = "9">The B, I, E labels have been widely used in previous work of Chinese word segmentation (Sun et al., 2009b).</S>
			<S sid ="116" ssid = "10">2 The operator × means a Cartesian product between two.</S>
			<S sid ="117" ssid = "11">sets.</S>
			<S sid ="118" ssid = "12">ADF learning algorithm 1: procedure ADF(q, c, α, β) 2: w ← 0, t ← 0, v ← 0, γ ← c 3: repeat until convergence 4: . Draw a sample zi at random 5: . v ← UPDATE(v, zi ) 6: . if t &gt; 0 and t mod q = 0 7: . . γ ← UPDATE(γ, v) 8: . . v ← 0 9: . g ← ∇w Lstoch (zi , w) 10: . w ← w + γ · g 11: . t ← t + 1 12: return w 13: 14: procedure UPDATE(v, zi ) 15: for k ∈ features used in sample zi 16: . vk ← vk + 1 17: return v 18: 19: procedure UPDATE(γ, v) 20: for k ∈ all features 21: . u ← vk /q 22: . η ← α − u(α − β) 23: . γk ← ηγk 24: return γ Figure 1: The proposed ADF online learning algorithm.</S>
			<S sid ="119" ssid = "13">q, c, α, and β are hyper-parameters.</S>
			<S sid ="120" ssid = "14">q is an integer representing window size.</S>
			<S sid ="121" ssid = "15">c is for initializing the learning rates.</S>
			<S sid ="122" ssid = "16">α and β are the upper and lower bounds of a scalar, with 0 &lt; β &lt; α &lt; 1.</S>
			<S sid ="123" ssid = "17">has its own learning rate, and we will show that this can significantly improve the convergence speed of online learning.</S>
			<S sid ="124" ssid = "18">In our proposed online learning method, the update formula is as follows: wt+1 = wt + γt · gt .</S>
			<S sid ="125" ssid = "19">(5) The update term gt is the gradient term of a randomly sampled instance: process.</S>
			<S sid ="126" ssid = "20">Our proposal is based on the intuition that a feature with higher frequency in the training process should be with a learning rate that decays faster.</S>
			<S sid ="127" ssid = "21">In other words, we assume a high frequency feature observed in the training process should have a small learning rate, and a low frequency feature should have a relatively larger learning rate in the training.</S>
			<S sid ="128" ssid = "22">Our assumption is based on the intuition that a weight with higher frequency is more adequately trained, hence smaller learning rate is preferable for fast convergence.</S>
			<S sid ="129" ssid = "23">Given a window size q (number of samples in a window), we use a vector v to record the feature frequency.</S>
			<S sid ="130" ssid = "24">The k’th entry vk corresponds to the frequency of the feature k in this window.</S>
			<S sid ="131" ssid = "25">Given a feature k, we use u to record the normalized frequency: u = vk /q. For each feature, an adaptation factor η is calculated based on the normalized frequency information, as follows: η = α − u(α − β), where α and β are the upper and lower bounds of a scalar, with 0 &lt; β &lt; α &lt; 1.</S>
			<S sid ="132" ssid = "26">As we can see, a feature with higher frequency corresponds to a smaller scalar via linear approximation.</S>
			<S sid ="133" ssid = "27">Finally, the learning rate is updated as follows: γk ← ηγk . With this setting, different features will correspond to different adaptation factors based on feature frequency information.</S>
			<S sid ="134" ssid = "28">Our ADF algorithm is summarized in Figure 1.</S>
			<S sid ="135" ssid = "29">The ADF training method is efficient, because the additional computation (compared with SGD) is only the derivation of the learning rates, which is simple and efficient.</S>
			<S sid ="136" ssid = "30">As we know, the regularization of SGD can perform efficiently via the optimization gt = ∇wt Lstoch (zi , wt ) = ∇wt ℓ(zi , wt )− || wt ||2 } based on sparse features (ShalevShwartz et al., 2007).</S>
			<S sid ="137" ssid = "31">Similarly, the derivation of γt can also In addition, γt ∈ Rf 2nσ2is a positive vector perform efficiently via the optimization based on sparse features.</S>
			<S sid ="138" ssid = "32">valued learning rate and · denotes component-wise (Hadamard) product of two vectors.</S>
			<S sid ="139" ssid = "33">We learn the learning rate vector γt based on feature frequency information in the updating 4.1 Convergence Analysis.</S>
			<S sid ="140" ssid = "34">Prior work on convergence analysis of existing online learning algorithms (Murata, 1998; Hsu et Da ta M et ho d Pa ss es Tr ain Ti m e (se c) N W D Re c Pr e R ec C W S F sc or e M S R Ba sel in e + Ne w fe at ur es + Ne w wo rd de tec tio n + A D F tra ini ng 5 0 5 0 5 0 1 0 4 . 7 e 3 1 . 2 e 4 1 . 2 e 4 2 . 3 e 3 7 2 . 6 7 5 . 3 7 8 . 2 7 7 . 5 96 .3 97 .2 97 .5 97 .6 95 .9 97 .0 96 .9 97 .2 9 6 . 1 9 7 . 1 9 7 . 2 9 7 . 4 C U Ba sel in e + Ne w fe at ur es + Ne w wo rd de tec tio n + A D F tra ini ng 5 0 5 0 5 0 1 0 2 . 9 e 3 7 . 5 e 3 7 . 5 e 3 1 . 5 e 3 6 8 . 5 6 8 . 0 6 8 . 8 6 8 . 8 94 .0 94 .4 94 .8 94 .8 93 .9 94 .5 94 .5 94 .7 9 3 . 9 9 4 . 4 9 4 . 7 9 4 . 8 P K U Ba sel in e + Ne w fe at ur es + Ne w wo rd de tec tio n + A D F tra ini ng 5 0 5 0 5 0 1 0 2 . 2 e 3 5 . 2 e 3 5 . 2 e 3 1 . 2 e 3 7 7 . 2 7 8 . 4 7 9 . 1 7 8 . 4 95 .0 95 .5 95 .8 95 .8 94 .0 94 .9 94 .9 94 .9 9 4 . 5 9 5 . 2 9 5 . 3 9 5 . 4 Table 2: Incremental evaluations, by incrementally adding new features (word features and high dimensional edge features), new word detection, and ADF training (replacing SGD training with ADF training).</S>
			<S sid ="141" ssid = "35">Number of passes is decided by empirical convergence of the training methods.</S>
			<S sid ="142" ssid = "36"># W . T . # W o r d # C .T . # C h a r M SR 8.8 × 10 4 2.4 × 10 6 5 × 10 3 4.1 × 10 6 C U 6.9 × 10 4 1.5 × 10 6 5 × 10 3 2.4 × 10 6 PK U 5.5 × 10 4 1.1 × 10 6 5 × 10 3 1.8 × 10 6 Table 1: Details of the datasets.</S>
			<S sid ="143" ssid = "37">W.T. represents word types; C.T. represents character types.</S>
			<S sid ="144" ssid = "38">al., 2009) can be extended to the proposed ADF training method.</S>
			<S sid ="145" ssid = "39">We can show that the proposed ADF learning algorithm has reasonable convergence properties.</S>
			<S sid ="146" ssid = "40">When we have the smallest learning rate γt+1 = βγt , the expectation of the obtained wt is t E(wt ) = w∗ + ∏ (I − γ0 βm H (w∗))(w0 − w∗), m=1 where w∗ is the optimal weight vector, and H is the Hessian matrix of the objective function.</S>
			<S sid ="147" ssid = "41">The rate of convergence is governed by the largest eigenvalue of</S>
	</SECTION>
	<SECTION title="Experiments. " number = "5">
			<S sid ="148" ssid = "1">5.1 Data and Metrics.</S>
			<S sid ="149" ssid = "2">We used benchmark datasets provided by the second International Chinese Word Segmentation Bakeoff to test our proposals.</S>
			<S sid ="150" ssid = "3">The datasets are from Microsoft Research Asia (MSR), City University of Hongkong (CU), and Peking University (PKU).</S>
			<S sid ="151" ssid = "4">Details of the corpora are listed in Table 1.</S>
			<S sid ="152" ssid = "5">We did not use any extra resources such as common surnames, parts-of-speech, and semantics.</S>
			<S sid ="153" ssid = "6">Four metrics were used to evaluate segmentation results: recall (R, the percentage of gold standard output words that are correctly segmented by the decoder), precision (P , the percentage of words in the decoder output that are segmented correctly), balanced F-score defined by 2P R/(P + R), and recall of new word detection (NWD recall).</S>
			<S sid ="154" ssid = "7">For more detailed information on the corpora, refer to Emerson (2005).</S>
			<S sid ="155" ssid = "8">the function C t = ∏t (I − γ0 βm H (w∗)).</S>
			<S sid ="156" ssid = "9">Then, 5.2 Features, Training,.</S>
			<S sid ="157" ssid = "10">and Tuning we can derive a bound of rate of convergence.</S>
			<S sid ="158" ssid = "11">Theorem 1 Assume ϕ is the largest eigenvalue of We employed the feature templates defined in the function C t = ∏t (I − γ0 βm H (w∗)).</S>
			<S sid ="159" ssid = "12">For Section 3.2.</S>
			<S sid ="160" ssid = "13">The feature sets are huge.</S>
			<S sid ="161" ssid = "14">There are 2.4 × 107 features for the MSR data, 4.1 × 107 the proposed ADF training, its convergence rate is bounded by ϕ, and we have ϕ ≤ exp { γ0 λβ }, β − 1 where λ is the minimum eigenvalue of H (w∗).</S>
			<S sid ="162" ssid = "15">features for the CU data, and 4.7 × 107 features for the PKU data.</S>
			<S sid ="163" ssid = "16">To generate word-based features, we extracted high-frequency word-based unigram and bigram lists from the training data.</S>
			<S sid ="164" ssid = "17">As for training, we performed gradient descent 97.5 97 96.5 96 95.5 MSR ADF SGD LBFGS (batch) CU 9 5 9 4 . 5 9 4 9 3 . 5 9 3 9 2 . 5 95.5 95 94.5 PKU 95 0 10 20 30 40 50 Number of Passes 92 0 10 20 30 40 50 Number of Passes 94 0 10 20 30 40 50 Number of Passes 97.5 97 96.5 96 95.5 MSR ADF SGD LBFGS (batch) CU 9 5 9 4 . 5 9 4 9 3 . 5 9 3 9 2 . 5 95.5 95 94.5 PKU 95 92 94 Figure 2: F-score curves on the MSR, CU, and PKU datasets: ADF learning vs. SGD and LBFGS training methods.</S>
			<S sid ="165" ssid = "18">with our proposed training method.</S>
			<S sid ="166" ssid = "19">To compare with existing methods, we chose two popular training methods, a batch training one and an online training one.</S>
			<S sid ="167" ssid = "20">The batch training method is the Limited-Memory BFGS (LBFGS) method (Nocedal and Wright, 1999).</S>
			<S sid ="168" ssid = "21">The online baseline training method is the SGD method, which we have introduced in Section 2.2.</S>
			<S sid ="169" ssid = "22">For the ADF training method, we need to tune the hyper-parameters q, c, α, and β.</S>
			<S sid ="170" ssid = "23">Based on automatic tuning within the training data (validation in the training data), we found it is proper to set q = n/10 (n is the number of training samples), c = 0.1, α = 0.995, and β = 0.6.</S>
			<S sid ="171" ssid = "24">To reduce overfitting, we employed an L2 Gaussian weight prior (Chen and Rosenfeld, 1999) for all training methods.</S>
			<S sid ="172" ssid = "25">We varied the σ with different values (e.g., 1.0, 2.0, and 5.0), and finally set the value to 1.0 for all training methods.</S>
			<S sid ="173" ssid = "26">5.3 Results and Discussion.</S>
			<S sid ="174" ssid = "27">First, we performed incremental evaluation in this order: Baseline (word segmentation model with SGD training); Baseline + New features; Baseline + New features + New word detection; Baseline + New features + New word detection + ADF training (replacing SGD training).</S>
			<S sid ="175" ssid = "28">The results are shown in Table 2.</S>
			<S sid ="176" ssid = "29">As we can see, the new features improved performance on both word segmentation and new word detection.</S>
			<S sid ="177" ssid = "30">However, we also noticed that the training cost became more expensive via adding high dimensional new features.</S>
			<S sid ="178" ssid = "31">Adding new word detection function further improved the segmentation quality and the new word recognition recall.</S>
			<S sid ="179" ssid = "32">Finally, by using the ADF training method, the training speed is much faster than the SGD training method.</S>
			<S sid ="180" ssid = "33">The ADF method can achieve empirical optimum in only a few passes, yet with better segmentation accuracies than the SGD training with 50 passes.</S>
			<S sid ="181" ssid = "34">To get more details of the proposed training method, we compared it with SGD and LBFGS training methods based on an identical platform, by varying the number of passes.</S>
			<S sid ="182" ssid = "35">The comparison was based on the same platform: Baseline + New features + New word detection.</S>
			<S sid ="183" ssid = "36">The F-score curves of the training methods are shown in Figure 2.</S>
			<S sid ="184" ssid = "37">Impressively, the ADF training method reached empirical convergence in only a few passes, while the SGD and LBFGS training converged much slower, requiring more than 50 passes.</S>
			<S sid ="185" ssid = "38">The ADF training is about an order magnitude faster than the SGD online training and more than an order magnitude faster than the LBFGS batch training.Finally, we compared our method with the state Da ta M et ho d Pr ob . Pr e R ecF sc or e M S R Be st0 5 (T se ng et al. , 20 05 ) C R F + rule sy ste m (Z ha ng et al. , 20 06 ) S e mi M ar k o v p er c e pt r o n ( Z h a n g a n d C la r k, 2 0 0 7 ) S e mi M ar k o v C R F ( G a o et al ., 2 0 0 7 ) La tent va ria bl e C R F (S un et al. , 20 09 b) O ur m et ho d (A Si ng le C R F) √ √ × √ √ √ 96 .2 97 .2 N / A N / A 97 .3 97 .6 96 .6 96 .9 N / A N / A 97 .3 97 .2 9 6 . 4 9 7 . 1 9 7 . 2 9 7 . 2 9 7 . 3 9 7 . 4 C U Be st0 5 (T se ng et al. , 20 05 ) C R F + r u l e s y s t e m ( Z h a n g e t a l . , 2 0 0 6 ) S e m i p e r c e p t r o n ( Z h a n g a n d C l a r k , 2 0 0 7 ) L a t e n t v a r i a b l e C R F ( S u n e t a l . , 2 0 0 9 b ) O u r m e t h o d ( A S i n g l e C R F ) √ √ × √ √ 94 .1 95 .2 N/ A 94 .7 94 .8 94 .6 94 .9 N/ A 94 .4 94 .7 9 4 . 3 9 5 . 1 9 5 . 1 9 4 . 6 9 4 . 8 P K U Be st0 5 (C he n et al. , 20 05 ) C R F + r u l e s y s t e m ( Z h a n g e t a l . , 2 0 0 6 ) s e m i p e r c e p t r o n ( Z h a n g a n d C l a r k , 2 0 0 7 ) L a t e n t v a r i a b l e C R F ( S u n e t a l . , 2 0 0 9 b ) O u r m e t h o d ( A S i n g l e C R F ) N / A √ × √ √ 95 .3 94 .7 N/ A 95 .6 95 .8 94 .6 95 .5 N/ A 94 .8 94 .9 9 5 . 0 9 5 . 1 9 4 . 5 9 5 . 2 9 5 . 4 Table 3: Comparing our method with the state-of-the-art CWS systems.</S>
			<S sid ="186" ssid = "39">of-the-art systems reported in the previous papers.</S>
			<S sid ="187" ssid = "40">The statistics are listed in Table 3.</S>
			<S sid ="188" ssid = "41">Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; CRF + rule-system represents confidence- based combination of CRF and rule-based models, presented in Zhang et al.</S>
			<S sid ="189" ssid = "42">(2006).</S>
			<S sid ="190" ssid = "43">Prob.</S>
			<S sid ="191" ssid = "44">indicates whether or not the system can provide probabilistic information.</S>
			<S sid ="192" ssid = "45">As we can see, our method achieved similar or even higher F-scores, compared with the best systems reported in previous papers.</S>
			<S sid ="193" ssid = "46">Note that, our system is a single Markov model, while most of the state-of-the-art systems are complicated heavy systems, with model-combinations (e.g., voting of multiple segmenters), semi-Markov relaxations, or latent-variables.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "6">
			<S sid ="194" ssid = "1">In this paper, we presented a joint model for Chinese word segmentation and new word detection.</S>
			<S sid ="195" ssid = "2">We presented new features, including word-based features and enriched edge features, for the joint modeling.</S>
			<S sid ="196" ssid = "3">We showed that the new features can improve the performance on the two tasks.</S>
			<S sid ="197" ssid = "4">On the other hand, the training of the model, especially with high-dimensional new features, became quite expensive.</S>
			<S sid ="198" ssid = "5">To solve this problem, we proposed a new training method, ADF training, for very fast training of CRFs, even given large- scale datasets with high dimensional features.</S>
			<S sid ="199" ssid = "6">We performed experiments and showed that our new training method is an order magnitude faster than existing optimization methods.</S>
			<S sid ="200" ssid = "7">Our final system can learn highly accurate models with only a few passes in training.</S>
			<S sid ="201" ssid = "8">The proposed fast learning method is a general algorithm that is not limited in this specific task.</S>
			<S sid ="202" ssid = "9">As future work, we plan to apply this fast learning method on other large-scale natural language processing tasks.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="203" ssid = "10">We thank Yaozhong Zhang and Weiwei Sun for helpful discussions on word segmentation techniques.</S>
			<S sid ="204" ssid = "11">The work described in this paper was supported by a Hong Kong RGC Project (No. PolyU 5230/08E), National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101), and National Natural Science Foundation of China (No.91024009, No.60973053).</S>
	</SECTION>
</PAPER>
