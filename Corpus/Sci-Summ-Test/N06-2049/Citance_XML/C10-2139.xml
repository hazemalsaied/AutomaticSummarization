<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a theoretical and empirical comparative analysis of the two dominant categories of approaches in Chinese word segmentation: word-based models and character-based models.</S>
		<S sid ="2" ssid = "2">We show that, in spite of similar performance overall, the two models produce different distribution of segmentation errors, in a way that can be explained by theoretical properties of the two models.</S>
		<S sid ="3" ssid = "3">The analysis is further exploited to improve segmentation accuracy by integrating a word-based segmenter and a character-based segmenter.</S>
		<S sid ="4" ssid = "4">A Bootstrap Aggregating model is proposed.</S>
		<S sid ="5" ssid = "5">By letting multiple segmenters vote, our model improves segmentation consistently on the four different data sets from the second SIGHAN bakeoff.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing.</S>
			<S sid ="7" ssid = "7">There are two dominant models for Chinese word segmentation.</S>
			<S sid ="8" ssid = "8">The first one is what we call “word-based” approach, where the basic predicting units are words themselves.</S>
			<S sid ="9" ssid = "9">This kind of segmenters sequentially decides whether the local sequence of characters make up a word.</S>
			<S sid ="10" ssid = "10">This word-by-word approach ranges from naive maximum matching (Chen and Liu, 1992) to complex solution based on semi-Markov conditional random fields (CRF) (Andrew, 2006).</S>
			<S sid ="11" ssid = "11">The second is “character-based” approach, where basic processing units are characters which compose words.</S>
			<S sid ="12" ssid = "12">Segmentation is formulated as a classification problem to predict whether a character locates at the beginning of, inside or at the end of a word.</S>
			<S sid ="13" ssid = "13">This character- by-character method was first proposed in (Xue, 2003), and a number of sequence labeling algorithms have been exploited.</S>
			<S sid ="14" ssid = "14">This paper is concerned with the behavior of different segmentation models in general.</S>
			<S sid ="15" ssid = "15">We present a theoretical and empirical comparative analysis of the two dominant approaches.</S>
			<S sid ="16" ssid = "16">Theoretically, these approaches are different.</S>
			<S sid ="17" ssid = "17">The word-based models do prediction on a dynamic sequence of possible words, while character- based models on a static character sequence.</S>
			<S sid ="18" ssid = "18">The former models have a stronger ability to represent word token features for disambiguation, while the latter models can better induce a word from its internal structure.</S>
			<S sid ="19" ssid = "19">For empirical analysis, we implement two segmenters, both using the Passive- Aggressive algorithm (Crammer et al., 2006) to estimate parameters.</S>
			<S sid ="20" ssid = "20">Our experiments indicate that despite similar performance in terms of overall F-score, the two models produce different types of errors, in a way that can be explained by theoretical properties.</S>
			<S sid ="21" ssid = "21">We will present a detailed analysis that reveals important differences of the two methods in Sec.</S>
			<S sid ="22" ssid = "22">4.</S>
			<S sid ="23" ssid = "23">The two types of approaches exhibit different behaviors, and each segmentation model has strengths and weaknesses.</S>
			<S sid ="24" ssid = "24">We further consider integrating word-based and character-based models in order to exploit their complementary strengths and thereby improve segmentation accuracy beyond what is possible by either model in isolation.</S>
			<S sid ="25" ssid = "25">We present a Bootstrap Aggregating model to combine multiple segmentation systems.</S>
			<S sid ="26" ssid = "26">By 1211 Coling 2010: Poster Volume, pages 1211–1219, Beijing, August 2010 letting multiple segmenters vote, our combination model improves accuracy consistently on all the four different segmentation data sets from the second SIGHAN bakeoff.</S>
			<S sid ="27" ssid = "27">We also compare our integrating system to the state-of-the-art segmentation and a function GEN that enumerates a set of segmentation candidates GEN(c) for c. In general, a segmenter solves the following “argmax” problem: systems.</S>
			<S sid ="28" ssid = "28">Our system obtains the highest reported F-scores on three data sets.</S>
	</SECTION>
	<SECTION title="Two Methods for Word Segmentation. " number = "2">
			<S sid ="29" ssid = "1">First of all, we distinguish two kinds of “words”: wˆ = arg max w∈GEN(c) = arg max w∈GEN(c) θ∗Φ(c, w) (1) |w| θ∗ ) φ(c, w[1:i] ) (2) i=1 (1) Words in dictionary are word types; (2) Words in sentences are word tokens.</S>
			<S sid ="30" ssid = "2">The goal of word segmentation is to identify word tokens in a running text, where a large dictionary (i.e. list of word types) and annotated corpora may be available.</S>
			<S sid ="31" ssid = "3">From the view of token, we divide segmentation models into two main categories: word- based models and character-based models.</S>
			<S sid ="32" ssid = "4">There are two key points of a segmentation model: (1) How to decide whether a local sequence of characters is a word?</S>
			<S sid ="33" ssid = "5">(2) How to do disambiguation if ambiguous segmentation occurs?</S>
			<S sid ="34" ssid = "6">For each model, we separately discuss the strategies for word prediction and segmentation disambiguation.</S>
			<S sid ="35" ssid = "7">2.1 Word-Based Approach.</S>
			<S sid ="36" ssid = "8">It may be the most natural idea for segmentation to find word tokens one by one.</S>
			<S sid ="37" ssid = "9">This kind of segmenters read the input sentences from left to right, predict whether current piece of continuous characters is a word token.</S>
			<S sid ="38" ssid = "10">After one word is found, segmenters move on and search for next possible word.</S>
			<S sid ="39" ssid = "11">There are different strategies for the word prediction and disambiguation problems.</S>
			<S sid ="40" ssid = "12">Take for example maximum matching, which was a popular algorithm at the early stage of research (Chen and Liu, 1992).</S>
			<S sid ="41" ssid = "13">For word prediction, if a sequence of characters appears in a dictionary, it is taken as a word candidate.</S>
			<S sid ="42" ssid = "14">For segmentation disambiguation, if more than one word types are matched, the algorithm chooses the longest one.In the last several years, machine learning techniques are employed to improve word-based seg where Φ and φ are global and local feature maps and θ is the parameter vector to learn.</S>
			<S sid ="43" ssid = "15">The inner product θ∗φ(c, w[1:i] ) can been seen as the con fidence score of whether wi is a word.</S>
			<S sid ="44" ssid = "16">The disambiguation takes into account confidence score of each word, by using the sum of local scores as its criteria.</S>
			<S sid ="45" ssid = "17">Markov assumption is necessary for computation, so φ is usually defined on a limited history.</S>
			<S sid ="46" ssid = "18">Perceptron and semi-Markov CRFs were used to estimate θ in previous work (Zhang and Clark, 2007; Andrew, 2006).</S>
			<S sid ="47" ssid = "19">2.2 Character-Based Approach.</S>
			<S sid ="48" ssid = "20">Most previous data-driven segmentation solutions took an alternative, character-based view.</S>
			<S sid ="49" ssid = "21">This approach observes that by classifying characters as different positions in words, segmentation can be treated as a sequence labeling problem, assigning labels to the characters in a sentence indicating whether a character ci is a single character word (S) or the begin (B), middle (I) or end (E) of a multi-character word.</S>
			<S sid ="50" ssid = "22">For word prediction, word tokens are inferred based on the character classes.</S>
			<S sid ="51" ssid = "23">The main difficulty of this model is character ambiguity that most Chinese characters can occur in different positions within different words.</S>
			<S sid ="52" ssid = "24">Linear models are also popular for character disambiguation (i.e. segmentation disambiguation).</S>
			<S sid ="53" ssid = "25">Denote a sequence of character labels y ∈ Yn, a linear model is defined as: yˆ = arg max θ∗Ψ(c, y) (3) y∈Y |c| mentation, where the above two problems are |c| = arg max θ∗ ) ψ(c, y[1:i] ) (4) solved in a uniform model.</S>
			<S sid ="54" ssid = "26">Given a sequence of n y∈Y |c| i=1 characters c ∈C (n is the number of characters), m denote a segmented sequence of words w ∈W (m is the number of words, i.e. m varies with w), Note that local feature map ψ is defined only on the sequence of characters and their labels.</S>
			<S sid ="55" ssid = "27">Several discriminative models have been exploited for parameter estimation, including per- ceptron, CRFs, and discriminative latent variable CRFs (Jiang et al., 2009; Tseng, 2005; Sun et al., 2009b).</S>
			<S sid ="56" ssid = "28">2.3 Theoretical Comparison.</S>
			<S sid ="57" ssid = "29">Theoretically, the two types of models are different.</S>
			<S sid ="58" ssid = "30">We compare them from four aspects.</S>
			<S sid ="59" ssid = "31">2.3.1 Internal Structure of Words Chinese words have internal structures.</S>
			<S sid ="60" ssid = "32">In most cases, Chinese character is a morpheme which is the smallest meaningful unit of the language.</S>
			<S sid ="61" ssid = "33">Though we cannot exactly infer the meaning of a word from its character components, the character structure is still meaningful.</S>
			<S sid ="62" ssid = "34">Partially characterizing the internal structures of words, one advantage of character-based models is the ability to induce new words.</S>
			<S sid ="63" ssid = "35">E.g., character “i!f/person” is usually used as a suffix meaning “one kind of people”.</S>
			<S sid ="64" ssid = "36">If a segmenter never sees “Ifi!f/worker” in training data, it may still rightly recognize this word by analyzing the prefix “If/work” with label BI and the suffix “i!f” with label E. In contrast, current word-based models only utilize the weighted features as word prediction criteria, and thus word formation information is not well explored.</S>
			<S sid ="65" ssid = "37">For more details about Chinese word fomation, see (Sun et al., 2009a).</S>
			<S sid ="66" ssid = "38">2.3.2 Linearity and Nonlinearity A majority of structured prediction models are linear models in the sense that the score functions are linear combination of parameters.</S>
			<S sid ="67" ssid = "39">Both previous solutions for word-based and character- based systems utilize linear models.</S>
			<S sid ="68" ssid = "40">However, both “linear” models incur nonlinearity to some extent.</S>
			<S sid ="69" ssid = "41">In general, a sequence classification itself involves nonlinearity in a way that the features of current token usually encode previous state information which is linear combination of features of previous tokens.</S>
			<S sid ="70" ssid = "42">The interested readers may consult (Liang et al., 2008) for preliminary discussion about the nonlinearity in structured models.</S>
			<S sid ="71" ssid = "43">This kind of nonlinearity exists in both word- based and character-based models.</S>
			<S sid ="72" ssid = "44">In addition, in most character-based models, a word should take a S label or start with a B label, end with E label, and only have I label inside.</S>
			<S sid ="73" ssid = "45">This inductive way for word prediction actually behaves nonlinearly.</S>
			<S sid ="74" ssid = "46">2.3.3 Dynamic Tokens or Static Tokens Since word-based models take the sum of part score of each individual word token, it increases the upper bound of the whole score to segment more words.</S>
			<S sid ="75" ssid = "47">As a result, word-based segmenter tends to segment words into smaller pieces.</S>
			<S sid ="76" ssid = "48">A difficult case occurs when a word token w consists of some word types which could be separated as words on their own.</S>
			<S sid ="77" ssid = "49">In such cases a word-based segmenter more easily splits the word into individual words.</S>
			<S sid ="78" ssid = "50">For example, in the phrase “l!lf =:8/4300 7\(/meter (4300 meters)”, the numeral “l!l f- =: 8” consists of two individual numeral types “l!lf- (4000)” and “=:8(300)”.</S>
			<S sid ="79" ssid = "51">A word- based segmenter more easily made a mistake to segment two word tokens.</S>
			<S sid ="80" ssid = "52">This phenomenon is very common in named entities.</S>
			<S sid ="81" ssid = "53">2.3.4 Word Token or Word Type Features In character-based models, features are usually defined by the character information in the neighboring n-character window.</S>
			<S sid ="82" ssid = "54">Despite a large set of valuable features that could be expressed, it is slightly less natural to encode predicted word token information.</S>
			<S sid ="83" ssid = "55">On the contrary, taking words as dynamic tokens, it is very easy to define word token features in a word-based model.</S>
			<S sid ="84" ssid = "56">Word- based segmenters hence have greater representational power.</S>
			<S sid ="85" ssid = "57">Despite of the lack of word token representation ability, character-based segmenters can use word type features by looking up a dictionary.</S>
			<S sid ="86" ssid = "58">For example, if a local sequence of characters following current token matches a word in a dictionary; these word types can be used as features.</S>
			<S sid ="87" ssid = "59">If a string matches a word type, it has a very high probability (ca.</S>
			<S sid ="88" ssid = "60">90%) to be a word token.</S>
			<S sid ="89" ssid = "61">So word type features are good approximation of word token features.</S>
	</SECTION>
	<SECTION title="Baseline Systems. " number = "3">
			<S sid ="90" ssid = "1">For empirical analysis, we implement segmenters in word-based and character-based architectures respectively.</S>
			<S sid ="91" ssid = "2">We introduce them from three aspects: basic models, parameter estimation and feature selection.</S>
			<S sid ="92" ssid = "3">Algorithm 1: The PA learning procedure.</S>
			<S sid ="93" ssid = "4">input : Data {(xt , yt),t = 1, 2, ..., n} 1 Initialize: w ← (0, ..., 0).</S>
			<S sid ="94" ssid = "5">2 for I = 1, 2, ... do 3 for t = 1, ..., n do</S>
	</SECTION>
	<SECTION title="Predict: y∗ =. " number = "4">
			<S sid ="95" ssid = "1">arg maxy∈GEN(xt ) w∗Φ(xt, y)</S>
	</SECTION>
	<SECTION title="Suffer loss: lt  = ρ(yt, y∗ )+. " number = "5">
			<S sid ="96" ssid = "1">w∗Φ(xt, y∗ ) − w∗Φ(xt, yt) learner processes all the instances (t is from 1 to n) in each iteration (I ).</S>
			<S sid ="97" ssid = "2">If current hypothesis (w) fails to predict xt, the learner update w through calculating the loss lt and the difference between Φ(xt, yt ) and Φ(xt, yt) (line 57).</S>
			<S sid ="98" ssid = "3">There are three variants in the update step.</S>
			<S sid ="99" ssid = "4">We here only present the PA-II rule1 , which performs best in our experiments.</S>
			<S sid ="100" ssid = "5">The PA algorithm utilizes a paradigm of cost- sensitive learning to resolve structured prediction.</S>
	</SECTION>
	<SECTION title="Set: τt. " number = "6">
			<S sid ="101" ssid = "1">= lt 2</S>
	</SECTION>
	<SECTION title="Update:. " number = "7">
			<S sid ="102" ssid = "1">t )−Φ(xt ,yt )|| +0.5C A cost function ρ is necessary to calculate the loss lt (line 5).</S>
			<S sid ="103" ssid = "2">For every pair of labels (y∗, y), users w ← w + τt(Φ(xt, yt ) − Φ(xt, y∗)) 8 end 9 end 3.1 Models.</S>
			<S sid ="104" ssid = "3">For both word-based and character-based segmenters, we use linear models introduced in the section above.</S>
			<S sid ="105" ssid = "4">We use a first order Markov models for training and testing.</S>
			<S sid ="106" ssid = "5">In particular, for word-based segmenter, the local feature map φ(c, w[1:i] ) is defined only on c, wi−1 and wi, and thereby Eq. 2 is defined as wˆ = arg max θ∗ I:|w| φ(c, wi 1 , w ).</S>
			<S sid ="107" ssid = "6">This w∈GEN(c) i=1 − i model has a first-order Semi-Markov structure.</S>
			<S sid ="108" ssid = "7">For decoding, Zhang and Clark (2007) used a beam search algorithm to get approximate solutions, and Sarawagi and Cohen (2004) introduced a Viterbi style algorithm for exact inference.</S>
			<S sid ="109" ssid = "8">Since the exact inference algorithm is efficient enough, we use this algorithm in our segmenter at both training and testing time.</S>
			<S sid ="110" ssid = "9">For our character-based segmenter, the local feature map ψ(c, y[1:i] ) is defined on c, yi−1 and yi, and Eq. 4 is defined as yˆ = arg max θ∗ I:|c| ψ(θ, yi 1, y ).</S>
			<S sid ="111" ssid = "10">In our y∈Y |c| i=1 − i character-based segmenter, we also use a Viterbi algorithm for decoding.</S>
			<S sid ="112" ssid = "11">3.2 Learning.</S>
			<S sid ="113" ssid = "12">We adopt Passive-Aggressive (PA) framework (Crammer et al., 2006), a family of margin based online learning algorithms, for the parameter estimation.</S>
			<S sid ="114" ssid = "13">It is fast and easy to implement.</S>
			<S sid ="115" ssid = "14">Alg.</S>
			<S sid ="116" ssid = "15">1 illustrates the learning procedure.</S>
			<S sid ="117" ssid = "16">The parameter vector w is initialized to (0, ..., 0).</S>
			<S sid ="118" ssid = "17">A PA should define a cost ρ(y∗, y) associated with predicting y∗ when the correct label is y. ρ should be defined differently for different purposes.</S>
			<S sid ="119" ssid = "18">There are two natural costs for segmentation: (1) sum of the number of wrong and missed word predictions and (2) sum of the number of wrongly classified characters.</S>
			<S sid ="120" ssid = "19">We tried both cost functions for both models.</S>
			<S sid ="121" ssid = "20">We find that the first one is suitable for word-based segmenter and the second one is suitable for character-based segmenter.</S>
			<S sid ="122" ssid = "21">We do not report segmentation performance with “weaker” cost in later sections.</S>
			<S sid ="123" ssid = "22">C (in line 6) is the slack variable.</S>
			<S sid ="124" ssid = "23">In our experiments, the segmentation performance is not sensitive to C . In the following experiments, we set C = 1.</S>
			<S sid ="125" ssid = "24">3.3 Features.</S>
			<S sid ="126" ssid = "25">3.3.1 Word-based Segmenter For the convenience of illustration, we denote a candidate word token wi with a context cj−1[wi−1 cj ...ck ][wi ck+1...cl ]cl+1.</S>
			<S sid ="127" ssid = "26">The character features includes, Boundary character unigram: cj , ck , ck+1, cl and cl+1; Boundary character bigram: ck ck+1 and cl cl+1.</S>
			<S sid ="128" ssid = "27">Inside character unigram: cs (k +1 &lt; s &lt; l); Inside character bigram: cscs+1 (k +1 &lt;s&lt; l).</S>
			<S sid ="129" ssid = "28">Length of current word.</S>
			<S sid ="130" ssid = "29">Whether ck+1 and ck+1 are identical.</S>
			<S sid ="131" ssid = "30">Combination Features: ck+1 and cl , The word token features includes, Word Unigram: previous word wi−1 and current word wi; Word Bigram: wi−1wi . 1 See the original paper for more details..</S>
			<S sid ="132" ssid = "31">The identity of wi, if it is a Single character word.</S>
			<S sid ="133" ssid = "32">Combination Features: wi−1 and length of wi, wi and length of wi−1.</S>
			<S sid ="134" ssid = "33">ck+1 and length of wi, cl and length of wi . 3.3.2 Character-based Segmenter We use the exact same feature templates dis- cribed in (Sun et al., 2009b).</S>
			<S sid ="135" ssid = "34">The features are divided into two types: character features and word type features.</S>
			<S sid ="136" ssid = "35">Note that the word type features are indicator functions that fire when the local character sequence matches a word unigram or bigram.</S>
			<S sid ="137" ssid = "36">Dictionaries containing word unigrams and bigrams was collected from the training data.</S>
			<S sid ="138" ssid = "37">Limited to the document length, we do not give M o d el P( %) R( %) F AS Ch ara cter 94.</S>
			<S sid ="139" ssid = "38">8 94.</S>
			<S sid ="140" ssid = "39">7 94.</S>
			<S sid ="141" ssid = "40">7 W o r d 93.</S>
			<S sid ="142" ssid = "41">5 94.</S>
			<S sid ="143" ssid = "42">8 94.</S>
			<S sid ="144" ssid = "43">2 CU Ch ara cter 95.</S>
			<S sid ="145" ssid = "44">5 94.</S>
			<S sid ="146" ssid = "45">6 95.</S>
			<S sid ="147" ssid = "46">0 W o r d 94.</S>
			<S sid ="148" ssid = "47">4 94.</S>
			<S sid ="149" ssid = "48">7 94.</S>
			<S sid ="150" ssid = "49">6 MS R Ch ara cter 96.</S>
			<S sid ="151" ssid = "50">1 96.</S>
			<S sid ="152" ssid = "51">5 96.</S>
			<S sid ="153" ssid = "52">3 W o r d 96.</S>
			<S sid ="154" ssid = "53">0 96.</S>
			<S sid ="155" ssid = "54">3 96.</S>
			<S sid ="156" ssid = "55">1 PK U Ch ara cter 94.</S>
			<S sid ="157" ssid = "56">6 94.</S>
			<S sid ="158" ssid = "57">9 94.</S>
			<S sid ="159" ssid = "58">8 W o r d 94.</S>
			<S sid ="160" ssid = "59">7 94.</S>
			<S sid ="161" ssid = "60">3 94.</S>
			<S sid ="162" ssid = "61">5 Table 1: Baseline performance.</S>
			<S sid ="163" ssid = "62">think the main reason is that we use a different learning architecture.</S>
			<S sid ="164" ssid = "63">4.3 Word Frequency Factors.</S>
			<S sid ="165" ssid = "64">the discription for the features.</S>
			<S sid ="166" ssid = "65">We suggest readers to refer to the original paper for details.</S>
			<S sid ="167" ssid = "66">4 Empirical Analysis.</S>
			<S sid ="168" ssid = "67">We present a series of experiments that relate segmentation performance to a set of properties of input words.</S>
			<S sid ="169" ssid = "68">We argue that the results can be correlated to specific theoretical aspects of each model.</S>
			<S sid ="170" ssid = "69">4.1 Experimental Setting.</S>
			<S sid ="171" ssid = "70">We used the data provided by the second SIGHAN Bakeoff (Emerson, 2005) to test the two segmentation models.</S>
			<S sid ="172" ssid = "71">The data contains four corpora 100 95 90 85 80 75 70 65 60 100 95 90 85 80 75 70 AS data set character-based word-based word occurances in training data CU data set character-based word-based word occurances in training data 100 95 90 85 80 75 70 65 60 100 95 90 85 80 75 70 65 60 MSR data set character-based word-based word occurances in training data PKU data set character-based word-based word occurances in training data from different sources: Academia Sinica Corpus (AS), City University of Hong Kong (CU), Microsoft Research Asia (MSR), and Peking University (PKU).</S>
			<S sid ="173" ssid = "72">There is no fixed standard for Chinese word segmentation.</S>
			<S sid ="174" ssid = "73">The four data sets above are annotated with different standards.</S>
			<S sid ="175" ssid = "74">To catch general properties, we do experiments on all the four data sets.</S>
			<S sid ="176" ssid = "75">Three metrics were used for evaluation: precision (P), recall (R) and balanced F-score (F) defined by 2PR/(P+R).</S>
			<S sid ="177" ssid = "76">4.2 Baseline Performance.</S>
			<S sid ="178" ssid = "77">Tab.</S>
			<S sid ="179" ssid = "78">1 shows the performance of our two segmenters.</S>
			<S sid ="180" ssid = "79">Numbers of iterations are respectively set to 15 and 20 for our word-based segmenter and character-based segmenter.</S>
			<S sid ="181" ssid = "80">The word-based segmenter performs slightly worse than the character- based segmenter.</S>
			<S sid ="182" ssid = "81">This is different from the experiments reported in (Zhang and Clark, 2007).</S>
			<S sid ="183" ssid = "82">We Figure 1: Segmentation recall relative to gold word frequency.</S>
			<S sid ="184" ssid = "83">Our theoretical analysis also suggests that character-based has stronger word induction ability because it focuses more on word internal structures and thereby expresses more nonlinearity.</S>
			<S sid ="185" ssid = "84">To test the word induction ability, we present the recall relative to word frequency.</S>
			<S sid ="186" ssid = "85">If a word appears in a training data many times, the learner usually works in a “memorizing” way.</S>
			<S sid ="187" ssid = "86">On the contrary, infrequent words should be correctly recognized in a somehow “inductive” way.</S>
			<S sid ="188" ssid = "87">Fig.</S>
			<S sid ="189" ssid = "88">1 shows the recall change relative to word frequency in each training data.</S>
			<S sid ="190" ssid = "89">Note that, the words with frequency 0 are out-of-vocabulary (OOV) words.</S>
			<S sid ="191" ssid = "90">We can clearly see that character-based model outperforms word-based model for infrequent word, especially OOV words, recognition.</S>
			<S sid ="192" ssid = "91">The “memoriz AS data set 98 96 character-based 94 word-based 92 90 88 86 84 82 80 78 76 1 2 3 4 word length MSR data set 98 97 character-based 96 word-based 95 94 93 92 91 90 89 88 1 2 3 4 word length AS data set 98 96 character-based 94 word-based 92 90 88 86 84 82 80 78 76 1 2 3 4 word length MSR data set 98 97 character-based 96 word-based 95 94 93 92 91 90 89 88 1 2 3 4 word length CU data set 98 character-based 96 word-based 94 92 90 88 86 84 1 2 3 4 word length PKU data set 98 96 character-based 94 word-based 92 90 88 86 84 82 80 78 1 2 3 4 word length CU data set 98 96 character-based 94 word-based 92 90 88 86 84 82 80 78 1 2 3 4 word length PKU data set 98 character-based 96 word-based 94 92 90 88 86 84 1 2 3 4 word length Figure 2: Segmentation precision/recall relative to gold word length in training data.</S>
			<S sid ="193" ssid = "92">ing” ability of the two models is similar; on the AS and CU data sets, the word-based model performs slightly better.</S>
			<S sid ="194" ssid = "93">Neither model is robust enough to reliably segment unfamiliar words.</S>
			<S sid ="195" ssid = "94">The recall of OOV words is much lower than in-vocabulary words.</S>
			<S sid ="196" ssid = "95">4.4 Length Factors.</S>
			<S sid ="197" ssid = "96">Table 2: Word length statistics on test sets.</S>
			<S sid ="198" ssid = "97">Tab.</S>
			<S sid ="199" ssid = "98">2 shows the statistics of word counts score varies with the length |w|.</S>
			<S sid ="200" ssid = "99">If a segmentation result is with more fragments, i.e. |w| is larger, the upper bound of its score is higher.</S>
			<S sid ="201" ssid = "100">As a result, in many cases, a word-based segmenter prefers shorter words, which may cause errors.</S>
			<S sid ="202" ssid = "101">4.5 Feature Factors.</S>
			<S sid ="203" ssid = "102">We would like to measure the effect of features empirically.</S>
			<S sid ="204" ssid = "103">In particular, we do not use dynamic word token features in our word-based segmenter, and word type features in our character- based segmenter as comparison with “standard” segmenters.</S>
			<S sid ="205" ssid = "104">The difference in performance can be seen as the contribution of word features.</S>
			<S sid ="206" ssid = "105">There are obvious drops in both cases.</S>
			<S sid ="207" ssid = "106">Though it is not a fair comparison, word token features seem more important, since the numerical decrease in the word-based experiment is larger.</S>
			<S sid ="208" ssid = "107">relative to word length on each test data sets.</S>
			<S sid ="209" ssid = "108">There are much less words with length more than 4.</S>
			<S sid ="210" ssid = "109">Analysis on long words may not be statis-.</S>
			<S sid ="211" ssid = "110">tical significant, so we only present length factors on small words (length is less than 5).</S>
			<S sid ="212" ssid = "111">Fig.</S>
			<S sid ="213" ssid = "112">2 shows the precision/recall of both segmentation models relative sentence length.</S>
			<S sid ="214" ssid = "113">We can see that word-based model tends to predict more single character words, but making more mistakes.</S>
			<S sid ="215" ssid = "114">Since about 50% word tokens are single-character words, this is one main source of error for word- segmenter.</S>
			<S sid ="216" ssid = "115">This can be explained by theoretical properties of dynamic token prediction discussed in Sec.</S>
			<S sid ="217" ssid = "116">2.3.3.</S>
			<S sid ="218" ssid = "117">The score of a word boundary assignment in a word-based segmenter is defined like θ∗ I:|w| φ(c, w[1:i]).</S>
			<S sid ="219" ssid = "118">The upper bound of this word-based character-based − + − + AS 93.1 94.2 94.1 94.7 CU 92.6 94.6 94.2 95.0 MSR 95.7 96.1 95.8 96.3 PKU 93.3 94.5 94.4 94.8 Table 3: F-score of two segmenters, with (−) and without (+) word token/type features.</S>
			<S sid ="220" ssid = "119">4.6 Discussion.</S>
			<S sid ="221" ssid = "120">The experiments highlight the fundamental difference between word-based and character-based models, which enlighten us to design new models.</S>
			<S sid ="222" ssid = "121">The above analysis indicates that the theoretical differences cause different error distribution.</S>
			<S sid ="223" ssid = "122">The two approaches are either based on a particular view of segmentation.</S>
			<S sid ="224" ssid = "123">Our analysis points out several drawbacks of each one.</S>
			<S sid ="225" ssid = "124">It may be helpful for both models to overcome their shortcomings.</S>
			<S sid ="226" ssid = "125">For example, one weakness of word-based model is its word induction ability which is partially caused by its neglect of internal structure of words.</S>
			<S sid ="227" ssid = "126">A word-based model may be improved by solving this problem.</S>
			<S sid ="228" ssid = "127">5 System Combination.</S>
			<S sid ="229" ssid = "128">The error analysis also suggests that there is still space for improvement, just by combining the two existing models.</S>
			<S sid ="230" ssid = "129">Here, we introduce a classifier ensemble method for system combination.</S>
			<S sid ="231" ssid = "130">5.1 Upper Bound of System Combination.</S>
			<S sid ="232" ssid = "131">To get an upper bound of the improvement that can be obtained by combining the strengths of each model, we have performed an oracle experiment.</S>
			<S sid ="233" ssid = "132">We think the optimal combination system should choose the right prediction when the two segmenters do not agree with each other.</S>
			<S sid ="234" ssid = "133">There is a gold segmenter that generates gold-standard segmentation results.</S>
			<S sid ="235" ssid = "134">In the oracle experiment, we let the three segmenters, i.e. baseline segmenters and the gold segmenter, vote.</S>
			<S sid ="236" ssid = "135">The three segmenters output three segmentation results, which are further transformed into IOB2 representation (Ramshaw and Marcus, 1995).</S>
			<S sid ="237" ssid = "136">Namely, each character has three B or I labels.</S>
			<S sid ="238" ssid = "137">We assign each character an oracle label which is chosn by at least two segmenters.</S>
			<S sid ="239" ssid = "138">When the baseline segmenters are agree with each other, the gold segmenter cannot change the segmentation whether it is right or wrong.</S>
			<S sid ="240" ssid = "139">In the situation that the two baseline segmenters disagree, the vote given by the gold segmenter will decide the right prediction.</S>
			<S sid ="241" ssid = "140">This kind of optimal performance is presented in Tab.</S>
			<S sid ="242" ssid = "141">4.</S>
			<S sid ="243" ssid = "142">Compared these results with Tab.</S>
			<S sid ="244" ssid = "143">1, we see a. significant increase in accuracy for the four data sets.</S>
			<S sid ="245" ssid = "144">The upper bound of error reduction with system combination is over 30%.</S>
			<S sid ="246" ssid = "145">5.2 Our Model.</S>
			<S sid ="247" ssid = "146">Bootstrap aggregating (Bagging) is a machine learning ensemble meta-algorithm to improve classification and regression models in terms of P( %) R( %) F ER (%) AS 96.</S>
			<S sid ="248" ssid = "147">6 96.</S>
			<S sid ="249" ssid = "148">9 96.</S>
			<S sid ="250" ssid = "149">7 3 7.</S>
			<S sid ="251" ssid = "150">7 CU 97.</S>
			<S sid ="252" ssid = "151">4 97.</S>
			<S sid ="253" ssid = "152">1 97.</S>
			<S sid ="254" ssid = "153">3 4 6.</S>
			<S sid ="255" ssid = "154">0 MS R 97.</S>
			<S sid ="256" ssid = "155">5 97.</S>
			<S sid ="257" ssid = "156">7 97.</S>
			<S sid ="258" ssid = "157">6 3 5.</S>
			<S sid ="259" ssid = "158">1 PK U 96.</S>
			<S sid ="260" ssid = "159">8 96.</S>
			<S sid ="261" ssid = "160">2 96.</S>
			<S sid ="262" ssid = "161">5 3 2.</S>
			<S sid ="263" ssid = "162">7 Table 4: Upper bound for combination.</S>
			<S sid ="264" ssid = "163">The error reduction (ER) rate is a comparison between the F-score produced by the oracle combination system and the character-based system (see Tab.</S>
			<S sid ="265" ssid = "164">1).</S>
			<S sid ="266" ssid = "165">stability and classification accuracy (Breiman, 1996).</S>
			<S sid ="267" ssid = "166">It also reduces variance and helps to avoid overfitting.</S>
			<S sid ="268" ssid = "167">Given a training set D of size n, Bagging generates m new training sets Di of size nt ≤ n, by sampling examples from D uniformly.The m models are fitted using the above m boot strap samples and combined by voting (for classification) or averaging the output (for regression).</S>
			<S sid ="269" ssid = "168">We propose a Bagging model to combine multiple segmentation systems.</S>
			<S sid ="270" ssid = "169">In the training phase, given a training set D of size n, our model gener ates m new training sets Di of size 63.2% × n by sampling examples from D without replacement.</S>
			<S sid ="271" ssid = "170">Namely no example will be repeated in each Di . Each Di is separately used to train a word-based segmenter and a character-based segmenter.</S>
			<S sid ="272" ssid = "171">Using this strategy, we can get 2m weak segmenters.</S>
			<S sid ="273" ssid = "172">Note that the sampling strategy is different from the standard one.</S>
			<S sid ="274" ssid = "173">Our experiment shows that there is no significant difference between the two sampling strategies in terms of accuracy.</S>
			<S sid ="275" ssid = "174">However, the non-placement strategy is more efficient.</S>
			<S sid ="276" ssid = "175">In the segmentation phase, the 2m models outputs 2m segmentation results, which are further transformed into IOB2 representation.</S>
			<S sid ="277" ssid = "176">In other words, each character has 2m B or I labels.</S>
			<S sid ="278" ssid = "177">The final segmentation is the voting result of these 2m labels.</S>
			<S sid ="279" ssid = "178">Note that since 2m is an even number, there may be equal number of B and I labels.</S>
			<S sid ="280" ssid = "179">In this case, our system prefer B to reduce error propagation.</S>
			<S sid ="281" ssid = "180">5.3 Results.</S>
			<S sid ="282" ssid = "181">Fig.</S>
			<S sid ="283" ssid = "182">4 shows the influence of m in the bagging algorithm.</S>
			<S sid ="284" ssid = "183">Because each new data set Di in bagging algorithm is generated by a random procedure, the performance of all bagging experiments are not the same.</S>
			<S sid ="285" ssid = "184">To give a more stable evaluation, we repeat 5 experiments for each m and show the 96.5 96 character-based word-based bagging 97.5 97 character-based word-based bagging 97 96.5 c h a r a c t e r b a s e d w o r d b a s e d b a g g i n g 95.5 96.5 9 6 96 95 95.5 95.5 94.5 95 95 94 94.5 94.5 93.5 AS CU MSR PKU 94 AS CU MSR PKU 94 AS CU MSR PKU Figure 3: Precision/Recall/F-score of different models.</S>
			<S sid ="286" ssid = "185">averaged F-score.</S>
			<S sid ="287" ssid = "186">We can see that the bagging model taking two segmentation models as basic systems consistently outperform the baseline systems and the bagging model taking either model in isolation as basic systems.</S>
			<S sid ="288" ssid = "187">An interesting phenomenon is that the bagging method can also improve word-based models.</S>
			<S sid ="289" ssid = "188">In contrast, there is no significant change in character-based models.</S>
			<S sid ="290" ssid = "189">AS C U MS R PK U (Zh ang et al., 200 6) 95.</S>
			<S sid ="291" ssid = "190">1 95.</S>
			<S sid ="292" ssid = "191">1 97.</S>
			<S sid ="293" ssid = "192">1 95.</S>
			<S sid ="294" ssid = "193">1 (Zh ang and Cla rk, 200 7) 94.</S>
			<S sid ="295" ssid = "194">6 95.</S>
			<S sid ="296" ssid = "195">1 97.</S>
			<S sid ="297" ssid = "196">2 94.</S>
			<S sid ="298" ssid = "197">5 (Su n et al., 200 9b) N/ A 94.</S>
			<S sid ="299" ssid = "198">6 97.</S>
			<S sid ="300" ssid = "199">3 95.</S>
			<S sid ="301" ssid = "200">2 Thi s pap er 95.</S>
			<S sid ="302" ssid = "201">2 95.</S>
			<S sid ="303" ssid = "202">6 96.</S>
			<S sid ="304" ssid = "203">9 95.</S>
			<S sid ="305" ssid = "204">2 Table 5: Segmentation performance presented in previous work and of our combination model.</S>
			<S sid ="306" ssid = "205">Tab.</S>
			<S sid ="307" ssid = "206">5 summarizes the performance of our final 95.5 95 94.5 94 93.5 93 96 95.5 95 94.5 94 93.5 A S d a t a s e t b a s e l i n e ( C ) b a s e l i n e ( W ) c h a r a c t e r b a g g i n g w o r d b a g gi n g b a g g i n g 1 2 3 4 5 6 7 8 9 10 11 12 13 Number of sampling data sets m C U d a t a s e t b a s e l i n e ( C ) b a s e l i n e ( W ) c h a r a c t e r b a g g i n g w o r d b a g g i n g b a g g i n g 1 2 3 4 5 6 7 8 9 10 11 12 13 Number of sampling data sets m 97 96.5 96 95.5 95 94.5 94 93.5 95.2 95 94.8 94.6 94.4 94.2 94 93.8 93.6 93.4 M S R d a t a s e t b a s e l i n e ( C ) b a s e l i n e ( W ) c h a r a c t e r b a g g i n g w o r d b a g gi n g b a g g i n g 1 2 3 4 5 6 7 8 9 10 11 12 13 Number of sampling data sets m P K U d a t a s e t b a s e l i n e ( C ) b a s e l i n e ( W ) c h a r a c t e r b a g g i n g w o r d b a g g i n g b a g g i n g 1 2 3 4 5 6 7 8 9 10 11 12 13 Number of sampling data sets m syste m and other syste ms report ed in a majori ty of previo us work.</S>
			<S sid ="308" ssid = "207">The left most colum n indica tes the refere nce of previo us syste ms that repres ent state- of the-art results . The.</S>
			<S sid ="309" ssid = "208">comp arison of the accura cy betwe en our integr ating syste m and the state of- the-art segme ntatio n syste ms in the literat ure indicate s that our combi nation syste m is comp etitive with the best syste ms, obtain ing the highes t reported F scores on three data sets.</S>
			<S sid ="310" ssid = "209">6 C o n c l u s i o n Figure 4: F-score of bagging models with different numbers of sampling data sets.</S>
			<S sid ="311" ssid = "210">Character- bagging means that the bagging system built on the single character-based segmenter.</S>
			<S sid ="312" ssid = "211">Word- bagging is named in the same way.</S>
			<S sid ="313" ssid = "212">Fig.</S>
			<S sid ="314" ssid = "213">3 shows the precision, recall, F-score of the two baseline systems and our final system for which we generate m = 15 new data sets for bagging.</S>
			<S sid ="315" ssid = "214">We can see significant improvements on the four datasets in terms of the balanced F- score.</S>
			<S sid ="316" ssid = "215">The improvement of precision and recall are not consistent.</S>
			<S sid ="317" ssid = "216">The improvement of AS and CU datasets is from the recall improvement; the improvement of PKU datasets is from the precision improvement.</S>
			<S sid ="318" ssid = "217">We think the different performance is mainly because the four datasets are annotated by using different standards.</S>
			<S sid ="319" ssid = "218">We have presented a thorough study of the difference between word-based and character-based segmentation approaches for Chinese.</S>
			<S sid ="320" ssid = "219">The theoretical and empirical analysis provides insights leading to better models.</S>
			<S sid ="321" ssid = "220">The strengths and weaknesses of the two methods are not exactly the same.</S>
			<S sid ="322" ssid = "221">To exploit their complementary strengths, we propose a Bagging model for system combination.</S>
			<S sid ="323" ssid = "222">Experiments show that the combination strategy is helpful.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="324" ssid = "223">The work is supported by the project TAKE (Technologies for Advanced Knowledge Extraction), funded under contract 01IW08003 by the German Federal Ministry of Education and Research.</S>
			<S sid ="325" ssid = "224">The author is also funded by German Academic Exchange Service (DAAD).</S>
	</SECTION>
</PAPER>
