<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">This paper describes the latest version of speech-to-speech translation systems developed by the team of NICTATR for over twenty years.</S>
		<S sid ="2" ssid = "2">The system is now ready to be deployed for the travel domain.</S>
		<S sid ="3" ssid = "3">A new noise-suppression technique notably improves speech recognition performance.</S>
		<S sid ="4" ssid = "4">Corpus-based approaches of recognition, translation, and synthesis enable coverage of a wide variety of topics and portability to other languages.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Speech recognition, speech synthesis, and machine translation research started about half a century ago.</S>
			<S sid ="6" ssid = "6">They have developed independently for a long time until speech-to-speech translation research was proposed in the 1980’s. The feasibility of speech-to-speech translation was the focus of research at the beginning because each component was difficult to build and their integration seemed more difficult.</S>
			<S sid ="7" ssid = "7">After groundbreaking work for two decades, corpus-based speech and language processing technology have recently enabled the achievement of speech-to-speech translation that is usable in the real world.</S>
			<S sid ="8" ssid = "8">This paper introduces (at ACL 2007) the state- of-the-art speech-to-speech translation system developed by NICTATR, Japan.</S>
	</SECTION>
	<SECTION title="SPEECH-TO-SPEECH	TRANSLA-. " number = "2">
			<S sid ="9" ssid = "1">TION SYSTEM A speech-to-speech translation system is very large and complex.</S>
			<S sid ="10" ssid = "2">In this paper, we prefer to describe recent progress.</S>
			<S sid ="11" ssid = "3">Detailed information can be found in [1, 2, 3] and their references.</S>
			<S sid ="12" ssid = "4">2.1 Speech recognition.</S>
			<S sid ="13" ssid = "5">To obtain a compact, accurate model from corpora with a limited size, we use MDLSSS [4] and composite multi-class N-gram models [5] for acoustic and language modeling, respectively.</S>
			<S sid ="14" ssid = "6">MDLSSS is an algorithm that automatically determines the appropriate number of parameters according to the size of the training data based on the Maximum Description Length (MDL) criterion.</S>
			<S sid ="15" ssid = "7">Japanese, English, and Chinese acoustic models were trained using the data from 4,200, 532, and 536 speakers, respectively.</S>
			<S sid ="16" ssid = "8">Furthermore, these models were adapted to several accents, e.g., US (the United States), AUS (Australia), and BRT (Britain) for English.</S>
			<S sid ="17" ssid = "9">A statistical language model was trained by using large-scale corpora (852 k sentences of Japanese, 710 k sentences of English, 510 k sentences of Chinese) drawn from the travel domain.</S>
			<S sid ="18" ssid = "10">Robust speech recognition technology in noisy situations is an important issue for speech translation in real-world environments.</S>
			<S sid ="19" ssid = "11">An MMSE (Minimum mean square error) estimator for log Mel-spectral energy coefficients using a GMM (Gaussian Mixture Model) [6] is introduced for suppressing interference and noise and for attenuating reverberation.</S>
			<S sid ="20" ssid = "12">Even when the acoustic and language models are trained well, environmental conditions such as variability of speakers, mismatches between the training and testing channels, and interference from environmental noise may cause recognition errors.</S>
			<S sid ="21" ssid = "13">These utterance recognition errors can be rejected by tagging them with a low confidence value.</S>
			<S sid ="22" ssid = "14">To do this we introduce generalized word 25 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 25–28, Prague, June 2007.</S>
			<S sid ="23" ssid = "15">Qc 2007 Association for Computational Linguistics posterior probability (GWPP)-based recognition error rejection for the post processing of the speech recognition [7, 8].</S>
			<S sid ="24" ssid = "16">2.2 Machine translation.</S>
			<S sid ="25" ssid = "17">The translation modules are automatically constructed from large-scale corpora: (1) TATR, a phrase-based SMT module and (2) EM, a simple memory-based translation module.</S>
			<S sid ="26" ssid = "18">EM matches a given source sentence against the source language parts of translation examples.</S>
			<S sid ="27" ssid = "19">If an exact match is achieved, the corresponding target language sentence will be output.</S>
			<S sid ="28" ssid = "20">Otherwise, TATR is called up.</S>
			<S sid ="29" ssid = "21">In TATR, which is built within the framework of feature-based exponential models, we used the following five features: phrase translation probability from source to target; inverse phrase translation probability; lexical weighting probability from source to target; inverse lexical weighting probability; and phrase penalty.</S>
			<S sid ="30" ssid = "22">Here, we touch on two approaches of TATR: novel word segmentation for Chinese, and language model adaptation.</S>
			<S sid ="31" ssid = "23">We used a subword-based approach for word segmentation of Chinese [9].</S>
			<S sid ="32" ssid = "24">This word segmentation is composed of three steps.</S>
			<S sid ="33" ssid = "25">The first is a dictionary-based step, similar to the word segmentation provided by LDC.</S>
			<S sid ="34" ssid = "26">The second is a subword- based IOB tagging step implemented by a CRF tagging model.</S>
			<S sid ="35" ssid = "27">The subword-based IOB tagging achieves a better segmentation than character- based IOB tagging.</S>
			<S sid ="36" ssid = "28">The third step is confidence- dependent disambiguation to combine the previous two results.</S>
			<S sid ="37" ssid = "29">The subword-based segmentation was evaluated with two different data from the Sighan Bakeoff and the NIST machine translation evaluation workshop.</S>
			<S sid ="38" ssid = "30">With the data of the second Sighan Bakeoff1 , our segmentation gave a higher F-score than the best published results.</S>
			<S sid ="39" ssid = "31">We also evaluated this segmentation in a translation scenario using the data of NIST translation evaluation 2 2005, where its BLEU score3 was 1.1% higher than that using the LDC-provided word segmentation.</S>
			<S sid ="40" ssid = "32">The language model that is used plays an important role in SMT.</S>
			<S sid ="41" ssid = "33">The effectiveness of the language 1 http://sighan.cs.uchicago.edu/bakeoff2005/ 2http://www.nist.gov/speech/tests/mt/mt05eval_official_ results_release_20050801_v3.html 3http://www.nist.gov/speech/tests/mt/resources/scoring.</S>
			<S sid ="42" ssid = "34">htm model is significant if the test data happen to have the same characteristics as those of the training data for the language models.</S>
			<S sid ="43" ssid = "35">However, this coincidence is rare in practice.</S>
			<S sid ="44" ssid = "36">To avoid this performance reduction, a topic adaptation technique is often used.</S>
			<S sid ="45" ssid = "37">We applied this adaptation technique to machine translation.</S>
			<S sid ="46" ssid = "38">For this purpose, a “topic” is defined as clusters of bilingual sentence pairs.</S>
			<S sid ="47" ssid = "39">In the decoding, for a source input sentence, f, a topic T is determined by maximizing P(f|T).</S>
			<S sid ="48" ssid = "40">To maximize P(f|T) we select cluster T that gives the highest probability for a given translation source sentence f. After the topic is found, a topic-dependent language model P(e|T) is used instead of P(e), the topic-independent language model.</S>
			<S sid ="49" ssid = "41">The topic- dependent language models were tested using IWSLT06 data 4 . Our approach improved the BLEU score between 1.1% and 1.4%.</S>
			<S sid ="50" ssid = "42">The paper of [10] presents a detailed description of this work.</S>
			<S sid ="51" ssid = "43">2.3 Speech synthesis.</S>
			<S sid ="52" ssid = "44">An ATR speech synthesis engine called XIMERA was developed using large corpora (a 110-hour corpus of a Japanese male, a 60-hour corpus of a Japanese female, and a 20-hour corpus of a Chinese female).</S>
			<S sid ="53" ssid = "45">This corpus-based approach makes it possible to preserve the naturalness and personality of the speech without introducing signal processing to the speech segment [11].</S>
			<S sid ="54" ssid = "46">XIMERA’s HMM (Hidden Markov Model)-based statistical prosody model is automatically trained, so it can generate a highly natural F0 pattern [12].</S>
			<S sid ="55" ssid = "47">In addition, the cost function for segment selection has been optimized based on perceptual experiments, thereby improving the naturalness of the selected segments [13].</S>
	</SECTION>
	<SECTION title="EVALUATION. " number = "3">
			<S sid ="56" ssid = "1">3.1 Speech and language corpora.</S>
			<S sid ="57" ssid = "2">We have collected three kinds of speech and language corpora: BTEC (Basic Travel Expression Corpus), MAD (Machine Aided Dialog), and FED (Field Experiment Data) [14, 15, 16, and 17].</S>
			<S sid ="58" ssid = "3">The BTEC Corpus includes parallel sentences in two languages composed of the kind of sentences one might find in a travel phrasebook.</S>
			<S sid ="59" ssid = "4">MAD is a dialog corpus collected using a speech-to-speech translation system.</S>
			<S sid ="60" ssid = "5">While the size of this corpus is relatively limited, the corpus is used for adaptation and 4 http://www.slt.atr.jp/IWSLT2006/ evaluation.</S>
			<S sid ="61" ssid = "6">FED is a corpus collected in Kansai International Airport uttered by travelers using the airport.</S>
			<S sid ="62" ssid = "7">3.2 Speech recognition system.</S>
			<S sid ="63" ssid = "8">The size of the vocabulary was about 35 k in canonical form and 50 k with pronunciation variations.</S>
			<S sid ="64" ssid = "9">Recognition results are shown in Table 1 for Japanese, English, and Chinese with a real-time factor5 of 5.</S>
			<S sid ="65" ssid = "10">Although the speech recognition performance for dialog speech is worse than that for read speech, the utterance correctness excluding erroneous recognition output using GWPP [8] was greater than 83% in all cases.</S>
			<S sid ="66" ssid = "11">The translation outputs were ranked A (perfect), B (good), C (fair), or D (nonsense) by professional translators.</S>
			<S sid ="67" ssid = "12">The percentage of ranks is shown in Table 3.</S>
			<S sid ="68" ssid = "13">This is in accordance with the above BLEU score.</S>
			<S sid ="69" ssid = "14">A A B A B C Ja pa neseto En gli sh 7 8 . 4 8 6 . 3 9 2 . 2 En glishto Ja pa nes e 7 4 . 3 8 5 . 7 9 3 . 9 Ja pa neseto Ch ine se 6 8 . 0 7 8 . 0 8 8 . 8 Ch ineseto Ja pa nes e 6 8 . 6 8 0 . 4 8 9 . 0 En glishto Ch ine se 5 2 . 5 6 7 . 1 7 9 . 4 Ch ineseto En gli sh 6 8 . 0 7 7 . 3 8 6 . 3 Table 3 Human Evaluation of translation</S>
	</SECTION>
	<SECTION title="System presented at ACL 2007. " number = "4">
			<S sid ="70" ssid = "1">The system works well in a noisy environment and translation can be performed for any combination of Japanese, English, and Chinese languages.</S>
			<S sid ="71" ssid = "2">The display of the current speech-to-speech translation system is shown below.</S>
			<S sid ="72" ssid = "3">Table 1 Evaluation of speech recognition 3.3 Machine Translation.</S>
			<S sid ="73" ssid = "4">The mechanical evaluation is shown, where there are sixteen reference translations.</S>
			<S sid ="74" ssid = "5">The performance is very high except for English-to-Chinese (Table 2).</S>
			<S sid ="75" ssid = "6">BL E U Ja pa neseto En gli sh 0.6 99 8 En glishto Ja pa nes e 0.7 49 6 Ja pa neseto Ch ine se 0.6 58 4 Ch ineseto Ja pa nes e 0.7 40 0 En glishto Ch ine se 0.5 52 0 Ch ineseto En gli sh 0.6 58 1 Table 2 Mechanical evaluation of translation Figure 1 Japanese-to-English Display of NICTATR Speech-to-Speech Translation System</S>
	</SECTION>
	<SECTION title="CONCLUSION. " number = "5">
			<S sid ="76" ssid = "1">This paper presented a speech-to-speech translation system that has been developed by NICTATR for two decades.</S>
			<S sid ="77" ssid = "2">Various techniques, such as noise suppression and corpus-based modeling for both speech processing and machine translation achieve robustness and portability.</S>
			<S sid ="78" ssid = "3">The evaluation has demonstrated that our system is both effective and useful in a real-world environment.</S>
	</SECTION>
</PAPER>
