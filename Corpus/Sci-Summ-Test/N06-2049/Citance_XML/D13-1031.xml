<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Nowadays supervised sequence labeling models can reach competitive performance on the task of Chinese word segmentation.</S>
		<S sid ="2" ssid = "2">However, the ability of these models is restricted by the availability of annotated data and the design of features.</S>
		<S sid ="3" ssid = "3">We propose a scalable semi-supervised feature engineering approach.</S>
		<S sid ="4" ssid = "4">In contrast to previous works using predeﬁned task- speciﬁc features with ﬁxed values, we dynamically extract representations of label distributions from both an in-domain corpus and an out-of-domain corpus.</S>
		<S sid ="5" ssid = "5">We update the representation values with a semi-supervised approach.</S>
		<S sid ="6" ssid = "6">Experiments on the benchmark datasets show that our approach achieve good results and reach an f-score of 0.961.</S>
		<S sid ="7" ssid = "7">The feature engineering approach proposed here is a general iterative semi-supervised method and not limited to the word segmentation task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Chinese is a language without natural word delimiters.</S>
			<S sid ="9" ssid = "9">Therefore, Chinese Word Segmentation (CWS) is an essential task required by further language processing.</S>
			<S sid ="10" ssid = "10">Previous research shows that sequence labeling models trained on labeled data can reach competitive accuracy on the CWS task, and supervised models are more accurate than unsupervised models (Xue, 2003; Low et al., 2005).</S>
			<S sid ="11" ssid = "11">However, the resource of manually labeled training corpora is limited.</S>
			<S sid ="12" ssid = "12">Therefore, semi-supervised learning has become one ∗Corresponding author of the most natural forms of training for CWS.</S>
			<S sid ="13" ssid = "13">Traditional semi-supervised methods focus on adding new unlabeled instances to the training set by a given criterion.</S>
			<S sid ="14" ssid = "14">The possible mislabeled instances, which are introduced from the automatically labeled raw data, can hurt the performance and not easy to exclude by setting a sound selecting criterion.</S>
			<S sid ="15" ssid = "15">In this paper, we propose a simple and scal- able semi-supervised strategy that works by providing semi-supervision at the level of representation.</S>
			<S sid ="16" ssid = "16">Previous works mainly assume that context features are helpful to decide the potential label of a character.</S>
			<S sid ="17" ssid = "17">However, when some of the context features do not appear in the training corpus, this assumption may fail.</S>
			<S sid ="18" ssid = "18">An example is shown in table 1.</S>
			<S sid ="19" ssid = "19">Although the context of “水”and “篮” is totally diﬀerent, they share a homo geneous structure as “verb-noun”.</S>
			<S sid ="20" ssid = "20">Therefore.</S>
			<S sid ="21" ssid = "21">Amuch better way is to map the context informa tion to a kind of representation.</S>
			<S sid ="22" ssid = "22">More precisely, the mapping should let the similar contexts map to similar representations, while let the distinct contexts map to distinct representations.</S>
			<S sid ="23" ssid = "23">吃水果 打篮球 Label B B Character 吃 水 果 打 篮 球 Context Features C-1= 吃 C0= 水 C1= 果 C-1= 打 C0= 篮 C1= 球 Table 1: Example of the context of “水” in “吃水 果 (Eat fruits)” and the context of “篮” in “打篮球 (Play basketball)” We use the label distribution information that 311 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 311–321, Seattle, Washington, USA, 1821 October 2013.</S>
			<S sid ="24" ssid = "24">Qc 2013 Association for Computational Linguistics is extracted from the unlabeled corpus as this representation to enhance the supervised model.</S>
			<S sid ="25" ssid = "25">We add “pseudo-labels” by tagging the unlabeled data with the trained model on the training corpus.</S>
			<S sid ="26" ssid = "26">These “pseudo-labels” are not accurate enough.</S>
			<S sid ="27" ssid = "27">Therefore, we use the label distribution, which is much more accurate.</S>
			<S sid ="28" ssid = "28">To accurately calculate the precise label distribution, we use a framework similar to the co- training algorithm to adjust the feature values iteratively.</S>
			<S sid ="29" ssid = "29">Generally speaking, unlabeled data can be classiﬁed as in-domain data and out-of- domain data.</S>
			<S sid ="30" ssid = "30">In previous works these two kinds of unlabeled data are used separately for diﬀerent purposes.</S>
			<S sid ="31" ssid = "31">In-domain data is mainly used to solve the problem of data sparseness (Sun and Xu, 2011).</S>
			<S sid ="32" ssid = "32">On the other hand, out-of domain data is used for domain adaptation (Chang and Han, 2010).</S>
			<S sid ="33" ssid = "33">In our work, we use in-domain and out-of-domain data together to adjust the labels of the unlabeled corpus.</S>
			<S sid ="34" ssid = "34">We evaluate the performance of CWS on the benchmark dataset of Peking University in the second International Chinese Word Segmentation Bakeoﬀ.</S>
			<S sid ="35" ssid = "35">Experiment results show that our approach yields improvements compared with the state-of-art systems.</S>
			<S sid ="36" ssid = "36">Even when the labeled data is insuﬃcient, our methods can still work better than traditional methods.</S>
			<S sid ="37" ssid = "37">Compared to the baseline CWS model, which has already achieved an f-score above 0.95, we further reduce the error rate by 15%.</S>
			<S sid ="38" ssid = "38">Our method is not limited to word segmentation.</S>
			<S sid ="39" ssid = "39">It is also applicable to other problems which can be solved by sequence labeling models.</S>
			<S sid ="40" ssid = "40">We also applied our method to the Chinese Named Entity Recognition task, and also achieved better results compared to traditional methods.</S>
			<S sid ="41" ssid = "41">The main contributions of our work are as follows: • We proposed a general method to utilize the label distribution given text contexts as representations in a semi-supervised framework.</S>
			<S sid ="42" ssid = "42">We let the co-training process adjust the representation values from label distribution instead of using manually pre deﬁned feature templates.</S>
			<S sid ="43" ssid = "43">• Compared with previous work, our method achieved a new state-of-art accuracy on the CWS task as well as on the NER task.</S>
			<S sid ="44" ssid = "44">The remaining part of this paper is organized as follows.</S>
			<S sid ="45" ssid = "45">Section 2 describes the details of the problem and our algorithm.</S>
			<S sid ="46" ssid = "46">Section 3 describes the experiment and presents the results.</S>
			<S sid ="47" ssid = "47">Section 4 reviews the related work.</S>
			<S sid ="48" ssid = "48">Section 5 concludes this paper.</S>
	</SECTION>
	<SECTION title="System Architecture. " number = "2">
			<S sid ="49" ssid = "1">2.1 Sequence Labeling.</S>
			<S sid ="50" ssid = "2">Nowadays the character-based sequence labeling approach is widely used for the Chinese word segmentation problem.</S>
			<S sid ="51" ssid = "3">It was ﬁrst proposed in Xue (2003), which assigns each character a label to indicate its position in the word.</S>
			<S sid ="52" ssid = "4">The most prevalent tag set is the BMES tag set, which uses 4 tags to carry word boundary information.</S>
			<S sid ="53" ssid = "5">This tag set uses B, M, E and S to represent the Beginning, the Middle, the End of a word and a Single character forming a word respectively.</S>
			<S sid ="54" ssid = "6">We use this tag set in our method.</S>
			<S sid ="55" ssid = "7">An example of the “BMES” representation is shown in table 2.</S>
			<S sid ="56" ssid = "8">Character: 我 爱 北 京 天 安 门 Tag: S S B E B M E Table 2: An example for the “BMES” representation.</S>
			<S sid ="57" ssid = "9">The sentence is “我爱北京天安门” (I love Bei jing Tian-an-men square), which consists of 4 Chinese words: “我” (I), “爱” (love), “北京” (Beijing), and “天安门” (Tian-an-men square).</S>
			<S sid ="58" ssid = "10">2.2 Unlabeled Data.</S>
			<S sid ="59" ssid = "11">Unlabeled data can be divided into in-domain data and out-of-domain data.</S>
			<S sid ="60" ssid = "12">In previous works, these two kinds of unlabeled data are used separately for diﬀerent purposes.</S>
			<S sid ="61" ssid = "13">In-domain data only solves the problem of data sparseness (Sun and Xu, 2011).</S>
			<S sid ="62" ssid = "14">Out-of domain data is used only for domain adaptation (Chang and Han, 2010).</S>
			<S sid ="63" ssid = "15">These two functionalities are not contradictory but complementary.</S>
			<S sid ="64" ssid = "16">Our study shows that by correctly designing features and algorithms, both in-domain unlabeled data and out- of-domain unlabeled data can work together to help enhancing the segmentation model.</S>
			<S sid ="65" ssid = "17">In our algorithm, the dynamic features learned from one corpus can be adjusted incrementally with the dynamic features learned from the other corpus.</S>
			<S sid ="66" ssid = "18">As for the out-of-domain data, it will be even better if the corpus is not limited to a speciﬁc domain.</S>
			<S sid ="67" ssid = "19">We choose a Chinese encyclopedia corpus which meets exactly this requirement.</S>
			<S sid ="68" ssid = "20">We use the corpus to learn a large set of informative features.</S>
			<S sid ="69" ssid = "21">In our experiment, two diﬀerent views of features on unlabeled data are considered: Static Statistical Features (SSFs): These features capture statistical information of characters and character n-grams from the unlabeled corpus.</S>
			<S sid ="70" ssid = "22">The values of these features are ﬁxed during the training process once the unlabeled corpus is given.</S>
			<S sid ="71" ssid = "23">Dynamic Statistical Features (DSFs): These features capture label distribution information from the unlabeled corpus given ﬁxed text contexts.</S>
			<S sid ="72" ssid = "24">As the training process proceeds, the value of these features will change, since the trained tagger at each training iteration may assign diﬀerent labels to the unlabeled data.</S>
			<S sid ="73" ssid = "25">2.3 Framework.</S>
			<S sid ="74" ssid = "26">Suppose we have labeled data L, two unlabeled corpora Ua and Ub (one is an in-domain corpus and the other is an out-of-domain corpus).</S>
			<S sid ="75" ssid = "27">Our algorithm is shown in Table 3.</S>
			<S sid ="76" ssid = "28">During each iteration, we tag the unlabeled corpus Ua using Tb to get pseudo-labels.</S>
			<S sid ="77" ssid = "29">Then we extract features from the pseudo-labels.</S>
			<S sid ="78" ssid = "30">We use the label distribution information as dynamic features.</S>
			<S sid ="79" ssid = "31">We add these features to the training data to train a new tagger Ta . To adjust the feature values, we extract features from one corpus and then apply the statistics to the other corpus.</S>
			<S sid ="80" ssid = "32">This is similar to the principle of co- training (Yarowsky, 1995; Blum and Mitchell, 1998; Dasgupta et al., 2002).</S>
			<S sid ="81" ssid = "33">The diﬀerence is that there are not diﬀerent views of features, but diﬀerent kinds of unlabeled data.</S>
			<S sid ="82" ssid = "34">Detailed description of features is given in the next section.</S>
			<S sid ="83" ssid = "35">Algorithm Init: Using baseline features only: Train an initial tagger T0 based on L () Label Ua and Ub individually using T0 BEGIN LOOP: Generate DSFs from tagged Ua Augment L with DSFs to get La Generate DSFs from tagged Ub Augment L with DSFs to get Lb Using baseline features, SSFs and DSFs: Train new tagger Ta using La Train new tagger Tb using Lb Label Ua using Tb Label Ub using Ta LOOP until performance does not improve RETURN the tagger which is trained with in-domain features.</S>
			<S sid ="84" ssid = "36">Table 3: Algorithm description 2.4 Features.</S>
			<S sid ="85" ssid = "37">2.4.1 Baseline Features Our baseline feature templates include the features described in previous works (Sun and Xu, 2011; Sun et al., 2012).</S>
			<S sid ="86" ssid = "38">These features are widely used in the CWS task.</S>
			<S sid ="87" ssid = "39">To be convenient, for a character ci with context . . .</S>
			<S sid ="88" ssid = "40">ci−1 ci ci+1 . . ., its baseline features are listed below: • Character uni-grams: ck (i − 3 &lt; k &lt; i + 3) • Character bi-grams: ck ck+1 (i − 3 &lt; k &lt; i + 2) • Whether ck and ck+1 are identical (i − 2 &lt; k &lt; i + 2) • Whether ck and ck+2 are identical (i − 4 &lt; k &lt; i + 2) The last two feature templates are designed to detect character reduplication, which is a morphological phenomenon in Chinese language.</S>
			<S sid ="89" ssid = "41">An example is “十全十美” (Perfect), which is a Chinese idiom with structure “ABAC”.</S>
			<S sid ="90" ssid = "42">2.4.2 Static statistical features Statistical features are statistics that distilled from the large unlabeled corpus.</S>
			<S sid ="91" ssid = "43">They are proved useful in the Chinese word segmentation task.</S>
			<S sid ="92" ssid = "44">We deﬁne Static Statistical Features (SSFs) as features whose value do not change during the training process.</S>
			<S sid ="93" ssid = "45">The SSFs in our approach includes Mutual information, Punctuation information and Accessor variety.</S>
			<S sid ="94" ssid = "46">Previous works have already explored the functions of the three static statistics in the Chinese word segmentation task, e.g. Feng et al.</S>
			<S sid ="95" ssid = "47">(2004); Sun and Xu (2011).</S>
			<S sid ="96" ssid = "48">We mainly follow their deﬁnitions while considering more details and giving some modiﬁcation.</S>
			<S sid ="97" ssid = "49">Mutual information Mutual information (MI) is a quantity that measures the mutual dependence of two random variables.</S>
			<S sid ="98" ssid = "50">Previous works showed that larger MI of two strings claims higher probability that the two strings should be combined.</S>
			<S sid ="99" ssid = "51">Therefore, MI can show the tendency of two strings forming one word.</S>
			<S sid ="100" ssid = "52">However, previous works mainly focused on the balanced case, i.e., the MI of strings with the same length.</S>
			<S sid ="101" ssid = "53">In our study we ﬁnd that, in Chinese, there remains large amount of imbalanced cases, like a string with length 1 followed by a string with length 2, and vice versa.</S>
			<S sid ="102" ssid = "54">We further considered the MI of these string pairs to capture more information.</S>
			<S sid ="103" ssid = "55">Punctuation information Punctuations can provide implicit labels for the characters before and after them.</S>
			<S sid ="104" ssid = "56">The character after punctuations must be the ﬁrst character of a word.</S>
			<S sid ="105" ssid = "57">The character before punctuations must be the last character of a word.</S>
			<S sid ="106" ssid = "58">When a string appears frequently after punctuations, it tends to be the beginning of a word.</S>
			<S sid ="107" ssid = "59">The situation is similar when a string appears frequently preceding punctuations.</S>
			<S sid ="108" ssid = "60">Besides, the probability of a string appears in the corpus also aﬀects this tendency.</S>
			<S sid ="109" ssid = "61">Considering all these factors, we propose “punctuation rate” (PR) to capture this information.</S>
			<S sid ="110" ssid = "62">For a string with length len and probability p in the corpus, we deﬁne the left punctuation rate LP Rlen as the number of times the string appears after punctuations, di vided by p. Similarly, the right punctuation rate RP Rlen is deﬁnes as the number of times it appears preceding punctuations divided by its probability p. The length of string we consider is from 1 to 4.</S>
			<S sid ="111" ssid = "63">Accessor variety Accessor variety (AV) is also known as letter successor variety (LSV) (Harris, 1955; Hafer and Weiss, 1974).</S>
			<S sid ="112" ssid = "64">If a string appears after or preceding many diﬀerent characters, this may provide some information of the string itself.</S>
			<S sid ="113" ssid = "65">Previous work of Feng et al.</S>
			<S sid ="114" ssid = "66">(2004), Sun and Xu (2011) used AV to represent this statistic.</S>
			<S sid ="115" ssid = "67">Similar to punctuation rate, we also consider both left AV and right AV.</S>
			<S sid ="116" ssid = "68">For a string s with length l, we deﬁne the left accessor variety (LAV) as the types of distinct characters preceding s in the corpus, and the right accessor variety (RAV) as the types of distinct characters after s in the corpus.</S>
			<S sid ="117" ssid = "69">The length of string we consider is also from 1 to 4.</S>
			<S sid ="118" ssid = "70">2.4.3 Dynamic statistical features The unlabeled corpus lacks precise labels.</S>
			<S sid ="119" ssid = "71">We can use the trained tagger to give the unlabeled data “pseudo-labels”.</S>
			<S sid ="120" ssid = "72">These labels cannot guarantee an acceptable precision.</S>
			<S sid ="121" ssid = "73">However, the label distribution will not be largely aﬀected by small mistakes.</S>
			<S sid ="122" ssid = "74">Using the label distribution information is more accurate than using the pseudo-labels directly.</S>
			<S sid ="123" ssid = "75">Based on this assumption, we propose “dynamic statistical features” (DSFs).</S>
			<S sid ="124" ssid = "76">The DSFs are intended to capture label distribution information given a text context.</S>
			<S sid ="125" ssid = "77">The word “Dynamic” is in accordance with the fact that these feature values will change during the training process.</S>
			<S sid ="126" ssid = "78">We give a formal description of DSFs.</S>
			<S sid ="127" ssid = "79">Suppose there are K labels in our task.</S>
			<S sid ="128" ssid = "80">For example, K = 4 if we take BMES labeling method.</S>
			<S sid ="129" ssid = "81">We deﬁne the whole character sequence with length n as X = (x1 , x2 · · · xj · · · xn ).</S>
			<S sid ="130" ssid = "82">Given a text context Ci , where i is current character position, the DSFs can be represented as a list, DSF (Ci ) = (DSF (Ci )1 , · · · , DSF (Ci )K ) Each element in the list represents the probability of the corresponding label in the distribution.</S>
			<S sid ="131" ssid = "83">For convenience, we further deﬁne function ‘count(condition)’ as the total number of times a ‘condition’ is true in the unlabeled corpus.</S>
			<S sid ="132" ssid = "84">For example, count (current=‘a’) represents the times the current character equals ‘a’, which isexactly the number of times character ‘a’ ap pears in the unlabeled corpus.</S>
			<S sid ="133" ssid = "85">According to diﬀerent types of text context We deﬁne Bigram DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: B(Ci )l = P (y = l|Ci = xi−j xi−j+1 ) = count(Ci = xi−j xi−j+1 ∧ y = l) count(Ci = xi−j xi−j+1 ) j can take value 0 and 1.</S>
			<S sid ="134" ssid = "86">In this equation, the numerator counts the number of times current context is xi−j xi−j+1with label l. The denominator counts the num Ci , we can divide DSFs into 3 types: 1.Basic DSF ber of times current context is xi−j x 3.WindowDSF i−j+1 . For Basic DSF of Ci , we deﬁne D(Ci ): D(Ci ) = (D(Ci )1 , . . .</S>
			<S sid ="135" ssid = "87">, D(Ci )K ) We deﬁne Basic DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: D(Ci )l = P (y = l|Ci = xi ) = count(Ci = xi ∧ y = l) count(Ci = xi ) In this equation, the numerator counts the number of times current character is xi with label l. The denominator counts the number of times current character is xi . We use the term “Basic” because this kind of DSFs only considers the character of position i Considering Basic DSF and Bigram DSF only might cause the over-ﬁtting problem, therefore we introduce another kind of DSF.</S>
			<S sid ="136" ssid = "88">We call it Window DSF, which considers the surrounding context of a character and omits the character itself.</S>
			<S sid ="137" ssid = "89">For Window DSF, we deﬁne W (Ci ): W (Ci ) = (W (Ci )1 , . . .</S>
			<S sid ="138" ssid = "90">, W (Ci )K ) We deﬁne Window DSF with current character position i, text context Ci and label l (the lth dimension in the list) as: W (Ci )l = P (y = l|Ci = xi−1 xi+1 ) = count(Ci = xi−1 xi+1 ∧ y = l) count(Ci = xi−1 xi+1 ) In this equation, the numerator counts theas its context.</S>
			<S sid ="139" ssid = "91">The text context refers to the cur rent character itself.</S>
			<S sid ="140" ssid = "92">This feature captures the number of times current context is xi−1 x i+1with label l. The denominator counts the num label distribution information given the character itself.</S>
			<S sid ="141" ssid = "93">ber of times current context is xi−1 x i+1 . 2.BigramDSFBasic DSF is simple and very easy to implement.</S>
			<S sid ="142" ssid = "94">The weakness is that it is less power ful to describe word-building features.</S>
			<S sid ="143" ssid = "95">Althoughcharacters convey context information, characters themselves in Chinese is sometimes meaningless.</S>
			<S sid ="144" ssid = "96">Character bi-grams can carry more con text information than uni-grams.</S>
			<S sid ="145" ssid = "97">We modify Basic DSFs to bi-gram level and propose Bigram DSFs.</S>
			<S sid ="146" ssid = "98">For Bigram DSF of Ci , we deﬁne B(Ci ): B(Ci ) = (B(Ci )1 , . . .</S>
			<S sid ="147" ssid = "99">, B(Ci )K ) 2.4.4 Discrete features VS. Continuous features The statistical features may be expressed as real values.</S>
			<S sid ="148" ssid = "100">A more natural way is to use discrete values to incorporate them into the sequence labeling models . Previous works like Sun and Xu (2011) solve this problem by setting thresholds and converting the real value into boolean values.</S>
			<S sid ="149" ssid = "101">We use a diﬀerent method to solve this, which does not need to consider tuning thresholds.</S>
			<S sid ="150" ssid = "102">In our method, we process static and dynamic statistical features using different strategies.</S>
			<S sid ="151" ssid = "103">For static statistical value: For mutual information, we round the real value to their nearest integer.</S>
			<S sid ="152" ssid = "104">For punctuation rate and accessor variety, as the values tend to be large, we ﬁrst get the log value of the feature and then use the nearest integer as the corresponding discrete value.</S>
			<S sid ="153" ssid = "105">For dynamic statistical value: Dynamic statistical features are distributions of a label.</S>
			<S sid ="154" ssid = "106">The values of DSFs are all percentage values.</S>
			<S sid ="155" ssid = "107">We can solve this by multiply the probability by an integer N and then take the integer part as the ﬁnal feature value.</S>
			<S sid ="156" ssid = "108">We set the value of N by cross-validation..</S>
			<S sid ="157" ssid = "109">2.5 Conditional Random Fields.</S>
			<S sid ="158" ssid = "110">Our algorithm is not necessarily limited to a speciﬁc baseline tagger.</S>
			<S sid ="159" ssid = "111">For simplicity and reliability, we use a simple Conditional Random Field (CRF) tagger, although other sequence labeling models like Semi-Markov CRF Gao et al.</S>
			<S sid ="160" ssid = "112">(2007) and Latent-variable CRF Sun et al.</S>
			<S sid ="161" ssid = "113">(2009) may provide better results than a single CRF.</S>
			<S sid ="162" ssid = "114">Detailed deﬁnition of CRF can be found in Laﬀerty et al.</S>
			<S sid ="163" ssid = "115">(2001); McCallum (2002); Pinto et al.</S>
			<S sid ="164" ssid = "116">(2003).</S>
	</SECTION>
	<SECTION title="Experiment. " number = "3">
			<S sid ="165" ssid = "1">3.1 Data and metrics.</S>
			<S sid ="166" ssid = "2">We used the benchmark datasets provided by the second International Chinese Word Segmentation Bakeoﬀ1 to test our approach.</S>
			<S sid ="167" ssid = "3">We chose the Peking University (PKU) data in our experiment.</S>
			<S sid ="168" ssid = "4">Although the benchmark provides another three data sets, two of them are data of traditional Chinese, which is quite diﬀerent from simpliﬁed Chinese.</S>
			<S sid ="169" ssid = "5">Another is the data from Microsoft Research (MSR).</S>
			<S sid ="170" ssid = "6">We experimented on this data and got 97.45% in f-score compared to the state-of-art 97.4% reported in Sun et al.</S>
			<S sid ="171" ssid = "7">(2012).</S>
			<S sid ="172" ssid = "8">However, this corpus is much larger than the PKU corpus.</S>
			<S sid ="173" ssid = "9">Using the labeled data alone can get a relatively good tagger and the unlabeled data contributes little to the performance.</S>
			<S sid ="174" ssid = "10">For simplicity and eﬃciency, our further 1 http://www.sighan.org/bakeoff2005/ experiments are all conducted on the PKU data.</S>
			<S sid ="175" ssid = "11">Details of the PKU data are listed in table 4.</S>
			<S sid ="176" ssid = "12">We also used two un-segmented corpora as unlabeled data.</S>
			<S sid ="177" ssid = "13">The ﬁrst one is Chinese Gigaword2 corpus.</S>
			<S sid ="178" ssid = "14">It is a comprehensive archive of newswire data.</S>
			<S sid ="179" ssid = "15">The second one is articles from Baike3 of baidu.com.</S>
			<S sid ="180" ssid = "16">It is a Chinese encyclopedia similar to Wikipedia but contains more Chinese items and their descriptions.</S>
			<S sid ="181" ssid = "17">In the experiment we used about 5 million characters from each corpus for eﬃciency.</S>
			<S sid ="182" ssid = "18">Details of unlabeled data can be found in table 5.</S>
			<S sid ="183" ssid = "19">In our experiment, we did not use any extra resources such as common surnames, part- of-speech or other dictionaries.</S>
			<S sid ="184" ssid = "20">F-score is used as the accuracy measure.</S>
			<S sid ="185" ssid = "21">We deﬁne precision P as the percentage of words in the output that are segmented correctly.</S>
			<S sid ="186" ssid = "22">We deﬁne recall R as the percentage of the words in reference that are correctly segmented.</S>
			<S sid ="187" ssid = "23">Then F-score is as follows: F = 2 × P × R P + R The recall of out-of-vocabulary is also taken into consideration, which measures the ability of the model to correctly segment out of vocabulary words.</S>
			<S sid ="188" ssid = "24">3.2 Main Results.</S>
			<S sid ="189" ssid = "25">Table 6 summarizes the segmentation results on test data with diﬀerent feature combinations.</S>
			<S sid ="190" ssid = "26">We performed incremental evaluation.</S>
			<S sid ="191" ssid = "27">In this table, we ﬁrst present the results of the tagger only using baseline features.</S>
			<S sid ="192" ssid = "28">Then we show the results of adding SSF and DSF individually.</S>
			<S sid ="193" ssid = "29">In the end we compare the results of combining SSF and DSF with baseline features.</S>
			<S sid ="194" ssid = "30">Because the baseline features is strong to reach a relative good result, it is not easy to largely enhance the performance.</S>
			<S sid ="195" ssid = "31">Nevertheless, there are signiﬁcant increases in f-score and OOV-Recall when adding these features.</S>
			<S sid ="196" ssid = "32">From table 6 we can see that by adding SSF and DSF individually, the F-score is improved by +1.1% 2 http://www.ldc.upenn.edu/Catalog/ catalogEntry.jsp?catalogId=LDC2003T09 3 http://baike.baidu.com/ Id en tic al w or ds To tal w or d Id en tic al C ha ra ct er To tal ch ar ac te r 5.</S>
			<S sid ="197" ssid = "33">5 × 10 4 1.</S>
			<S sid ="198" ssid = "34">1 × 10 6 5 × 10 3 1.</S>
			<S sid ="199" ssid = "35">8 × 10 6 Table 4: Details of the PKU data C or p us C h ar ac te r us ed Gi ga w or d 50 00 19 3 Ba ik e 50 00 14 7 Table 5: Details of the unlabeled data.</S>
			<S sid ="200" ssid = "36">P R F O O V Ba sel in e 0.</S>
			<S sid ="201" ssid = "37">95 0 0.</S>
			<S sid ="202" ssid = "38">94 3 0.</S>
			<S sid ="203" ssid = "39">94 6 0.</S>
			<S sid ="204" ssid = "40">67 6 + S S F 0.</S>
			<S sid ="205" ssid = "41">96 1 0.</S>
			<S sid ="206" ssid = "42">95 3 0.</S>
			<S sid ="207" ssid = "43">95 7 0.</S>
			<S sid ="208" ssid = "44">72 8 + D S F 0.</S>
			<S sid ="209" ssid = "45">95 8 0.</S>
			<S sid ="210" ssid = "46">95 3 0.</S>
			<S sid ="211" ssid = "47">95 5 0.</S>
			<S sid ="212" ssid = "48">67 8 +S S F + D S F 0.</S>
			<S sid ="213" ssid = "49">96 5 0.</S>
			<S sid ="214" ssid = "50">95 8 0.</S>
			<S sid ="215" ssid = "51">96 1 0.</S>
			<S sid ="216" ssid = "52">73 1 Table 6: Segmentation results on test data with diﬀerent feature combinations.</S>
			<S sid ="217" ssid = "53">The symbol “+” means this feature conﬁguration contains features set containing the baseline features and all features after ‘+’.</S>
			<S sid ="218" ssid = "54">The size of unlabeled data is ﬁxed as 5 million characters.</S>
			<S sid ="219" ssid = "55">and +0.9%.</S>
			<S sid ="220" ssid = "56">The OOV-Recall is also improved, especially after adding SSFs.</S>
			<S sid ="221" ssid = "57">When considering SSF and DSF together, the f-score is improved by +1.5% while the OOV-Recall is improved by +5.5%.</S>
			<S sid ="222" ssid = "58">To compare the contribution of unlabeleddata, we conduct experiments of using diﬀer ent sizes of unlabeled data.</S>
			<S sid ="223" ssid = "59">Note that the SSFs are still calculated using all the unlabeled data.</S>
			<S sid ="224" ssid = "60">However, each iteration in the algorithm uses unlabeled data with diﬀerent sizes.</S>
			<S sid ="225" ssid = "61">Table 7 shows the results when changing the size of unlabeled data.</S>
			<S sid ="226" ssid = "62">We experimented on three diﬀerent sizes: 0.5 million, 1 million and 5 million characters.</S>
			<S sid ="227" ssid = "63">P R F O O V D SF (0.</S>
			<S sid ="228" ssid = "64">5 M ) 0.</S>
			<S sid ="229" ssid = "65">96 2 0.</S>
			<S sid ="230" ssid = "66">95 4 0.</S>
			<S sid ="231" ssid = "67">95 8 0.</S>
			<S sid ="232" ssid = "68">72 7 D SF (1 M ) 0.</S>
			<S sid ="233" ssid = "69">96 3 0.</S>
			<S sid ="234" ssid = "70">95 5 0.</S>
			<S sid ="235" ssid = "71">95 9 0.</S>
			<S sid ="236" ssid = "72">72 8 D SF (5 M ) 0.</S>
			<S sid ="237" ssid = "73">96 5 0.</S>
			<S sid ="238" ssid = "74">95 8 0.</S>
			<S sid ="239" ssid = "75">96 1 0.</S>
			<S sid ="240" ssid = "76">73 1 Table 7: Comparison of results when changing the size of unlabeled data.</S>
			<S sid ="241" ssid = "77">(0.5 million, 1 million and 5 million characters).</S>
			<S sid ="242" ssid = "78">We further experimented on unlabeled corpus with larger size (up to 100 million characters).</S>
			<S sid ="243" ssid = "79">However the performance did not change significantly.</S>
			<S sid ="244" ssid = "80">Besides, because the number of features in our method is very large, using too large unlabeled corpus is intractable in real applications due to the limitation of memory.</S>
			<S sid ="245" ssid = "81">Our method can keep working well even when the labeled data are insuﬃcient.</S>
			<S sid ="246" ssid = "82">Table 8 shows the comparison of f-scores when changing the size of labeled data.</S>
			<S sid ="247" ssid = "83">We compared the results of using all labeled data with 3 diﬀerent situations: using 1/10, 1/2 and 1/4 of all the labeled data.</S>
			<S sid ="248" ssid = "84">In fact, the best system on the Second International Chinese Word Segmentation bakeoﬀ reached 0.95 in f-score by using all labeled data.</S>
			<S sid ="249" ssid = "85">From table 8 we can see that our algorithm only needs 1/4 of all labeled data to achieve the same f-score.</S>
			<S sid ="250" ssid = "86">Ba sel in e +S S F + D S F I m pr ov e 1/ 10 0.</S>
			<S sid ="251" ssid = "87">93 4 0.</S>
			<S sid ="252" ssid = "88">94 3 +0 .9 6 % 1/ 4 0.</S>
			<S sid ="253" ssid = "89">94 6 0.</S>
			<S sid ="254" ssid = "90">95 1 +0 .5 3 % 1/ 2 0.</S>
			<S sid ="255" ssid = "91">95 2 0.</S>
			<S sid ="256" ssid = "92">95 6 +0 .4 2 % Al l 0.</S>
			<S sid ="257" ssid = "93">95 7 0.</S>
			<S sid ="258" ssid = "94">96 1 +0 .4 2 % Table 8: Comparison of f-scores when changing the size of labeled data.</S>
			<S sid ="259" ssid = "95">(1/10, 1/4, 1/2 and all labeled data.</S>
			<S sid ="260" ssid = "96">The size of unlabeled data is ﬁxed as 5 million characters.)</S>
			<S sid ="261" ssid = "97">We also explored how the performance changes as iteration increases.</S>
			<S sid ="262" ssid = "98">Figure 1 shows the change of F-score during the ﬁrst 10 iterations.</S>
			<S sid ="263" ssid = "99">From ﬁgure 1 we ﬁnd that f-score has a fast improvement in the ﬁrst few iterations, and then stables at a ﬁxed point.</S>
			<S sid ="264" ssid = "100">Besides, as the size of labeled data increases, it converges faster.</S>
			<S sid ="265" ssid = "101">Using an in-domain corpus and an out-of- domain corpus is better than use one corpus alone.</S>
			<S sid ="266" ssid = "102">We compared our approach with the method which uses only one unlabeled corpus.</S>
			<S sid ="267" ssid = "103">To use only one corpus, we modify our algorithm to extract DSFs from the Chinese Giga word corpus and apply the learned features to itself.</S>
			<S sid ="268" ssid = "104">Figure 1: Learning curve of using diﬀerent size of labeled data Table 9 shows the result.</S>
			<S sid ="269" ssid = "105">We can see that our method outperforms by +0.2% in f-score and +0.7% in OOV-Recall.</S>
			<S sid ="270" ssid = "106">Finally, we compared our method with thestate-of-art systems reported in the previous papers.</S>
			<S sid ="271" ssid = "107">Table 10 listed the results.</S>
			<S sid ="272" ssid = "108">Best05 represents the best system reported on the Second International Chinese Word Segmentation Bakeoﬀ.</S>
			<S sid ="273" ssid = "109">CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.</S>
			<S sid ="274" ssid = "110">(2006).</S>
			<S sid ="275" ssid = "111">Other three systems all represent the methods using their cor responding model in the corresponding papers.</S>
			<S sid ="276" ssid = "112">Note that these state-of-art systems are eitherusing complicated models with semi-Markov relaxations or latent variables, or modifying mod els to ﬁt special conditions.</S>
			<S sid ="277" ssid = "113">Our system uses a single CRF model.</S>
			<S sid ="278" ssid = "114">As we can see in table 10, our method achieved higher F-scores than the previous best systems.</S>
			<S sid ="279" ssid = "115">3.3 Results on NER task.</S>
			<S sid ="280" ssid = "116">Our method is not limited to the CWS problem.</S>
			<S sid ="281" ssid = "117">It is applicable to all sequence labeling problems.</S>
			<S sid ="282" ssid = "118">We applied our method on the Chinese NER task.</S>
			<S sid ="283" ssid = "119">We used the MSR corpus of the sixth SIGHAN Workshop on Chinese Language Processing.</S>
			<S sid ="284" ssid = "120">It is the only NER corpus using simpliﬁed Chinese in that workshop.</S>
			<S sid ="285" ssid = "121">We compared our method with the pure sequence labeling approach in He and Wang (2008).</S>
			<S sid ="286" ssid = "122">We re- implemented their method to eliminate the difference of various CRFs implementations.</S>
			<S sid ="287" ssid = "123">Experiment results are shown in table 11.</S>
			<S sid ="288" ssid = "124">We can see that our methods works better, especially when handling the out-of-vocabulary named entities;</S>
	</SECTION>
	<SECTION title="Related work. " number = "4">
			<S sid ="289" ssid = "1">Recent studies show that character sequence labeling is an eﬀective method of Chinese word segmentation for machine learning (Xue, 2003; Low et al., 2005; Zhao et al., 2006a,b).</S>
			<S sid ="290" ssid = "2">These supervised methods show good results.</S>
			<S sid ="291" ssid = "3">Unsupervised word segmentation (Maosong et al., 1998; Peng and Schuurmans, 2001; Feng et al., 2004; Goldwater et al., 2006; Jin and TanakaIshii, 2006) takes advantage of the huge amount of raw text to solve Chinese word segmentation problems.</S>
			<S sid ="292" ssid = "4">These methods need no annotated corpus, and most of them use statistics to help model the problem.</S>
			<S sid ="293" ssid = "5">However, they usually are less accurate than supervised ones.</S>
			<S sid ="294" ssid = "6">Currently “feature-engineering” methodshave been successfully applied into NLP ap plications.</S>
			<S sid ="295" ssid = "7">Miller et al.</S>
			<S sid ="296" ssid = "8">(2004) applied this method to named entity recognition.</S>
			<S sid ="297" ssid = "9">Koo et al.(2008) applied this method to dependency pars ing.</S>
			<S sid ="298" ssid = "10">Turian et al.</S>
			<S sid ="299" ssid = "11">(2010) applied this method toboth named entity recognition and text chunk ing.</S>
			<S sid ="300" ssid = "12">These papers shared the same concept of word clustering.</S>
			<S sid ="301" ssid = "13">However, we cannot simply equal Chinese character to English word becausecharacters in Chinese carry much less informa tion than words in English and the clustering results is less meaningful.</S>
			<S sid ="302" ssid = "14">Features extracted from large unlabeled corpus in previous works mainly focus on statistical information of characters.</S>
			<S sid ="303" ssid = "15">Feng et al.</S>
			<S sid ="304" ssid = "16">(2004) used the accessor variety criterion to extract word types.</S>
			<S sid ="305" ssid = "17">Li and Sun (2009) used punctuation information in Chinese word segmentation by introducing extra labels ’L’ and ’R’.</S>
			<S sid ="306" ssid = "18">Chang and Han (2010), Sun and Xu (2011) used rich statistical information as discrete features in a sequence labeling framework.</S>
			<S sid ="307" ssid = "19">All these approaches can be viewed as using static statistics features in a supervised approach.</S>
			<S sid ="308" ssid = "20">Our method is diﬀerent from theirs.</S>
			<S sid ="309" ssid = "21">For the static statistics features in our approach, we not only consider richer string pairs with the diﬀerent lengths, but also consider term frequency when processing P R F O O V Us in g on e co rp us 0.</S>
			<S sid ="310" ssid = "22">96 3 0.</S>
			<S sid ="311" ssid = "23">95 5 0.</S>
			<S sid ="312" ssid = "24">95 9 0.</S>
			<S sid ="313" ssid = "25">72 4 O ur m et ho d 0.</S>
			<S sid ="314" ssid = "26">96 5 0.</S>
			<S sid ="315" ssid = "27">95 8 0.</S>
			<S sid ="316" ssid = "28">96 1 0.</S>
			<S sid ="317" ssid = "29">73 1 Table 9: Comparison of our approach with using only the Gigaword corpus M et ho d P RF sc or e Be st0 5 (C he n et al.</S>
			<S sid ="318" ssid = "30">(2 00 5) ) 0.</S>
			<S sid ="319" ssid = "31">95 3 0.</S>
			<S sid ="320" ssid = "32">94 6 0.</S>
			<S sid ="321" ssid = "33">95 0 C R F + rule sy ste m (Z ha ng et al.</S>
			<S sid ="322" ssid = "34">(2 00 6) ) 0.</S>
			<S sid ="323" ssid = "35">94 7 0.</S>
			<S sid ="324" ssid = "36">95 5 0.</S>
			<S sid ="325" ssid = "37">95 1 Se mi pe rc ep tr on (Z ha ng an d Cl ar k (2 00 7) ) N /A N /A 0.</S>
			<S sid ="326" ssid = "38">94 5 La te nt va ria bl e C R F (S un et al.</S>
			<S sid ="327" ssid = "39">(2 00 9) ) 0.</S>
			<S sid ="328" ssid = "40">95 6 0.</S>
			<S sid ="329" ssid = "41">94 8 0.</S>
			<S sid ="330" ssid = "42">95 2 A DF C R F (S un et al.</S>
			<S sid ="331" ssid = "43">(2 01 2) ) 0.</S>
			<S sid ="332" ssid = "44">95 8 0.</S>
			<S sid ="333" ssid = "45">94 9 0.</S>
			<S sid ="334" ssid = "46">95 4 O ur m et ho d 0.</S>
			<S sid ="335" ssid = "47">96 5 0.</S>
			<S sid ="336" ssid = "48">95 8 0.</S>
			<S sid ="337" ssid = "49">96 1 Table 10: Comparison of our approach with the state-of-art systems P R F O O V Tr ad iti on al 0.</S>
			<S sid ="338" ssid = "50">92 5 0.</S>
			<S sid ="339" ssid = "51">87 2 0.</S>
			<S sid ="340" ssid = "52">89 8 0.</S>
			<S sid ="341" ssid = "53">71 2 O ur m et ho d 0.</S>
			<S sid ="342" ssid = "54">91 6 0.</S>
			<S sid ="343" ssid = "55">88 7 0.</S>
			<S sid ="344" ssid = "56">90 2 0.</S>
			<S sid ="345" ssid = "57">73 7 Table 11: Comparison of our approach with traditional NER systems punctuation features.There are previous works using features extracted from label distribution of unlabeled cor pus in NLP tasks.</S>
			<S sid ="346" ssid = "58">Schapire et al.</S>
			<S sid ="347" ssid = "59">(2002) use a set of features annotated with majority labels to boost a logistic regression model.</S>
			<S sid ="348" ssid = "60">We are diﬀerent from their approach because there isno pseudo-example labeling process in our ap proach.</S>
			<S sid ="349" ssid = "61">Qi et al.</S>
			<S sid ="350" ssid = "62">(2009) investigated on largeset of distribution features and used these fea tures in a self-training way.</S>
			<S sid ="351" ssid = "63">They applied themethod on three tasks: named entity recogni tion, POS tagging and gene name recognition and got relatively good results.</S>
			<S sid ="352" ssid = "64">Our approach is diﬀerent from theirs.</S>
			<S sid ="353" ssid = "65">Although we all consider label distribution, the way we use features arediﬀerent.</S>
			<S sid ="354" ssid = "66">Besides, our approach uses two unla beled corpora which can mutually enhancing to get better result.</S>
	</SECTION>
	<SECTION title="Conclusion and  Perspectives. " number = "5">
			<S sid ="355" ssid = "1">In this paper, we presented a semi-supervised method for Chinese word segmentation.</S>
			<S sid ="356" ssid = "2">Two kinds of new features are used for the iterative modeling: static statistical features and dy namic statistical features.</S>
			<S sid ="357" ssid = "3">The dynamic statistical features use label distribution information for text contexts, and can be adjusted automatically during the co-training process.</S>
			<S sid ="358" ssid = "4">Experimental results show that the new features can improve the performance on the Chinese word segmentation task.</S>
			<S sid ="359" ssid = "5">We further conducted experiments to show that the performance is largely improved, especially when the labeled data is insuﬃcient.</S>
			<S sid ="360" ssid = "6">The proposed iterative semi-supervised method is not limited to the Chinese word segmentation task.</S>
			<S sid ="361" ssid = "7">It can be easily extended to any sequence labeling task.</S>
			<S sid ="362" ssid = "8">For example, it works well on the NER task as well.</S>
			<S sid ="363" ssid = "9">As our future work, we plan to apply our method to other natural language processing tasks, such as text chunking.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="364" ssid = "10">This research was partly supported by Major National Social Science Fund of China(No. 12&amp;ZD227),National High Technology Research and Development Program of China (863 Program) (No. 2012AA011101) and National Natural Science Foundation of China (No.91024009).</S>
			<S sid ="365" ssid = "11">We also thank Xu Sun and Qiuye Zhao for proofreading the paper.</S>
	</SECTION>
</PAPER>
