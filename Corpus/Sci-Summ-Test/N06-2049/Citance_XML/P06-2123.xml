<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging.</S>
		<S sid ="2" ssid = "2">The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods.</S>
		<S sid ="3" ssid = "3">We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments.</S>
		<S sid ="4" ssid = "4">In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation.</S>
		<S sid ="5" ssid = "5">This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate.</S>
		<S sid ="6" ssid = "6">Our techniques were evaluated using the test data from Sighan Bakeoff 2005.</S>
		<S sid ="7" ssid = "7">We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971).</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="8" ssid = "8">Many approaches have been proposed in Chinese word segmentation in the past decades.</S>
			<S sid ="9" ssid = "9">Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine ∗ Now the second author is affiliated with NTT.</S>
			<S sid ="10" ssid = "10">(SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004).</S>
			<S sid ="11" ssid = "11">By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation.</S>
			<S sid ="12" ssid = "12">This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging.</S>
			<S sid ="13" ssid = "13">Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word.</S>
			<S sid ="14" ssid = "14">For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”.</S>
			<S sid ="15" ssid = "15">Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging.</S>
			<S sid ="16" ssid = "16">For new test data, word boundaries are determined based on the results of tagging.</S>
			<S sid ="17" ssid = "17">While the IOB tagging approach has been widely used in Chinese word segmentation, we found that so far all the existing implementations were using character-based IOB tagging.</S>
			<S sid ="18" ssid = "18">In this work we propose a subword-based IOB tagging, which assigns tags to a predefined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters.</S>
			<S sid ="19" ssid = "19">If only Chinese characters are used, the subword-based IOB tagging is downgraded to a character-based one.</S>
			<S sid ="20" ssid = "20">Taking thesame example mentioned above, “全北京市” is la 961 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961–968, Sydney, July 2006.</S>
			<S sid ="21" ssid = "21">Qc 2006 Association for Computational Linguistics beled as “全/O 北京/B 市/I” in the subword-based tagging, where “北京/B” is labeled as one unit.</S>
			<S sid ="22" ssid = "22">We will give a detailed description of this approach in Section 2.</S>
			<S sid ="23" ssid = "23">There exists a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov).</S>
			<S sid ="24" ssid = "24">In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower.</S>
			<S sid ="25" ssid = "25">While OOV recognition is very important in word segmentation, a higher IV rate is also desired.</S>
			<S sid ="26" ssid = "26">In this work we propose a confidence measure approach to lessen this weakness.</S>
			<S sid ="27" ssid = "27">By this approach we can change the R-oov and R-iv and find an optimal tradeoff.</S>
			<S sid ="28" ssid = "28">This approach will be described in Section 2.3.</S>
			<S sid ="29" ssid = "29">In addition, we illustrate our word segmentation process in Section 2, where the subword-based tagging is described by the MaxEnt method.</S>
			<S sid ="30" ssid = "30">Section 3 presents our experimental results.</S>
			<S sid ="31" ssid = "31">The effects using the MaxEnts and CRFs are shown in this section.</S>
			<S sid ="32" ssid = "32">Section 4 describes current state-of-the-art methods with Chinese word segmentation, with which our results were compared.</S>
			<S sid ="33" ssid = "33">Section 5 provides the con input 咘㣅᯹ԣ೼࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Dictionary-based word segmentation 咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ +XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJFLW\ Subword-based IOB tagging 咘/%/0.94 㣅/,/0.98 ᯹/,/0.91 ԣ/2/0.9 ೼/2/0.89 ࣫Ҁ/%/0.94 Ꮦ/,/0.98 +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, Confidence-based disambiguation 咘/%/0.75 㣅/,/0.78 ᯹/,/0.73 ԣ/2/0.92 ೼/2/0.91 ࣫Ҁ/%/0.95 Ꮦ/,/0.98 +XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/, output 咘㣅᯹ ԣ ೼ ࣫ҀᏖ +XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJFLW\ Figure 1: Outline of word segmentation process W = wt0 wt1 wt2 . . .</S>
			<S sid ="34" ssid = "34">wtM , which satisfies wt0 = c0 . . .</S>
			<S sid ="35" ssid = "35">ct0 , wt1 = ct0 +1 . . .</S>
			<S sid ="36" ssid = "36">ct1 wti = cti−1 +1 . . .</S>
			<S sid ="37" ssid = "37">cti , wtM = ctM−1 +1 . . .</S>
			<S sid ="38" ssid = "38">ctM ti &gt; ti−1, 0 ≤ ti ≤ N, 0 ≤ i ≤ M such that W = arg max P(W |C) = arg max P(W )P(C|W ) cluding remarks and outlines future goals.</S>
			<S sid ="39" ssid = "39">W W = arg max P(wt0 wt1 . . .</S>
			<S sid ="40" ssid = "40">wtM )δ(c0 . . .</S>
			<S sid ="41" ssid = "41">ct0 , wt0 ) W</S>
	</SECTION>
	<SECTION title="Chinese word segmentation framework. " number = "2">
			<S sid ="42" ssid = "1">δ(ct +1 . . .</S>
			<S sid ="43" ssid = "2">ct t t +1 M t Our word segmentation process is illustrated inFig.</S>
			<S sid ="44" ssid = "3">1.</S>
			<S sid ="45" ssid = "4">It is composed of three parts: a dictionary 0 1 , w 1 ) . . .</S>
			<S sid ="46" ssid = "5">δ(c M−1 . . .</S>
			<S sid ="47" ssid = "6">c , w M ) (1) based N-gram word segmentation for segmenting IV words, a maximum entropy subword-based tagger for recognizing OOVs, and a confidence-dependent word disambiguation used for merging the results of both the dictionary-based and the IOB-tagging- based.</S>
			<S sid ="48" ssid = "7">An example exhibiting each step’s results is also given in the figure.</S>
			<S sid ="49" ssid = "8">2.1 Dictionary-based N-gram word.</S>
			<S sid ="50" ssid = "9">segmentation This approach can achieve a very high R-iv, but no OOV detection.</S>
			<S sid ="51" ssid = "10">We combined with it the N-gram language model (LM) to solve segmentation ambiguities.</S>
			<S sid ="52" ssid = "11">For a given Chinese character sequence, C = c0c1c2 . . .</S>
			<S sid ="53" ssid = "12">cN , the problem of word segmentation can be formalized as finding a word sequence, We applied Bayes’ law in the above derivation.</S>
			<S sid ="54" ssid = "13">Because the word sequence must keep consistent with the character sequence, P(C|W ) is expanded to be a multiplication of a Kronecker delta function series, δ(u, v), equal to 1 if both arguments are the same and 0 otherwise.</S>
			<S sid ="55" ssid = "14">P(wt0 wt1 . . .</S>
			<S sid ="56" ssid = "15">wtM ) is a language model that can be expanded by the chain rule.</S>
			<S sid ="57" ssid = "16">If trigram LMs are used, we have P(w0)P(w1|w0)P(w2|w0w1) · · · P(wM |wM−2wM−1) where wi is a shorthand for wti .Equation 1 indicates the process of dictionary based word segmentation.</S>
			<S sid ="58" ssid = "17">We looked up the lexicon to find all the IVs, and evaluated the word sequences by the LMs.</S>
			<S sid ="59" ssid = "18">We used a beam search (Jelinek, 1998) instead of a viterbi search to decode the best word sequence because we found that a beam search can speed up the decoding.</S>
			<S sid ="60" ssid = "19">N-gram LMs were used to score all the hypotheses, of which the one with the highest LM scores is the final output.</S>
			<S sid ="61" ssid = "20">The experimental results are presented in Section 3.1, where we show the comparative results as we changed the order of LMs.</S>
			<S sid ="62" ssid = "21">2.2 Subword-based IOB tagging.</S>
			<S sid ="63" ssid = "22">There are several steps to train a subword-based IOB tagger.</S>
			<S sid ="64" ssid = "23">First, we extracted a word list from the training data sorted in decreasing order by their counts in the training data.</S>
			<S sid ="65" ssid = "24">We chose all the single characters and the top multi-character words as a lexicon subset for the IOB tagging.</S>
			<S sid ="66" ssid = "25">If the subset consists of Chinese characters only, it is a character-based IOB tagger.</S>
			<S sid ="67" ssid = "26">We regard the words in the subset as the sub- words for the IOB tagging.</S>
			<S sid ="68" ssid = "27">Second, we re-segmented the words in the training data into subwords of the subset, and assigned IOB tags to them.</S>
			<S sid ="69" ssid = "28">For the character- based IOB tagger, there is only one possibility for re-segmentation.</S>
			<S sid ="70" ssid = "29">However, there are multi ple choices for the subword-based IOB　tagger.</S>
			<S sid ="71" ssid = "30">For example, “北 京 市(Beijing-city)” can be segmented as “北 京 市(Beijing-city)/O,” or “北 京(Beijing)/B 市(city)/I,” or ”北(north)/B京(capital)/I 市(city)/I.” In this work we used for ward maximal match (FMM) for disambiguation.</S>
			<S sid ="72" ssid = "31">Because we carried out FMMs on each words in the manually segmented training data, the accuracy of FMM was much higher than applying it on whole sentences.</S>
			<S sid ="73" ssid = "32">Of course, backward maximal match (BMM) or other approaches are also applicable.</S>
			<S sid ="74" ssid = "33">We did not conduct comparative experiments due to trivial differences in the results of these approaches.</S>
			<S sid ="75" ssid = "34">In the third step, we used the maximum entropy (MaxEnt) approach (the results of CRF are given in Section 3.4) to train the IOB tagger (Xue and Shen, 2003).</S>
			<S sid ="76" ssid = "35">The mathematical expression for the MaxEnt model is word and tag sequences; fi, a binary feature equal to 1 if the i-th defined feature is activated and 0 otherwise; Z, a normalization coefficient; and λi, the weight of the i-th feature.</S>
			<S sid ="77" ssid = "36">Many kinds of features can be defined for improving the tagging accuracy.</S>
			<S sid ="78" ssid = "37">However, to conform to the constraints of closed test in Bakeoff 2005, some features, such as syntactic information and character encodings for numbers and alphabetical characters, are not allowed.</S>
			<S sid ="79" ssid = "38">Therefore, we used the features available only from the provided training corpus.</S>
			<S sid ="80" ssid = "39">• Contextual information: w0, t−1, w0t−1, w0t−1w1, t−1w1, t−1t−2, w0t−1t−2, w0w1, w0w1w2, w−1, w0w−1, w0w−1w1, w−1w1, w−1w−2, w0w−1w−2, w1, w1w2 where w stands for word and t, for IOB tag.</S>
			<S sid ="81" ssid = "40">The subscripts are position indicators, where 0 means the current word/tag; −1, −2, the first or second word/tag to the left; 1, 2, the first or second word/tag to the right.</S>
			<S sid ="82" ssid = "41">• Prefixes and suffixes.</S>
			<S sid ="83" ssid = "42">These are very useful features.</S>
			<S sid ="84" ssid = "43">Using the same approach as in (Tseng et al., 2005), we extracted the most frequent words tagged with “B”, indicating a prefix, and the last words tagged with “I”, denoting a suffix.</S>
			<S sid ="85" ssid = "44">Features containing prefixes and suffixes were used in the following combinations with other features, where p stands for prefix; s, suffix; p0 means the current word is a prefix and s1 denotes that the right first word is a suffix, and so on.</S>
			<S sid ="86" ssid = "45">p0, w0 p−1, w0 p1, s0, w0 s−1, w0 s1, p0w−1, p0w1, s0w−1, s0w−2 • Word length.</S>
			<S sid ="87" ssid = "46">This is defined as the number of characters in a word.</S>
			<S sid ="88" ssid = "47">The length of a Chinese word has discriminative roles for word composition.</S>
			<S sid ="89" ssid = "48">For example, single-character words are more apt to form new words than are multiple-character words.</S>
			<S sid ="90" ssid = "49">Features using word length are listed below, where l0 means   the word length of the current word.</S>
			<S sid ="91" ssid = "50">Others can P(t|h) = exp  λi fi(h, t) /Z, Z = P(t|h) (2) be inferred similarly.</S>
			<S sid ="92" ssid = "51">   l0, w0l , w l , w l l , l l , l l i t −1 0 1 0 −1 1 0 −1 0 1 where t is a tag, “I,O,B,” of the current word; h, the context surrounding the current word, including As to feature selection, we simply adopted the absolute count for each feature in the training data as the metric, and defined a cutoff value for each feature type.</S>
			<S sid ="93" ssid = "52">We used IIS to train the maximum entropy model.</S>
			<S sid ="94" ssid = "53">For details, refer to (Lafferty et al., 2001).</S>
			<S sid ="95" ssid = "54">The tagging algorithm is based on the beam- search method (Jelinek, 1998).</S>
			<S sid ="96" ssid = "55">After the IOB tagging, each word is tagged with a B/I/O tag.</S>
			<S sid ="97" ssid = "56">The word segmentation is obtained immediately.</S>
			<S sid ="98" ssid = "57">The experimental effect of the word-based tagger and its comparison with the character-based tagger are made in section 3.2.</S>
			<S sid ="99" ssid = "58">2.3 Confidence-dependent word segmentation.</S>
			<S sid ="100" ssid = "59">In the last two steps we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging.</S>
			<S sid ="101" ssid = "60">However, neither was perfect.</S>
			<S sid ="102" ssid = "61">The dictionary-based segmentation produced a result with a higher R-iv but lower R-oov while the IOB tagging yielded the contrary results.</S>
			<S sid ="103" ssid = "62">In this section we introduce a confidence measure approach to combine the two results.</S>
			<S sid ="104" ssid = "63">We define a confidence measure, C M(tiob|w), to measure the confidence of the results produced by the IOB tagging by using the results from the dictionary- based segmentation.</S>
			<S sid ="105" ssid = "64">The confidence measure comes from two sources: IOB tagging and dictionary-based word segmentation.</S>
			<S sid ="106" ssid = "65">Its calculation is defined as: C M(tiob|w) = αC Miob(tiob|w) + (1 − α)δ(tw, tiob)ng (3) where tiob is the word w’s IOB tag assigned by the IOB tagging; tw, a prior IOB tag determined by the results of the dictionary-based segmentation.</S>
			<S sid ="107" ssid = "66">After the dictionary-based word segmentation, the words are re-segmented into subwords by FMM before being fed to IOB tagging.</S>
			<S sid ="108" ssid = "67">Each subword is given a prior IOB tag, tw.</S>
			<S sid ="109" ssid = "68">C Miob(t|w), a confidence probability derived in the process of IOB tagging, which is defined as C M (t w) = hi P(t|w, hi) t hi P(t|w, hi) where hi is a hypothesis in the beam search.</S>
			<S sid ="110" ssid = "69">δ(tw, tiob)ng denotes the contribution of the dictionary-based segmentation.</S>
			<S sid ="111" ssid = "70">δ(tw, tiob)ng is a Kronecker delta function defined as δ(t , t ) = 1 if tw = tiob 0 otherwise In Eq. 3, α is a weighting between the IOB tagging and the dictionary-based word segmentation.</S>
			<S sid ="112" ssid = "71">We found an empirical value 0.8 for α.</S>
			<S sid ="113" ssid = "72">By Eq. 3 the results of IOB tagging were reevaluated.</S>
			<S sid ="114" ssid = "73">A confidence measure threshold, t, was defined for making a decision based on the value.</S>
			<S sid ="115" ssid = "74">If the value was lower than t, the IOB tag was rejected and the dictionary-based segmentation was used; otherwise, the IOB tagging segmentation was used.</S>
			<S sid ="116" ssid = "75">A new OOV was thus created.</S>
			<S sid ="117" ssid = "76">For the two extreme cases, t = 0 is the case of the IOB tagging while t = 1 is that of the dictionary-based approach.</S>
			<S sid ="118" ssid = "77">In Section 3.3 we will present the experimental segmentation results of the confidence measure approach.</S>
			<S sid ="119" ssid = "78">In a real application, we can actually change the confidence threshold to obtain a satisfactory balance between R-iv and R-oov.</S>
			<S sid ="120" ssid = "79">An example is shown in Figure 1.</S>
			<S sid ="121" ssid = "80">In the stage of IOB tagging, a confidence is attached to each word.</S>
			<S sid ="122" ssid = "81">In the stage of confidence-based, a new confidence was made after merging with dictionary-based results where all single-character words are labeled as “O” by default except “Beijing-city” labeled as “Beijing/B” and “city/I”.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "3">
			<S sid ="123" ssid = "1">We used the data provided by Sighan Bakeoff 2005 to test our approaches described in the previous sections.</S>
			<S sid ="124" ssid = "2">The data contain four corpora from different sources: Academia sinica, City University of Hong Kong, Peking University and Microsoft Research (Beijing).</S>
			<S sid ="125" ssid = "3">The statistics concerning the corpora is listed in Table 3.</S>
			<S sid ="126" ssid = "4">The corpora provided both unicode coding and Big5/GB coding.</S>
			<S sid ="127" ssid = "5">We used the Big5 and CP936 encodings.</S>
			<S sid ="128" ssid = "6">Since the main purpose of this work is to evaluate the proposed subword- based IOB tagging, we carried out the closed test only.</S>
			<S sid ="129" ssid = "7">Five metrics were used to evaluate the segmentation results: recall (R), precision (P), F-score (F), OOV rate (R-oov) and IV rate (R-iv).</S>
			<S sid ="130" ssid = "8">For a detailed explanation of these metrics, refer to (Sproat and Emerson, 2003).</S>
			<S sid ="131" ssid = "9">Co rp us Ab bre v. En co din gs Tra inin g siz e (w or ds) Tes t siz e (w or ds) Ac ad em ia Si nic a AS Bi g5/ Un ico de 5.4 5M 12 2K Bei jin g Un ive rsit y PK U CP 93 6/ Un ico de 1.1 M 10 4K Cit y Uni ver sity of Ho ng Ko ng CI TY U Bi g5/ Un ico de 1.4 6M 41 K Mi cro sof t Re sea rch (B eiji ng ) M SR CP 93 6/ Un ico de 2.3 7M 10 7K Table 1: Corpus statistics in Sighan Bakeoff 2005 3.1 Effects of N-gram LMs.</S>
			<S sid ="132" ssid = "10">We obtained a word list from the training data as the vocabulary for dictionary-based segmentation.</S>
			<S sid ="133" ssid = "11">N- gram LMs were generated using the SRI LM toolkit.</S>
			<S sid ="134" ssid = "12">Table 2 shows the performance of N-gram segmentation by changing the order of N-grams.</S>
			<S sid ="135" ssid = "13">We found that bigram LMs can improve segmentation over unigram, though we observed no effect from the trigram LMs.</S>
			<S sid ="136" ssid = "14">For the PKU corpus, there was a relatively strong improvement due to using bi- grams rather than unigrams, posssibly because the PKU corpus’ training size was smaller than the others.</S>
			<S sid ="137" ssid = "15">For a sufficiently large training corpus, the unigram LMs may be enough for segmentation.</S>
			<S sid ="138" ssid = "16">This experiment revealed that language models above bi- grams do not improve word segmentation.</S>
			<S sid ="139" ssid = "17">Since there were some single-character words present in test data but not in the training data, the R-oov rates were not zero in this experiment.</S>
			<S sid ="140" ssid = "18">In fact, we did not use any OOV detection for the dictionary-based approach.</S>
			<S sid ="141" ssid = "19">3.2 Comparisons of Character-based and.</S>
			<S sid ="142" ssid = "20">Subword-based tagger In Section 2.2 we described the character-based and subword-based IOB tagging methods.</S>
			<S sid ="143" ssid = "21">The main difference between the two is the lexicon subset used for re-segmentation.</S>
			<S sid ="144" ssid = "22">For the subword-based IOB tagging, we need to add some multiple-character words into the lexicon subset.</S>
			<S sid ="145" ssid = "23">Since it is hard to decide the optimal number of words to add, we test three different lexicon sizes, as shown in Table 3.</S>
			<S sid ="146" ssid = "24">The first one, s1, consisting of all the characters, is a character-based approach.</S>
			<S sid ="147" ssid = "25">The second, s2, added 2,500 top words from the training data to the lexicon of s1.</S>
			<S sid ="148" ssid = "26">The third, s3, added another 2,500 top words to the lexicon of s2.</S>
			<S sid ="149" ssid = "27">All the words were among the most frequent in the training corpora.</S>
			<S sid ="150" ssid = "28">After choosing the subwords, the training data were re- segmented using the subwords by FMM.</S>
			<S sid ="151" ssid = "29">The final A S CI TY U M S R P K U s1 6, 08 7 4, 9 1 6 5,1 50 4,6 85 s2 8, 33 2 7, 3 3 8 7,4 64 7,0 14 s3 10, 87 6 9, 9 9 6 9,9 90 9,0 53 Table 3: Three different vocabulary sizes used in subword- based tagging.</S>
			<S sid ="152" ssid = "30">s1 contains all the characters.</S>
			<S sid ="153" ssid = "31">s2 and s3 contains some common words.</S>
			<S sid ="154" ssid = "32">lexicons were collected again, consisting of single- character words and multiple-character words.</S>
			<S sid ="155" ssid = "33">Table 3 shows the sizes of the final lexicons.</S>
			<S sid ="156" ssid = "34">Therefore, the minus of the lexicon size of s2 to s1 are not 2,500, exactly.</S>
			<S sid ="157" ssid = "35">The segmentation results of using three lexicons are shown in Table 4.</S>
			<S sid ="158" ssid = "36">The numbers are separated by a “/” in the sequence of “s1/s2/s3.” We found although the subword-based approach outperformed the character-based one significantly, there was no obvious difference between the two subword-based approaches, s2 and s3, adding respective 2,500 and 5,000 subwords to s1.</S>
			<S sid ="159" ssid = "37">The experiments show that we cannot find an optimal lexicon size from 2,500 to 5,000.</S>
			<S sid ="160" ssid = "38">However, there might be an optimal point less than 2,500.</S>
			<S sid ="161" ssid = "39">We did not take much effort to find the optimal point, and regarded 2,500 as an acceptable size for practical usages.</S>
			<S sid ="162" ssid = "40">The F-scores of IOB tagging shown in Table 4 are better than that of N-gram word segmentation in Table 2, which proves that the IOB tagging is effective in recognizing OOV.</S>
			<S sid ="163" ssid = "41">However, we found there was a large decrease in the R-ivs, which shows the weakness of the IOB tagging approach.</S>
			<S sid ="164" ssid = "42">We use the confidence measure approach to deal with this problem in next section.</S>
			<S sid ="165" ssid = "43">3.3 Effects of the confidence measure.</S>
			<S sid ="166" ssid = "44">Up to now we had two segmentation results by using the dictionary-based word segmentation and the IOB tagging.</S>
			<S sid ="167" ssid = "45">In Section 2.3, we proposed a confidence measure approach to reevaluate the results of IOB tagging by combining the two results.</S>
			<S sid ="168" ssid = "46">The effects of R P F R o o v R i v A S 0.9 34/ 0.9 42/ 0.9 41 0.8 84/ 0.8 81/ 0.8 81 0.9 09/ 0.9 10/ 0.9 10 0.0 41/ 0.0 40/ 0.0 38 0.9 75/ 0.9 83/ 0.9 82 CI TY U 0.9 24/ 0.9 29/ 0.9 28 0.8 51/ 0.8 51/ 0.8 51 0.8 86/ 0.8 88/ 0.8 88 0.1 62/ 0.1 62/ 0.1 64 0.9 84/ 0.9 90/ 0.9 89 P K U 0.9 38/ 0.9 49/ 0.9 48 0.9 09/ 0.9 12/ 0.9 12 0.9 24/ 0.9 30/ 0.9 30 0.4 07/ 0.4 03/ 0.4 08 0.9 71/ 0.9 82/ 0.9 81 M S R 0.9 65/ 0.9 69/ 0.9 68 0.9 27/ 0.9 27/ 0.9 27 0.9 46/ 0.9 47/ 0.9 47 0.0 36/ 0.0 36/ 0.0 48 0.9 91/ 0.9 94/ 0.9 93 Table 2: Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005.</S>
			<S sid ="169" ssid = "47">A “/” separates the results of unigram, bigram and trigram.</S>
			<S sid ="170" ssid = "48">R P F R o o v R i v A S 0.9 22/ 0.9 42/ 0.9 43 0.9 14/ 0.9 30/ 0.9 30 0.9 18/ 0.9 36/ 0.9 37 0.6 41/ 0.6 28/ 0.6 09 0.9 35/ 0.9 56/ 0.9 59 CI TY U 0.9 06/ 0.9 33/ 0.9 34 0.9 05/ 0.9 29/ 0.9 27 0.9 06/ 0.9 31/ 0.9 30 0.6 68/ 0.6 71/ 0.6 71 0.9 25/ 0.9 54/ 0.9 55 P K U 0.9 13/ 0.9 34/ 0.9 36 0.9 22/ 0.9 38/ 0.9 40 0.9 18/ 0.9 36/ 0.9 38 0.7 44/ 0.7 24/ 0.7 13 0.9 24/ 0.9 46/ 0.9 49 M S R 0.9 29/ 0.9 53/ 0.9 53 0.9 34/ 0.9 55/ 0.9 52 0.9 32/ 0.9 54/ 0.9 52 0.6 56/ 0.6 84/ 0.6 65 0.9 36/ 0.9 61/ 0.9 61 Table 4: Segmentation results by the pure subword-based IOB tagging.</S>
			<S sid ="171" ssid = "49">The separator “/” divides the results by three lexicon sizes as illustrated in Table 3.</S>
			<S sid ="172" ssid = "50">The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3).</S>
			<S sid ="173" ssid = "51">0.94 0.95 0.96 0.97 0.98 0.99 1 t=0 t=0 t=0 t=0 t=1 AS CITYU PKU MSR t=0 t=1 t=1 iv’s varing as the thresho ld is shown in Fig.</S>
			<S sid ="174" ssid = "52">2, where R-oovs and R ivs are moving in differe nt directi ons.</S>
			<S sid ="175" ssid = "53">When the confide nce thresh old t = 0, the case for the IOB tagging, R oovs are maxim al. When t = 1, repres enting the dictionary based segme ntation , R- oovs are the minim al. The R-oovs and R ivs varied largely at the start and end point but little around the middle sectio n. 3 . 4 S u b w o r d b a s e d t a g g i n g b y C R F s 0.8 0.7 0.6 0.5 0.4 R-oov 0.3 0.2 0.1 0 Our proposed approach es were presented and eval Figure 2: R-iv and R-oov varing as the confidence threshold, t. the confidence measure are shown in Table 5, where we used α = 0.8 and confidence threshold t = 0.7.</S>
			<S sid ="176" ssid = "54">These are empirical numbers.</S>
			<S sid ="177" ssid = "55">We obtained the optimal values by multiple trials on held-out data.</S>
			<S sid ="178" ssid = "56">The numbers in the slots of Table 5 are divided by a separator “/” and displayed as the sequence “s1/s2/s3”, just as Table 4.</S>
			<S sid ="179" ssid = "57">We found that the results in Table 5 were better than those in Table 4 and Table 2, which proved that using the confidence measure approach yielded the best performance over the N-gram segmentation and the IOB tagging approaches.</S>
			<S sid ="180" ssid = "58">Even with the use of the confidence measure, the subword-based IOB tagging still outperformed the character-based IOB tagging, proving that the proposed subword-based IOB tagging was very effective.</S>
			<S sid ="181" ssid = "59">Though the improvement under the confidence measure was decreasing, it was still significant.</S>
			<S sid ="182" ssid = "60">We can change the R-oov and R-iv by changing the confidence threshold.</S>
			<S sid ="183" ssid = "61">The effect of R-oov and R uated using the MaxEnt method in the previous sections.</S>
			<S sid ="184" ssid = "62">When we turned to CRF-based tagging, we found a same effect as the MaxEnt method.</S>
			<S sid ="185" ssid = "63">Our subword-based tagging by CRFs was implemented by the package “CRF++” from the site “http://www.chasen.org/˜taku/software.” We repeated the previous sections’ experiments using the CRF approach except that we did one of the two subword-based tagging, the lexicon size s3.</S>
			<S sid ="186" ssid = "64">The same values of the confidence measure threshold and α were used.</S>
			<S sid ="187" ssid = "65">The results are shown in Table 6.</S>
			<S sid ="188" ssid = "66">We found that the results using the CRFs were much better than those of the MaxEnts.</S>
			<S sid ="189" ssid = "67">However, the emphasis here was not to compare CRFs and MaxEnts but the effect of subword-based IOB tagging.</S>
			<S sid ="190" ssid = "68">In Table 6, the results before ”/” are the character-based IOB tagging and after ”/”, the subword-based.</S>
			<S sid ="191" ssid = "69">It was clear that the subword-based approaches yielded better results than the character- based approach though the improvement was not as higher as that of the MaxEnt approaches.</S>
			<S sid ="192" ssid = "70">There was R P F R o o v R i v A S 0.9 38/ 0.9 50/ 0.9 53 0.9 45/ 0.9 46/ 0.9 51 0.9 41/ 0.9 48/ 0.9 48 0.6 74/ 0.6 41/ 0.6 06 0.9 50/ 0.9 64/ 0.9 69 CI TY U 0.9 32/ 0.9 49/ 0.9 46 0.9 44/ 0.9 33/ 0.9 44 0.9 38/ 0.9 41/ 0.9 45 0.7 05/ 0.5 97/ 0.6 67 0.9 50/ 0.9 77/ 0.9 68 P K U 0.9 41/ 0.9 48/ 0.9 49 0.9 45/ 0.9 47/ 0.9 47 0.9 43/ 0.9 48/ 0.9 48 0.6 72/ 0.6 62/ 0.6 60 0.9 58/ 0.9 66/ 0.9 66 M S R 0.9 44/ 0.9 59/ 0.9 61 0.9 59/ 0.9 64/ 0.9 63 0.9 51/ 0.9 61/ 0.9 62 0.6 71/ 0.6 74/ 0.6 31 0.9 51/ 0.9 67/ 0.9 70 Table 5: Effects of combination using the confidence measure.</S>
			<S sid ="193" ssid = "71">Here we used α = 0.8 and confidence threshold t = 0.7.</S>
			<S sid ="194" ssid = "72">The separator “/” divides the results of s1, s2, and s3.</S>
			<S sid ="195" ssid = "73">no change on F-score for AS corpus, but a better recall rate was found.</S>
			<S sid ="196" ssid = "74">Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora.</S>
			<S sid ="197" ssid = "75">Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006).</S>
	</SECTION>
	<SECTION title="Discussion and Related works. " number = "4">
			<S sid ="198" ssid = "1">The IOB tagging approach adopted in this work is not a new idea.</S>
			<S sid ="199" ssid = "2">It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods.</S>
			<S sid ="200" ssid = "3">Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001).</S>
			<S sid ="201" ssid = "4">However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv.</S>
			<S sid ="202" ssid = "5">This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMM- based or CRF-based approaches.</S>
			<S sid ="203" ssid = "6">(Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly.</S>
			<S sid ="204" ssid = "7">We used confidence measure to determine a better balance between R-oov and R-iv.</S>
			<S sid ="205" ssid = "8">The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs.</S>
			<S sid ="206" ssid = "9">In this work we used it more than that.</S>
			<S sid ="207" ssid = "10">By way of the confidence measure we combined results from the dictionary-based and the IOB- tagging-based and as a result, we could achieve the optimal performance.</S>
			<S sid ="208" ssid = "11">Our main contribution is to extend the IOB tagging approach from being a character-based to a subword-based one.</S>
			<S sid ="209" ssid = "12">We proved that the new approach enhanced the word segmentation signifi cantly in all the experiments, MaxEnts, CRFs and using confidence measure.</S>
			<S sid ="210" ssid = "13">We tested our approach using the standard Sighan Bakeoff 2005 data set in the closed test.</S>
			<S sid ="211" ssid = "14">In Table 7 we align our results with some top runners’ in the Bakeoff 2005.</S>
			<S sid ="212" ssid = "15">Our results were compared with the best performers’ results in the Bakeoff 2005.</S>
			<S sid ="213" ssid = "16">Two participants’ results were chosen as bases: No.15-b, ranked the first in the AS corpus, and No.14, the best performer in CITYU, MSR and PKU.</S>
			<S sid ="214" ssid = "17">The No.14 used CRF-modeled IOB tagging while No.15-b used MaxEnt-modeled IOB tagging.</S>
			<S sid ="215" ssid = "18">Our results produced by the MaxEnt are denoted as “ours(ME)” while “ours(CRF)” for the CRF approaches.</S>
			<S sid ="216" ssid = "19">We achieved the highest F-scores in three corpora except the AS corpus.</S>
			<S sid ="217" ssid = "20">We think the proposed subword- based approach played the important role for the achieved good results.</S>
			<S sid ="218" ssid = "21">A second advantage of the subword-based IOB tagging over the character-based is its speed.</S>
			<S sid ="219" ssid = "22">The subword-based approach is faster because fewer words than characters needed to be labeled.</S>
			<S sid ="220" ssid = "23">We observed a speed increase in both training and testing.</S>
			<S sid ="221" ssid = "24">In the training stage, the subword approach was almost two times faster than the character-based.</S>
	</SECTION>
	<SECTION title="Conclusions. " number = "5">
			<S sid ="222" ssid = "1">In this work, we proposed a subword-based IOB tagging method for Chinese word segmentation.</S>
			<S sid ="223" ssid = "2">The approach outperformed the character-based method using both the MaxEnt and CRF approaches.</S>
			<S sid ="224" ssid = "3">We also successfully employed the confidence measure to make a confidence-dependent word segmentation.</S>
			<S sid ="225" ssid = "4">By setting the confidence threshold, R-oov and R-iv can be changed accordingly.</S>
			<S sid ="226" ssid = "5">This approach is effective for performing desired segmentation based on users’ requirements to R-oov and R-iv.</S>
			<S sid ="227" ssid = "6">R P F R o o v R i v A S 0.9 53/ 0.9 56 0.9 44/ 0.9 47 0.9 48/ 0.9 51 0.6 07/ 0.6 49 0.9 69/ 0.9 69 CI TY U 0.9 43/ 0.9 52 0.9 48/ 0.9 49 0.9 46/ 0.9 51 0.6 82/ 0.7 41 0.9 64/ 0.9 69 P K U 0.9 42/ 0.9 47 0.9 57/ 0.9 55 0.9 49/ 0.9 51 0.7 75/ 0.7 48 0.9 52/ 0.9 59 M S R 0.9 60/ 0.9 72 0.9 66/ 0.9 69 0.9 63/ 0.9 71 0.6 74/ 0.7 12 0.9 67/ 0.9 76 Table 6: Effects of using CRF.</S>
			<S sid ="228" ssid = "7">The separator “/” divides the results of s1, and s3.</S>
			<S sid ="229" ssid = "8">Par tici pa nts R P FR oo vR iv Hong Kong City University ou rs( C R F) 0.9 52 0.9 49 0.9 51 0.</S>
			<S sid ="230" ssid = "9">74 1 0.9 69 o ur s( M E) 0.9 46 0.9 44 0.9 45 0.</S>
			<S sid ="231" ssid = "10">66 7 0.9 68 1 4 0.9 41 0.9 46 0.9 43 0.</S>
			<S sid ="232" ssid = "11">69 8 0.9 61 1 5 b 0.9 37 0.9 46 0.9 41 0.</S>
			<S sid ="233" ssid = "12">73 6 0.9 53 A c a d e m i a S i n i c a 1 5 b 0.9 52 0.9 51 0.9 52 0.</S>
			<S sid ="234" ssid = "13">69 6 0.9 63 ou rs( C R F) 0.9 56 0.9 47 0.9 51 0.</S>
			<S sid ="235" ssid = "14">64 9 0.9 69 o ur s( M E) 0.9 53 0.9 43 0.9 48 0.</S>
			<S sid ="236" ssid = "15">60 8 0.9 69 1 4 0.</S>
			<S sid ="237" ssid = "16">95 0.9 43 0.9 47 0.</S>
			<S sid ="238" ssid = "17">71 8 0.9 60 M i c r o s o f t R e s e a r c h ou rs( C R F) 0.9 72 0.9 69 0.9 71 0.</S>
			<S sid ="239" ssid = "18">71 2 0.9 76 1 4 0.9 62 0.9 66 0.9 64 0.</S>
			<S sid ="240" ssid = "19">71 7 0.9 68 o ur s( M E) 0.9 61 0.9 63 0.9 62 0.</S>
			<S sid ="241" ssid = "20">63 1 0.9 70 1 5 b 0.9 52 0.9 64 0.9 58 0.</S>
			<S sid ="242" ssid = "21">71 8 0.9 58 P e k i n g U n i v e r s i t y ou rs( C R F) 0.9 47 0.9 55 0.9 51 0.</S>
			<S sid ="243" ssid = "22">74 8 0.9 59 1 4 0.9 46 0.9 54 0.9 50 0.</S>
			<S sid ="244" ssid = "23">78 7 0.9 56 o ur s( M E) 0.9 49 0.9 47 0.9 48 0.</S>
			<S sid ="245" ssid = "24">66 0 0.9 66 1 5 b 0.</S>
			<S sid ="246" ssid = "25">93 0.9 51 0.9 41 0.</S>
			<S sid ="247" ssid = "26">7 6 0.9 41 Table 7: List of results in Sighan Bakeoff 2005</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="248" ssid = "27">The authors thank the reviewers for the comments and advice on the paper.</S>
			<S sid ="249" ssid = "28">Some related software for this work will be released very soon.</S>
	</SECTION>
</PAPER>
