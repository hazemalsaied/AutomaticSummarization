<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Since the first Chinese Word Segmentation (CWS) Bakeoff on 2003, CWS has experienced a prominent flourish because Bakeoff provides a platform for the participants, which helps them recognize the merits and drawbacks of their segmenters.</S>
		<S sid ="2" ssid = "2">However, the evaluation metric of bakeoff is not sufficient enough to measure the performance thoroughly, sometimes even misleading.</S>
		<S sid ="3" ssid = "3">One typical example caused by this insufficiency is that there is a popular belief existing in the research field that segmentation based on word can yield a better result than character-based tagging (CT) on in-vocabulary (IV) word segmentation even within closed tests of Bakeoff.</S>
		<S sid ="4" ssid = "4">Many efforts were paid to balance the performance on IV and out-of- vocabulary (OOV) words by combining these two methods according to this belief.</S>
		<S sid ="5" ssid = "5">In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance.</S>
		<S sid ="6" ssid = "6">Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff * The work is done when the first author is working in MSRA as an intern.</S>
		<S sid ="7" ssid = "7">closed tests.</S>
		<S sid ="8" ssid = "8">Furthermore, our analysis shows that using confidence measure to combine the two segmentation results should be under certain limitation.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Chinese Word Segmentation (CWS) has been witnessed a prominent progress in the last three Bakeoffs (Sproat and Emerson, 2003), (Emerson, 2005), (Levow, 2006).</S>
			<S sid ="10" ssid = "10">One of the reasons for this progress is that Bakeoff provides standard corpora and objective metric, which makes the result of each system comparable.</S>
			<S sid ="11" ssid = "11">Through those evaluations researchers can recognize the advantage and disadvantage of their methods and improve their systems accordingly.</S>
			<S sid ="12" ssid = "12">However, in the evaluation metric of Bakeoff, only the overall F measure, precision, recall, IV (in- vocabulary) recall and OOV (out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and OOV word segmentation need to be evaluated.</S>
			<S sid ="13" ssid = "13">An important issue is that segmentation based on which, word or character, can yield the better performance on IV words.</S>
			<S sid ="14" ssid = "14">We give a detailed explanation about this issue as following.</S>
			<S sid ="15" ssid = "15">Since CWS was firstly treated as a character- based tagging task (we call it “CT” for short hereafter) in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006).</S>
			<S sid ="16" ssid = "16">Relatively to dictionary-based segmentation (we call it “DS” for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole.</S>
			<S sid ="17" ssid = "17">Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006.</S>
			<S sid ="18" ssid = "18">Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT even in the restriction of Bakeoff closed test.</S>
			<S sid ="19" ssid = "19">Consequently, many strategies are proposed to balance the IV and OOV performance (Goh et al., 2005), (Zhang et al., 2006a).</S>
			<S sid ="20" ssid = "20">Among these strategies, the confidence measure used to combine the results of CT and DS is a straightforward one, which is introduced in (Zhang et al., 2006a).</S>
			<S sid ="21" ssid = "21">The basic assumption of such combination is that DS method performs better on IV words and Zhang derives this belief from the fact that DS achieves higher IV recall rate as Table 1 shows.</S>
			<S sid ="22" ssid = "22">In which AS, CityU, MSRA and PKU are four corpora used in Bakeoff 2005 (also see Table 2 for detail).</S>
			<S sid ="23" ssid = "23">We provide a more detailed evaluation metric to analyze these two methods, including precision and F measure of IV and OOV respectively and our experiments show that CT outperforms DS on both IV and OOV words within Bakeoff closed test.</S>
			<S sid ="24" ssid = "24">The precision and F measure are existing metrics and the definitions of them are clear.</S>
			<S sid ="25" ssid = "25">Here we just employ them to evaluate segmentation results.</S>
			<S sid ="26" ssid = "26">Furthermore, our error analysis on the results of combination reveals that confidence measure in (Zhang et al., 2006a) has a representation flaw and we propose an EIV tag method to revise it.</S>
			<S sid ="27" ssid = "27">Finally, we give an empirical comparison between existing pure CT method and combination, which shows that pure CT method can produce state-of-the-art results on both IV word and overall segmentation.</S>
			<S sid ="28" ssid = "28">Corpus RIV ROOV DS CT DS CT AS 0.982 0.967 0.038 0.647 CityU 0.989 0.967 0.164 0.736 MSRA 0.993 0.972 0.048 0.716 PKU 0.981 0.955 0.408 0.754 Table 1 IV and OOV recall in (Zhang et al., 2006a) The rest of this paper is organized as follows.</S>
			<S sid ="29" ssid = "29">In Section 2, we give a brief introduction to Zhang‟s DS method and subword-based tagging, which is a special CT method.</S>
			<S sid ="30" ssid = "30">And by comparing the results of this special CT method and DS according our detailed metric, we show that CT performs better on both IV and OOV.</S>
			<S sid ="31" ssid = "31">We review in Section 3 how confidence measure works and indicate its representation flaw.</S>
			<S sid ="32" ssid = "32">Furthermore, an “EIV” tag method is proposed to revise the confidence measure.</S>
			<S sid ="33" ssid = "33">In Section 4, the experimental results of existing pure CT method are demonstrated to compare with combination result, based on which we discuss the related work.</S>
			<S sid ="34" ssid = "34">In Section 5, we conclude the contributions of this paper and discuss the future work.</S>
	</SECTION>
	<SECTION title="Comparison   between   DS   and   CT. " number = "2">
			<S sid ="35" ssid = "1">Based on Detailed Metric We proposed a detailed evaluation metric for IV and OOV word identification in this section and experiments based on the new metric show that CT outperforms DS not only on OOV words but also on IV words with F-measure of IV.</S>
			<S sid ="36" ssid = "2">All the experiments in this paper conform to the constraints of closed test in Bakeoff 2005 (Emerson, 2005).</S>
			<S sid ="37" ssid = "3">It means that any resource beyond the training corpus is excluded.</S>
			<S sid ="38" ssid = "4">We first review how DS and CT work and then present our evaluation metric and experiment results.</S>
			<S sid ="39" ssid = "5">There is one thing should be emphasized, by comparing DS and CT result we just want to verify that our new metric can show the performance on IV words more objectively.</S>
			<S sid ="40" ssid = "6">Since either DS or CT implementation has specific setting here we should not extend the comparison result to a general sense between those generative models and discriminative models.</S>
			<S sid ="41" ssid = "7">2.1 Dictionary-based segmentation.</S>
			<S sid ="42" ssid = "8">For the dictionary-based word segmentation, we collect a dictionary from training corpus first.</S>
			<S sid ="43" ssid = "9">Instead of Maximum Match, trigram language model 2 trained on training corpus is employed for disambiguation.</S>
			<S sid ="44" ssid = "10">During the disambiguation procedure, a beam search decoder is used to seek the most possible segmentation.</S>
			<S sid ="45" ssid = "11">Since the setting in our paper is consistent with the closed test of 2 Language model used in this paper is SLRIM from.</S>
			<S sid ="46" ssid = "12">http://www.speech.sri.com/projects/srilm/ Bakeoff, we can only use the information we learn from training corpus though other open resources may be helpful to improve the performance further.</S>
			<S sid ="47" ssid = "13">For detail, the decoder reads characters from the input sentence one at a time, and generates candidate segmentations incre- mentally.</S>
			<S sid ="48" ssid = "14">At each stage, the next incoming character is combined with an existing candidate in two different ways to generate new candidates: it is either appended to the last word in the candidate, or taken as the start of a new word.</S>
			<S sid ="49" ssid = "15">This method guarantees exhaustive generation of possible segmentations for any input sentence.</S>
			<S sid ="50" ssid = "16">However, the exponential time and space of the length of the input sentence are needed for such a search and it is always intractable in practice.</S>
			<S sid ="51" ssid = "17">Thus, we use the trigram language model to select top B (B is a constant predefined before search and in our experiment 3 is used) best candidates with highest probability at each stage so that the search algorithm can work in practice.</S>
			<S sid ="52" ssid = "18">Finally, when the whole sentence has been read, the best candidate with the highest probability will be selected as the segmentation result.</S>
			<S sid ="53" ssid = "19">Here, the term “dictionary-based” is exactly the method implemented in (Zhang et al., 2006a), it does not mean the generative language model in general.</S>
			<S sid ="54" ssid = "20">2.2 Character-based tagging.</S>
			<S sid ="55" ssid = "21">Under CT scheme, each character in one sentence is labeled as „B‟ if it is the beginning of a word, „O‟ tag means the current character is a single-character word, other character is labeled as „I‟. For example, “全中国 (whole China)” is labeled as “ 全 (whole)/O 中 (central)/B 国 (country)/I”.</S>
			<S sid ="56" ssid = "22">In (Zhang et al., 2006a), the above CT method is developed as subword-based tagging.</S>
			<S sid ="57" ssid = "23">First, the most frequent multi-character words and all single characters in training corpus are collected as subwords.</S>
			<S sid ="58" ssid = "24">During the subword- based tagging, a subword is viewed as an unit instead of several separate characters and given only one tag.</S>
			<S sid ="59" ssid = "25">For example, in subword-based tagging, “全中国 (whole China)” is labeled as “ 全 (whole)/O 中国 (China)/O”, if the word “中 国 (China)” is collected as a subword.</S>
			<S sid ="60" ssid = "26">As the preprocessing, both training and test corpora are segmented by maximum match with subword set as dictionary.</S>
			<S sid ="61" ssid = "27">After this preprocessing, every sentence in both training and test corpora becomes subword sequence.</S>
			<S sid ="62" ssid = "28">Finally, the tagger is trained by CRFs approach3 on the training data.</S>
			<S sid ="63" ssid = "29">Although word information is integrated into this method, it still works in the scheme of “IOB” tagging.</S>
			<S sid ="64" ssid = "30">Thus, we still call subword- based tagging as a special CT method and in the reminder of this paper “CT” means subword- based tagging in Zhang‟s paper and “Pure CT” means CT without subword.</S>
			<S sid ="65" ssid = "31">2.3 A detailed evaluation metric In this paper, data provided by Bakeoff 2005 is used in our experiments in order to compare with the published results in (Zhang et al., 2006a).</S>
			<S sid ="66" ssid = "32">The statistics of the corpora for Bakeoff 2005 are listed in Table 2 (Emerson, 2005).</S>
			<S sid ="67" ssid = "33">Co rp us En co din g # T r a i n i n g w o r d s # T es t w or ds O O V ra te A S B i g 5 5 . 4 5 M 12 2K 0.0 43 Ci ty U B i g 5 1 . 4 6 M 4 1 K 0.0 74 M SR A G B 2 . 3 7 M 10 7K 0.0 26 P K U G B 1 . 1 M 10 4K 0.0 58 Table 2 Corpora statistics of Bakeoff 2005 Evaluation standard is also provided by Bakeoff, including overall precision, recall, F measure, IV recall and OOV recall (Sproat and Emerson, 2003), (Emerson, 2005).</S>
			<S sid ="68" ssid = "34">However, some important metrics, such as F measure and precision of both IV and OOV words are omitted, which are necessary when the performance of IV or OOV word identification need to be judged.</S>
			<S sid ="69" ssid = "35">Thus, in order to judge the results of each experiment, a more detailed evaluation with precision and F measure of both IV and OOV words included is used.</S>
			<S sid ="70" ssid = "36">To calculate the IV and OOV precision and recall, we firstly divide words of the segmenter‟s output and gold data into IV word and OOV word sets respectively with the dictionary collected from the training corpus.</S>
			<S sid ="71" ssid = "37">Then, for IV and OOV word sets respectively, the IV (or OOV) recall is the proportion of the correctly segmented IV (or OOV) word tokens to all IV (or OOV) word tokens in the gold data, and IV (or OOV) precision is the proportion of the correctly segmented IV</S>
	</SECTION>
	<SECTION title="CRF tagger in this paper  is implemented by CRF++. " number = "3">
			<S sid ="72" ssid = "1">downloaded from http://crfpp.sourceforge.net/ (or OOV) word tokens to all IV (or OOV) word tokens in the segmenter‟s output.</S>
			<S sid ="73" ssid = "2">One thing have to be emphasized is that the single character in test corpus will be defined as OOV if it does not appear in training corpus.</S>
			<S sid ="74" ssid = "3">We will see later in this section, by this evaluation, some facts covered by the bakeoff evaluation can be illustrated by our new evaluation metric.</S>
			<S sid ="75" ssid = "4">Here, we repeat two experiments described in (Zhang et al., 2006a), namely dictionary-based approach and subword-based tagging.</S>
			<S sid ="76" ssid = "5">For CT method, top 2000 most frequent multi-character words and all single characters in training corpus are selected as subwords and the feature templates used for CRF model is listed in Table 3.</S>
			<S sid ="77" ssid = "6">We present all the segmentation results in Table6 to see the strength and weakness of each me thod conveniently.</S>
			<S sid ="78" ssid = "7">Based on IV and OOV recall as we show inTable 1, Zhang argues that the DS performs bet ter on IV word identification while CT performs better on OOV words.</S>
			<S sid ="79" ssid = "8">But we can see from the results in Table 6 (the lines about DS and CT), the IV precision of DS approach is much lower than that of CT on all the four corpora, which also causes a lower F measure of IV.</S>
			<S sid ="80" ssid = "9">The reason for low IV precision of DS is that many OOV words are segmented into two IV words by DS.</S>
			<S sid ="81" ssid = "10">For example, OOV word “ 歌唱班(choral)” is segmented into“ 歌唱 (sing) 班 (class)” by DS.</S>
			<S sid ="82" ssid = "11">These wrongly identified IV words increase the number of all IV words in the segmenter‟s output and cause the low IV precision of the DS result.</S>
			<S sid ="83" ssid = "12">Since the F measure of IV is a more reasonable metric of performance of IV than IV recall only, Table 6 shows that CT method outperforms the DS on IV word segmentation over all four corpora.</S>
			<S sid ="84" ssid = "13">The comparison also shows that CT outperforms the DS on OOV and overall segmentation as well.</S>
			<S sid ="85" ssid = "14">Ty pe Fe atu re Fu nct ion Un igr amC 2, C 1, C0, C1, C2 Pre vio us tw o, cur ren t an d ne xt tw o su bw ord Bi gra m C-2 C 1, C-1 C0, C0 C1, C1 C2 Tw o adj ace nt su bw ord s Ju mp C-1 C1 Pre vio us ch ara cte r an d ne xt su bw ord s Table 3 Feature templates used for CRF in our experiments 3 Balance between IV and OOV Per-.</S>
			<S sid ="86" ssid = "15">formance There are other strategies such as (Goh et al., 2005) trying to seek balance between IV and When both results of CT and DS are available, the CM can be calculated according to the following formula in (Zhang et al., 2006a): CM(tiob | w)  CM iob (tiob | w)  (1  ) (tw , tiob )ngOOV performance.</S>
			<S sid ="87" ssid = "16">In (Goh et al, 2005), infor Here, w is a subword, tiob is “IOB” tag given mation in a dictionary is used in a statistical model.</S>
			<S sid ="88" ssid = "17">In this way, the dictionary-based ap by CT and tw is “IOB” tag generated by DS.</S>
			<S sid ="89" ssid = "18">In the first term of the right hand side of the formu proach and the statistical model are combined.We choose the confidence measure to study be la, CM iob (tiob | w) is the marginal probability ofcause it is straightforward.</S>
			<S sid ="90" ssid = "19">We show in this sec tiob (we call this marginal probability “MP” for tion that there is a representation flaw in the short).</S>
			<S sid ="91" ssid = "20">And in the second term,  (tw , tiob )ng is a formula of confidence measure in (Zhang et al., 2006a).</S>
			<S sid ="92" ssid = "21">And we propose an “EIV” tag method to Kronecker delta function, returning 1 if and only solve this problem.</S>
			<S sid ="93" ssid = "22">Our experiments show that if t w and tiob are identical, else returning 0.</S>
			<S sid ="94" ssid = "23">But confidence measure with EIV tag outperformsif  (t w , tiob ) ng  1 , there is no requirement of re CT and DS alone.</S>
			<S sid ="95" ssid = "24">placement at all.</S>
			<S sid ="96" ssid = "25">While if  (t w , tiob ) ng  0 , when 3.1 Confidence measure.</S>
			<S sid ="97" ssid = "26">Confidence Measure (CM) means to seek an optimal tradeoff between performance on IV and OOV words.</S>
			<S sid ="98" ssid = "27">The basic idea of CM comes from the belief that CT performs better on OOV words while DS performs better on IV words.</S>
			<S sid ="99" ssid = "28">t w  tiob , CM depends on the first term of its right hand side only and  is unnecessary to be set as a weight.</S>
			<S sid ="100" ssid = "29">Finally,  in the formula is a weight to seek balance between CT tag and DS tag.</S>
			<S sid ="101" ssid = "30">Another parameter here is a threshold t for the CM.</S>
			<S sid ="102" ssid = "31">If CM is less than t , t w replaces tiob as the final tag, otherwise tiob will be remained as the final tag.</S>
			<S sid ="103" ssid = "32">However, two parameters in the CM, namely  and t , are unnecessary, because when MP is greater than or equal to t /  , tiob will be kept, otherwise it will be replaced with t w . Thus, the CM ultimately is the marginal probability of the “IOB” tag (MP).</S>
			<S sid ="104" ssid = "33">In the experiment of this paper, MP is used as CM because it is equivalent to Zhang‟s CM but more convenient to express.</S>
			<S sid ="105" ssid = "34">3.2 Experiments and error analysis about.</S>
			<S sid ="106" ssid = "35">combination We repeat the experiments about CM in Zhang‟s paper (Zhang et al., 2006a) and show that there is a representation flaw in the CM formula.</S>
			<S sid ="107" ssid = "36">Furthermore, we propose an EIV tag method to make CM yield a better result.</S>
			<S sid ="108" ssid = "37">In this paper,  = 0.8 and t = 0.7 (Parameters in two papers, Zhang et al. 2006a and Zhang et al. 2006b, are different.</S>
			<S sid ="109" ssid = "38">And our parameters are consistent with Zhang et al. 2006b which is con firmed by Dr Zhang through email) are used in CM, namely MP= 0.875 is the threshold.</S>
			<S sid ="110" ssid = "39">Here, in Table 4, we provide some statistics on the results of CT when MP is less than 0.875.</S>
			<S sid ="111" ssid = "40">From Table 4 we can see that even with MP less than 0.875, most of the subwords are still tagged correctly by CT and should not be revised by DS result.</S>
			<S sid ="112" ssid = "41">Besides, lots of the subwords with low MP contained by OOV words in test data, especially for the corpus whose OOV rate is high (i.e. on CityU corpus more than one third sub- words with low MP belong to OOV word) and performance on OOV recognition is the advantage of CT rather than that of DS approach.</S>
			<S sid ="113" ssid = "42">Thus when combining the results of the two methods, it is the tiob should be maintained if the subword is contained by an OOV word.</S>
			<S sid ="114" ssid = "43">Therefore, the CM formula seems somewhat unreasonable.</S>
			<S sid ="115" ssid = "44">The error analysis about how many original errors are eliminated and how many new errors are introduced by CM is provided in Table 5 (the columns about CM).</S>
			<S sid ="116" ssid = "45">Table 5 illustrates that, after combining the two results, most original er rors on IV words are corrected because DS can achieve higher IV recall as described in Zhang‟s paper.</S>
			<S sid ="117" ssid = "46">But on OOV part, more new errors are introduced by CM and these new errors decrease the precision of the IV words.</S>
			<S sid ="118" ssid = "47">For example, the OOV words “警卫队员 (guard member)” and “ 设计费 (design fee)” is recognized correctly byCT but with low CM.</S>
			<S sid ="119" ssid = "48">In the combining proce dure, these words are wrongly split as IV errors: “警卫 (guard) 队员 (member)” and “设计 (design) 费 (fee)”.</S>
			<S sid ="120" ssid = "49">Thus, for two corpora (i.e. CityU and AS), F measure of IV and overall F measure decreases since there are more new errors introduced than original ones eliminated and only on the other two corpora (MSRA and PKU), overall F measure of combination method is higher than CT alone, which is shown in Table 6 by the lines about combination.</S>
			<S sid ="121" ssid = "50">3.3 EIV tag method.</S>
			<S sid ="122" ssid = "51">Since combining the two results by CM may produce an even worse performance in some case, it is worthy to study how to use this CM to get an enhanced result.</S>
			<S sid ="123" ssid = "52">Intuitively, if we can change only the CT tags of the subwords which contained in IV word while keep the CT tags of those contained in OOV words unchanged, we will improve the final result according to our error analysis in Table 5.</S>
			<S sid ="124" ssid = "53">Unfortunately, only from the test data, we can get the information whether a subword contained in an IV word, just as what we do to get Table 4.</S>
			<S sid ="125" ssid = "54">However, we can get an approximate estimation from DS result.</S>
			<S sid ="126" ssid = "55">When using subwords to re-segment DS result4, all the fractions re-segmented out of multiple- character words, including both multiple- character words and single characters, will be given an “EIV” tag, which means that the current multiple-character word or single character is contained in an IV word with high probability.</S>
			<S sid ="127" ssid = "56">For example, “人力资源 (human resource)” in DS result is a whole word.</S>
			<S sid ="128" ssid = "57">However, only “资源 (resource)” belongs to the subword set, so during the re-segmentation “人力资源 (human resource)” will be re-segmented as “人 (people) 力 (force) 资源 (resource)”.</S>
			<S sid ="129" ssid = "58">All these three frac tions will be labeled with an “EIV” tag respectively.</S>
			<S sid ="130" ssid = "59">It is reasonable because all the multiple- character words in the DS result can match an IV word.</S>
			<S sid ="131" ssid = "60">After this procedure, when combining</S>
	</SECTION>
	<SECTION title="For the detail, please refer to (Zhang et al., 2006a).. " number = "4">
			<S sid ="132" ssid = "1">C o r p u s A S Cit yU M SR A PK U # su bw ord tok ens bel on g to IV 10 01 0 44 04 9 5 5 2 96 19 # su bw ord tok ens bel on g to IV an d tag ge d cor rec tly by CT 74 52 34 34 7 4 5 2 72 13 # su bw ord tok ens bel on g to IV an d tag ge d wr on gly by CT 25 58 9 7 0 2 1 0 0 24 06 # su bw ord tok ens bel on g to O O V 59 24 25 24 2 6 8 5 35 80 # su bw ord tok ens bel on g to O O V an d tag ge d cor rec tly by CT 31 77 16 56 1 7 2 5 22 08 # su bw ord tok ens bel on g to O O V an d tag ge d wr on gly by CT 27 47 8 6 8 9 6 0 13 72 Table 4 Results of CT when MP is less than 0.875 Co rp us A S City U MS RA P K U Me tho d C M EI V C M EI V C M EI V C M EI V #o rig ina l err ors eli mi nat ed on IV 19 05 10 03 90 4 46 9 19 59 10 77 19 23 11 87 #o rig ina l err ors eli mi nat ed on O O V 75 5 7 5 15 5 8 0 1 0 4 3 0 23 0 7 6 #o rig ina l err ors eli mi nat ed tot all y 26 60 10 78 10 59 54 9 20 63 11 07 21 53 12 63 #n ew err ors int ro du ce d on IV 44 1 18 5 8 0 5 0 1 4 8 6 8 21 1 11 8 #n ew err ors int ro du ce d on O O V 24 87 7 7 13 20 10 3 15 17 5 7 16 81 5 8 # ne w err ors int ro du ce d tot all y 29 28 26 2 14 00 15 3 16 65 12 5 18 92 17 6 Table 5 Error analysis of confidence measure with and without EIV tag the two results, only the CT tag with EIV tags and low MP will be replaced by DS tag, otherwise the original CT tag will be maintained.</S>
			<S sid ="133" ssid = "2">Under this condition the errors introduced by OOV will not happen and enhanced results are listed in Table 6 lines about EIV.</S>
			<S sid ="134" ssid = "3">We can see that on all four corpora the overall F measure of EIV result is higher than that of CT alone, which show that our EIV method works well.</S>
			<S sid ="135" ssid = "4">Now, let‟s check what changes happened in the number of error tags after EIV condition added into the CM.</S>
			<S sid ="136" ssid = "5">We can see from the Table 5 columns about EIV, there are more errors eliminated than the new errors introduced after EIV condition added into CM and most CT tags of subwords contained in OOV words maintained unchanged as we supposed.</S>
			<S sid ="137" ssid = "6">And then, our results (in Table 6 lines about EIV) are comparable with that in Zhang‟s paper.</S>
			<S sid ="138" ssid = "7">Thus, there may be some similar strategies in Zhang‟s CM too but not presented in Zhang‟s paper.</S>
			<S sid ="139" ssid = "8">4 Discussion and Related Works.</S>
			<S sid ="140" ssid = "9">Although the method such as confidence measure can be helpful at some circumstance, our experiment shows that pure character-based tagging (pure CT) can work well with reasonable features and tag set.</S>
			<S sid ="141" ssid = "10">In (Zhao et al., 2006), an enhanced CRF tag set is proposed to distinguish different positions in the multi-character words when the word length is less than 6.</S>
			<S sid ="142" ssid = "11">In this method, feature templates are almost the same as shown in Table 3 with a 3-character window and a 6-tag set {B, B2, B3, M, E, O} is used.</S>
			<S sid ="143" ssid = "12">Here, tag B and E stand for the first and the last position in a multi-character word, respectively.</S>
			<S sid ="144" ssid = "13">S stands up a single-character word.</S>
			<S sid ="145" ssid = "14">B2 and B3 stand for the second and the third position in a multi-character word, whose length is larger than two-character or three-character.</S>
			<S sid ="146" ssid = "15">M stands for the fourth or more rear position in a multi- character word, whose length is larger than four- character.</S>
			<S sid ="147" ssid = "16">In Table 6, the lines about “pure CT” provide the results generated by pure CT with 6-tag set.</S>
			<S sid ="148" ssid = "17">We can see from the Table 6 this pure CT approach achieves the state-of-the-art results on all the corpora.</S>
			<S sid ="149" ssid = "18">On three of the four corpora (AS, MSRA and PKU) this pure CT method gets the best result.</S>
			<S sid ="150" ssid = "19">Even on IV word, this pure CT approach outperforms Zhang‟s CT method and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too.</S>
			<S sid ="151" ssid = "20">Moreover, this character-based tagging approach is more clear and simple than the confidence measure method.</S>
			<S sid ="152" ssid = "21">Although character-based tagging became mainstream approach in the last two Bakeoffs, it does not mean that word information is valueless in Chinese word segmentation.</S>
			<S sid ="153" ssid = "22">A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007), which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff.</S>
			<S sid ="154" ssid = "23">Co rp us Me tho d R P F RIV PIV FIV RO OV PO OV FO OV AS DS 0.9 43 0.8 81 0.9 11 0.9 84 0.8 92 0.9 35 0.0 44 0.2 17 0.0 76 CT 0.9 54 0.9 38 0.9 46 0.9 67 0.9 60 0.9 64 0.6 66 0.6 06 0.6 35 Co mb ina tio n 0.9 58 0.9 29 0.9 43 0.9 80 0.9 45 0.9 62 0.4 87 0.5 93 0.5 35 EI V tag 0.9 60 0.9 42 0.9 51 0.9 73 0.9 62 0.9 68 0.6 67 0.6 24 0.6 45 Pu re CT 0.9 58 0.9 47 0.9 53 0.9 71 0.9 63 0.9 67 0.6 82 0.6 18 0.6 48 Cit yU DS 0.9 28 0.8 48 0.8 86 0.9 89 0.8 65 0.9 23 0.1 62 0.3 53 0.2 23 CT 0.9 47 0.9 40 0.9 44 0.9 63 0.9 64 0.9 64 0.7 39 0.7 17 0.7 28 Co mb ina tio n 0.9 54 0.9 22 0.9 38 0.9 84 0.9 38 0.9 61 0.5 81 0.6 93 0.6 32 EI V tag 0.9 53 0.9 49 0.9 51 0.9 70 0.9 68 0.9 69 0.7 44 0.7 50 0.7 47 Pu re CT 0.9 47 0.9 48 0.9 48 0.9 67 0.9 73 0.9 70 0.6 92 0.6 60 0.6 76 M SR A DS 0.9 69 0.9 27 0.9 47 0.9 94 0.9 30 0.9 61 0.0 36 0.3 58 0.0 66 CT 0.9 63 0.9 64 0.9 63 0.9 70 0.9 79 0.9 75 0.6 98 0.6 62 0.6 80 Co mb ina tio n 0.9 77 0.9 61 0.9 69 0.9 90 0.9 70 0.9 80 0.5 11 0.6 53 0.5 74 EI V tag 0.9 72 0.9 70 0.9 71 0.9 80 0.9 82 0.9 81 0.6 96 0.6 79 0.6 88 Pu re CT 0.9 72 0.</S>
			<S sid ="155" ssid = "24">97 5 0.9 73 0.9 78 0.9 86 0.9 82 0.7 50 0.6 32 0.6 86 PK U DS 0.9 48 0.9 11 0.9 29 0.9 81 0.9 20 0.9 50 0.4 03 0.7 11 0.5 15 CT 0.9 44 0.9 45 0.9 45 0.9 55 0.9 66 0.9 61 0.7 63 0.7 27 0.7 45 Co mb ina tio n 0.9 55 0.9 42 0.9 49 0.9 73 0.9 53 0.9 63 0.6 64 0.7 82 0.7 18 EI V tag 0.9 50 0.9 52 0.9 51 0.9 61 0.9 70 0.9 66 0.7 68 0.7 53 0.7 60 Pu re CT 0.9 46 0.9 57 0.9 51 0.9 56 0.9 73 0.9 64 0.6 72 0.5 80 0.6 23 Table 6 Results of different approach used in our experiments (White background lines are the results we repeat Zhang‟s methods and they have some trivial difference with Table 1.)</S>
			<S sid ="156" ssid = "25">Therefore, the most important thing worth to pay attention in future study is how to integrate linguistic information into the statistical model effectively, no matter character or word information.</S>
	</SECTION>
	<SECTION title="Conclusions and Future Work. " number = "5">
			<S sid ="157" ssid = "1">In this paper, we first provided a detailed evaluation metric, which provides the necessary information to judge the performance of each method on IV and OOV word identification.</S>
			<S sid ="158" ssid = "2">Second, by this evaluation metric, we show that character- based tagging outperforms dictionary-based segmentation not only on OOV words but also on IV words within Bakeoff closed tests.</S>
			<S sid ="159" ssid = "3">Furthermore, our experiments show that confidence measure in Zhang‟s paper has a representation flaw and we propose an EIV tag method to revise the combination.</S>
			<S sid ="160" ssid = "4">Finally, our experiments show that pure character-based approach also can achieve good IV word and overall performance.</S>
			<S sid ="161" ssid = "5">Perhaps, there are two reasons that existing combination results don‟t outperform the pure CT. One is that most information contained in statistic language model is already captured by the CT feature templates in CRF framework.</S>
			<S sid ="162" ssid = "6">The other is that confidence measure may not be the effective way to combine the DS and CT results.</S>
			<S sid ="163" ssid = "7">In the future work, our research will focus on how to integrate word information into CRF features rather than using it to modify the results of CRF tagging.</S>
			<S sid ="164" ssid = "8">In this way, we can capture the word information meanwhile avoid destroying the optimal output of CRF tagging.</S>
	</SECTION>
	<SECTION title="Acknowledgement">
			<S sid ="165" ssid = "9">The authors appreciate Dr. Hai Zhao in City University of Hong Kong and Dr. Ruiqiang Zhang in Spoken Language Communications Lab, ATR, Japan providing a lot of help for this paper.</S>
			<S sid ="166" ssid = "10">Thank those reviewers who gave the valuable comment to improve this paper.</S>
	</SECTION>
</PAPER>
