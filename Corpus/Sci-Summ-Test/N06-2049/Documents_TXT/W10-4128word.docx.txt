HMM Revises Low Marginal Probability by CRF
for Chinese Word Segmentation


Degen Huang, Deqin Tong, Yanyan Luo 
Department of Computer Science and Engineering 
Dalian University of Technology
huangdg@dlut.edu.cn, {tongdeqin, ziyanluoyu}@gmail.com





Abstract

This paper presents a Chinese word 
segmentation system for CIPS-SIGHAN
2010 Chinese language processing task. 
Firstly,  based  on  Conditional  Random
Field (CRF) model, with local features 
and global features, the character-based 
tagging  model  is  designed.  Secondly,
Hidden Markov Models (HMM) is used 
to revise the substrings with low marginal
probability by CRF. Finally, confidence 
measure is used to regenerate the result
and simple rules to deal with the strings 
within letters and numbers. As is well 
known that character-based approach has
outstanding   capability   of   discovering 
out-of-vocabulary (OOV) word, but ex-
ternal information of word lost. HMM 
makes use of word information to in- 
crease in-vocabulary (IV) recall. We par-
ticipate in the simplified Chinese word 
segmentation both closed and open test
on all four corpora, which belong to dif- 
ferent domains. Our system achieves bet- 
ter performance.

1     Introduction

Chinese Word Segmentation (CWS) has wit- 
nessed a prominent progress in the first four 
SIGHAN Bakeoffs. Since Xue (2003) used 
character-based tagging, this method has at- 
tracted more and more attention. Some previous 
work (Peng et al., 2004; Tseng et al., 2005; Low 
et al., 2005) illustrated the effectiveness of using 
characters as tagging units, while literatures 
(Zhang et al., 2006; Zhao and Kit, 2007a; Zhang 
and  Clark,  2007)  focus  on  employing  lexical


words or subwords as tagging units. Because the 
word-based models can capture the word-level 
contextual information and IV knowledge. Be- 
sides, many strategies are proposed to balance 
the  IV  and  OOV  performance  (Wang  et  al.,
2008).
  CRF has been widely used in sequence label- 
ing tasks and has a good performance (Lafferty
et al., 2001). Zhao and Kit (2007b; 2008) at-
tempt to integrate global information with local 
information to further improve CRF-based tag- 
ging method of CWS, which provides a solid 
foundation for strengthening CRF learning with 
unsupervised learning outcomes.
  In order to increase the accuracy of tagging 
using CRF, we adopt the strategy, which is: if the 
marginal probability of characters is lower than a 
threshold, the modified component based on 
HMM will be trigged; combining the confidence 
measure the results will be regenerated.

2     Our word segmentation system

In this section, we describe our system in more 
details. Three modules are included in our sys- 
tem: a basic character-based CRF tagger, HMM 
which revises the substrings with low marginal 
probability and confidence measure which com- 
bines them to regenerate the result. In addition, 
we also use some rules to deal with the strings 
within letters and numbers.

2.1     Character-based CRF tagger

Tag Set A 6-tag set is adopted in our system. It 
includes six tags: B, B2, B3, M, E and S. Here, 
Tag B and E stand for the first and the last posi- 
tion in a multi-character word, respectively. S 
stands for a single-character word. B2 and B3 
stand for the second and the third position in a




The work described in this paper is supported by Microsoft Research Asia Funded Project.


multi-character word. M stands for the fourth or 
more  rear  position  in  a  multi-character  word 
with more than four characters. The 6-tag set is 
proved to work more effectively than other tag 
sets in improving the segmentation performance 
of CRFs by Zhao et al. (2006).
  Feature templates In our system, six n-gram 
templates,  namely,  C-1,  C0,  C1,  C-1C0,  C0C1,


that character.
  In the open test, we only add another feature 
of ‘FRE’, the basic idea of which is if a string 
matches a word in an existing dictionary, it may 
be a clue that the string is likely a true word. 
Then more word boundary information can be 
obtained, which may be helpful for CRF learn- 
ing   on   CWS.   The   dictionary   we   used   is
①


C-1C1 are selected as features, where C stands for


downloaded from the Internet


and consists of


a character and the subscripts -1, 0 and 1 stand
for the previous, current and next character, re- 
spectively. Furthermore, another one is character
type  feature  template  T-1T0T1.  We  use  four 
classes of character sets which are predefined as: 
class N represents numbers, class L represents
non-Chinese letters, class P represents punctua- 
tion labels and class C represents Chinese char- 
acters.
  Except for the character feature, we also em- 
ploy global word feature templates. The basic 
idea of using global word information for CWS 
is to inform the supervised learner how likely it 
is that the subsequence can be a word candidate.


108,750 words with length of one to four char-
acters. We get FRE features similar to the AV
features.

2.2	HMM revises substrings with low mar- 
ginal probability

The MP (short for marginal probability) of each 
character labeled with one of the six tags can be 
got separately through the basic CRF tagger. Here, 
B replaces ‘B’ and ‘S’ , and I represents other 
tags (‘B2’, ‘B3’, ‘M’, ‘E’). So each character has 
corresponding new MP as defined in formula (3) 
and (4).


The accessor variety (AV) (Feng et al., 2005) is 
opted as global word feature, which is integrated 
into CRF successfully in literatures (Zhao and 
Kit, 2007b; Zhao and Kit, 2008). The AV value


P 	( PS
B 	∑


PB  )
Pt



(3)


of a substring  s  is defined as:


( P 	P 	PM
P 	2	3


PE )


(4)




AV (s)	min




Lav (s), Rav (s)




(1)


I

Where


∑ Pt
t 	S, B, B2 , B3 , M, E 	and




Pt  can  be


Where the left and right AV values


Lav (s)


calculated  by  using  forward-backward  
algorithm and more details are in Lafferty 
et al. (2001).


and


Rav 
(s)


are  
defin
ed,  
respe
ctivel
y,  as  
the


A 
low 
confi
dent 
word 
refer
s to 
a 
word 
with


number  of  its  distinct  predecessors  and  the
number of its distinct successors.
  Multiple feature templates are used to repre- 
sent word candidates of various lengths identi- 
fied by the AV criterion. Meanwhile, in order to 
alleviate the sparse data problem, we follow the 
feature function definition for a word candidate


word boundary ambiguity which can be reflected 
by the MP of the first character of a word. That 
is, it’s a low confident word if the MP of the first 
character of the word is lower than a threshold 
(it’s an empirical value and can be obtained
by experiments). After getting the new MP, all


s  with a score AV (s)
namely:


in Zhao and Kit (2008),


these low confident 
candidate words are 
recom-
bined  with  their  
direct  predecessors  
until  the occurrence 
of a word that the 
MP of its first


fn (s)


t ,  2


t	AV (s)
	2t  1


(2)


character is 
above the 
threshold	, and then a


  In order to improve the efficiency, all candi- 
dates longer than five characters are given up. 
The AV features of word candidates can’t di- 
rectly be utilized to direct CRF learning before 
being transferred to the information of characters. 
So we only choose the one with the greatest AV 
score to activate the above feature function for


new substring is generated for post processing.
  Then, we use class-based HMM to re-segment 
the substrings mentioned above. Given a word


①http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Inform 
ation%20Processing/Source_Code/Chapter_8/Lexicon_full. 
zip


wi, a word class ci is the word itself. Let  W  be 
the word sequence, let  C  be its class sequence,
W #


all punctuations) is half-width and the string 
be- fore or after are composed of letters and 
numbers,
combine all into a string as a whole. For an ex-


and let


be the 
segment
ation 
result 
with the


ample, 
‘.’, ‘/’, 
‘:’, ‘%’ 
and ‘\’ 
are 
usually 
recog-


maximum likelihood. Then, a class-based HMM
model (Liu, 2004) can be got.

W #	arg max P(W )
W
=  arg max P(W | C )P(C )
W
m


nized as split tokens. So, it needs handling addi- 
tionally.

3     Experiments results and analysis

We evaluate our system on the corpora given by 
CIPS-SIGHAN 2010. There are four test 
corpora which belong to different domains. 
The details


=  arg max
w1w2 ...wm

=  arg max
w1w2 ...wm


p '(wi | ci 
)P(ci | ci  1 )
i  1
m
P(ci | ci  1 )
i  1





(5)


are showed in 
table 1.


Where


P(ci  | ci  1 )


indicates  
the  
transitive


probability from one class to another and it can 
be obtained from training corpora.
The word boundary of results from HMM is
also represented by tag ‘B’ and ‘I’ which mean- 
ing are the same as mentioned in above.

2.3	Confidence measure and post process- 
ing for final result

There are two segmentation results for substrings 
with low MP candidates after reprocessing using 
HMM. Analyzing experiments data, we find 
wrong tags labeled by CRF are mainly: OOV 
words in test data, IV words and incorrect words 
recognized by CRF. Rectifying the tags with 
lower MP simply may produce an even worse 
performance in some case. For example, some 
OOV words are recognized correctly by CRF but 
with low MP. So, we can’t accept the revised 
results completely. A confidence measure ap- 
proach is used to resolve this problem. Its calcu- 
lation is defined as:




Table 1. Test corpora details
  A, B, C and D represent literature, computer 
science, medical science and finance, respec- 
tively.

3.1     Closed 
test

The rule for the closed test in Bakeoff is that no 
additional information beyond training corpora is 
allowed. Following the rule, the closed test is 
designed to compare our system with other CWS 
systems. Five metrics of SIGHAN Bakeoff are 
used to evaluate the segmentation results: F-score 
(F), recall (R), precision (P), the recall on IV 
words (RIV) and the recall on OOV words (Roov). 
The closed test results are presented in table 2.





PC	PC


(1  P  )
o


(6)


  PC   is the MP of the character as ‘I’,       is the 
premium coefficient. Based on the new value, a 
threshold  t  was used, if the value was lower 
than  t , the original tag ‘I’ will be rejected and 
changed into the tag ‘B’ which is labeled by 
HMM.









Table 2. Evaluation closed results on all data sets


At last, we use a simple rule to post-process the	 	


result directed at the strings that containing letters,


②  In order to analyze our results, we got value of R


from


numbers and punctuations. If the punctuation (not


the organizers because it can’t be obtained from the scoring 
system on http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/#.


  In each domain, the first line shows the results 
of our basic CRF segmenter and the second one 
shows the final results dealt with HMM through


confidence measure, which make it clear that 
using the confidence measure can improve the 
overall F-score by increasing value of R and P.



Do
ma
in
I
D
R
P
F
R
oo
v
R
IV


A
5
0.
94
5
0.
94
6
0.
94
6
0.
81
6
0.
95
4

o
u
r
0.
94
0
0.
94
2
0.
94
1
0.
64
9
0.
96
1

1
2
0.
93
7
0.
93
7
0.
93
7
0.
65
2
0.
95
8


B
o
u
r
0.
95
3
0.
95
0
0.
95
1
0.
82
7
0.
97
5

1
1
0.
94
8
0.
94
5
0.
94
7
0.
85
3
0.
96
5

1
2
0.
94
1
0.
94
0
0.
94
0
0.
75
7
0.
97
4


C
o
u
r
0.
94
2
0.
93
6
0.
93
9
0.
75
0
0.
96
5

1
8
0.
93
7
0.
93
4
0.
93
6
0.
76
1
0.
95
9

5
0.
94
0
0.
92
8
0.
93
4
0.
76
1
0.
96
2


D
o
u
r
0.
95
9
0.
96
0
0.
95
9
0.
82
7
0.
97
2

1
2
0.
95
7
0.
95
6
0.
95
7
0.
81
3
0.
97
1

9
0.
95
6
0.
95
5
0.
95
6
0.
85
7
0.
96
5
Table 3. Comparison our closed results with the top three in all test sets




  Next, we compare it with other top three sys- 
tems. From the table 3 we can see that our system 
achieves better performance on closed test. In 
contrast, the values of RIV of our method are su- 
perior to others’, which contributes to the model 
we use. Whether the features of AV for charac- 
ter-based CRF tagger or HMM revising, they all 
make good use of word information of training 
corpora.

3.2	Open test

In the open test, the only additional source we 
use is the dictionary mentioned above. We get 
one first and two third best. Our result is showed 
in table 4. Compared with closed test, the value 
of RIV  is increased in all test corpora. But we 
only get the higher value of F in domain of lit- 
erature. The reasons will be analyzed as follows: 
In the open test, the OOV words are split into 
pieces because our model may be more depend- 
ent on the dictionary information. Consequently, 
we get higher value of R but lower P. The train- 
ing corpora are the same as closed test, but it is 
different that FRE features are added. The addi- 
tional features enhance the original information 
of IV words, so the value of RIV  is improved to 
some extent. However, they have side effects for 
OOV segmentation. We will continue to solve


this problem in the future 
work.

Do
ma
in
R
P
F
R
oo
v
R
IV

A
0.
95
6
0.
94
7
0.
95
2
0.
63
6
0.
98
0

  
0.958 
	
0.95
3 	
0.95
5 	
0.65
5 	
0.9
81 	

B
0.
94
3
0.
92
1
0.
93
2
0.
71
6
0.
98
5

  
0.948 
	
0.92
9 	
0.93
9 	
0.73
5 	
0.9
86 	

C
0.
94
7
0.
91
5
0.
93
1
0.
65
9
0.
98
3

  
0.951 	
0.9
2 	
0.93
5 	
0.6
7 	
0.9
86 	

D
0.
96
2
0.
94
8
0.
95
5
0.
76
0
0.
98
1

  
0.964 	
0.9
5 	
0.95
7 	
0.76
3 	
0.9
83 	
Table 4. Evaluation open results on all test sets

4     Conclusions and future work

In this paper, a detailed description on a Chinese 
segmentation system is presented. Based on 
intermediate results from a CRF tagger, which 
employs local features and global features, we 
use class-based HMM to revise the substrings 
with low marginal probabilities. Then, a confi- 
dence measure is introduced to combine the two 
results.  Finally,  we  post  process  the  strings 
within letters, numbers and punctuations using 
simple rules. The results above show that our 
system achieves the state-of-the-art performance.


The MP plays the important role in our method 
and HMM revises some errors identified by CRF. 
Besides, the word features are proved to be in- 
formative cues in obtaining high quality MP. 
Therefore, our future work will focus on how to 
make CRF generate more reliable MP of char- 
acters, including exploring other word informa- 
tion or more unsupervised segmentation infor- 
mation.


References

Feng Haodi, Kang Chen, Chuyu Kit, Xiaotie Deng.
2005. Unsupervised segmentation of Chinese cor- 
pus using accessor variety, In: Natural Language 
Processing IJCNLP, pages 694-703, Sanya, China.

Lafferty John, Andrew McCallum and Fernando 
Pereira. 2001. Conditional Random Fields: prob- 
abilistic models for segmenting and labeling se- 
quence data, In: Proceedings of ICML-18, pages
282-289, Williams College, USA.

Liu Qun, Huaping Zhang, Hongkui Yu and Xueqi 
Chen. 2004. Chinese lexical analysis using cas- 
caded Hidden Markov Model, Journal of computer 
research and development 41(8): 1421-1429.

Low Kiat Jin, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word 
Segmentation. In: Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Proc- 
essing, pages 161-164, Jeju Island, Korea.

Peng Fuchun, Fangfang Feng and Andrew McCallum.
2004. Chinese segmentation and new word detec- 
tion using Conditional Random Fields, In: COL- 
ING 2004, pages 562-568, Geneva, Switzerland.

Tseng Huihsin, Pichuan Chang et al. 2005. A Condi- 
tional Random Field Word Segmenter for SIGHAN 
Bakeoff 2005. In: Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language Proc- 
essing, pages 168-171, Jeju Island, Korea.

Wang Zhenxing, Changning Huang and Jingbo Zhu.
2008. Which perform better on in-vocabulary word 
segmentation: based on word or character? In: 
Processing of the Sixth SIGHAN Workshop on 
Chinese Language Processing, pages 61-68, Hy- 
derabad, India.

Xue Nianwen. 2003. Chinese word segmentation as 
character tagging, Computational Linguistics and 
Chinese Language Processing 8(1): 29-48.

Zhang Yue and Stephen Clark. 2007. Chinese Seg- 
mentation with a Word-Based Perceptron Algo- 
rithm. In: Proceedings of the 45th Annual Meeting


of the Association for Computational Linguistics, 
pages 840-847, Prague, Czech Republic.

Zhang Ruiqiang, Genichiro Kikui and Eiichiro Sumita.
2006. Subword-based  tagging  by  Conditional 
Random Fields for Chinese word segmentation, In: 
Proceedings   of   the   Human   Language 
Technology  Conference  of  the  NAACL,  pages
193-196, New York, USA.

Zhao Hai, Changning Huang, Mu Li and Baoliang Lu.
2006. Effective tag set selection in Chinese word 
segmentation via Conditional Random Field mod- 
eling, In: PACLIC-20, pages 87-94, Wuhan, China.

Zhao Hai and Chunyu Kit. 2007a. Effective subse- 
quence based tagging for Chinese word segmenta- 
tion,  Journal of  Chinese Information Processing
21(5): 8-13.

Zhao Hai and Chunyu Kit. 2007b. Incorporating 
global information into supervised learning for 
Chinese word segmentation, In: PACLING-2007, 
pages 66-74, Melbourne, Australia.

Zhao Hai and Chunyu Kit. 2008. Unsupervised seg- 
mentation helps supervised learning of character 
tagging for word segmentation and named entity 
recognition, In: Proceedings of the Six SIGHAN 
Workshop on Chinese Language Processing, pages
106-111, Hyderabad, India.
