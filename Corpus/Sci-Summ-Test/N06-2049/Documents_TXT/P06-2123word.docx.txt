Subword-based Tagging for Confidence-dependent Chinese Word
Segmentation


Ruiqiang Zhang1,2 and Genichiro Kikui∗ and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp







Abstract

We proposed a subword-based tagging for 
Chinese  word segmentation  to improve 
the existing character-based tagging.  The 
subword-based tagging was implemented 
using the maximum entropy (MaxEnt) 
and the conditional random fields (CRF) 
methods. We  found that the proposed 
subword-based tagging outperformed the 
character-based tagging in all compara- 
tive  experiments.    In addition, we pro- 
posed a confidence  measure approach  to 
combine the results of a dictionary-based 
and a subword-tagging-based  segmenta- 
tion.    This approach  can produce an 
ideal tradeoff between  the in-vocaulary 
rate and out-of-vocabulary  rate. Our tech- 
niques were evaluated using the test data 
from Sighan Bakeoff 2005. We achieved 
higher F-scores  than the best results in 
three of the four corpora: PKU(0.951), 
CITYU(0.950) and MSR(0.971).

1   Introduction

Many approaches have been proposed  in Chinese 
word segmentation in the past decades.   Segmen- 
tation performance has been improved significantly, 
from the earliest maximal match (dictionary-based) 
approaches to HMM-based  (Zhang et al., 2003) ap- 
proaches and recent state-of-the-art  machine learn- 
ing approaches such as maximum  entropy  (Max- 
Ent) (Xue and Shen, 2003), support vector machine
∗ Now the second author is affiliated with NTT.


(SVM) (Kudo and Matsumoto, 2001), conditional 
random fields (CRF) (Peng and McCallum,  2004), 
and minimum  error rate training (Gao et al., 2004). 
By analyzing the top results in the first and second 
Bakeoffs, (Sproat and Emerson, 2003) and (Emer- 
son, 2005), we found the top results were produced 
by direct or indirect  use of so-called “IOB” tagging, 
which converts the problem of word segmentation 
into one of character tagging so that part-of-speech 
tagging  approaches can be used for word segmen- 
tation. This approach was also called “LMR” (Xue 
and Shen, 2003) or “BIES” (Asahara et al., 2005) 
tagging.  Under the scheme,  each character  of a 
word is labeled as ”B” if it is the first character of a 
multiple-character word, or ”I” otherwise, and ”O” 
if the character functioned  as an independent word.
For example, “全(whole) 北京市(Beijing city)” is
labeled as “全/O 北/B 京/I 市/I”. Thus, the training
data in word sequences are turned into IOB-labeled 
data in character sequences, which  are then used as 
the training data for tagging. For new test data, word 
boundaries  are determined  based on the results of 
tagging.

  While the IOB tagging approach has been widely 
used in Chinese word segmentation, we found that 
so far all the existing implementations were using 
character-based IOB tagging.  In this work we pro- 
pose a subword-based  IOB tagging, which assigns 
tags to a pre-defined lexicon subset consisting of the 
most frequent multiple-character words in addition 
to single Chinese characters. If only Chinese char- 
acters are used, the subword-based IOB tagging is 
downgraded to a character-based  one. Taking the
same example mentioned above, “全北京市” is la-








961

Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961–968, 
Sydney, July 2006. Qc 2006 Association for Computational Linguistics


beled as “全/O 北京/B 市/I” in the subword-based 
tagging, where “北京/B” is labeled as one unit. We
will give a detailed description  of this approach in
Section 2.
  There exists a clear weakness with the IOB tag- 
ging approach:  It yields a very low in-vocabulary 
rate (R-iv) in return for a higher  out-of-vocabulary 
(OOV) rate (R-oov).  In the results of the closed 
test in Bakeoff 2005 (Emerson,  2005), the work 
of (Tseng et al., 2005), using CRFs for the IOB tag- 
ging, yielded  a very high R-oov in all of the four 
corpora used, but the R-iv rates were lower. While 
OOV recognition is very important in word segmen- 
tation, a higher IV rate is also desired. In this work 
we propose a confidence measure approach to lessen 
this weakness. By this approach we can change the 
R-oov and R-iv and find an optimal  tradeoff. This 
approach will be described in Section 2.3.
  In addition, we illustrate our word segmentation 
process in Section 2, where the subword-based tag- 
ging is described by the MaxEnt method. Section 3 
presents our experimental  results. The effects using 
the MaxEnts  and CRFs are shown in this section. 
Section 4 describes current state-of-the-art methods 
with Chinese word segmentation, with which our re- 
sults were compared.  Section 5 provides the con-


input
咘㣅᯹ԣ೼࣫ҀᏖ
+XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJ-FLW\



Dictionary-based word segmentation
咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ
+XDQJ <LQJ &KXQ OLYHV LQ %HLMLQJ-FLW\




Subword-based IOB tagging
咘/%/0.94 㣅/,/0.98 ᯹/,/0.91 ԣ/2/0.9 ೼/2/0.89 ࣫Ҁ/%/0.94 Ꮦ/,/0.98
+XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/,


Confidence-based disambiguation
咘/%/0.75 㣅/,/0.78 ᯹/,/0.73 ԣ/2/0.92 ೼/2/0.91 ࣫Ҁ/%/0.95 Ꮦ/,/0.98
+XDQJ/% <LQJ/, &KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/,


output
咘㣅᯹ ԣ ೼ ࣫ҀᏖ
+XDQJ<LQJ&KXQ OLYHV LQ %HLMLQJ-FLW\


Figure 1: Outline of word segmentation process



W = wt0 wt1 wt2 . . . wtM , which satisfies

wt0 = c0 . . . ct0 ,  wt1 = ct0 +1 . . . ct1
wti  = cti−1 +1 . . . cti ,  wtM  = ctM−1 +1 . . . ctM
ti > ti−1,  0 ≤ ti ≤ N,  0 ≤ i ≤ M

such that


W = arg max P(W |C) = arg max P(W )P(C|W )


cluding remarks and outlines future goals.	W	W
= arg max P(wt0 wt1 . . . wtM )δ(c0 . . . ct0 , wt0 )
W


2	Chinese word segmentation framework



δ(ct +1 . . . ct	t



t	+1	M	t


Our word segmentation   process  is illustrated in
Fig. 1. It is composed of three parts:  a dictionary-


0	1 , w 1 ) . . . δ(c M−1


. . . c


, w M )
(1)


based N-gram  word segmentation  for segmenting IV 
words,  a maximum  entropy  subword-based tagger 
for recognizing  OOVs, and a confidence-dependent 
word disambiguation  used for merging the results 
of both the dictionary-based  and the IOB-tagging- 
based. An example exhibiting  each step’s results is 
also given in the figure.

2.1	Dictionary-based N-gram word 
segmentation

This approach can achieve a very high R-iv, but no 
OOV detection. We combined with it the N-gram 
language model (LM) to solve segmentation ambi- 
guities.  For a given Chinese character sequence, 
C = c0c1c2 . . . cN , the problem of word segmenta- 
tion can be formalized  as finding  a word sequence,


We applied Bayes’ law in the above derivation.
Because the word sequence must keep consistent 
with the character  sequence, P(C|W ) is expanded 
to be a multiplication  of a Kronecker  delta function 
series, δ(u, v), equal to 1 if both arguments are the 
same and 0 otherwise. P(wt0 wt1 . . . wtM ) is a lan- 
guage model that can be expanded by the chain rule. 
If trigram LMs are used, we have

P(w0)P(w1|w0)P(w2|w0w1) · · · P(wM |wM−2wM−1)

where wi is a shorthand for wti .
Equation 1 indicates the process of dictionary-
based word segmentation.  We looked up the lexicon 
to find all the IVs, and evaluated the word sequences 
by the LMs. We used a beam search (Jelinek, 1998) 
instead of a viterbi search to decode the best word


sequence because we found  that a beam search can 
speed up the decoding.  N-gram LMs were used to 
score all the hypotheses, of which the one with the 
highest LM scores is the final output. The exper- 
imental  results are presented in Section 3.1, where 
we show the comparative results as we changed the 
order of LMs.

2.2   Subword-based IOB tagging

There are several steps to train a subword-based  IOB 
tagger. First, we extracted a word list from the train- 
ing data sorted in decreasing order by their counts 
in the training data. We chose all the single charac- 
ters and the top multi-character  words as a lexicon 
subset for the IOB tagging. If the subset consists of 
Chinese characters only, it is a character-based  IOB 
tagger. We regard the words in the subset as the sub- 
words for the IOB tagging.
  Second, we re-segmented the words in the train- 
ing data into subwords  of  the subset, and as- 
signed IOB  tags to  them.    For the character- 
based  IOB tagger, there is only one possibility 
for re-segmentation. However, there are  multi-
ple choices  for the subword-based  IOB　tagger.
For example,  “北 京 市(Beijing-city)” can    be
segmented         as   “北 京 市(Beijing-city)/O,”   or
“北 京(Beijing)/B    市(city)/I,”   or  ”北(north)/B
京(capital)/I 市(city)/I.” In this work we used for-
ward maximal match (FMM) for disambiguation. 
Because we carried out FMMs  on each words in the 
manually  segmented training  data, the accuracy of 
FMM was much higher than applying  it on whole 
sentences.    Of course,  backward  maximal match 
(BMM) or other approaches are also applicable.  We 
did not conduct comparative experiments due to triv- 
ial differences in the results of these approaches.
  In the third step, we used the maximum  entropy 
(MaxEnt)  approach (the results of CRF are given in 
Section 3.4) to train the IOB tagger (Xue and Shen,
2003). The mathematical expression for the MaxEnt 
model is


word and tag sequences;  fi, a binary  feature equal 
to 1 if the i-th defined feature is activated and 0 oth- 
erwise; Z, a normalization  coefficient;   and λi, the 
weight of the i-th feature.
  Many kinds of features can be defined for improv- 
ing the tagging accuracy.  However, to conform to 
the constraints of closed test in Bakeoff 2005, some 
features, such as syntactic information  and character 
encodings for numbers and alphabetical characters, 
are not allowed. Therefore, we used the features 
available only from the provided training corpus.

• Contextual information: w0, t−1, w0t−1, w0t−1w1, 
t−1w1, t−1t−2, w0t−1t−2, w0w1, w0w1w2, w−1, 
w0w−1, w0w−1w1,
w−1w1, w−1w−2, w0w−1w−2, w1, w1w2
where w stands for word and t, for IOB 
tag.
The subscripts are position  indicators,  where
0 means the current word/tag; −1, −2, the first 
or second word/tag to the left; 1, 2, the first or 
second word/tag to the right.

• Prefixes and suffixes.  These are very useful fea- 
tures. Using the same approach  as in (Tseng 
et al., 2005), we extracted the most frequent 
words tagged with “B”, indicating a prefix, and 
the last words tagged with “I”, denoting  a suf- 
fix.  Features containing  prefixes and suffixes 
were used in the following combinations with 
other features, where p stands for prefix; s, suf- 
fix; p0 means the current word is a prefix and 
s1  denotes that the right first word is a suffix, 
and so on.
p0, w0 p−1, w0 p1, s0, w0 s−1, w0 s1,
p0w−1, p0w1, s0w−1, s0w−2

• Word length. This is defined  as the number 
of characters in a word. The length of a Chi- 
nese  word has discriminative   roles for word 
composition.	For example,  single-character 
words  are more apt to form new words than 
are multiple-character  words. Features using 
word length are listed below, where l0  means


		the word length of the current word. Others can


P(t|h) = exp      λi fi(h, t) /Z,   Z =      P(t|h)  
(2)


be inferred similarly.



	



l0, w0l


, w l , w l


l , l l


, l l


i 	t	−1	0 1


0 −1 1


0 −1   0 1





  where t is a tag, “I,O,B,” of the current word; h, 
the context surrounding the current word, including
  

As to feature selection, we simply adopted the ab- 
solute count for each feature in the training  data as


the metric,  and defined a cutoff value for each fea- 
ture type.
  We used IIS to train the maximum entropy model. 
For details, refer to (Lafferty et al., 2001).
  The tagging algorithm is based  on the beam- 
search method (Jelinek, 1998). After the IOB tag- 
ging, each word is tagged with a B/I/O tag. The 
word segmentation is obtained immediately. The 
experimental effect of the word-based tagger and 
its comparison with the character-based tagger are 
made in section 3.2.

2.3   Confidence-dependent word segmentation

In the last two steps we produced two segmentation 
results: the one by the dictionary-based  approach 
and the one by the IOB tagging. However, nei- 
ther was perfect. The dictionary-based segmenta- 
tion produced a result with a higher  R-iv but lower 
R-oov while the IOB tagging yielded the contrary 
results. In this section we introduce  a confidence 
measure approach to combine the two results. We 
define a confidence measure, C M(tiob|w), to measure 
the confidence of the results produced by the IOB 
tagging by using the results from the dictionary- 
based segmentation.  The confidence measure comes 
from two sources: IOB tagging and dictionary-based 
word segmentation. Its calculation is defined as:



C M(tiob|w) = αC Miob(tiob|w) + (1 − α)δ(tw, tiob)ng
                                          (3) 
where tiob is the word w’s IOB tag assigned by the 
IOB tagging; tw, a prior IOB tag determined by the 
results of the dictionary-based segmentation.  After 
the dictionary-based word segmentation, the words 
are re-segmented into subwords by FMM before be- 
ing fed to IOB tagging. Each subword is given a 
prior IOB tag, tw. C Miob(t|w), a confidence proba- 
bility derived in the process of IOB tagging, which
is defined as

C M	(t w) = 	 hi P(t|w, hi)
 t   hi P(t|w, hi)

where hi   is  a  hypothesis   in  the beam search. 
δ(tw, tiob)ng    denotes the   contribution  of   the 
dictionary-based segmentation.
δ(tw, tiob)ng is a Kronecker delta function defined


as
δ(t  , t	)	= 	1   if tw = tiob
0   otherwise


  In Eq. 3, α is a weighting  between the IOB tag- 
ging and the dictionary-based word segmentation. 
We found an empirical value 0.8 for α.
  By Eq. 3 the results of IOB tagging were re- 
evaluated.  A confidence measure threshold, t, was 
defined for making  a decision  based on the value. 
If the value was lower than t, the IOB tag was re- 
jected and the dictionary-based  segmentation was 
used; otherwise, the IOB tagging segmentation was 
used. A new OOV was thus created.  For the two 
extreme  cases, t  =  0 is the case of the IOB tag- 
ging while t = 1 is that of the dictionary-based ap- 
proach. In Section 3.3 we will present the experi- 
mental segmentation results of the confidence mea- 
sure approach. In a real application,  we can actually 
change the confidence threshold to obtain a satisfac- 
tory balance between R-iv and R-oov.
  An example is shown in Figure 1. In the stage of 
IOB tagging, a confidence is attached to each word. 
In the stage of confidence-based, a new confidence 
was made after merging with dictionary-based re- 
sults where all single-character  words are labeled 
as “O” by default except “Beijing-city” labeled  as 
“Beijing/B” and “city/I”.


3   Experiments

We used the data provided by Sighan Bakeoff 2005 
to test our approaches described in the previous sec- 
tions.  The data contain  four corpora from differ- 
ent sources:  Academia  sinica, City University of 
Hong Kong, Peking University and Microsoft Re- 
search (Beijing). The statistics concerning the cor- 
pora is listed in Table 3. The corpora provided both 
unicode coding and Big5/GB coding. We used the 
Big5 and CP936 encodings. Since the main purpose 
of this work is to evaluate the proposed subword- 
based IOB tagging, we carried out the closed test 
only.  Five metrics were used to evaluate the seg- 
mentation results: recall (R), precision (P), F-score 
(F), OOV rate (R-oov) and IV rate (R-iv). For a de- 
tailed explanation of these metrics, refer to (Sproat 
and Emerson, 2003).



Co
rp
us
Ab
bre
v.
En
co
din
gs
Tra
inin
g 
siz
e 
(w
or
ds)
Tes
t 
siz
e 
(w
or
ds)
Ac
ad
em
ia 
Si
nic
a
AS
Bi
g5/
Un
ico
de
5.4
5M
12
2K
Bei
jin
g 
Un
ive
rsit
y
PK
U
CP
93
6/
Un
ico
de
1.1
M
10
4K
Cit
y 
Uni
ver
sity 
of 
Ho
ng 
Ko
ng
CI
TY
U
Bi
g5/
Un
ico
de
1.4
6M
41
K
Mi
cro
sof
t 
Re
sea
rch 
(B
eiji
ng
)
M
SR
CP
93
6/
Un
ico
de
2.3
7M
10
7K

Table 1: Corpus statistics in Sighan Bakeoff 2005




3.1   Effects of N-gram LMs

We obtained a word list from  the training  data as the 
vocabulary for dictionary-based  segmentation.   N- 
gram LMs were generated using the SRI LM toolkit. 
Table 2 shows the performance of N-gram segmen- 
tation by changing the order of N-grams.
  We found that bigram LMs can improve segmen- 
tation over unigram, though we observed no effect 
from the trigram LMs. For the PKU corpus, there 
was a relatively strong improvement due to using bi- 
grams rather than unigrams, posssibly because the 
PKU corpus’ training size was smaller than the oth- 
ers. For a sufficiently  large training corpus, the un- 
igram LMs may be enough for segmentation.  This 
experiment revealed that language models above bi- 
grams do not improve word segmentation.   Since 
there were some single-character  words present in 
test data but not in the training data, the R-oov rates 
were not zero in this experiment. In fact, we did not 
use any OOV detection for the dictionary-based ap- 
proach.

3.2   Comparisons of Character-based and
Subword-based tagger

In Section 2.2 we described the character-based and 
subword-based IOB tagging methods. The main dif- 
ference between the two is the lexicon  subset used 
for re-segmentation.   For the subword-based IOB 
tagging,  we need to add some multiple-character 
words into the lexicon subset. Since it is hard to 
decide the optimal number of words to add, we test 
three different  lexicon sizes,  as shown  in Table 3. 
The first one, s1, consisting  of all the characters, is 
a character-based approach.  The second, s2, added
2,500 top words from the training  data to the lexi- 
con of s1. The third, s3, added another 2,500 top 
words to the lexicon of s2.  All  the words were 
among the most frequent in the training corpora. Af- 
ter choosing the subwords, the training  data were re- 
segmented using the subwords by FMM. The final




A
S
CI
TY
U
M
S
R
P
K
U
s1
6,
08
7
4,
9
1
6
5,1
50
4,6
85
s2
8,
33
2
7,
3
3
8
7,4
64
7,0
14
s3
10,
87
6
9,
9
9
6
9,9
90
9,0
53

Table 3:  Three different vocabulary  sizes used in subword- 
based tagging.  s1 contains all the characters. s2 and s3 contains 
some common words.


lexicons were collected again, consisting of single- 
character words and multiple-character  words. Ta- 
ble 3 shows the sizes of the final lexicons. There- 
fore, the minus of the lexicon size of s2 to s1 are not
2,500, exactly.
  The segmentation results of using three lexicons 
are shown  in Table 4.  The numbers  are separated 
by a “/” in the sequence of “s1/s2/s3.” We found al- 
though the subword-based approach outperformed 
the character-based one significantly,  there was no 
obvious difference between the two subword-based 
approaches, s2 and s3, adding respective 2,500 and
5,000 subwords to s1. The experiments show that 
we cannot find an optimal  lexicon size from 2,500 
to 5,000. However, there might be an optimal point 
less than 2,500. We did not take much effort to find 
the optimal  point, and regarded 2,500 as an accept- 
able size for practical usages.
  The F-scores of IOB tagging shown in Table 4 are 
better than that of N-gram word segmentation in Ta- 
ble 2, which proves that the IOB tagging is effective 
in recognizing OOV. However, we found there was a 
large decrease in the R-ivs, which shows the weak- 
ness of the IOB tagging approach. We use the con- 
fidence measure approach to deal with this problem 
in next section.

3.3   Effects of the confidence measure

Up to now we had two segmentation results by using 
the dictionary-based  word segmentation and the IOB 
tagging. In Section 2.3, we proposed a confidence 
measure approach to re-evaluate the results of IOB 
tagging by combining the two results. The effects of




R
P
F
R
-
o
o
v
R
-
i
v
A
S
0.9
34/
0.9
42/
0.9
41
0.8
84/
0.8
81/
0.8
81
0.9
09/
0.9
10/
0.9
10
0.0
41/
0.0
40/
0.0
38
0.9
75/
0.9
83/
0.9
82
CI
TY
U
0.9
24/
0.9
29/
0.9
28
0.8
51/
0.8
51/
0.8
51
0.8
86/
0.8
88/
0.8
88
0.1
62/
0.1
62/
0.1
64
0.9
84/
0.9
90/
0.9
89
P
K
U
0.9
38/
0.9
49/
0.9
48
0.9
09/
0.9
12/
0.9
12
0.9
24/
0.9
30/
0.9
30
0.4
07/
0.4
03/
0.4
08
0.9
71/
0.9
82/
0.9
81
M
S
R
0.9
65/
0.9
69/
0.9
68
0.9
27/
0.9
27/
0.9
27
0.9
46/
0.9
47/
0.9
47
0.0
36/
0.0
36/
0.0
48
0.9
91/
0.9
94/
0.9
93

Table 2: Segmentation results of dictionary-based  segmentation in closed test of Bakeoff 2005. A “/”  separates the results of 
unigram, bigram and trigram.



R
P
F
R
-
o
o
v
R
-
i
v
A
S
0.9
22/
0.9
42/
0.9
43
0.9
14/
0.9
30/
0.9
30
0.9
18/
0.9
36/
0.9
37
0.6
41/
0.6
28/
0.6
09
0.9
35/
0.9
56/
0.9
59
CI
TY
U
0.9
06/
0.9
33/
0.9
34
0.9
05/
0.9
29/
0.9
27
0.9
06/
0.9
31/
0.9
30
0.6
68/
0.6
71/
0.6
71
0.9
25/
0.9
54/
0.9
55
P
K
U
0.9
13/
0.9
34/
0.9
36
0.9
22/
0.9
38/
0.9
40
0.9
18/
0.9
36/
0.9
38
0.7
44/
0.7
24/
0.7
13
0.9
24/
0.9
46/
0.9
49
M
S
R
0.9
29/
0.9
53/
0.9
53
0.9
34/
0.9
55/
0.9
52
0.9
32/
0.9
54/
0.9
52
0.6
56/
0.6
84/
0.6
65
0.9
36/
0.9
61/
0.9
61

Table 4: Segmentation results by the pure subword-based IOB tagging. The separator “/” divides the results by three lexicon sizes 
as illustrated in Table 3. The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3).




0.94

0.95

0.96

0.97

0.98

0.99

1




t=0





t=0
     t=0 t=0











t=1



   AS CITYU 
PKU MSR







t=0


 	






 t=1 t=1


iv’s 
varing  
as the 
thresho
ld  is 
shown 
in Fig. 
2, 
where 
R-oovs 
and R-
ivs are 
moving 
in 
differe
nt 
directi
ons. 
When 
the 
confide
nce 
thresh
old t 
= 0, 
the 
case 
for the 
IOB 
taggin
g, R-
oovs 
are 
maxim
al.  
When 
t = 1, 
repres
enting 
the 
diction
ary-
based 
segme
ntation
, R- 
oovs 
are the 
minim
al. The 
R-oovs 
and R-
ivs 
varied 
largely 
at the 
start 
and 
end 
point 
but 
little 
around 
the 
middle 
sectio
n.

3
.
4
   
S
u
b
w
o
r
d
-
b
a
s
e
d
 
t
a
g
g
i
n
g
 
b
y
 
C
R
F
s


0.8


0.7


0.6


0.5


0.4
R-oov


0.3


0.2


0.1 	0



Our 
proposed 
approach
es were 
presented 
and eval-


Figure 2: R-iv and R-oov varing as the confidence threshold, t.


the confidence measure are shown in Table 5, where 
we used α = 0.8 and confidence threshold t = 0.7. 
These are empirical  numbers. We obtained the opti- 
mal values by multiple trials on held-out data. The 
numbers in the slots of Table 5 are divided by a sep- 
arator “/” and displayed  as the sequence “s1/s2/s3”, 
just as Table 4. We found that the results in Table 5 
were better than those in Table 4 and Table 2, which 
proved that using the confidence measure approach 
yielded the best performance over the N-gram seg- 
mentation and the IOB tagging approaches.
  Even with the use of the confidence measure, the 
subword-based IOB tagging still outperformed the 
character-based IOB tagging, proving that the pro- 
posed subword-based  IOB tagging was very effec- 
tive. Though the improvement under the confidence 
measure was decreasing, it was still significant.
  We can change the R-oov and R-iv by changing 
the confidence threshold. The effect of R-oov and R-


uated  using the MaxEnt method  in the previous 
sections. When we turned to CRF-based tagging, 
we found a  same  effect as  the MaxEnt method. 
Our subword-based tagging  by CRFs was imple- 
mented  by the package  “CRF++” from the site 
“http://www.chasen.org/˜taku/software.”
  We repeated the previous sections’ experiments 
using the CRF approach except that we did one of 
the two subword-based tagging, the lexicon size s3. 
The same values of the confidence measure thresh- 
old and α were used. The results are shown in Ta- 
ble 6.
  We found that the results using the CRFs were 
much better than those  of the MaxEnts.  How- 
ever, the emphasis here was not to compare CRFs 
and MaxEnts  but the effect of subword-based IOB 
tagging.  In Table 6, the results  before ”/”  are 
the character-based IOB tagging and after ”/”,  the 
subword-based. It was clear that the subword-based 
approaches yielded better results than the character- 
based approach though the improvement  was not as 
higher as that of the MaxEnt approaches. There was




R
P
F
R
-
o
o
v
R
-
i
v
A
S
0.9
38/
0.9
50/
0.9
53
0.9
45/
0.9
46/
0.9
51
0.9
41/
0.9
48/
0.9
48
0.6
74/
0.6
41/
0.6
06
0.9
50/
0.9
64/
0.9
69
CI
TY
U
0.9
32/
0.9
49/
0.9
46
0.9
44/
0.9
33/
0.9
44
0.9
38/
0.9
41/
0.9
45
0.7
05/
0.5
97/
0.6
67
0.9
50/
0.9
77/
0.9
68
P
K
U
0.9
41/
0.9
48/
0.9
49
0.9
45/
0.9
47/
0.9
47
0.9
43/
0.9
48/
0.9
48
0.6
72/
0.6
62/
0.6
60
0.9
58/
0.9
66/
0.9
66
M
S
R
0.9
44/
0.9
59/
0.9
61
0.9
59/
0.9
64/
0.9
63
0.9
51/
0.9
61/
0.9
62
0.6
71/
0.6
74/
0.6
31
0.9
51/
0.9
67/
0.9
70

Table 5: Effects of combination  using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The 
separator “/” divides the results of s1, s2, and s3.




no change on F-score for AS corpus, but a better re- 
call rate was found. Our results are better than the 
best one of Bakeoff 2005 in PKU, CITYU and MSR 
corpora.
Detailed descriptions about subword tagging by
CRF can be found in our paper (Zhang et al., 2006).

4   Discussion and Related works

The IOB tagging approach adopted in this work is 
not a new idea. It was first implemented in Chi- 
nese word segmentation by (Xue and Shen, 2003) 
using the maximum entropy methods. Later, (Peng 
and McCallum, 2004) implemented  the idea us- 
ing the CRF-based approach,  which yielded bet- 
ter results than the maximum entropy approach be- 
cause it could solve the label bias problem (Laf- 
ferty et al., 2001). However,  as we mentioned be- 
fore, this approach does not take advantage of the 
prior knowledge  of in-vocabulary  words; It pro- 
duced a higher R-oov but a lower R-iv. This prob- 
lem has been observed by some participants  in the 
Bakeoff 2005 (Asahara et al., 2005), where they 
applied the IOB tagging to recognize OOVs, and 
added the OOVs to the lexicon  used in the HMM- 
based or CRF-based approaches. (Nakagawa, 2004) 
used hybrid HMM models to integrate word level 
and character  level information seamlessly. We 
used confidence measure to determine  a better bal- 
ance between  R-oov and R-iv.   The idea of us- 
ing the confidence  measure has appeared in (Peng 
and McCallum,  2004), where it was used to recog- 
nize the OOVs. In this work we used it more than 
that. By way of the confidence measure we com- 
bined results from the dictionary-based and the IOB- 
tagging-based and as a result, we could achieve the 
optimal performance.
  Our main contribution is to extend the IOB tag- 
ging approach from being a character-based   to a 
subword-based one. We proved that the new ap- 
proach enhanced  the word segmentation  signifi-


cantly in all the experiments, MaxEnts, CRFs and 
using confidence measure. We tested our approach 
using the standard Sighan Bakeoff 2005 data set in 
the closed test. In Table 7 we align our results with 
some top runners’ in the Bakeoff 2005.
  Our results were compared with the best perform- 
ers’ results in the Bakeoff 2005. Two participants’ 
results were chosen as bases: No.15-b,  ranked the 
first in the AS corpus,  and No.14, the best per- 
former in CITYU, MSR and PKU. .  The No.14 
used CRF-modeled IOB tagging while No.15-b used 
MaxEnt-modeled IOB tagging.  Our results  pro- 
duced by the MaxEnt are denoted  as “ours(ME)” 
while “ours(CRF)” for the CRF approaches.   We 
achieved the highest F-scores in three corpora ex- 
cept the AS corpus. We think the proposed subword- 
based approach  played  the important role for the 
achieved good results.
  A second advantage  of the subword-based IOB 
tagging over the character-based is its speed.  The 
subword-based  approach  is faster because  fewer 
words than characters needed to be labeled. We ob- 
served a speed increase in both training and testing. 
In the training  stage, the subword approach was al- 
most two times faster than the character-based.



5   Conclusions


In this work, we proposed a subword-based IOB tag- 
ging method for Chinese word segmentation.  The 
approach outperformed the character-based method 
using both the MaxEnt and CRF approaches.  We 
also successfully employed the confidence measure 
to make a confidence-dependent word segmentation. 
By setting the confidence threshold, R-oov and R-iv 
can be changed accordingly.  This approach is effec- 
tive for performing  desired segmentation based on 
users’ requirements to R-oov and R-iv.




R
P
F
R
-
o
o
v
R
-
i
v
A
S
0.9
53/
0.9
56
0.9
44/
0.9
47
0.9
48/
0.9
51
0.6
07/
0.6
49
0.9
69/
0.9
69
CI
TY
U
0.9
43/
0.9
52
0.9
48/
0.9
49
0.9
46/
0.9
51
0.6
82/
0.7
41
0.9
64/
0.9
69
P
K
U
0.9
42/
0.9
47
0.9
57/
0.9
55
0.9
49/
0.9
51
0.7
75/
0.7
48
0.9
52/
0.9
59
M
S
R
0.9
60/
0.9
72
0.9
66/
0.9
69
0.9
63/
0.9
71
0.6
74/
0.7
12
0.9
67/
0.9
76

Table 6: Effects of using CRF. The separator “/” divides the results of s1, and s3.





Par
tici
pa
nts
R
P
F
R-
oo
v
R-
iv
Hong Kong 
City 
University
ou
rs(
C
R
F)
0.9
52
0.9
49
0.9
51
0.
74
1
0.9
69
o
ur
s(
M
E)
0.9
46
0.9
44
0.9
45
0.
66
7
0.9
68
1
4
0.9
41
0.9
46
0.9
43
0.
69
8
0.9
61
1
5
-
b
0.9
37
0.9
46
0.9
41
0.
73
6
0.9
53
A
c
a
d
e
m
i
a
 
S
i
n
i
c
a
1
5
-
b
0.9
52
0.9
51
0.9
52
0.
69
6
0.9
63
ou
rs(
C
R
F)
0.9
56
0.9
47
0.9
51
0.
64
9
0.9
69
o
ur
s(
M
E)
0.9
53
0.9
43
0.9
48
0.
60
8
0.9
69
1
4
0.
95
0.9
43
0.9
47
0.
71
8
0.9
60
M
i
c
r
o
s
o
f
t
 
R
e
s
e
a
r
c
h
ou
rs(
C
R
F)
0.9
72
0.9
69
0.9
71
0.
71
2
0.9
76
1
4
0.9
62
0.9
66
0.9
64
0.
71
7
0.9
68
o
ur
s(
M
E)
0.9
61
0.9
63
0.9
62
0.
63
1
0.9
70
1
5
-
b
0.9
52
0.9
64
0.9
58
0.
71
8
0.9
58
P
e
k
i
n
g
 
U
n
i
v
e
r
s
i
t
y
ou
rs(
C
R
F)
0.9
47
0.9
55
0.9
51
0.
74
8
0.9
59
1
4
0.9
46
0.9
54
0.9
50
0.
78
7
0.9
56
o
ur
s(
M
E)
0.9
49
0.9
47
0.9
48
0.
66
0
0.9
66
1
5
-
b
0.
93
0.9
51
0.9
41
0.
7
6
0.9
41

Table 7: List of results in Sighan Bakeoff 2005



Acknowledgements

The authors thank the reviewers for the comments 
and advice on the paper. Some related software for 
this work will be released very soon.


References

Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi- 
Ling Goh, Yotaro Watanabe,  Yuji  Matsumoto,  and 
Takashi Tsuzuki. 2005.  Combination of machine 
learning methods  for  optimum chinese  word seg- 
mentation. In Forth SIGHAN Workshop on Chinese 
Language Processing, Proceedings of the Workshop, 
pages 134–137, Jeju, Korea.

Thomas Emerson.  2005. The second international  chi- 
nese word segmentation bakeoff. In Proceedings of 
the Fourth SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea.

Jianfeng Gao, Andi Wu, Mu Li,  Chang-Ning  Huang, 
Hongqiao Li, Xinsong Xia, and Haowei Qin.  2004. 
Adaptive  chinese word segmentation.   In ACL-2004, 
Barcelona, July.


Frederick Jelinek. 1998. Statistical methods for speech 
recognition. the MIT Press.

Taku Kudo and Yuji Matsumoto.  2001. Chunking with 
support vector machine. In Proc. of NAACL-2001, 
pages 192–199.

John Lafferty,  Andrew McCallum,  and Fernando Pereira.
2001. Conditional random fields: probabilistic models 
for segmenting and labeling  sequence data. In Proc. of 
ICML-2001, pages 591–598.

Tetsuji Nakagawa.   2004. Chinese and japanese word 
segmentation using word-level  and character-level in- 
formation. In Proceedings  of Coling  2004, pages 466–
472, Geneva, August.

Fuchun  Peng and Andrew McCallum. 2004. Chinese 
segmentation  and new word detection using condi- 
tional random fields. In Proc. of Coling-2004,  pages
562–568, Geneva, Switzerland.

Richard Sproat and Tom Emerson. 2003. The first inter- 
national chinese word segmentation bakeoff. In Pro- 
ceedings of the Second SIGHAN Workshop on Chinese 
Language Processing, Sapporo, Japan, July.

Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel 
Jurafsky, and Christopher Manning. 2005. A condi- 
tional random field word segmenter for Sighan bake- 
off 2005. In Proceedings of the Fourth SIGHAN Work- 
shop on Chinese Language Processing, Jeju, Korea.

Nianwen Xue and Libin Shen.  2003.  Chinese word 
segmentation  as LMR tagging. In Proceedings of the 
Second SIGHAN  Workshop on Chinese Language Pro- 
cessing.

Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003.  HHMM-based  Chinese lexical analyzer ICT- 
CLAS. In Proceedings of the Second SIGHAN Work- 
shop on Chinese Language  Processing,  pages 184–
187.

Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random 
fields for chinese word segmentation. In Proc. of HLT- 
NAACL.

