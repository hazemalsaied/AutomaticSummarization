Chinese Segmentation with a Word-Based Perceptron Algorithm



Yue Zhang and Stephen Clark Oxford 
University Computing Laboratory 
Wolfson Building, Parks Road
Oxford OX1 3QD, UK
{yue.zhang,stephen.clark}@comlab.ox.ac.uk








Abstract

Standard approaches to Chinese word seg- 
mentation treat the problem as a tagging 
task, assigning labels to the characters in 
the sequence indicating whether the char-


of characters which have themselves been seen 
as words; here an automatic segmentor may 
split the O OV  word into individual single-
character words. Typical examples of unseen 
words include Chinese names, translated 
foreign names and idioms.
The segmentation of known words can also 
be


acter marks a word boundary.  Discrimina-


ambiguous. For example, “


” should be “


tively trained models based on local char-


(here)


(flour)” in the s             “


acter features are used to make the tagging


” (flour and rice are expensive here) or “


(here)


decisions, with Viterbi decoding finding the


(inside)” in the sentence “


  ” (it’s


highest scoring segmentation. In this paper



we propose an alternative, word-based seg- 
mentor, which uses features based on com-


cold inside here).  The ambiguity can be 
resolved
with information about the neighboring words.  
In comparison, for the sentences “	”,


plete words and word sequences. The gener-


possible segmentations include


� *    m t -


“	(the discus


alized perceptron algorithm is used for dis-


sion)


(will)


(very)


(b � * essful)” and


criminative training, and we use a beam-


“	   (the d       ssion


mt
meetin


(very)


(be


search decoder. Closed tests on the first and


s�*   l)”.  The ambiguity ca        ly be mt   
ed


uccessfu


n on


resolv


second S I G H A N bakeoffs show that our sys-
tem is competitive with the best in the litera- 
ture, achieving the highest reported F-scores 
for a number of corpora.

1   Introduction

Words are the basic units to process for most N L P 
tasks.  The problem of Chinese word segmentation 
(C W S) is to find these basic units for a given sen- 
tence, which is written as a continuous sequence of 
characters. It is the initial step for most Chinese pro- 
cessing applications.


with contextual information outside the sentence. 
Human readers often use semantics, contextual in- 
formation about the document and world knowledge 
to resolve segmentation ambiguities.
  There is no fixed standard for Chinese word seg- 
mentation.   Experiments have shown that there is 
only about 75% agreement among native speakers 
regarding the correct word segmentation (Sproat et 
al., 1996). Also, specific N L P tasks may require dif- 
ferent segmentation criteria. For example, “
” could be treated as a single word (Bank   
 
jing) for machine translation, while it is more natu-


Chinese character sequences are ambiguous, of-


rally segmented into “


(Beijing)


(bank)”


ten requiring knowledge from a variety of sources 
for disambiguation. Out-of-vocabulary (O OV) words 
are a major source of ambiguity.  For example, a 
difficult case occurs when an O OV  word consists
840


for tasks such as text-to-speech synthesis.  There-
fore, supervised learning with specifically defined 
training data has become the dominant approach.
Following Xue (2003), the standard approach to


Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 840–847, 
Prague, Czech Republic, June 2007. Qc 2007 Association for Computational Linguistics


supervised learning for C W S is to treat it as a tagging 
task. Tags are assigned to each character in the sen- 
tence, indicating whether the character is a single- 
character word or the start, middle or end of a multi- 
character word. The features are usually confined to 
a five-character window with the current character 
in the middle.  In this way, dynamic programming 
algorithms such as the Viterbi algorithm can be used 
for decoding.
  Several discriminatively trained models have re- 
cently been applied to the C W S  problem.   Exam- 
ples include Xue (2003), Peng et al. (2004) and Shi 
and Wang (2007); these use maximum entropy (M E) 
and conditional random field (C R F) models (Ratna- 
parkhi, 1998; Lafferty et al., 2001).  An advantage 
of these models is their flexibility in allowing knowl- 
edge from various sources to be encoded as features.
  Contextual information plays an important role in 
word segmentation decisions; especially useful is in- 
formation about surrounding words.  Consider the


beam and the importance of word-based features. 
We compare the accuracy of our final system to the 
state-of-the-art C W S systems in the literature using 
the first and second S I G H A N bakeoff data. Our sys- 
tem is competitive with the best systems, obtaining 
the highest reported F-scores on a number of the 
bakeoff corpora. These results demonstrate the im- 
portance of word-based features for C W S.  Further- 
more, our approach provides an example of the po- 
tential of search-based discriminative training meth- 
ods for N L P tasks.

2   The Perceptron Training Algorithm

We formulate the C W S problem as finding a mapping 
from an input sentence x ∈ X to an output sentence 
y ∈ Y , where X is the set of possible raw sentences
and Y is the set of possible segmented sentences. 
Given an input sentence x, the correct output seg- 
mentation F (x) satisfies:


sentence “


”, which 
can be from 
“


F (x) = 
arg max 
Score(y)


(among w                      oreign)


(compan -!! 


y∈GEN(x)



or “


hich)	(f
   
h
i
n
a
)


ies)”,
(for           
omp
anies
)




where 
GEN(
x)




deno
tes 
the 
set 
of 
possi
ble 
segm
en-


(   ess)”. Note       e five-character wind  



tations for an input sentence



, consistent with nota-


busin


that th	ow	x


surrounding “


” is the same in 
both cases, 
making


tion from 
Collins (2002).


the tagging d       ion for that character difficult given 
the local window. However, the correct decision can 
be made by comparison of the two three-word win- 
dows containing this character.
  In order to explore the potential of word-based 
models, we adapt the perceptron discriminative 
learning algorithm to the C W S  problem.   Collins 
(2002) proposed the perceptron as an alternative to 
the C R F method for H M M-style taggers.  However, 
our model does not map the segmentation problem 
to a tag sequence learning problem, but defines fea- 
tures on segmented sentences directly.  Hence we 
use a beam-search decoder during training and test- 
ing; our idea is similar to that of Collins and Roark 
(2004) who used a beam-search decoder as part of 
a perceptron parsing model.  Our work can also be 
seen as part of the recent move towards search-based 
learning methods which do not rely on dynamic pro- 
gramming and are thus able to exploit larger parts of 
the context for making decisions (Daume III, 2006).
  We study several factors that influence the per- 
formance of the perceptron word segmentor, includ- 
ing the averaged perceptron method, the size of the



  The score for a segmented sentence is computed 
by first mapping it into a set of features. A feature 
is an indicator of the occurrence of a certain pattern 
in a segmented sentence. For example, it can be the 
occurrence of “       ” as a single word, or the occur- 
rence of “   ” separated from “   ” in two adjacent 
words. By defining features, a segmented sentence 
is mapped into a global feature vector, in which each 
dimension represents the count of a particular fea- 
ture in the sentence. The term “global” feature vec- 
tor is used by Collins (2002) to distinguish between 
feature count vectors for whole sequences and the 
“local” feature vectors in M E tagging models, which 
are Boolean valued vectors containing the indicator 
features for one element in the sequence.
  Denote the global feature vector for segmented 
sentence y with Φ(y) ∈ Rd, where d is the total
number of features in the model; then Score(y) is 
computed by the dot product of vector Φ(y) and a
parameter vector α ∈ Rd, where αi is the weight for
the ith feature:
Score(y) = Φ(y) · α


Inputs: training examples (xi, yi)
Initialization: set α = 0
Algorithm:
for t = 1..T , i = 1..N
calculate zi  = arg maxy∈GEN(xi ) Φ(y) · α
if zi  1= yi
α = α + Φ(yi) − Φ(zi)
Outputs: α

Figure 1: the perceptron learning algorithm, adapted 
from Collins (2002)


The perceptron training algorithm is used to deter- 
mine the weight values α.
  The training algorithm initializes the parameter 
vector as all zeros, and updates the vector by decod- 
ing the training examples.  Each training sentence 
is turned into the raw input form, and then decoded 
with the current parameter vector.  The output seg- 
mented sentence is compared with the original train- 
ing example. If the output is incorrect, the parameter 
vector is updated by adding the global feature vector 
of the training example and subtracting the global 
feature vector of the decoder output. The algorithm 
can perform multiple passes over the same training 
sentences. Figure 1 gives the algorithm, where N is 
the number of training sentences and T is the num- 
ber of passes over the data.
  Note that the algorithm from Collins (2002) was 
designed for discriminatively training an H M M-style 
tagger.  Features are extracted from an input se- 
quence x and its corresponding tag sequence y:
Score(x, y) = Φ(x, y) · α

Our algorithm is not based on an H M M. For a given 
input sequence x, even the length of different candi- 
dates y (the number of words) is not fixed. Because 
the output sequence y (the segmented sentence) con- 
tains all the information from the input sequence x 
(the raw sentence), the global feature vector Φ(x, y) 
is replaced with Φ(y), which is extracted from the 
candidate segmented sentences directly.
Despite the above differences, since the theorems



2.1   The averaged perceptron

The averaged perceptron algorithm (Collins, 2002) 
was proposed as a way of reducing overfitting on 
the training data.   It was motivated by the voted- 
perceptron algorithm (Freund and Schapire, 1999) 
and has been shown to give improved accuracy over 
the non-averaged perceptron on a number of tasks. 
Let N be the number of training sentences, T the 
number of training iterations, and αn,t the parame- 
ter vector immediately after the nth sentence in the
tth iteration. The averaged parameter vector γ ∈ Rd
is defined as:

   	1	αn,t
N T
n=1..N,t=1..T


  To compute the averaged parameters γ, the train- 
ing algorithm in Figure 1 can be modified by keep-
ing a total parameter vector σn,t  = I: αn,t, which is
updated using α after each training example. After 
the final iteration, γ is computed as σn,t/N T . In the 
averaged perceptron algorithm, γ is used instead of 
α as the final parameter vector.
  With a large number of features, calculating the 
total parameter vector σn,t after each training exam- 
ple is expensive.  Since the number of changed di- 
mensions in the parameter vector α after each train- 
ing example is a small proportion of the total vec- 
tor, we use a lazy update optimization for the train- 
ing process.1   Define an update vector τ to record 
the number of the training sentence n and iteration 
t when each dimension of the averaged parameter 
vector was last updated.  Then after each training 
sentence is processed, only update the dimensions 
of the total parameter vector corresponding to the 
features in the sentence. (Except for the last exam- 
ple in the last iteration, when each dimension of τ 
is updated, no matter whether the decoder output is 
correct or not).
  Denote the sth dimension in each vector before 
processing the nth example in the tth iteration as


n−1,t


n−1,t


n−1,t


of convergence and their proof (Collins, 2002) are 
only dependent on the feature vectors, and not on


αs 	, σs	and τs 	= (nτ,s, tτ,s).  Suppose
that the decoder output zn,t  is different from the


the source of the feature definitions, the perceptron


training example yn.  Now αn,t


n,t


n,t


algorithm is applicable to the training of our C W S
model.


s   , σs	and τs 	can


1 Daume III (2006) describes a similar algorithm.



be updated in the following way:


Input: raw sentence sent – a list 
of characters
Initialization: set agendas src = 
[[]], tgt = []


σn,t


n−1,t


n−1,t


Varia
bles: 
candi
date 
sente
nce 
item 
– a 
list of 
word
s


s	= σs	+ αs 	× (tN +n −tτ,sN − nτ,s)


αn,t


n−1,t


Algo
rith
m:


s	= αs 	+ Φ(yn) − Φ(zn,t)


for index = 0..sent.length−1:


σn,t


n,t


s	= σs    + Φ(yn) − Φ(zn,t)
s	= (n, t)

  We found that this lazy update method was signif- 
icantly faster than the naive method.

3   The Beam-Search Decoder

The decoder reads characters from the input sen- 
tence one at a time, and generates candidate seg- 
mentations incrementally. At each stage, the next in- 
coming character is combined with an existing can- 
didate in two different ways to generate new candi- 
dates: it is either appended to the last word in the 
candidate, or taken as the start of a new word. This 
method guarantees exhaustive generation of possible 
segmentations for any input sentence.
  Two agendas are used: the source agenda and the 
target agenda.  Initially the source agenda contains 
an empty sentence and the target agenda is empty. 
At each processing stage, the decoder reads in a 
character from the input sentence, combines it with 
each candidate in the source agenda and puts the 
generated candidates onto the target agenda.  After 
each character is processed, the items in the target 
agenda are copied to the source agenda, and then the 
target agenda is cleaned, so that the newly generated 
candidates can be combined with the next incom- 
ing character to generate new candidates. After the 
last character is processed, the decoder returns the 
candidate with the best score in the source agenda. 
Figure 2 gives the decoding algorithm.
For a sentence with length l, there are 2l−1 differ-
ent possible segmentations. To guarantee reasonable 
running speed, the size of the target agenda is lim- 
ited, keeping only the B best candidates.

4   Feature templates

The feature templates are shown in Table 1. Features
1 and 2 contain only word information, 3 to 5 con- 
tain character and length information, 6 and 7 con- 
tain only character information, 8 to 12 contain word 
and character information, while 13 and 14 contain


var char = sent[index]
foreach item in src:
// append as a new word to the candidate 
var item1  = item 
item1.append(char.toWord()) 
tgt.insert(item1)
// append the character to the last word
if item.length > 1:

var item2  = item 
item2[item2.length−1].append(char)
tgt.insert(item2)
src = tgt 
tgt = []
Outputs: src.best item

Figure 2: The decoding algorithm


word and length information. Any segmented sen- 
tence is mapped to a global feature vector according 
to these templates. There are 356, 337 features with 
non-zero values after 6 training iterations using the 
development data.
  For this particular feature set, the longest range 
features are word bigrams. Therefore, among partial 
candidates ending with the same bigram, the best 
one will also be in the best final candidate.   The 
decoder can be optimized accordingly: when an in- 
coming character is combined with candidate items 
as a new word, only the best candidate is kept among 
those having the same last word.

5   Comparison with Previous Work

Among the character-tagging C W S models, Li et al. 
(2005) uses an uneven margin alteration of the tradi- 
tional perceptron classifier (Li et al., 2002).  Each 
character is classified independently, using infor- 
mation in the neighboring five-character window. 
Liang (2005) uses the discriminative perceptron al- 
gorithm (Collins, 2002) to score whole character tag 
sequences, finding the best candidate by the global 
score. It can be seen as an alternative to the M E and 
C R F models (Xue, 2003; Peng et al., 2004), which


1	word w
2	word bigram w1w2
3	single-character word w
4	a word starting with character c and having 
length l
5	a word ending with character c and having 
length l
6	space-separated characters c1 and c2
7	character bigram c1c2 in any word
8	the first and last characters c1 and c2 of any 
word
9	word w immediately before character c
10	character c immediately before word w
11	the starting characters c1 and c2 of two con- 
secutive words
12	the ending characters c1 and c2 of two con- 
secutive words
13	a word of length l and the previous word w
14	a word of length l and the next word w

Table 1: feature templates


do not involve word information. Wang et al. (2006) 
incorporates an N-gram language model in M E tag- 
ging, making use of word information to improve 
the character tagging model. The key difference be- 
tween our model and the above models is the word- 
based nature of our system.
  One existing method that is based on sub-word in- 
formation, Zhang et al. (2006), combines a C R F and 
a rule-based model.  Unlike the character-tagging 
models,  the  C R F  submodel  assigns  tags  to  sub- 
words, which include single-character words and 
the most frequent multiple-character words from the 
training corpus. Thus it can be seen as a step towards 
a word-based model.  However, sub-words do not 
necessarily contain full word information.   More- 
over, sub-word extraction is performed separately 
from feature extraction.   Another difference from 
our model is the rule-based submodel, which uses a 
dictionary-based forward maximum match method 
described by Sproat et al. (1996).

6   Experiments

Two sets of experiments were conducted. The first, 
used for development, was based on the part of Chi- 
nese Treebank 4 that is not in Chinese Treebank


3 (since C T B3 was used as part of the first bake- 
off).  This corpus contains 240K characters (150K 
words and 4798 sentences).  80% of the sentences 
(3813) were randomly chosen for training and the 
rest (985 sentences) were used as development test- 
ing data. The accuracies and learning curves for the 
non-averaged and averaged perceptron were com- 
pared.  The influence of particular features and the 
agenda size were also studied.
  The second set of experiments used training and 
testing sets from the first and second international 
Chinese word segmentation bakeoffs (Sproat and 
Emerson, 2003; Emerson, 2005). The accuracies are 
compared to other models in the literature.
  F-measure is used as the accuracy measure. De- 
fine precision p as the percentage of words in the de- 
coder output that are segmented correctly, and recall 
r as the percentage of gold standard output words 
that are correctly segmented by the decoder.  The 
(balanced) F-measure is 2pr/(p + r).
   C W S systems are evaluated by two types of tests. 
The closed tests require that the system is trained 
only with a designated training corpus.  Any extra 
knowledge is not allowed, including common sur- 
names, Chinese and Arabic numbers, European let- 
ters, lexicons, part-of-speech, semantics and so on. 
The open tests do not impose such restrictions.
  Open tests measure a model’s capability to utilize 
extra information and domain knowledge, which can 
lead to improved performance, but since this extra 
information is not standardized, direct comparison 
between open test results is less informative.
  In this paper, we focus only on the closed test. 
However, the perceptron model allows a wide range 
of features, and so future work will consider how to 
integrate open resources into our system.

6.1   Learning curve
In this experiment, the agenda size was set to 16, for 
both training and testing.  Table 2 shows the preci- 
sion, recall and F-measure for the development set 
after 1 to 10 training iterations, as well as the num- 
ber of mistakes made in each iteration.  The corre- 
sponding learning curves for both the non-averaged 
and averaged perceptron are given in Figure 3.
  The table shows that the number of mistakes made 
in each iteration decreases, reflecting the conver- 
gence of the learning algorithm. The averaged per-



Ite
rat
io
n
1	2	3	4	5	6	7	8	9	10
P 
(n
on
-
av
g)
R
 
(
n
o
n
-
a
v
g
)
 
F
 
(
n
o
n
-
a
v
g
)
89
.0	91.6	92.0	92.3	92.5	92.5	92.5	92.7	92.6	92.6
88
.3	91.4	92.2	92.6	92.7	92.8	93.0	93.0	93.1	93.2
88
.6	91.5	92.1	92.5	92.6	92.6	92.7	92.8	92.8	92.9
P 
(a
vg
)
R
 
(
a
v
g
)
 
F
 
(
a
v
g
)
91
.7	92.8	93.1	92.2	93.1	93.2	93.2	93.2	93.2	93.2
91
.6	92.9	93.3	93.4	93.4	93.5	93.5	93.5	93.6	93.6
91
.6	92.9	93.2	93.3	93.3	93.4	93.3	93.3	93.4	93.4
#
W
ro
ng 
se
nt
en
ce
s
34
01	1652	945	621	463	288	217	176	151	139

Table 2: accuracy using non-averaged and averaged perceptron.
P - precision (%), R - recall (%), F - F-measure.

B
2
4
8
16
32
64
12
8
25
6
51
2
10
24
Tr
66
0
61
0
68
3
83
0
11
11
16
45
25
45
49
22
91
04
15
59
8
Se
g
18
.6
5
18
.1
8
28
.8
5
26
.5
2
36
.5
8
56
.4
5
95
.4
5
17
3.
38
32
5.
99
55
9.
87
F
86
.9
0
92
.9
5
93
.3
3
93
.3
8
93
.2
5
93
.2
9
93
.1
9
93
.0
7
93
.2
4
93
.3
4

Table 3: the influence of agenda size.
B - agenda size, Tr - training time (seconds), Seg - testing time (seconds), F - F-measure.




0.94

0.93

0.92

0.91

0.9

0.89

0.88

0.87

0.86













n
o
n
-
a
v
e
r
a
g
e
d
 
a
v
e
r
a
g
e
d




1	2	3	4	5	6	7	8	9	10
n
u
m
b
e
r
 
o
f
 
t
r
a
i
n
i
n
g
 
i
t
e
r
a
t
i
o
n
s


also 
affects 
the 
training 
time, 
and 
resultin
g 
model, 
since 
the 
perceptr
on 
training 
algorith
m uses 
the de- 
coder 
output 
to 
adjust 
the 
model 
paramet
ers. 
Table 3 
shows 
the 
accuraci
es with 
ten 
differen
t agenda 
sizes, 
each 
used for 
both 
training 
and 
testing.
  Accu
racy 
does 
not 
increase 
beyond 
B  = 
16. 
Moreov
er, the 
accurac
y is 
quite 
competi
tive 
even 
with B 
as low 
as 4. 
This 
reflects 
the fact 
that the 
best 
segment
ation is 
often 
within 
the 
current 
top few 
can- 
didates 
in the 
agenda.
2 Since 
the 
training 
and 
testing 
time 
generall
y 
increase
s as N 
increase
s, the 
agenda 
size is 
fixed to 
16 for 
the 
remaini
ng 
experim
ents.


6
.
3
   
T
h
e
 
i
n
f
l
u
e
n
c
e
 
o
f
 
p
a
r
t
i
c
u
l
a
r
 
f
e
a
t
u
r
e
s



Figure 3: learning curves of the averaged and non- 
averaged perceptron algorithms


ceptron algorithm improves the segmentation ac- 
curacy at each iteration, compared with the non- 
averaged perceptron.  The learning curve was used 
to fix the number of training iterations at 6 for the 
remaining experiments.

6.2   The influence of agenda size

Reducing the agenda size increases the decoding 
speed, but it could cause loss of accuracy by elimi- 
nating potentially good candidates. The agenda size



Our C W S model is highly dependent upon word in- 
formation. Most of the features in Table 1 are related 
to words. Table 4 shows the accuracy with various 
features from the model removed.
  Among the features, vocabulary words (feature 1) 
and length prediction by characters (features 3 to 5) 
showed strong influence on the accuracy, while word 
bigrams (feature 2) and special characters in them 
(features 11 and 12) showed comparatively weak in- 
fluence.

  2 The optimization in Section 4, which has a pruning effect, 
was applied to this experiment. Similar observations were made 
in separate experiments without such optimization.



Fe
at
ur
es
F
Fe
at
ur
es
F
A
l
l
w
/
o
 
2
 
w
/
o
 
6
 
w
/
o
 
8
w
/o 
1
1, 
1
2
93
.3
8
93
.3
6
93
.1
3
93
.1
4
93
.3
8
w/
o 
1
w
/
o 
3
, 
4
, 
5 
w
/
o 
7
w
/
o 
9
, 
1
0 
w
/
o 
1
3
, 
1
4
92
.8
8
92
.7
2
93
.1
3
93
.3
1
93
.2
3

Table 4:  the influence of features.  (F: F-measure. 
Feature numbers are from Table 1)




6.4   Closed test on the S I G H A N bakeoffs

Four training and testing corpora were used in the 
first bakeoff (Sproat and Emerson, 2003), including 
the Academia Sinica Corpus (AS), the Penn Chinese 
Treebank Corpus (CTB), the Hong Kong City Uni- 
versity Corpus (CU) and the Peking University Cor- 
pus (PU). However, because the testing data from 
the Penn Chinese Treebank Corpus is currently un- 
available, we excluded this corpus. The corpora are 
encoded in GB (PU, CTB) and BIG5 (AS, CU). In 
order to test them consistently in our system, they 
are all converted to UTF8 without loss of informa- 
tion.
  The results are shown in Table 5. We follow the 
format from Peng et al. (2004).  Each row repre- 
sents a C W S model.  The first eight rows represent 
models from Sproat and Emerson (2003) that partic- 
ipated in at least one closed test from the table, row 
“Peng” represents the C R F model from Peng et al. 
(2004), and the last row represents our model. The 
first three columns represent tests with the AS, CU 
and PU corpora, respectively. The best score in each 
column is shown in bold. The last two columns rep- 
resent the average accuracy of each model over the 
tests it participated in (SAV), and our average over 
the same tests (OAV), respectively. For each row the 
best average is shown in bold.
  We achieved the best accuracy in two of the three 
corpora, and better overall accuracy than the major- 
ity of the other models.  The average score of S10 
is 0.7% higher than our model, but S10 only partici- 
pated in the HK test.
  Four training and testing corpora were used in 
the second bakeoff (Emerson, 2005), including the 
Academia Sinica corpus (AS), the Hong Kong City 
University Corpus (CU), the Peking University Cor- 
pus (PK) and the Microsoft Research Corpus (MR).




Table 5: the accuracies over the first S I G H A N bake- 
off data.


A
S	CU	PK	MR
SA
V
O
A
V
S1
4
S1
5b
S2
7
94
.7	94.3	95.0	96.4
95
.2	94.1	94.1	95.8
94
.5	94.0	95.0	96.0
95
.1
94
.8
94
.9
95
.4
95
.4
95
.4
Zh
-a
Zh
-b
94
.7	94.6	94.5	96.4
95
.1	95.1	95.1	97.1
95
.1
95
.6
95
.4
95
.4

94
.6	95.1	94.5	97.2



Table 6:  the accuracies over the second S I G H A N
bakeoff data.


Different encodings were provided, and the UTF8 
data for all four corpora were used in this experi- 
ment.
  Following the format of Table 5, the results for 
this bakeoff are shown in Table 6.   We chose the 
three models that achieved at least one best score 
in the closed tests from Emerson (2005), as well as 
the sub-word-based model of Zhang et al. (2006) for 
comparison. Row “Zh-a” and “Zh-b” represent the 
pure sub-word C R F model and the confidence-based 
combination of the C R F and rule-based models, re- 
spectively.
  Again, our model achieved better overall accu- 
racy than the majority of the other models. One sys- 
tem to achieve comparable accuracy with our sys- 
tem is Zh-b, which improves upon the sub-word C R F 
model (Zh-a) by combining it with an independent 
dictionary-based submodel and improving the accu- 
racy of known words. In comparison, our system is 
based on a single perceptron model.
  In summary, closed tests for both the first and the 
second bakeoff showed competitive results for our


system compared with the best results in the litera- 
ture.  Our word-based system achieved the best F- 
measures over the AS (96.5%) and CU (94.6%) cor- 
pora in the first bakeoff, and the CU (95.1%) and 
MR (97.2%) corpora in the second bakeoff.

7   Conclusions and Future Work

We proposed a word-based C W S  model using the 
discriminative perceptron learning algorithm.  This 
model is an alternative to the existing character- 
based tagging models, and allows word information 
to be used as features. One attractive feature of the 
perceptron training algorithm is its simplicity, con- 
sisting of only a decoder and a trivial update process. 
We use a beam-search decoder, which places our 
work in the context of recent proposals for search- 
based discriminative learning algorithms.  Closed 
tests using the first and second S I G H A N C W S bake- 
off data demonstrated our system to be competitive 
with the best in the literature.
  Open features, such as knowledge of numbers and 
European letters, and relationships from semantic 
networks (Shi and Wang, 2007), have been reported 
to improve accuracy. Therefore, given the flexibility 
of the feature-based perceptron model, an obvious 
next step is the study of open features in the seg- 
mentor.
  Also, we wish to explore the possibility of in- 
corporating P O S  tagging and parsing features into 
the discriminative model, leading to joint decod- 
ing.  The advantage is two-fold:  higher level syn- 
tactic information can be used in word segmenta- 
tion, while joint decoding helps to prevent bottom- 
up error propagation among the different processing 
steps.

Acknowledgements

This work is supported by the ORS and Clarendon 
Fund. We thank the anonymous reviewers for their 
insightful comments.


References

Michael Collins and Brian Roark.  2004.  Incremental parsing 
with the perceptron algorithm.  In Proceedings of ACL’04, 
pages 111–118, Barcelona, Spain, July.

Michael Collins.  2002.  Discriminative training methods for 
hidden markov models: Theory and experiments with per-



ceptron algorithms.  In Proceedings of EMNLP, pages 1–8, 
Philadelphia, USA, July.

Hal Daume III. 2006. Practical Structured Learning for Natu- 
ral Language Processing. Ph.D. thesis, USC.

Thomas Emerson.   2005.   The second international Chinese 
word segmentation bakeoff.  In Proceedings of The Fourth 
SIGHAN Workshop, Jeju, Korea.

Y. Freund and R. Schapire. 1999.  Large margin classification 
using the perceptron algorithm. In Machine Learning, pages
277–296.

J. Lafferty, A. McCallum, and F. Pereira.  2001.  Conditional 
random fields: Probabilistic models for segmenting and la- 
beling sequence data.   In Proceedings of the 18th ICML, 
pages 282–289, Massachusetts, USA.

Y. Li, Zaragoza, R. H., Herbrich, J. Shawe-Taylor, and J. Kan- 
dola. 2002. The perceptron algorithm with uneven margins. 
In Proceedings of the 9th ICML, pages 379–386, Sydney, 
Australia.

Yaoyong Li, Chuanjiang Miao, Kalina Bontcheva, and Hamish 
Cunningham.  2005.  Perceptron learning for Chinese word 
segmentation. In Proceedings of the Fourth SIGHAN Work- 
shop, Jeju, Korea.

Percy Liang.  2005.  Semi-supervised learning for natural lan- 
guage. Master’s thesis, MIT.

F. Peng, F. Feng, , and A. McCallum. 2004. Chinese segmenta- 
tion and new word detection using conditional random fields. 
In Proceedings of COLING, Geneva, Switzerland.

Adwait Ratnaparkhi. 1998. Maximum Entropy Models for Nat- 
ural Language Ambiguity Resolution. Ph.D. thesis, UPenn.

Yanxin Shi and Mengqiu Wang.   2007.   A dual-layer CRF 
based joint decoding method for cascade segmentation and 
labelling tasks. In Proceedings of IJCAI, Hyderabad, India.

Richard Sproat and Thomas Emerson. 2003. The first interna- 
tional Chinese word segmentation bakeoff.  In Proceedings 
of The Second SIGHAN Workshop, pages 282–289, Sapporo, 
Japan, July.

R. Sproat, C. Shih, W. Gail, and N. Chang.  1996.  A stochas- 
tic finite-state word-segmentation algorithm for Chinese. In 
Computational Linguistics, volume 22(3), pages 377–404.

Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, and Xihong 
Wu. 2006. Chinese word segmentation with maximum en- 
tropy and n-gram language model.   In Proceedings of the 
Fifth SIGHAN Workshop, pages 138–141, Sydney, Australia, 
July.

N. Xue.  2003.  Chinese word segmentation as character tag- 
ging. In International Journal of Computational Linguistics 
and Chinese Language Processing, volume 8(1).

Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006.
Subword-based tagging by conditional random fields for 
Chinese word segmentation.  In Proceedings of the Human 
Language Technology Conference of the NAACL, Compan- 
ion, volume Short Papers, pages 193–196, New York City, 
USA, June.

