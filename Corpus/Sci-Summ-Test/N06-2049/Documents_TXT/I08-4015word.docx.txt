



The Character-based CRF Segmenter of MSRA&NEU

for the 4th Bakeoff

Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu1
1 Institute of Computer Software and Theory, Northeastern University,
Shenyang, China, 110004
2 Microsoft Research Asia, 49, Zhichun Road,
Haidian District, Beijing, China, 100080
zxwang@ics.neu.edu.cn v-
cnh@microsoft.com 
zhujingbo@mail.neu.edu.cn



Abstract

This paper describes the Chinese Word 
Segmenter for the fourth International 
Chinese Language Processing Bakeoff. 
Base on Conditional Random Field (CRF) 
model, a basic segmenter is designed as a 
problem of character-based tagging.   To 
further improve the performance of our 
segmenter, we employ a word-based ap- 
proach to increase the in-vocabulary (IV) 
word recall and a post-processing to in- 
crease the out-of-vocabulary (OOV) word 
recall. We participate in the word segmen- 
tation closed test on all five corpora and 
our system achieved four second best and 
one the fifth in all the five corpora.

1     Introduction

Since Chinese Word Segmentation was firstly 
treated as a character-based tagging task in (Xue 
and Converse, 2002), this method has been widely 
accepted and further developed by researchers 
(Peng et al., 2004), (Tseng et al., 2005), (Low et 
al., 2005), (Zhao et al., 2006). Thus, as a powerful 
sequence tagging model, CRF became the domi- 
nant method in the Bakeoff 2006 (Levow, 2006).
In this paper, we improve basic segmenter un-
der the CRF work frame in two aspects, namely 
IV and OOV identification respectively. We use 
the result from word-based segmentation to revise 
the CRF output so that we gain a higher IV word 
recall. For the OOV part a post-processing rule is 
proposed to find those OOV words which are 
wrongly  segmented  into  several  fractions.  Our


system performs well in the Fourth Bakeoff, 
achieving four second best and on the fifth in all 
the five corpora. In the following of this paper, we 
describe our method in more detail.
   The rest of this paper is organized as follows. 
In Section 2, we first give a brief review to the 
basic CRF tagging approach and then we propose 
our methods to improve IV and OOV performance 
respectively. In Section 3 we give the experiment 
results on the fourth Bakeoff corpora to show that 
our method is effective to improve the perfor- 
mance of the segmenter. In Section 4, we con- 
clude our work.

2     Our Word Segmentation System

In this section, we describe our system in more 
detail. Our system includes three modules: a basic 
CRF tagger, a word-base segmenter to improve 
the  IV  recall  and  a   post-processing  rule  to 
improve the OOV recall. In the following of this 
section, we introduce these three modules 
respectively.

2.1     Basic CRF tagger

Sequence tagging approach treat Word Segmenta- 
tion task as a labeling problem. Every character in 
input sentences will be given a label which indi- 
cates whether this character is a word boundary. 
Our basic CRF1 tagger is almost the same as the 
system described in (Zhao et al., 2006) except we 
add a feature to incorporate word information, 
which is learned from training corpus.

1 CRF tagger in this paper  is implemented by CRF++
which is downloaded from http://crfpp.sourceforge.net/





Type
Feature
Function
Unigram
C-1, C0, C1
Previous, current and next character
Bigram
C-1 C0, C0 C1
Two adjacent character
Jump
C-1 C1
Previous character and next character
Word Flag
F0 F1
Whether adjacent characters form an IV word
Table 1 Feature templates used for CRF in our system



   Under the CRF tagging scheme, each character 
in  one sentence  will  be given  a  label  by  CRF 
model to indicate which position this character 
occupies in a word. In our system, CRF tag set is 
proposed to distinguish different positions in the 
multi-character  words  when  the  word  length  is 
less than 6, namely 6-tag set {B, B2, B3, M, E, 
O}. Here, Tag B and E stand for the first and the 
last position in a multi-character word, respective- 
ly. S stands up a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character word, whose length is larger than 
two-character or three-character. M stands for the 
fourth or more rear position in a multi-character 
word, whose length is larger than four-character.
   We add a new feature, which also used in 
maximum entropy model for word segmentation 
task by (Low et al., 2005), to the feature templates 
for CRF model while keep the other features same 
as (Zhao et al., 2006). The feature templates are 
defined in table 1. In the feature template, only the 
Word Flag feature needs an explanation. The bi-
nary function F0 = 1 if and only if C-1 C0 form a IV



mentations for any input sentence. However, the 
exponential time and space of the length of the 
input sentence are needed for such a search and it 
is always intractable in practice. Thus, we use the 
trigram language model to select top B (B is a 
constant predefined before search and in our expe- 
riment 3 is used) best candidates with highest 
probability at each stage so that the search algo- 
rithm  can  work  in  practice.  Finally,  when  the 
whole sentence has been read, the best candidate 
with the highest probability will be selected as the 
segmentation result.
   After we get word-based segmentation result, 
we use it to revise the CRF tagging result similar 
to (Zhang et al., 2006). Since word-based segmen- 
tation result also corresponds to a tag sequence 
according to the 6-tag set, we now have two tags 
for each character, word-based tag (WT) and CRF 
tag (CT). Which tag will be kept as the final result 
depends on Marginal Probability (MP) of the CT.
   Here, we give a short explanation about what 
is the MP of the CT. Suppose there is a sentence


C  c0c1...cM   , where ci


is the character this sen-


word, else F0  = 0 and F1  = 1 if and only if C0  C1


tence containing. CRF model gives this sentence a


form a IV word, else F1 = 0.


optimal tag sequence T  
t0t1...tM



, where ti is the


2.2	Word based segmenter and revise rules


tag  for


c .  If t


 t and t  {B, B , B , M , E, S} ,


i	i 	2 	3


For the word-based word segmentation, we collect
dictionary from training corpus  first.  Instead of


the MP of ti is defined as:


Maximum   Match,	trigram   language   model  2



MP(t



 t) 


T ,t t


P(T | C)


trained on training corpus is employed for disam-
biguation. During the disambiguation procedure, a


i 	P(T | C)
T


beam  search  decoder  is  used  to  seek  the  most


Here,


P(T | C) is the conditional probability giv-


possible  segmentation.  For  detail,  the  decoder 
reads characters from the input sentence one at a 
time, and generates candidate segmentations in- 
crementally. At each stage, the next incoming cha- 
racter is combined with an existing candidate in 
two different ways to generate new candidates: it 
is either appended to the last word in the candidate, 
or taken as the start of a new word. This method 
guarantees exhaustive generation of possible seg-

2 Language model used in this paper is SLRIM down- 
loaded from http://www.speech.sri.com/projects/srilm/


en by CRF model. For more detail about how to 
calculate this conditional probability, please refer 
to (Lafferty et al., 2001).
   Assume that the tag assigned to the current 
character is CT by CRF and WT by word-based 
segmenter respectively. The rules under which we 
revise CRF result with word-based result is that if 
MP(CT) of a character is less than a predefined 
threshold and WT is not “S”, the WT of this cha- 
racter will be kept as the final result, else the CT 
of the character will be kept as the final result.







   The restriction that WT should not be “S” is 
reasonable because word-based segmentation is 
incapable to recognize the OOV word and always 
segments OOV word into single characters. Be- 
sides CRF model is better at dealing with OOV 
word  than  our  word-based segmentation.  When 
WT is “S” it is possible that current word is an 
OOV word and segmented into single character 
wrongly by the word-based segmenter, so the CT 
of the character should be kept under such situa- 
tion.  For  more detail  about  this  analysis  please 
refer to (Wang et al., 2008).

2.3     Post-processing rule

The rules we described in last subsection is help- 
ful to improve the IV word recall and now we in- 
troduce our post-processing rule to improve the 
OOV recall.
Our post-processing rule is designed to deal
with one typical type of OOV errors, namely an 
OOV word wrongly segmented into several parts. 
In practice many OOV errors belong to such type.
   The rule is quite simple. When we read a sen- 
tence from the result we get by the last step, we 
also kept the last N sentences in memory, in our 
system we set N equals to 20. We do this because 
adjacent sentences are always relevant and some 
named entity likely occurs repeatedly in these sen- 
tences. Then, we scan these sentences to find all 
n-grams (n from 2 to 7) and count their occur- 
rence. If certain n-gram appears more than a thre- 
shold and this n-gram never appears in training 
corpus, the n-gram will be selected as a word can- 
didate. Then, we filter these word candidates ac- 
cording  to  the context  entropy  (Luo  and  Song,
2004).  Assume  w  is  a  word  candidate  appears
n times in the current sentence and last N sen-


threshold will be bind as a whole word in test cor- 
pus no matter what tag sequence the segmenter 
giving it. If a shorter n-gram is contained in a 
longer n-gram and both of them satisfy the above 
condition, the shorter n-gram will be overlooked 
and the longer n-gram is bind as a whole word.

3     Evaluation of Our System

On the corpora of the Fourth Bakeoff, we evaluate 
our system.   We carry out our evaluation on the 
closed tracks. It means that we do not use any ad- 
ditional  knowledge  beyond  the  training  corpus. 
The thresholds set for MP and CE on each corpus 
are tuned on left-out data of training corpus by 
cross validation. To analyze our methods on IV 
and OOV words, we use a detailed evaluation me- 
tric than Bakeoff 2006 (Levow, 2006) which in- 
cludes Foov and Fiv. Our results are shown in Ta- 
ble 2. In Table 2, the row “Basic Model” means 
the results produced by our basic CRF tagger, the 
row “+IV” means the results produced by the 
combination of CRF tagger and word-based seg- 
menter and the row “+IV+OOV” means the result 
we get by executing post-processing rule on the 
combination results. The F measure of the basic 
CRF tagger alone in the Table 2 is within the top 
three in the closed tests except Cityu. Performance 
on Cityu corpus is not so good because the incon- 
sistencies existing in Cityu training and test corpo- 
ra. In the training corpus the quotation marks are
「」while in test corpus quotation marks 
are“”,
which never  apper in the training corpus.  As a 
reult, a lot of errors were caused by quotation 
marks. For example, the following four character
“事業”were combined as a one word in 
our
result and fragment“越位”was tagged as 
two
“越 and 位”. Because CRF tagger 
never


tences and   {a0 , a1 ,..., al } is the set of left side 
characters of w . Left Context Entropy (LCE) can 
be defined as:


words
met “ and ” in training corpus so the 
tagger
gave the most common tags, namely B and E to 
the  quotation  marks,  which  cause  segmentation


LCE (w)


 1 
n a 


C (ai , w) 
log


n
C (ai , w)


errors not 
only on 
quotation 
marks 
themselv
es but 
also  on  
the  
character
s  
adjacent  
to  them.  
We
remove  
these  
inconsiste
ncies  
munually  
and  got


Here,


C (ai , 
w) is  
the  
count  
of  
concur
rence  
of



the F 
measu
re 0.5 
percen
tage 
higer 
than 
the 
rusult


ai and w . For the Right Context Entropy, the de-
finition is the same except change left into right. 
Now, we define Context Entropy (CE) of a word


in table 2. This result is within the top three in the 
closed tests. On all the five corpora, our “+IV” 
module  can  increase  the  Fiv  and  our  “+OOV”


candidate  w  as


min(LCE(w), 
RCE(w))


.   The


module can 
increase Foov 
respectively. 
However,


word candidates with CE larger than a predefined


these improvements are not significant.





Co
rp
us
Me
tho
d
R
P
F
RO
OV
PO
OV
FO
OV
RIV
PIV
FIV

C
K
IP
Ba
sic 
M
od
el
0.9
46
0.9
23
0.9
40
0.6
51
0.7
19
0.6
83
0.9
69
0.9
48
0.9
58

+ 
IV
0.9
49
0.9
35
0.9
42
0.6
47
0.7
41
0.6
91
0.9
73
0.9
48
0.9
60

+ 
IV 
+ 
O
O
V
0.9
50
0.9
36
0.9
43
0.6
56
0.7
48
0.6
99
0.9
73
0.9
49
0.9
61

Ci
ty
U
Ba
sic 
M
od
el
0.9
44
0.9
34
0.9
39
0.6
54
0.7
21
0.6
86
0.9
70
0.9
51
0.9
60

+ 
IV
0.9
46
0.9
36
0.9
41
0.6
55
0.7
38
0.6
94
0.9
72
0.9
51
0.9
62

+ 
IV 
+ 
O
O
V
0.9
49
0.9
37
0.9
43
0.6
78
0.7
59
0.7
16
0.9
73
0.9
51
0.9
62

C
T
B
Ba
sic 
M
od
el
0.9
53
0.9
51
0.9
52
0.7
03
0.7
27
0.7
15
0.9
67
0.9
64
0.9
65

+ 
IV
0.9
54
0.9
52
0.9
53
0.6
97
0.7
47
0.7
21
0.9
69
0.9
63
0.9
66

+ 
IV 
+ 
O
O
V
0.9
54
0.9
53
0.9
53
0.7
03
0.7
49
0.7
25
0.9
69
0.9
64
0.9
66

N
C
C
Ba
sic 
M
od
el
0.9
40
0.9
28
0.9
34
0.4
38
0.5
80
0.4
99
0.9
65
0.9
40
0.9
52

+ 
IV
0.9
44
0.9
30
0.9
36
0.4
34
0.6
03
0.5
04
0.9
69
0.9
41
0.9
55

+ 
IV 
+ 
O
O
V
0.9
45
0.9
32
0.9
39
0.4
50
0.6
20
0.5
22
0.9
70
0.9
43
0.9
56

S
X
U
Ba
sic 
M
od
el
0.9
60
0.9
53
0.9
56
0.6
36
0.6
74
0.6
54
0.9
77
0.9
67
0.9
72

+ 
IV
0.9
62
0.9
55
0.9
58
0.6
37
0.6
96
0.6
65
0.9
80
0.9
67
0.9
73

+ 
IV 
+ 
O
O
V
0.9
62
0.9
55
0.9
59
0.6
45
0.7
02
0.6
73
0.9
79
0.9
68
0.9
74
Table 2 performance each step of our system achieves



4     Conclusions and Future Work

In this paper, we propose a three-stage strategy in 
Chinese Word Segmentation. Based on the results 
produced by basic CRF, our word-based segmen- 
tation module and post-processing module are 
designed to improve IV and OOV performance 
respectively. The results above show that our sys- 
tem achieves the state-of-the-art performance. 
Since only the CRF tagger is good enough as we 
shown in our experiment, in the future work we 
will pay effort on the semi-supervised learning for 
CRF model in order to mining more useful infor- 
mation from training and test corpus for CRF tag- 
ger.

References

John Lafferty, Andrew McCallum, and Fernando Perei- 
ra. 2001. Conditional random fields: probabilistic 
models for segmenting and labeling sequence data. 
In Proceedings of ICML-2001, pages 591–598.

Gina-Anne Levow. 2006. The Third International Chi- 
nese Language Processing Bakeoff: Word Segmen- 
tation and Named Entity Recognition. In   Proceed- 
ings of the Fifth SIGHAN Workshop on Chinese 
Language Processing , pages 108-117, Sydney: Ju- 
ly.

Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word 
Segmentation. In Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 161-
164, Jeju Island, Korea.

Zhiyong Luo, Rou Song, 2004. “An integrated method 
for Chinese unknown word extraction”, In Proceed-



ings of Third SIGHAN Workshop on Chinese Lan- 
guage Processing, pages 148-154. Barcelona, Spain.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection 
using conditional random fields. In COLING 2004, 
pages 562–568. Geneva, Switzerland.

Huihsin Tseng, Pichuan Chang et al. 2005. A Conditional
Random Field Word Segmenter for SIGHAN Bakeoff
2005. In Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, pages 168-171, Jeju 
Island, Korea.

Zhenxing Wang, Changning Huang and Jingbo Zhu.
2008.  Which  Performs  Better  on  In-Vocabulary
Word Segmentation: Based on Word or Character? 
In Proceeding of the Sixth Sighan Workshop on 
Chinese Language Processing. To be published.

Neinwen Xue and Susan P. Converse. 2002. Combin- 
ing Classifiers for Chinese Word Segmentation. In 
Proceedings of the First SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Taipei, 
Taiwan.

Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.
2006. Subword-based Tagging by Conditional Ran- 
dom Fields for Chinese Word Segmentation. In Pro- 
ceedings of the Human Language Technology Con- 
ference of the NAACL, Companion volume, pages
193-196. New York, USA.

Hai Zhao, Changning Huang et al. 2006. Effective Tag 
Set Selection in Chinese Word Segmentation via 
Conditional Random Field Modeling. In Proceed- 
ings of PACLIC-20. pages 87-94. Wuhan, China, 
Novemeber.




