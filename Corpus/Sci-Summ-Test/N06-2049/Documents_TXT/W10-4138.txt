Term Contributed Boundary Tagging by Conditional Random 
Fields for SIGHAN 2010 Chinese Word Segmentation Bakeoff 
Tian-Jian Jiang?? Shih-Hung Liu*? Cheng-Lung Sung*? Wen-Lian Hsu?? 
?Department of 
Computer Science 
National Tsing-Hua University 
*Department of 
Electrical Engineering 
National Taiwan University 
?Institute of 
Information Science 
Academia Sinica 
{tmjiang,journey,clsung,hsu}@iis.sinica.edu.tw 
 
Abstract 
This paper presents a Chinese word 
segmentation system submitted to the 
closed training evaluations of CIPS-
SIGHAN-2010 bakeoff. The system uses 
a conditional random field model with 
one simple feature called term contri-
buted boundaries (TCB) in addition to 
the ?BI? character-based tagging ap-
proach. TCB can be extracted from unla-
beled corpora automatically, and seg-
mentation variations of different do-
mains are expected to be reflected impli-
citly. The experiment result shows that 
TCB does improve ?BI? tagging domain-
independently about 1% of the F1 meas-
ure score. 
1 Introduction 
The CIPS-SIGHAN-2010 bakeoff task of Chi-
nese word segmentation is focused on cross-
domain texts. The design of data set is challeng-
ing particularly. The domain-specific training 
corpora remain unlabeled, and two of the test 
corpora keep domains unknown before releasing, 
therefore it is not easy to apply ordinary machine 
learning approaches, especially for the closed 
training evaluations. 
2 Methodology 
2.1 The ?BI? Character-Based Tagging of 
Conditional Random Field as Baseline 
The character-based ?OBI? tagging of 
Conditional Random Field (Lafferty et al, 2001) 
has been widely used in Chinese word 
segmentation recently (Xue and Shen, 2003; 
Peng and McCallum, 2004; Tseng et al, 2005). 
Under the scheme, each character of a word is 
labeled as ?B? if it is the first character of a 
multiple-character word, or ?I? otherwise. If the 
character is a single-character word itself, ?O? 
will be its label. As Table 1 shows, the lost of 
performance is about 1% by replacing ?O? with 
?B? for character-based CRF tagging on the 
dataset of CIPS-SIGHAN-2010 bakeoff task of 
Chinese word segmentation, thus we choose 
?BI? as our baseline for simplicity, with this 1% 
lost bearing in mind. In tables of this paper, SC 
stands for Simplified Chinese and TC represents 
for Traditional Chinese. Test corpora of SC and 
TC are divided into four domains, where suffix 
A, B, C and D attached, for texts of literature, 
computer, medicine and finance, respectively. 
  R P F OOV 
SC-A OBI 0.906 0.916 0.911 0.539 
 BI 0.896 0.907 0.901 0.508 
SC-B OBI 0.868 0.797 0.831 0.410 
 BI 0.850 0.763 0.805 0.327 
SC-C OBI 0.897 0.897 0.897 0.590 
 BI 0.888 0.886 0.887 0.551 
SC-D OBI 0.900 0.903 0.901 0.472 
 BI 0.888 0.891 0.890 0.419 
TC-A OBI 0.873 0.898 0.886 0.727 
 BI 0.856 0.884 0.870 0.674 
TC-B OBI 0.906 0.932 0.919 0.578 
 BI 0.894 0.920 0.907 0.551 
TC-C OBI 0.902 0.923 0.913 0.722 
 BI 0.891 0.914 0.902 0.674 
TC-D OBI 0.924 0.934 0.929 0.765 
 BI 0.908 0.922 0.915 0.722 
Table 1. OBI vs. BI; where the lost of F > 1%, 
such as SC-B, is caused by incorrect English 
segments that will be discussed in the section 4. 
2.2 Term Contributed Boundary 
The word boundary and the word frequency are 
the standard notions of frequency in corpus-
based natural language processing, but they lack 
the correct information about the actual boun-
dary and frequency of a phrase?s occurrence. 
The distortion of phrase boundaries and frequen-
cies was first observed in the Vodis Corpus 
when the bigram ?RAIL ENQUIRIES? and tri-
gram ?BRITISH RAIL ENQUIRIES? were ex-
amined and reported by O'Boyle (1993). Both of 
them occur 73 times, which is a large number for 
such a small corpus. ?ENQUIRIES? follows 
?RAIL? with a very high probability when it is 
preceded by ?BRITISH.? However, when 
?RAIL? is preceded by words other than ?BRIT-
ISH,? ?ENQUIRIES? does not occur, but words 
like ?TICKET? or ?JOURNEY? may. Thus, the 
bigram ?RAIL ENQUIRIES? gives a misleading 
probability that ?RAIL? is followed by ?EN-
QUIRIES? irrespective of what precedes it. This 
problem happens not only with word-token cor-
pora but also with corpora in which all the com-
pounds are tagged as units since overlapping N-
grams still appear, therefore corresponding solu-
tions such as those of Zhang et al (2006) were 
proposed. 
We uses suffix array algorithm to calculate ex-
act boundaries of phrase and their frequencies 
(Sung et al, 2008), called term contributed 
boundaries (TCB) and term contributed fre-
quencies (TCF), respectively, to analogize simi-
larities and differences with the term frequencies 
(TF). For example, in Vodis Corpus, the original 
TF of the term ?RAIL ENQUIRIES? is 73. 
However, the actual TCF of ?RAIL ENQUI-
RIES? is 0, since all of the frequency values are 
contributed by the term ?BRITISH RAIL EN 
QUIRIES?. In this case, we can see that ?BRIT-
ISH RAIL ENQUIRIES? is really a more fre-
quent term in the corpus, where ?RAIL EN-
QUIRIES? is not. Hence the TCB of ?BRITISH 
RAIL ENQUIRIES? is ready for CRF tagging as 
?BRITISH/TB RAIL/TB ENQUIRIES/TI,? for 
example. 
3 Experiments 
Besides submitted results, there are several 
different experiments that we have done. The 
configuration is about the trade-off between data 
sparseness and domain fitness. For the sake of 
OOV issue, TCBs from all the training and test 
corpora are included in the configuration of 
submitted results. For potentially better consis-
tency to different types of text, TCBs from the 
training corpora and/or test corpora are grouped 
by corresponding domains of test corpora. Table 
2 and Table 3 provide the details, where the 
baseline is the character-based ?BI? tagging, and 
others are ?BI? with additional different TCB 
configurations: TCBall stands for the submitted 
results; TCBa, TCBb, TCBta, TCBtb, TCBtc, 
TCBtd represents TCB extracted from the train-
ing corpus A, B, and the test corpus A, B, C, D, 
respectively. Table 2 indicates that F1 measure 
scores can be improved by TCB about 1%, do-
main-independently. Table 3 gives a hint of the 
major contribution of performance is from TCB 
of each test corpus. 
Table 2. Baseline vs. Submitted Results 
 
 
 
 
 
 
  R P F OOV 
SC-A BI 0.896 0.907 0.901 0.508 
 TCBall 0.917 0.921 0.919 0.699 
SC-B BI 0.850 0.763 0.805 0.327 
 TCBall 0.876 0.799 0.836 0.456 
SC-C BI 0.888 0.886 0.887 0.551 
 TCBall 0.900 0.896 0.898 0.699 
SC-D BI 0.888 0.891 0.890 0.419 
 TCBall 0.910 0.906 0.908 0.562 
TC-A BI 0.856 0.884 0.870 0.674 
 TCBall 0.871 0.891 0.881 0.670 
TC-B BI 0.894 0.920 0.907 0.551 
 TCBall 0.913 0.917 0.915 0.663 
TC-C BI 0.891 0.914 0.902 0.674 
 TCBall 0.900 0.915 0.908 0.668 
TC-D BI 0.908 0.922 0.915 0.722 
 TCBall 0.929 0.922 0.925 0.732 
  F OOV 
SC-A TCBta 0.918 0.690 
 TCBa 0.917 0.679 
 TCBta + TCBa 0.917 0.690 
 TCBall 0.919 0.699 
SC-B TCBtb 0.832 0.465 
 TCBb 0.828 0.453 
 TCBtb + TCBb 0.830 0.459 
 TCBall 0.836 0.456 
SC-C TCBtc 0.897 0.618 
 TCBall 0.898 0.699 
SC-D  TCBtd 0.905 0.557 
 TCBall 0.910 0.562 
Table 3a. Simplified Chinese Domain-specific 
TCB vs. TCBall 
  F OOV 
TC-A TCBta 0.889 0.706 
 TCBa 0.888 0.690 
 TCBta + TCBa 0.889 0.710 
 TCBall 0.881 0.670 
TC-B TCBtb 0.911 0.636 
 TCBb 0.921 0.696 
 TCBtb + TCBb 0.912 0.641 
 TCBall 0.915 0.663 
TC-C TCBtc 0.918 0.705 
 TCBall 0.908 0.668 
TC-D TCBtd 0.927 0.717 
 TCBall 0.925 0.732 
Table 3b. Traditional Chinese Domain-specific 
TCB vs. TCBall 
 
4 Error Analysis 
The most significant type of error in our results 
is unintentionally segmented English words. Ra-
ther than developing another set of tag for Eng-
lish alphabets, we applies post-processing to fix 
this problem under the restriction of closed train-
ing by using only alphanumeric character infor-
mation. Table 4 compares F1 measure score of 
the Simplified Chinese experiment results before 
and after the post-processing. 
 
 
 F1 measure score 
before after 
SC-A OBI 0.911 0.918 
 BI 0.901 0.908 
 TCBta 0.918 0.920 
 TCBta + TCBa 0.917 0.920 
 TCBall 0.919 0.921 
SC-B OBI 0.831 0.920 
 BI 0.805 0.910 
 TCBtb 0.832 0.917 
 TCBtb + TCBb 0.830 0.916 
 TCBall 0.836 0.916 
SC-C OBI 0.897 0.904 
 BI 0.887 0.896 
 TCBtc 0.897 0.901 
 TCBall 0.898 0.902 
SC-D OBI 0.901 0.919 
 BI 0.890 0.908 
 TCBtd 0.905 0.915 
 TCBall 0.908 0.918 
Table 4. F1 measure scores before and after 
English Problem Fixed 
The major difference between gold standards 
of the Simplified Chinese corpora and the Tradi-
tional Chinese corpora is about non-Chinese 
characters. All of the alphanumeric and the 
punctuation sequences are separated from Chi-
nese sequences in the Simplified Chinese corpo-
ra, but can be part of the Chinese word segments 
in the Traditional Chinese corpora. For example, 
a phrase ??? / simvastatin / ? / statins? / ? / ? /
? / ?? (?/? represents the word boundary) from 
the domain C of the test data cannot be either 
recognized by ?BI? and/or TCB tagging ap-
proaches, or post-processed. This is the reason 
why Table 4 does not come along with Tradi-
tional Chinese experiment results. 
Some errors are due to inconsistencies in the 
gold standard of non-Chinese character, For ex-
ample, in the Traditional Chinese corpora, some 
percentage digits are separated from their per-
centage signs, meanwhile those percentage signs 
are connected to parentheses right next to them. 
5 Conclusion 
This paper introduces a simple CRF feature 
called term contributed boundaries (TCB) for 
Chinese word segmentation. The experiment 
result shows that it can improve the basic ?BI? 
tagging scheme about 1% of the F1 measure 
score, domain-independently. 
Further tagging scheme for non-Chinese cha-
racters are desired for recognizing some sophis-
ticated gold standard of Chinese word segmenta-
tion that concatenates alphanumeric characters 
to Chinese characters. 
Acknowledgement 
The CRF model used in this paper is developed based 
on CRF++, http://crfpp.sourceforge.net/ 
Term Contributed Boundaries used in this paper are 
extracted by YASA, http://yasa.newzilla.org/ 
References 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: proba-
bilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference of Machine Learning, 591?598. 
Peter O'Boyle. 1993. A Study of an N-Gram Lan-
guage Model for Speech Recognition. PhD thesis. 
Queen's University Belfast. 
Fuchun Peng and Andrew McCallum. 2004. Chinese 
segmentation and new word detection using condi-
tional random fields. In Proceedings of Interna-
tional Conference of Computational linguistics, 
562?568, Geneva, Switzerland. 
Cheng-Lung Sung, Hsu-Chun Yen, and Wen-Lian 
Hsu. 2008. Compute the Term Contributed Fre-
quency. In Proceedings of the 2008 Eighth Inter-
national Conference on Intelligent Systems Design 
and Applications, 325-328, Washington, D.C., 
USA. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Da-
niel Jurafsky, and Christopher Manning. 2005. A 
conditional random field word segmenter for Sig-
han bakeoff 2005. In Proceedings of the Fourth 
SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea. 
Nianwen Xue and Libin Shen. 2003. Chinese word-
segmentation as LMR tagging. In Proceedings of 
the Second SIGHAN Workshop on Chinese Lan-
guage Processing. 
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumi-
ta. 2006. Subword-based tagging by conditional 
random fields for Chinese word segmentation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 193-
196, New York, USA. 
 
