Improved Statistical Machine Translation by Multiple Chinese Word
Segmentation



Ruiqiang Zhang1,2 and Keiji Yasuda1,2   and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai,  Science City, Kyoto, 619-0288, Japan
{ruiqiang.zhang,keiji.yasuda,eiichiro.sumita}@atr.jp







Abstract


Chinese word segmentation  (CWS) is a 
necessary step in Chinese-English statisti- 
cal machine translation (SMT) and its per- 
formance has an impact  on the results of 
SMT. However,  there are many settings in- 
volved in creating a CWS system such as 
various specifications and CWS methods. 
This paper investigates the effect of these 
settings  to SMT. We tested dictionary- 
based  and CRF-based  approaches  and 
found there was no significant difference 
between the two in the qualty of the result- 
ing translations. We also found the corre- 
lation between the CWS F-score and SMT 
BLEU score was very weak. This paper 
also proposes two methods of combining 
advantages of different specifications:    a 
simple concatenation of training data and 
a feature interpolation  approach in which 
the same types of features of translation 
models from various CWS schemes are 
linearly  interpolated.  We found these ap- 
proaches were very effective  in improving 
quality of translations.


1   Introduction

Chinese word segmentation (CWS) is a necessary 
step in Chinese-English statistical machine transla- 
tion (SMT). The research on CWS independently 
from SMT has been conducted  for decades.  As an 
evidence, the CWS evaluation campaign, the Sighan


Bakeoff (Emerson, 2005),1, has been held four times 
since 2004. However, works on relations between 
CWS and SMT are scarce.
  Generally, two factors need to be considered in 
constructing   a CWS system. The first one is the 
specifications for CWS, i.e., the rules or guidelines 
for word segmentation,  and the second one is the 
CWS methods. There are many CWS specifications 
used by different organizations. Unfortunately, these 
organizations do not seem to have any intention  of 
reaching a unified specification.   More than five or 
six specifications  have been used in the four Sighan 
Bakeoffs.  There is also significant disagreement on 
the specifications, although much of their contents is 
the same. One of the aims of this work was therefore 
to establish whether inconsistencies in specifications 
significantly affect the quality of SMT.
  The second factor is CWS methods. We grouped 
all of the CWS methods into two classes: the class 
without out-of-vocabulary (OOV) recognition and 
the class with OOV recognition,  represented by the 
dictionary-based CWS and the CRF-based CWS, re- 
spectively. Out-of-vocabulary recognition may have 
two-sided effects on SMT performance.  The CRF- 
based CWS that supports OOV recognition produces 
word segmentations with a  higher F-score,  but a 
huge number of new words recognized correctly and 
incorrectly that can incur data sparseness in training 
the SMT models. On the other hand, the dictionary- 
based approach that does not support OOV recogni- 
tion produced a lower F-score, but with a relatively 
weak data spareness problem.  Which  approach pro-

  1 A CWS competition organized by the ACL special interest 
group on Chinese.




216
Proceedings of the Third Workshop on Statistical Machine Translation, pages 216–223, 
Columbus, Ohio, USA, June 2008. Qc 2008 Association for Computational Linguistics


Table 1: Examples of disagreement in segmentation guidelines


ChineseName
EnglishName
Time
AS
DENGXIAOPING
GEORGE	BUSH
1997YEAR 7MONTH  1DAY
CITYU
DENGXIAOPING
GEORGEBUSH
1997 YEAR 7  MONTH 1 DAY
MSR
DENGXIAOPING
GEORGEBUSH
1997YEAR7MONTH1DAY
PKU
DENG XIAOPING
GEORGEBUSH
1997YEAR 7MONTH  1DAY


Table 2: A second example of disagreement in segmentation guidelines


Composite words
Composite words
AS
FUJITSUCOMPANY
EUROZONE
CITYU
FUJITSU COMPANY
EUROZONE
MSR
FUJITSUCOMPANY
EURO ZONE
PKU
FUJITSU COMPANY
EUROZONE




duces a better SMT result is our research interest in 
this work.
  The performance of CWS is usually measured by 
the F-score, while that of SMT is measured using 
the BLEU score. Does  a CWS with a higher F- 
score produce  a better  translation? In this paper 
we answer this question by comparing F-scores with 
BLEU scores.
  In this work, we also propose approaches to make 
use of all the Sighan training  data regardless of the 
specifications.   Two methods  are proposed:   (1) a 
simple combination of all the training  data, and (2) 
implementing linear interpolation of multiple trans- 
lation models. Linear interpolation is widely used in 
language modeling for speech recognition.   We in- 
terpolated multiple  translation models generated by 
the CWS schemes and found our approaches were 
very effective in improving the translations.

2	CWS specifications and corpora from 
the second Sighan Bakeoff

A Chinese word is composed of one or more char- 
acters. There  are no spaces between  the words. 
Automatic word segmentation is required for ma- 
chine translation.  Usually  a specification is needed 
to carry out word segmentation. Unfortunately, there 
are many different  versions of specifications. Differ- 
ent tasks give rise to different requirements and the 
CWS specifications must be adjusted accordingly. 
For example, shorter segmentation  has been shown


to be better for speech recognition. A composite 
word (numbers, dates, times, etc.) is split into char- 
acters even if it is one word defined by linguists. In 
contrast, longer segmentation is preferred for named 
entity recognition consisting of longer character se- 
quences, such as the name of people, places, and or- 
ganizations.

  This work investigated  four well-known spec- 
ifications created  by four different organizations: 
Academia  Sinica (AS), City University of Hong 
Kong (CITYU), Microsoft Research  (Beijing) 
(MSR), and Beijing University  (PKU). These specs 
were used in the second Sighan Bakeoff (Emerson,
2005). When we compared the four specifications 
and the manual segmentations in the Sighan Bakeoff 
training  data, we found there were many inconsis- 
tencies among the four specifications.  Some exam- 
ples are shown in Table 1 and 2. For instance, the 
AS and PKU specifications are distinct  in splitting 
both Chinese and English names. We also found the 
MSR specification  generated more composite words 
and grouped longer character sequences into a word. 
Using this specification could generate tens of thou- 
sands of new words, which can cause data sparse- 
ness for SMT.

  In addition to using the four specifications, we 
also downloaded the training and test corpora of the 
second Sighan Bakeoff. We used each of the train- 
ing corpora provided to create a CWS  scheme and 
evaluated the performance of the schemes on our test


data. This enabled us to examine the effect of CWS
specifications on SMT.
  We  used a Chinese  word segmentation tool, 
Achilles, to implement word segmentation.  Part of 
the work using this tool was described by (Zhang 
et al., 2006).  The approach was reported to achieve 
the highest word segmentation accuracy using the 
data from the second Sighan Bakeoff. Moreover, 
this tool meets our need to test the effect of the two 
kinds of CWS approaches for SMT. We can easily 
train a dictionary-based  and a CRF-based CWS by 
using this tool. By turning the program’s option for 
the CRF model on and off, we can use the Achilles 
as a dictionary-based  approach and as a CRF-based 
CWS. In fact, the dictionary-based approach is the 
default approach for Achilles.

3   Experiments

3.1   SMT resources
We followed the instructions for the 2005 NIST MT 
evaluation campaign. Training the translation mod- 
els for our SMT system used the available LDC par- 
allel data except the UN corpus. To train the lan- 
guage models  for English, we used all the avail- 
able English parallel data plus Xinhua News of the 
LDC Gigaword English corpus, LDC2005T12. In 
summary, we used 2.4 million parallel  sentences for 
training  the translation model. We used the test data 
defined in the NIST MT05 evaluation which is de- 
fined in the LDC corpus as LDC2006E38.  We used 
the corpus, LDC2006E43,  as the development data 
for loglinear model optimization.
  We used a phrase-based SMT  system that is based 
on a log-linear   model incorporating multiple fea- 
tures. The training and decoding system of our SMT 
used the publicly available Pharaoh (Koehn  et al.,
2003)2. GIZA++ was used for word alignment.
  The Pharaoh  decoder  was  used exclusively   in 
all the experiments.    No additional features  but 
the defaults defined by Pharaoh were used. The 
feature weights were optimized  against the BLEU 
scores (Och, 2003).
  We chose automatic metrics to evaluate CWS and 
SMT. We used the F-score for CWS and BLEU for 
SMT. The BLEU is BLEU4, computed using the 
NIST-provided “mt-eval” script.

2 http://www.iccs.informatics.ed.ac.uk/˜pkoehn


3.2   Implementation of CWS schemes

To determine the effect of CWS on SMT, we cre- 
ated 14 CWS schemes  which are  shown in Ta- 
ble 3.  Schemes 1 to 12 were implemented using 
the in-house tool, Achilles, and schemes 13 and 14 
using off-the-shelf tools.  The CWS schemes are 
named according to the specifications (AS, CITYU, 
MSR, PKU), implementing methods (CRF-based or 
dictionary-based), and lexicon sources (Sighan  or 
LDC corpus). The table also shows the results of 
segmentation on the SMT training and test data, i.e., 
number of total tokens,  unique words, and OOV 
words.
  We divided  the schemes into two groups for sim- 
plicity.  The first group includes  schemes 1 to 12, 
which were trained using a specific Sighan corpus. 
For example, schemes 1 to 3 were trained using the 
AS corpus,  schemes 4 to 6 using the CITYU cor- 
pus, and so on.  The meaning of the name of the 
CWS scheme can be derived from the table – the 
name is defined by specifications, methods and lexi- 
con sources. For example, the CRF-AS  scheme per- 
forms CRF-based segmentation;  and its lexicon is 
from the AS corpus provided by the Sighan. The 
CRF-AS segmenter  can be easily trained, as  de- 
scribed by Achilles.
The second group contains two schemes 13 and
14. The ICTCLAS is a HHMM-based hierarchical 
HMM segmenter  (Zhang et al., 2003) that uses the 
specifications of PKU. This segmenter incorporates 
parts-of-speech information  in the probability mod- 
els and generates multiple HMM models for solving 
segmentation ambiguities. The MSRSEG  was de- 
veloped by Gao et al. (Gao et al., 2004). This seg- 
menter is based on the MSR specifications.  It uses a 
log-linear model that integrates multiple features.
  The segmenters   of  the first  group, dict-AS 
and dict-LDC-AS, are two dictionary-based CWS 
schemes.   They differ in lexicon size and lexicon 
extracting source. The former used a lexicon ex- 
tracted directly from the Sighan AS training data 
while the latter used a lexicon from LDC parallel 
corpora. It took some efforts to get the lexicon.  First, 
we used the CRF-AS  to segment the LDC corpora. 
We extracted a unique word list from the segmented 
data and sorted it in decreasing order according to 
word frequency.   Because OOV was recognized by


Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemes


N
o.
C
W
S
 
s
c
h
e
m
e
s
Sp
eci
fic
ati
on
s
M
et
ho
ds
Le
xi
co
n
T
o
k
e
n
s
U
ni
qu
e 
w
or
ds
O
O
Vs
1
C
R
F-
A
S
A
S
C
R
F
Si
g
h
a
n
4
7
,
9
3
4
,
0
8
8
4
1
3
,
5
8
8
1,
1
9
3
2
di
ct-
A
S
A
S
D
i
c
t
Si
g
h
a
n
5
1
,
6
6
4
,
6
7
5
8
9
,
3
4
6
2
3
7
3
di
ct-
L
D
C-
A
S
A
S
D
i
c
t
L
D
C
4
8
,
6
6
5
,
3
6
4
1
0
2
,
9
1
9
2
7
3
4
C
R
F-
CI
T
Y
U
C
I
T
Y
U
C
R
F
Si
g
h
a
n
4
7
,
9
6
3
,
5
4
1
4
2
6
,
2
7
3
1,
1
5
5
5
di
ct-
CI
T
Y
U
C
I
T
Y
U
D
i
c
t
Si
g
h
a
n
5
1
,
2
5
1
,
7
2
9
5
6
,
9
9
6
3
6
2
6
di
ct-
L
D
C-
CI
T
Y
U
C
I
T
Y
U
D
i
c
t
L
D
C
4
8
,
7
8
7
,
1
5
4
1
0
2
,
7
5
4
2
1
7
7
C
R
F-
M
S
R
M
S
R
C
R
F
Si
g
h
a
n
4
6
,
4
8
3
,
9
2
3
5
2
3
,
7
8
8
1,
2
9
7
8
di
ct-
M
S
R
M
S
R
D
i
c
t
Si
g
h
a
n
5
1
,
3
0
2
,
5
0
9
6
0
,
2
4
7
2
4
8
9
di
ct-
L
D
C-
M
S
R
M
S
R
D
i
c
t
L
D
C
4
7
,
4
6
9
,
2
7
1
1
0
2
,
3
9
0
2
1
7
1
0
C
R
F-
P
K
U
P
K
U
C
R
F
Si
g
h
a
n
4
8
,
0
2
2
,
6
9
7
4
4
0
,
1
1
4
1,
1
3
6
1
1
di
ct-
P
K
U
P
K
U
D
i
c
t
Si
g
h
a
n
5
2
,
7
2
1
,
8
0
9
4
7
,
1
7
6
2
1
1
1
2
di
ct-
L
D
C-
P
K
U
P
K
U
D
i
c
t
L
D
C
4
8
,
7
2
1
,
7
9
5
1
0
2
,
2
1
3
2
5
6
1
3
IC
T
C
L
A
S
P
K
U
H
H
M
M
-
5
0
,
7
5
1
,
4
0
2
1
6
2
,
2
2
2
8
3
5
1
4
M
S
R
S
E
G
M
S
R
-
-
　
48
,7
34
,1
13
2
7
4
,
4
1
1
1,
4
4
3





the CRF-AS, a huge word list was generated(see Ta- 
ble 3). We chose the most frequent 100,000 words 
as the dictionary for the dict-LDC-AS  3. The LM for 
the dict-AS  was trained using the AS corpus while 
the LM for the dict-LDC-AS  was trained using the 
segmented SMT training  corpus.
  Therefore, the dict-LDC-AS  used a larger lexicon 
than the dict-AS. This lexicon contained the most 
frequent OOV words recognized by the CRF-AS. 
Our aim was to investigate whether the dict-LDC- 
AS, whose lexicon consisted of the lexicon of dict- 
AS and new words recognized by CRF-AS, could 
improve SMT.
  As shown in Table 3, using CRF-AS  generated a 
huge number of unique words for the training  data 
and OOV words for the test data. We found that 
the CRF-AS  generated three times more OOVs for 
the test data than the dictionary-based  CWS,dict-AS 
(see OOVs in Table 3).
  Other schemes  in the first group were imple- 
mented similarly  to the “AS”.
  Table 3 lists the segmentation statistics for the 
training  and test data of all the tested CWS schemes, 
where “Tokens” indicates the total number of words 
in the training data. “Unique words” and “OOVs”

  3 Only those words that appeared at least five times in the 
lexicon were considered.



Table 4: BLEU scores for CWS 
schemes


C
W
S
A
S
CI
T
Y
U
M
S
R
P
K
U
C
R
F
2
3.
7
0
2
3
.
5
5
22
.5
0
2
3
.
6
1
di
ct
2
3.
4
6
2
3
.
7
2
23
.3
3
2
3
.
6
1
di
ct-
L
D
C
2
3.
5
2
2
3
.
3
6
23
.1
6
2
3
.
7
4
IC
T
C
L
A
S
-
-
-
2
4
.
1
2
M
S
R
S
E
G
-
-
19
.7
2
-
B
E
S
T
2
3.
7
0
2
3
.
7
2
23
.3
3
23
.7
4 
(2
4.
12
)



mean the lexicon size of the segmented training  
data and the unknown words in the test data, 
respectively.

3.3   Effect of CWS  specifications on 
SMT

Our first concern was the effect of CWS 
specifica- tions on SMT. The results in Table 4 
show the rela- tionships that were found. 
The last row gives the best BLEU scores 
obtained  for each of the CWS specifications.  
The scores for AS, CITYU, MSR and PKU 
were 23.70 (CRF-AS),  23.72 (dict-CITYU),
23.33 (dict-MSR) and 23.74 (dict-PKU-
LDC), re- spectively.  We found there were no 
observable dif- ferences between AS, CITYU, 
and PKU. However, the specification that 
produced the worst transla- tions was the 
MSR. The MSR specification appears


to have been designed for recognizing named enti- 
ties (NE) (See the examples of segmentation in Ta- 
ble 1). Many NEs are regarded as words by MSR, 
while they are more appropriately  split into sepa- 
rate words by other specifications. For example, the 
long word, “1997YEAR7MONTH1DAY” (“July 1,
1997”). As a result, the CRF-MSR  generated 20% 
more words in the vocabulary than the other CWS 
schemes in segmenting the SMT training data. The 
larger  vocabulary  can trigger  data sparseness prob- 
lems and result in SMT degradation. The segmenter, 
MSRSEG,  produced an even  lower BLEU score 
(19.72) than the Achilles.
  The  results  were  verified  by   significance 
test (Zhang et al., 2004).  We found the systems 
with  the BLEU  scores  higher than 23.70 were 
significantly  better than those lower than 23.70.


3.4   Correlation  between BLEU score and
F-score

The values of the F-scores and BLEU scores  are 
listed in parallel in Table 5.  We tied the F-scores 
and specifications  together because comparing  the 
value of the F-score across specs is meaningless.  We 
separated the F-score and BLEU score for different 
corpus. The F-score was calculated using the Sighan 
test data. The CRF-based approach usually  gives a 
higher F-score, but its corresponding BLEU scores 
were not always higher.  The F-score and BLEU 
score correlated  well for ICTCLAS and CRF-AS 
but less well for CRF-CITYU, CRF-PKU and CRF- 
MSR. Obviously, there is no strong correlation be- 
tween the F-score and BLEU score.


4   Effect of combining multiple CWS
schemes

We  used the Sighan Bakeoff corpora of different 
CWS specifications separately in the previous ex- 
periments. Here, we propose two approaches to us- 
ing all the resources combined.  The first approach 
is to concatenate all the training  data of the Sighan 
Bakeoff, regardless of the specifications and train- 
ing a new CWS for segmenting SMT training data. 
The second approach involves linear integration of 
translation models. We found that both approaches 
produced an improvement in translation quality.


4.1	Effect of combining training data from 
multiple CWS specifications

The CWS specifications  are very different  and the 
corresponding Sighan training data are segmented 
in different ways. We used these data separately 
in the previous work as if they were incompatible. 
However,  creating data manually  is laborious and 
costly. It would therefore be a significant   advan- 
tage if all the data could be used, regardless of the 
different  specifications.  We therefore created a new 
CWS scheme, called “dict-hybrid”. This CWS was 
trained by concatenating all the Sighan Bakeoff cor- 
pora regardless of the different specifications.  The 
“dict-hybrid” was trained using Achilles. It uses a 
dictionary-based  approach, and its lexicon and lan- 
guage model were obtained as follows.
  First, we created  a hybrid corpus by combining 
all the Sighan training corpora: AS, CITYU, MSR, 
PKU. The hybrid corpus was used to train a CRF- 
based CWS.  This CWS was then used to segment 
the SMT training corpus and then we extracted  a 
lexicon of 100,000 from the top frequent words of 
the segmented SMT corpus. This lexicon was used 
as the lexicon  of the “dict-hybrid.” The LM of “dict- 
hybrid” was also trained on the segmented corpus. 
Note a lexicon and a LM are the only needed re- 
sources for building a dictionary-based  CWS,  like 
the “dict-hybrid.” (Zhang et al., 2006)
  We used the “dict-hybrid” to segment the SMT 
training corpus and test data.  This segmentation 
generated 49,546,231 tokens, 112,072 unique words 
for the training data and 693 OOVs for the test data.
  The segmentation  data were used for training a 
new SMT model. We tested the model using the 
same approach and found the BLEU score obtained 
by this CWS scheme was 23.91. This score was 
better than those in Table 4 obtained by any of the 
Achilles CWS schemes except  ICTCLAS. There- 
fore, the CWS scheme “dict-hybrid” produced better 
translations than other schemes implemented  using 
Achilles, indicating that using multiple CWS cor- 
pora can improve  SMT even if their specifications 
are different.
  Significance testing also showed that the results 
for ICTCLAS and “dict-hybrid” were not signifi- 
cantly different.  The results of “dict-hybrid” are sig- 
nificantly better than those in the Table 4 which have


Table 5: Correlation between F-score and BLEU

P
K
U

F-
sc
or
e
B
L
E
U
C
R
F
0
.
9
3
9
2
3.
6
1
d
i
c
t
0
.
9
3
0
2
3.
6
1
di
ct
-
L
D
C
0
.
9
3
1
2
3.
7
4
IC
T
C
L
A
S
0
.
9
4
8
2
4.
1
2

C
I
T
Y
U

F-
sc
or
e
B
L
E
U
C
R
F
0
.
9
2
0
2
3.
5
5
d
i
c
t
0
.
8
7
3
2
3.
7
2
di
ct
-
L
D
C
0
.
8
8
6
2
3.
3
6




a BLEU score lower than 23.70.

4.2	Effect of feature interpolation of 
translation models

We  investigated the effect of linearly integrating 
multiple features of the same type. We generated 
multiple translation models by using different word 
segmenters. Each translation model corresponded to 
a word segmenter.  The same type of features  as in 
the log-linear  model were added linearly.  For exam- 
ple, the phrase translation model p(e| f ) can be lin-


by ICTCLAS. “tst-B” means the test data segmented 
by “dict-hybrid”, and so on. The second line gives 
baseline results showing  the original results with- 
out the use of feature integration.  For different test 
data, the baseline is different. The baseline of ICT- 
CLAS was tested on “tst-A” only. The baseline of 
“dict-hybrid” was tested on “tst-B” only. From the 
third line we gradually  added a translation  model 
to the models  used in the baseline. For example, 
“A+B” integrates models made using ICTCLAS and 
“dict-hybrid.” Each integration  models were tested


early interpolated as, p(e| f ) = l..S


αi pi(e| f ) where


only on the test data participated  in 
the integration.


pi(e| f ) is the phrase translation model correspond- 
ing to the i-th CWSs. αi is the weight,  and S is the


Hence, some slots in Table 6 are blank.
We did not carry out parameter optimization  with


total number of models. l..S


αi = 1.


regards to the αs.  Instead, we 
used equal αs for all


  αs can be obtained by maximizing the likelihood 
or BLEU scores of the development data. Optimiz- 
ing the α has been described elsewhere (Foster and 
Kuhn, 2007). p(e| f ) is the phrase translation model 
generated.
  In addition to the phrase translation  model,  we 
used  the same  approach  to integrate  three other 
features:  phrase inverse probability p( f |e), lexical 
probability lex(e| f , a), and lexical inverse probabil- 
ity lex( f |e, a).
  We integrated the CWS schemes ranked  in the 
top five  in Table 4:  ICTCLAS, dict-hybrid, dict- 
LDC-PKU, dict-CITYU, and CRF-AS. We labeled 
the five schemes A, B, C, D, and E, respectively, 
as shown  in Table 6. The first line of Table 6 rep- 
resents the test data segmented  by the five  CWS 
schemes. “tst-A” means the test data was segmented


the features. For example, all αs equal 0.5 for A+B,
and 0.25 for A+B+C+D. Each cell in Table 6 indi- 
cates the BLEU score of the integration in relation 
to the test data. We found our approach improved 
the baseline results significantly.   The more models 
integrated, the better the results.  The improvement 
was positive for all of the test data. With regards to 
the integration, if a phrase pair exists in one model 
only, we suppose the values of probabilities  are zero 
in other models.
  To better understand the effects of feature inter- 
polation, we blended the features of the translation 
models, as shown in Table 7, by simply combining 
the phrase pairs without probability interpolation. 
When we merged two models, we defined one model 
as the master model and the other as the supple- 
mentary model. Only phrase pairs that were in the


supplementary models but not in the master model 
were appended to the master model. Their feature 
probabilities  were not changed. Hence, the com- 
bined model was a blend of phrase pairs from the 
master model and supplementary model. There was 
no probability  integration, that was significantly dif- 
ferent from the feature interpolation  approach. For 
each set of test data in Table 7, the master model 
was the model using the same CWS as the test data. 
While there was one row for each type of combina- 
tion, the cells in the row contained different models. 
For example, “A+B” for test data “A” uses “A” as the 
master model and “B” as the supplementary model, 
while the opposite holds for test data “B”.
  Comparing Table 6 and 7 showed  that feature 
interpolation  outperformed feature blending. Fea- 
ture interpolation yielded surprisingly good results. 
The performance consistently improved when more 
models were integrated, but this was not the case 
for feature blending. This shows that probability 
integration is very effective.   Increasing the size of 
phrase pairs, as feature blending  does, is not as ef- 
fective.
  We used equal values for the αs. Optimal values 
may be obtained using the optimization  approach 
of maximizing BLEU or the likelihood  of develop- 
ment  data as has been reported  previously   (Foster 
and Kuhn, 2007). However, optimization is compu- 
tationally  expensive and the effect was not satisfac- 
tory. Therefore, we decided not optimizing  the αs in 
this work.

5   Related work  and Discussions

CWS has  been  the subject  of intensive  research 
in  recent years, as   is  evident   from  the  last 
four international  evaluations,  the Sighan Bake- 
offs,  and many approaches  have  been proposed 
over the past decade. Segmentation performance 
has  been improved significantly, from the earli- 
est maximal  match (dictionary-based)  approaches to 
CRF (Peng and McCallum, 2004) approach. We 
used  dictionary-based   and CRF-based  CWS ap- 
proaches to demonstrate the effect of CWS on SMT, 
both without and with OOV recognition.
  SMT is a very complicated system to study. Its 
response to CWS schemes is intractable and it is 
very hard to use one or two measures to describe


the relationship between CWS and SMT, in a similar 
way to describing the relationship between the align- 
ment error rate (AER) and SMT (Fraser and Marcu,
2007). The CWS and SMT are related by a series of 
factors such as the specifications, OOVs, lexicons, 
and F-scores. None of these factors can be directly 
related to the SMT. While we have completed many 
experiments, based on changing the CWS specifica- 
tions and methods used, to determine the relation- 
ship between CWS and SMT, we have not estab- 
lished any overwhelming rules. However, we be- 
lieve the following guidelines are appropriate in con- 
sidering a CWS system for SMT. Firstly, the F-score 
is not a reliable guide to SMT quality. A very high 
F-score may produce the lowest quality translations, 
as was found for the MSRSEG. Secondly, it is better 
to design a specification  with smaller word units to 
reduce data sparseness. Specifications like those for 
MSR will produce an inferior translation.  Thirdly, 
do not use a huge  lexicon for word segmentation. 
A huge lexicon will  result in data sparseness and 
segmentation complexity. And lastly, using multi- 
ple word segmentation results and approaches does 
work. We used two approaches that combined mul- 
tiple word segmentation - dict-hybrid  and feature in- 
tegration - and both improved the translations signif- 
icantly.
  The BLEU scores in our experiments were rela- 
tively low in comparison with current state-of-the art 
results. However, our system was very similar to the 
system (Koehn et al., 2005) that gave a BLEU score 
of 24.3, comparable to ours. The BLEU score can 
be raised if we do post-editing,  use more data for 
language modeling and other methods.

6   Conclusions

We investigated the effect of CWS on SMT from 
two points of view.  Firstly, we analyzed multiple 
CWS specifications and built a CWS for each one to 
examine how they affected translations.  Secondly, 
we investigated the advantages and disadvantages of 
various CWS approaches, both dictionary-based and 
CRF-based, and built CWSs using these approaches 
to examine their effect on translations.
  We  proposed  a new approach to linear interpo- 
lation of translation features. This approach pro- 
duced a significant  improvement  in translation and


Table 6: Feature interpolation  of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-AS

M
o
d
e
l
ts
t-
A
ts
t-
B
ts
t-
C
ts
t-
D
ts
t-
E
B
a
s
e
l
i
n
e
24
.1
2
23
.9
1
23
.7
4
23
.7
2
23
.7
0
A
+
B
24
.2
5
24
.2
0



A
+
B
+
C
24
.4
9
24
.3
1
23
.8
4


A
+
B
+
C
+
D
24
.6
0
24
.4
3
24
.0
5
24
.2
7

A
+
B
+
C
+
D
+
E
24
.6
1
24
.5
5
24
.1
6
24
.3
9
24
.1
7


Table 7: Feature blending of translation models

M
o
d
e
l
ts
t-
A
ts
t-
B
ts
t-
C
ts
t-
D
ts
t-
E
B
a
s
e
l
i
n
e
24
.1
2
23
.9
1
23
.7
4
23
.7
2
23
.7
0
A
+
B
24
.2
0
24
.2
4



A
+
B
+
C
24
.2
7
24
.1
4
23
.6
9


A
+
B
+
C
+
D
23
.9
2
24
.2
9
23
.6
1
24
.0
0

A
+
B
+
C
+
D
+
E
23
.8
6
24
.3
1
23
.6
9
24
.0
5
23
.7
6




achieved  the best BLEU score of all the CWS
schemes.
  We  have  published  a much more detailed  pa- 
per (Zhang et al., 2008) to describe the relations be- 
tween CWS and SMT.


References

Thomas Emerson.  2005. The second international  chi- 
nese word segmentation bakeoff. In Proceedings of 
the Fourth SIGHAN Workshop on Chinese Language 
Processing, Jeju, Korea.

George Foster and Roland Kuhn. 2007. Mixture-model 
adaptation for SMT.  In Proceedings of the Second 
Workshop on Statistical Machine Translation, pages
128–135, Prague, Czech Republic,  June. Association 
for Computational Linguistics.

Alexander Fraser and Daniel Marcu. 2007. Measuring 
word alignment quality for statistical machine transla- 
tion. In Computational linguistics, Squibs Discussion, 
volume 33 of 3, pages 293–303, September.

Jianfeng Gao, Andi Wu, Mu Li,  Chang-Ning  Huang, 
Hongqiao Li, Xinsong Xia, and Haowei Qin.  2004. 
Adaptive  chinese word segmentation.   In ACL-2004, 
pages 462–469, Barcelona, July.

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical  phrase-based translation. In HLT-NAACL
2003: Main Proceedings,  pages 127–133.

Philipp  Koehn,  Amittai  Axelrod,  Alexandra Birch
Mayne, Miles Osborne Chris Callison-Burch, David


Talbot, and Michael White.  2005.  Edinburgh sys- 
tem description for the 2005 nist mt evaluation.   In 
Proceedings of Machine Translation Evaluation Work- 
shop.

Franz Josef Och.  2003.  Minimum error rate training 
in statistical machine translation.  In Proc. of the 41st 
Annual Meeting of the Association for Computational 
Linguistics (ACL), pages 160–167.

Fuchun  Peng and Andrew McCallum. 2004. Chinese 
segmentation  and new word detection using condi- 
tional random fields. In Proc. of Coling-2004,  pages
562–568, Geneva, Switzerland.

Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003.  HHMM-based  Chinese lexical analyzer ICT- 
CLAS. In Proceedings of the Second SIGHAN Work- 
shop on Chinese Language  Processing,  pages 184–
187.

Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In- 
terpreting BLEU/NIST scores:  How much improve- 
ment do we need to have a better system?  In Proceed- 
ings of the LREC.

Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random 
fields for chinese word segmentation. In Proceedings 
of the HLT-NAACL,  Companion Volume: Short Pa- 
pers, pages 193–196, New York City, USA, June.

Ruiqiang Zhang, Keiji  Yasuda,  and Eiichiro Sumita.
2008. Chinese word segmentation and statistical ma- 
chine translation. ACM Trans. Speech Lang. Process.,
5(2), May.

