



Which Performs Better on In-Vocabulary Word Segmentation: 
Based on Word or Character?
Zhenxing Wang1,2, Changning Huang2 and Jingbo Zhu1
1 Institute of Computer Software and Theory, Northeastern University,
Shenyang, China, 110004
2 Microsoft Research Asia, 49, Zhichun Road,
Haidian District, Beijing, China, 100080
zxwang@ics.neu.edu.cn v-
cnh@microsoft.com 
zhujingbo@mail.neu.edu.cn




Abstract

Since the first Chinese Word Segmenta- 
tion (CWS) Bakeoff on 2003, CWS has 
experienced a prominent flourish be- 
cause Bakeoff  provides  a  platform for 
the participants, which helps them rec- 
ognize the merits and drawbacks of their 
segmenters. However, the evaluation 
metric  of bakeoff is not  sufficient 
enough to measure the performance tho- 
roughly,   sometimes   even   misleading. 
One typical example caused by this in- 
sufficiency is that there is a popular be- 
lief existing in the research field that 
segmentation based on word can yield a 
better  result  than  character-based  tag- 
ging (CT) on in-vocabulary (IV) word 
segmentation even within closed tests of 
Bakeoff. Many efforts were paid to bal- 
ance the performance on IV and out-of- 
vocabulary (OOV) words by combining 
these two methods according to this be- 
lief. In this paper, we provide a more de- 
tailed evaluation metric of IV and OOV 
words than Bakeoff to analyze CT me- 
thod and combination method, which is 
a typical way to seek such balance. Our 
evaluation metric shows that CT outper- 
forms   dictionary-based   (or   so   called 
word-based in general) segmentation on 
both IV and OOV words within Bakeoff


* The work is done when the first author is working 
in MSRA as an intern.


closed tests. Furthermore, our analysis 
shows that using confidence measure to 
combine the two segmentation results 
should be under certain limitation.

1     Introduction

Chinese Word Segmentation (CWS) has been 
witnessed a prominent progress in the last three 
Bakeoffs (Sproat and Emerson, 2003), (Emer- 
son, 2005), (Levow, 2006). One of the reasons 
for this progress is that Bakeoff provides stan- 
dard corpora and objective metric, which makes 
the result of each system comparable. Through 
those evaluations researchers can recognize the 
advantage  and  disadvantage  of  their  methods 
and improve their systems accordingly. Howev- 
er, in the evaluation metric of Bakeoff, only the 
overall F measure, precision, recall, IV (in- 
vocabulary) recall and OOV (out-of-vocabulary) 
recall are included and such a metric is not suffi- 
cient to give a completely measure on the per- 
formance, especially when the performance on 
IV and OOV word segmentation need to be eva- 
luated. An important issue is that segmentation 
based on which, word or character, can yield the 
better performance on IV words. We give a de- 
tailed explanation about this issue as following.
   Since CWS was firstly treated as a character- 
based tagging task (we call it “CT” for short he- 
reafter) in (Xue and Converse, 2002), this me- 
thod has been widely accepted and further de- 
veloped  by  researchers  (Peng  et  al.,  2004), 
(Tseng et al., 2005), (Low et al., 2005), (Zhao et 
al.,    2006).    Relatively    to    dictionary-based







segmentation (we call it “DS” for short hereaf- 
ter), CT method can achieve a higher accuracy 
on OOV word recognition and a better perfor- 
mance of segmentation in whole. Thus, CT has 
drawn more and more attention and became the 
dominant method in the Bakeoff 2005 and 2006.
Although CT has shown its merits in word
segmentation  task,  some researchers  still  hold 
the belief that on IV words DS can perform bet- 
ter than CT even in the restriction of Bakeoff 
closed test. Consequently, many strategies are 
proposed to balance the IV and OOV perfor- 
mance (Goh et al., 2005), (Zhang et al., 2006a). 
Among these strategies, the confidence measure 
used to combine the results of CT and DS is a 
straight-forward one, which is introduced in 
(Zhang et al., 2006a). The basic assumption of 
such combination is that DS method performs 
better on IV words and Zhang derives this belief 
from the fact that DS achieves higher IV recall 
rate as Table 1 shows. In which AS, CityU, 
MSRA and PKU are four corpora used in Ba- 
keoff 2005 (also see Table 2 for detail). We pro- 
vide a more detailed evaluation metric to ana- 
lyze these two methods, including precision and 
F measure of IV and OOV respectively and our 
experiments show that CT outperforms DS on 
both IV and OOV words within Bakeoff closed 
test. The precision and F measure are existing 
metrics  and the definitions  of them are clear. 
Here we just employ them to evaluate segmenta- 
tion results.   Furthermore, our error analysis on 
the results of combination reveals that confi- 
dence measure in (Zhang et al., 2006a) has a 
representation flaw and we propose an EIV tag 
method to revise it. Finally, we give an empiri- 
cal comparison between existing pure CT me- 
thod and combination,  which shows  that  pure 
CT method can produce state-of-the-art results 
on both IV word and overall segmentation.


Corpus
RIV
ROOV

DS
CT
DS
CT
AS
0.982
0.967
0.038
0.647
CityU
0.989
0.967
0.164
0.736
MSRA
0.993
0.972
0.048
0.716
PKU
0.981
0.955
0.408
0.754
Table 1 IV and OOV recall in
(Zhang et al.,   2006a)
   The rest of this paper is organized as fol- 
lows. In Section 2, we give a brief introduction


to Zhang‟s  DS method and subword-based tag- 
ging, which is a special CT method. And by 
comparing the results of this special CT method 
and DS according our detailed metric, we show 
that CT performs better on both IV and OOV. 
We review in Section 3 how confidence measure 
works and indicate its representation flaw. Fur- 
thermore, an “EIV” tag method is proposed to 
revise the confidence measure. In Section 4, the 
experimental results of existing pure CT method 
are demonstrated to compare with combination 
result, based on which we discuss the related 
work. In Section 5, we conclude the contribu- 
tions of this paper and discuss the future work.

2	Comparison   between   DS   and   CT 
Based on Detailed Metric

We proposed a detailed evaluation metric for IV 
and OOV word identification in this section and 
experiments based on the new metric show that 
CT outperforms DS not only on OOV words but 
also on IV words with F-measure of IV.  All the 
experiments in this paper conform to the 
constraints of closed test in Bakeoff 2005 
(Emerson, 2005). It means that any resource 
beyond the training corpus is excluded. We first 
review how DS and CT work and then present 
our evaluation metric and experiment results. 
There is one thing should be emphasized, by 
comparing DS and CT result we just want to 
verify that our new metric can show the 
performance  on  IV  words  more  objectively. 
Since either DS or CT implementation has 
specific setting here we should not extend the 
comparison result to a general sense between 
those generative models and discriminative 
models.

2.1     Dictionary-based segmentation

For the dictionary-based word segmentation, we 
collect a dictionary from training corpus first. 
Instead of Maximum Match, trigram language 
model 2 trained on training corpus is employed 
for disambiguation. During the disambiguation 
procedure, a beam search decoder is used to seek 
the most possible segmentation. Since the setting 
in our paper is consistent with the closed test of


2 Language model used in this paper is SLRIM from 
http://www.speech.sri.com/projects/srilm/







Bakeoff,  we can only use the information  we 
learn from training corpus though other open 
resources may be helpful to improve the perfor- 
mance further. For detail, the decoder reads cha- 
racters from the input sentence one at a time, 
and generates candidate segmentations incre- 
mentally. At each stage, the next incoming cha- 
racter is combined with an existing candidate in 
two different ways to generate new candidates: it 
is either appended to the last word in the candi- 
date, or taken as the start of a new word. This 
method guarantees exhaustive generation of 
possible segmentations for any input sentence. 
However, the exponential time and space of the 
length of the input sentence are needed for such 
a search and it is always intractable in practice. 
Thus, we use the trigram language model to se- 
lect top B (B is a constant predefined before 
search and in our experiment 3 is used) best 
candidates with highest probability at each stage 
so that the search algorithm can work in practice. 
Finally, when the whole sentence has been read, 
the best candidate with the highest probability 
will  be  selected  as  the  segmentation  result. 
Here, the term “dictionary-based” is exactly the 
method implemented in (Zhang et al., 2006a), it 
does not mean the generative language model in 
general.

2.2     Character-based tagging

Under CT scheme, each character in one sen- 
tence is labeled as „B‟ if it is the beginning of a 
word, „O‟ tag means the current character is a 
single-character word, other character is labeled
as „I‟. For example, “全中国 (whole China)” is
labeled  as  “ 全  (whole)/O  中  (central)/B  国
(country)/I”.
   In (Zhang et al., 2006a), the above CT me- 
thod is developed as subword-based tagging. 
First, the most  frequent  multi-character  words 
and all single characters in training corpus are 
collected  as  subwords.  During  the  subword- 
based tagging, a subword is viewed as an unit 
instead of several separate characters and given 
only  one tag.  For  example,  in  subword-based
tagging, “全中国 (whole China)” is labeled as “
全 (whole)/O 中国 (China)/O”, if the word “中
国 (China)” is collected as a subword. As the
preprocessing, both training and test corpora are
segmented by maximum match with subword set


as dictionary. After this preprocessing, every 
sentence in both training and test corpora be- 
comes subword sequence. Finally, the tagger is 
trained by CRFs approach3 on the training data. 
Although  word  information  is  integrated  into 
this  method,  it  still  works  in  the  scheme  of 
“IOB”  tagging.  Thus,  we  still  call  subword- 
based tagging as a special CT method and in the 
reminder of this paper “CT” means subword- 
based tagging in Zhang‟s paper and “Pure CT” 
means CT without subword.

2.3     A detailed evaluation metric

In this paper, data provided by Bakeoff 2005 is 
used  in  our  experiments  in  order  to  compare 
with  the  published  results  in  (Zhang  et  al.,
2006a).     The statistics of the corpora for Ba- 
keoff 2005 are listed in Table 2 (Emerson, 2005).


Co
rp
us

En
co
din
g
#
T
r
a
i
n
i
n
g
 
w
o
r
d
s
#
T
es
t 
w
or
ds
O
O
V
ra
te
A
S
B
i
g
5
5
.
4
5
M
12
2K
0.0
43
Ci
ty
U
B
i
g
5
1
.
4
6
M
4
1
K
0.0
74
M
SR
A
G
B
2
.
3
7
M
10
7K
0.0
26
P
K
U
G
B
1
.
1
M
10
4K
0.0
58
Table 2 Corpora statistics of Bakeoff 2005

   Evaluation standard is also provided by Ba- 
keoff, including overall precision, recall, F 
measure, IV recall and OOV recall (Sproat and 
Emerson,   2003),   (Emerson,   2005).   However, 
some important metrics, such as F measure and 
precision of both IV and OOV words are omit- 
ted, which are necessary when the performance 
of IV or OOV word identification need to be 
judged.  Thus,  in order  to judge the results  of 
each  experiment,  a  more  detailed  evaluation 
with precision and F measure of both IV and 
OOV words included is used. To calculate the 
IV and OOV precision and recall, we firstly di- 
vide words of the segmenter‟s output and gold 
data into IV word and OOV word sets respec- 
tively with the dictionary collected from the 
training corpus. Then,  for  IV and OOV  word 
sets respectively, the IV (or OOV) recall is the 
proportion of the correctly segmented IV (or 
OOV) word tokens to all IV (or OOV) word to- 
kens in the gold data, and IV (or OOV) precision 
is the proportion of the correctly segmented IV


3 CRF tagger in this paper  is implemented by CRF++
downloaded from http://crfpp.sourceforge.net/







(or OOV) word tokens to all IV (or OOV) word 
tokens in the segmenter‟s output. One thing have 
to be emphasized is that the single character in 
test corpus will be defined as OOV if it does not 
appear in training corpus. We will see later in 
this section, by this evaluation, some facts cov- 
ered by the bakeoff evaluation can be illustrated 
by our new evaluation metric.
   Here, we repeat two experiments described in 
(Zhang et al., 2006a), namely dictionary-based 
approach and subword-based tagging. For CT 
method, top 2000 most frequent multi-character 
words and all single characters in training corpus 
are selected as subwords and the feature tem- 
plates used for CRF model is listed in Table 3. 
We present all the segmentation results in Table
6 to see the strength and weakness of each me-
thod conveniently.
Based on IV and OOV recall as we show in
Table 1, Zhang argues that the DS performs bet-


ter on IV word identification while CT performs 
better on OOV words. But we can see from the 
results in Table 6 (the lines about DS and CT), 
the IV precision of DS approach is much lower 
than that of CT on all the four corpora, which 
also causes a lower F measure of IV. The reason 
for low IV precision of DS is that many OOV 
words are segmented into two IV words by DS.
For example, OOV word “ 歌唱班(choral)” is
segmented into“ 歌唱 (sing)  班 (class)” by DS.
These wrongly identified IV words increase the 
number of all IV words in the segmenter‟s out- 
put and cause the low IV precision of the DS 
result. Since the F measure of IV is a more rea- 
sonable metric of performance of IV than IV 
recall only, Table 6 shows that CT method out- 
performs the DS on IV word segmentation over 
all four corpora. The comparison also shows that 
CT outperforms the DS on OOV and overall 
segmentation as well.



Ty
pe
Fe
atu
re
Fu
nct
ion
Un
igr
am
C-
2, 
C-
1, 
C0, 
C1, 
C2
Pre
vio
us 
tw
o, 
cur
ren
t 
an
d 
ne
xt 
tw
o 
su
bw
ord
Bi
gra
m
C-2 
C-
1, 
C-1 
C0, 
C0 
C1, 
C1 
C2
Tw
o 
adj
ace
nt 
su
bw
ord
s
Ju
mp
C-1 
C1
Pre
vio
us 
ch
ara
cte
r 
an
d 
ne
xt 
su
bw
ord
s
Table 3 Feature templates used for CRF in our experiments



3	Balance between  IV and  OOV  Per- 
formance

There are other strategies such as (Goh et al.,
2005) trying to seek balance between  IV and


When both results of CT and DS are available, 
the CM can be calculated according to the fol- 
lowing formula in (Zhang et al., 2006a):
CM(tiob | w)  CM iob (tiob  | w)  (1  ) (tw , tiob )ng


OOV performance. In (Goh et al, 2005), infor-


Here, w is a subword,


tiob  is “IOB” tag given


mation  in  a  dictionary  is  used  in  a  statistical 
model.  In  this  way,  the  dictionary-based  ap-


by CT and tw is “IOB” tag generated by DS. In 
the first term of the right hand side of the formu-


proach and the statistical model are combined.
We choose the confidence measure to study be-


la, CM



iob


(tiob


| w)


is  the marginal probability  of


cause it is straight-forward. We show in this sec-


tiob (we call this marginal probability “MP” for


tion  that  there is  a representation  flaw in  the


short). And in the second term,


 (tw , tiob )ng


is a


formula of confidence measure in (Zhang et al.,
2006a). And we propose an “EIV” tag method to


Kronecker delta function, returning 1 if and only


solve this problem. Our experiments show that


if t w  and


tiob  are identical, else returning 0. But


confidence measure with EIV tag outperforms


if  (t w , tiob ) ng    1 , there is no requirement of re-


CT and DS alone.


placement at all. 
While if  (t w


, tiob


) ng


 0 , when



3.1     Confidence measure

Confidence Measure (CM) means to seek an 
optimal tradeoff between performance on IV and 
OOV words. The basic idea of CM comes from 
the  belief  that  CT  performs  better  on  OOV 
words while DS performs better on IV words.


t w   tiob  , CM depends on the first term of its 
right hand side only and  is unnecessary to be 
set as a weight. Finally,  in the formula is a 
weight to seek balance between CT tag and DS 
tag. Another parameter here is a threshold t for
the CM. If CM is less than t , t w  replaces tiob as







the final tag, otherwise tiob will be remained as 
the final tag. However, two parameters in the
CM, namely  and t , are unnecessary, because
when MP is greater than or equal to t /  , tiob 
will be kept, otherwise it will be replaced with 
t w .   Thus, the CM ultimately is the marginal 
probability of the “IOB” tag (MP). In the expe- 
riment of this paper, MP is used as CM because 
it is equivalent to Zhang‟s CM but more conve- 
nient to express.

3.2	Experiments and error analysis about 
combination

We repeat the experiments about CM in Zhang‟s 
paper (Zhang et al., 2006a) and show that there 
is a representation flaw in the CM formula. Fur- 
thermore,  we  propose  an  EIV  tag  method  to 
make CM yield a better result.
   In this paper,  = 0.8 and t = 0.7 (Parameters 
in two papers, Zhang et al. 2006a and Zhang et
al. 2006b, are different. And our parameters are 
consistent with Zhang et al. 2006b which is con-
firmed by Dr Zhang through email) are used in 
CM, namely MP= 0.875 is the threshold. Here, 
in Table 4, we provide some statistics  on the
results of CT when MP is less than 0.875. From
Table 4 we can see that even with MP less than
0.875, most of the subwords are still tagged cor- 
rectly by CT and should not be revised by DS 
result. Besides, lots of the subwords with low 
MP contained by OOV words in test data, espe- 
cially for  the corpus whose OOV rate is high 
(i.e. on CityU corpus more than one third sub- 
words with low MP belong to OOV word) and 
performance on OOV recognition is the advan- 
tage of CT rather than that of DS approach. Thus 
when combining the results of the two methods, 
it is the tiob should be maintained if the subword 
is contained by an OOV word. Therefore, the 
CM formula seems somewhat unreasonable.
   The error analysis about how many original 
errors are eliminated and how many new errors
are introduced by CM is provided in Table 5 (the 
columns about CM). Table 5 illustrates that, af- 
ter combining the two results, most original er-
rors on IV words are corrected because DS can 
achieve higher IV recall as described in Zhang‟s 
paper. But on OOV part, more new errors are


introduced by CM and these new errors decrease 
the precision of the IV words. For example, the
OOV words “警卫队员 (guard member)” and “
设计费 (design fee)” is recognized correctly by
CT but with low CM. In the combining proce-
dure, these words are wrongly split as IV errors: 
“警卫 (guard) 队员 (member)” and “设计 (de- 
sign)  费  (fee)”.    Thus,  for  two  corpora  (i.e.
CityU and AS), F measure of IV and overall F 
measure decreases since there are more new er- 
rors  introduced  than  original  ones  eliminated 
and only on the other two corpora (MSRA and 
PKU), overall F measure of combination method 
is higher than CT alone, which is shown in Ta- 
ble 6 by the lines about combination.

3.3     EIV tag method

Since combining the two results by CM may 
produce an even worse performance in some 
case, it is worthy to study how to use this CM to 
get an enhanced result. Intuitively, if we can 
change only the CT tags of the subwords which 
contained in IV word while keep the CT tags of 
those contained in OOV words unchanged, we 
will improve the final result  according to  our 
error analysis in Table 5. Unfortunately, only 
from the test data, we can get the information 
whether a subword contained in an IV word, just 
as what we do to get Table 4. However, we can 
get an approximate estimation from DS result.
When using subwords to re-segment DS result4,
all the fractions re-segmented out of multiple- 
character words, including both multiple- 
character words and single characters, will be 
given an “EIV” tag, which means that the cur- 
rent multiple-character word or single character 
is contained in an IV word with high probability.
For example, “人力资源 (human resource)” in
DS result is a whole word. However, only “资源
(resource)” belongs to the subword set, so dur- 
ing the re-segmentation “人力资源 (human re- 
source)” will be re-segmented as “人 (people) 力 
(force) 资源 (resource)”. All these three frac-
tions will be labeled with an “EIV” tag respec- 
tively. It is reasonable because all the multiple- 
character words in the DS result can match an 
IV word. After this procedure, when combining


4 For the detail, please refer to (Zhang et al., 2006a).





C
o
r
p
u
s
A
S
Cit
yU
M
SR
A
PK
U
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
IV
10
01
0
44
04
9
5
5
2
96
19
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
IV 
an
d 
tag
ge
d 
cor
rec
tly 
by 
CT
74
52
34
34
7
4
5
2
72
13
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
IV 
an
d 
tag
ge
d 
wr
on
gly 
by 
CT
25
58
9
7
0
2
1
0
0
24
06
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
O
O
V
59
24
25
24
2
6
8
5
35
80
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
O
O
V 
an
d 
tag
ge
d 
cor
rec
tly 
by 
CT
31
77
16
56
1
7
2
5
22
08
# 
su
bw
ord 
tok
ens 
bel
on
g 
to 
O
O
V 
an
d 
tag
ge
d 
wr
on
gly 
by 
CT
27
47
8
6
8
9
6
0
13
72
Table 4 Results of CT when MP is less than 0.875

Co
rp
us
A
S
City
U
MS
RA
P
K
U
Me
tho
d
C
M
EI
V
C
M
EI
V
C
M
EI
V
C
M
EI
V
#o
rig
ina
l 
err
ors 
eli
mi
nat
ed 
on 
IV
19
05
10
03
90
4
46
9
19
59
10
77
19
23
11
87
#o
rig
ina
l 
err
ors 
eli
mi
nat
ed 
on 
O
O
V
75
5
7
5
15
5
8
0
1
0
4
3
0
23
0
7
6
#o
rig
ina
l 
err
ors 
eli
mi
nat
ed 
tot
all
y
26
60
10
78
10
59
54
9
20
63
11
07
21
53
12
63
#n
ew 
err
ors 
int
ro
du
ce
d 
on 
IV
44
1
18
5
8
0
5
0
1
4
8
6
8
21
1
11
8
#n
ew 
err
ors 
int
ro
du
ce
d 
on 
O
O
V
24
87
7
7
13
20
10
3
15
17
5
7
16
81
5
8
# 
ne
w 
err
ors 
int
ro
du
ce
d 
tot
all
y
29
28
26
2
14
00
15
3
16
65
12
5
18
92
17
6
Table 5 Error analysis of confidence measure with and without EIV tag



the two results, only the CT tag with EIV tags 
and low MP will be replaced by DS tag, other- 
wise the original CT tag will be maintained. Un- 
der this condition the errors introduced by OOV 
will not happen and enhanced results are listed 
in Table 6 lines about EIV. We can see that on 
all four corpora the overall F measure of EIV 
result  is  higher  than that  of CT  alone,  which 
show that  our  EIV method works  well.  Now, 
let‟s check what changes happened in the num- 
ber of error tags after EIV condition added into 
the CM. We can see from the Table 5 columns 
about EIV, there are more errors eliminated than 
the new errors introduced after EIV condition 
added into CM and most CT tags of subwords 
contained in OOV words maintained unchanged 
as we supposed. And then, our results (in Table
6 lines about EIV) are comparable with that in 
Zhang‟s paper. Thus, there may be some similar 
strategies in Zhang‟s  CM too but not presented 
in Zhang‟s paper.

4      Discussion and Related Works

Although the method such as confidence meas- 
ure can be helpful at some circumstance, our 
experiment shows that pure character-based tag- 
ging (pure CT) can work well with reasonable 
features and tag set. In (Zhao et al., 2006), an 
enhanced CRF tag set is proposed to distinguish 
different positions in the multi-character words 
when the word length is less than 6. In this me- 
thod, feature templates are almost the same as 
shown in Table 3 with a 3-character window and


a 6-tag set {B, B2, B3, M, E, O} is used. Here, 
tag B and E stand for the first and the last posi- 
tion in a multi-character word, respectively. S 
stands up a single-character word. B2 and B3 
stand for the second and the third position in a 
multi-character  word,  whose  length  is  larger 
than two-character or three-character. M stands 
for the fourth or more rear position in a multi- 
character word, whose length is larger than four- 
character.
   In Table 6, the lines about “pure CT” provide 
the results generated by pure CT with 6-tag set. 
We can see from the Table 6 this pure CT ap- 
proach achieves the state-of-the-art results on all 
the corpora. On three of the four corpora (AS, 
MSRA and PKU) this pure CT method gets the 
best result. Even on IV word, this pure CT ap- 
proach outperforms Zhang‟s CT method and 
produces comparable results with combination 
with EIV tags, which shows that pure CT me- 
thod can perform well on IV words too. Moreo- 
ver,  this  character-based  tagging  approach  is 
more clear and simple than the confidence 
measure method.
  Although character-based tagging became 
mainstream approach in the last two Bakeoffs, it 
does not mean that word information is valueless 
in Chinese word segmentation.  A word-based 
perceptron  algorithm  is  proposed  recently 
(Zhang and Clark, 2007), which views Chinese 
word segmentation task from a new angle in- 
stead of character-based tagging and gets com- 
parable results with the best results of Bakeoff.







Co
rp
us
Me
tho
d
R
P
F
RIV
PIV
FIV
RO
OV
PO
OV
FO
OV
AS
DS
0.9
43
0.8
81
0.9
11
0.9
84
0.8
92
0.9
35
0.0
44
0.2
17
0.0
76

CT
0.9
54
0.9
38
0.9
46
0.9
67
0.9
60
0.9
64
0.6
66
0.6
06
0.6
35

Co
mb
ina
tio
n
0.9
58
0.9
29
0.9
43
0.9
80
0.9
45
0.9
62
0.4
87
0.5
93
0.5
35

EI
V 
tag
0.9
60
0.9
42
0.9
51
0.9
73
0.9
62
0.9
68
0.6
67
0.6
24
0.6
45

Pu
re 
CT
0.9
58
0.9
47
0.9
53
0.9
71
0.9
63
0.9
67
0.6
82
0.6
18
0.6
48
Cit
yU
DS
0.9
28
0.8
48
0.8
86
0.9
89
0.8
65
0.9
23
0.1
62
0.3
53
0.2
23

CT
0.9
47
0.9
40
0.9
44
0.9
63
0.9
64
0.9
64
0.7
39
0.7
17
0.7
28

Co
mb
ina
tio
n
0.9
54
0.9
22
0.9
38
0.9
84
0.9
38
0.9
61
0.5
81
0.6
93
0.6
32

EI
V 
tag
0.9
53
0.9
49
0.9
51
0.9
70
0.9
68
0.9
69
0.7
44
0.7
50
0.7
47

Pu
re 
CT
0.9
47
0.9
48
0.9
48
0.9
67
0.9
73
0.9
70
0.6
92
0.6
60
0.6
76
M
SR
A
DS
0.9
69
0.9
27
0.9
47
0.9
94
0.9
30
0.9
61
0.0
36
0.3
58
0.0
66

CT
0.9
63
0.9
64
0.9
63
0.9
70
0.9
79
0.9
75
0.6
98
0.6
62
0.6
80

Co
mb
ina
tio
n
0.9
77
0.9
61
0.9
69
0.9
90
0.9
70
0.9
80
0.5
11
0.6
53
0.5
74

EI
V 
tag
0.9
72
0.9
70
0.9
71
0.9
80
0.9
82
0.9
81
0.6
96
0.6
79
0.6
88

Pu
re 
CT
0.9
72
0.
97
5
0.9
73
0.9
78
0.9
86
0.9
82
0.7
50
0.6
32
0.6
86
PK
U
DS
0.9
48
0.9
11
0.9
29
0.9
81
0.9
20
0.9
50
0.4
03
0.7
11
0.5
15

CT
0.9
44
0.9
45
0.9
45
0.9
55
0.9
66
0.9
61
0.7
63
0.7
27
0.7
45

Co
mb
ina
tio
n
0.9
55
0.9
42
0.9
49
0.9
73
0.9
53
0.9
63
0.6
64
0.7
82
0.7
18

EI
V 
tag
0.9
50
0.9
52
0.9
51
0.9
61
0.9
70
0.9
66
0.7
68
0.7
53
0.7
60

Pu
re 
CT
0.9
46
0.9
57
0.9
51
0.9
56
0.9
73
0.9
64
0.6
72
0.5
80
0.6
23
Table 6 Results of different approach used in our experiments (White background lines are
the results we repeat Zhang‟s methods and they have some trivial difference with Table 1.)



Therefore, the most important thing worth to pay 
attention in future study is how to integrate lin- 
guistic information into the statistical model effec- 
tively, no matter character or word information.

5     Conclusions and Future Work

In this paper, we first provided a detailed evalua- 
tion metric, which provides the necessary infor- 
mation to judge the performance of each method 
on IV and OOV word identification. Second, by 
this evaluation metric, we show that character- 
based tagging outperforms dictionary-based seg- 
mentation not only on OOV words but also on IV 
words within Bakeoff closed tests. Furthermore, 
our experiments show that confidence measure in 
Zhang‟s paper has a representation flaw and we 
propose an EIV tag method to revise the combina- 
tion. Finally, our experiments show that pure cha- 
racter-based approach also can achieve good IV 
word and overall performance. Perhaps, there are 
two  reasons  that  existing  combination  results 
don‟t outperform the pure CT. One is that most 
information contained in statistic language model 
is already captured by the CT feature templates in 
CRF  framework.  The  other  is  that  confidence



measure may not be the effective way to combine 
the DS and CT results.
   In the future work, our research will focus on 
how to integrate word information into CRF fea- 
tures rather than using it to modify the results of 
CRF  tagging.  In  this  way,  we  can  capture  the 
word information meanwhile avoid destroying the 
optimal output of CRF tagging.

Acknowledgement

The authors appreciate Dr. Hai Zhao in City Uni- 
versity of Hong Kong and Dr. Ruiqiang Zhang in 
Spoken Language Communications Lab, ATR, 
Japan providing a lot of help for this paper. Thank 
those reviewers who gave the valuable comment 
to improve this paper.

References

Thomas Emerson. 2005. The Second International Chi- 
nese Word Segmentation Bakeoff. In Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing, pages 123-133, Jeju Island, Korea:

Chooi-Ling Goh, Masayuku Asahara and Yuji Matsu- 
moto. 2005. Chinese Word Segmentatin by Classifi- 
cation of Characters. Computational Linguistics and







Chinese  Language  Processing,  Vol.  10(3):  pages
381-396.

Gina-Anne Levow. 2006. The Third International Chi- 
nese Language Processing Bakeoff: Word Segmen- 
tation and Named Entity Recognition. In   Proceed- 
ings of the Fifth SIGHAN Workshop on Chinese 
Language Processing , pages 108-117, Sydney: Ju- 
ly.

Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word 
Segmentation. In Proceedings of the Fourth SIGHAN 
Workshop on Chinese Language Processing, pages 161-
164, Jeju Island, Korea.

Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection 
using conditional random fields. In COLING 2004, 
pages 562–568. Geneva, Switzerland.

Richard Sproat  and Thomas Emerson. 2003. The First 
International Chinese Word Segmentation Bakeoff. In 
Proceedings of the Second SIGHAN Workshop on Chi- 
nese Language Processing, pages 133-143, Sapporo, Ja- 
pan: July 11-12,

Huihsin Tseng, Pichuan Chang et al. 2005. A Conditional
Random Field Word Segmenter for SIGHAN Bakeoff
2005. In Proceedings of the Fourth SIGHAN Workshop 
on Chinese Language Processing, pages 168-171, Jeju 
Island, Korea.

Neinwen Xue and Susan P. Converse. 2002. Combin- 
ing Classifiers for Chinese Word Segmentation. In 
Proceedings of the First SIGHAN Workshop on 
Chinese Language Processing, pages 63-70, Taipei, 
Taiwan.

Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.
2006a. Subword-based Tagging by Conditional 
Random Fields for Chinese Word Segmentation. In 
Proceedings of the Human Language Technology 
Conference of the NAACL, Companion volume, pag- 
es 193-196. New York, USA.

Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.
2006b. Subword-based Tagging for Confidence- 
dependent Chinese Word Segmentaion. In Proceed- 
ings of the COLING/ACL, Main Conference Poster 
Sessions, pages 961-968. Sydney, Australia.

Yue Zhang and Stephen Clark. 2007. Chinese Segmenta- 
tion with a Word-Based Perceptron Algorithm. In 
Proceedings of the 45th Annual Meeting of the As- 
sociation for Computational Linguistics, pages 840-
847. Prague, Czech Republic.

Hai Zhao, Changning Huang et al. 2006. Effective Tag 
Set Selection in Chinese Word Segmentation via 
Conditional Random Field Modeling. In  Proceed-


ings  of  PACLIC-20.  pages  87-94.  Wuhan,  China, 
Novemeber.




