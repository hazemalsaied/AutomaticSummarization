<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a novel method to recognise semantic equivalents of biomedical terms in language pairs.</S>
		<S sid ="2" ssid = "2">We hypothesise that biomedical term are formed by semantically similar textual units across languages.</S>
		<S sid ="3" ssid = "3">Based on this hypothesis, we employ a Random Forest (RF) classifier that is able to automatically mine higher order associations between textual units of the source and target language when trained on a corpus of both positive and negative examples.</S>
		<S sid ="4" ssid = "4">We apply our method on two language pairs: one that uses the same character set and another with a different script, English-French and EnglishChinese, respectively.</S>
		<S sid ="5" ssid = "5">We show that English-French pairs of terms are highly transliterated in contrast to the EnglishChinese pairs.</S>
		<S sid ="6" ssid = "6">Nonetheless, our method performs robustly on both cases.</S>
		<S sid ="7" ssid = "7">We evaluate RF against a state-of-the-art alignment method, GIZA++, and we report a statistically significant improvement.</S>
		<S sid ="8" ssid = "8">Finally, we compare RF against Support Vector Machines and analyse our results.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">Given a term in a source language and term in a target language the task of this paper is to classify this pair as a translation or not.</S>
			<S sid ="10" ssid = "10">We investigate the performance of the proposed classifier by applying it on a balanced classification problem, i.e. our experimental datasets contain an equal number of positive and negative examples.</S>
			<S sid ="11" ssid = "11">The proposed classification model can be used as a component of a larger system that automatically compiles bilingual dictionaries of technical terms across languages.</S>
			<S sid ="12" ssid = "12">Bilingual dictionaries of terms are important resources for many Natural Language Processing (NLP) applications including Statistical Machine Translation (SMT) (Feng et al., 2004; Huang and Vogel, 2002; Wu et al., 2008), Cross- Language Information Retrieval (Ballesteros and Croft, 1997) and Question Answering systems (AlOnaizan and Knight, 2002).</S>
			<S sid ="13" ssid = "13">Especially in the biomedical domain, manually creating and more importantly updating such resources is an expensive process, due to the vast amount of neologisms, i.e. newly introduced terms (Pustejovsky et al., 2001).</S>
			<S sid ="14" ssid = "14">The UMLS metathesaurus which is one the most popular hub of multilingual resources in the biomedical domain, contains technical terms in 21 languages that are linked together using a concept identifier.</S>
			<S sid ="15" ssid = "15">In Spanish, the second most popular language in UMLS, only 16.44% of the 7.6M English terms are covered while other languages fluctuate between 0.0052% (for Hebrew terms) to 3.26% (for Japanese terms).</S>
			<S sid ="16" ssid = "16">Hence, these lexica are far for complete and methods that semi- automatically (i.e., in a post-processing step, curators can manually remove erroneous dictionary entries) discover pairs of terms across languages are needed to enrich such multilingual resources.</S>
			<S sid ="17" ssid = "17">Our method can be applied to parallel, aligned corpora, where we expect approximately the same, balanced classification problem.</S>
			<S sid ="18" ssid = "18">However, in comparable corpora the search space of candidate alignments is of vast size, i.e., quadratic the the size of the input data.</S>
			<S sid ="19" ssid = "19">To cope with this heavily unbalanced classification problem, we would need to narrow down the number of negative instances before classification.</S>
			<S sid ="20" ssid = "20">We hypothesise that there are language independent rules that apply to biomedical terms across many languages.</S>
			<S sid ="21" ssid = "21">Often the same or similar textual units (e.g., morphemes and suffixes) are concatenated to realise the same terms in different languages.</S>
			<S sid ="22" ssid = "22">For example, Table 1 illustrates how a morpheme expressing pain (ache in English) is used to realise the same terms in English, Chinese and French.</S>
			<S sid ="23" ssid = "23">The realisations of the term “head 95 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95–104, Sofia, Bulgaria, August 8, 2013.</S>
			<S sid ="24" ssid = "24">Qc 2013 Association for Computational Linguistics English Morpheme: -ache Chinese Morpheme: 痛 French Morpheme: -mal headache 头-痛 mal de teˆte backache 腰-痛 mal au dos earache 耳朵-痛 mal d’oreille Table 1: An example of English, Chinese and French terms consisting of the same morphemes ache” is expected to consist of the units for “head” and “ache” regardless of the language of realisation.</S>
			<S sid ="25" ssid = "25">Hence, knowing the translations of “head” and “ache” allows the reconstruction “headache” in a target language.</S>
			<S sid ="26" ssid = "26">In our method, we use a Random Forest (RF) classifier (Breiman, 2001) to learn the underlying rules according to which terms are being constructed across languages.</S>
			<S sid ="27" ssid = "27">An RF is an ensemble of Decision Trees voting for the most popular class.</S>
			<S sid ="28" ssid = "28">RF classifiers are popular in the biomedical domain for various tasks: classification of microarray data (D´ıazUriarte and De Andres, 2006), compound classification in cheminformatics (Svetnik et al., 2003), classification of microRNA data (Jiang et al., 2007) and protein-protein interactions in Systems Biology (Chen and Liu, 2005).</S>
			<S sid ="29" ssid = "29">In NLP, RF classifiers have been used for: Language Modelling (Xu and Jelinek, 2004) and semantic parsing (Nielsen and Pradhan, 2004).</S>
			<S sid ="30" ssid = "30">To the best of the authors’ knowledge, this is the first attempt to employ RF for identifying translation equivalents of biomedical terms.</S>
			<S sid ="31" ssid = "31">We prefer RF over other traditional machine learning approaches such as Support Vector Machines (SVMs) for a number of reasons.</S>
			<S sid ="32" ssid = "32">Firstly, RF is able to automatically construct correlation paths from the feature space, i.e. decision rules that correspond to the translation rules that we intend to capture.</S>
			<S sid ="33" ssid = "33">Secondly, RF is considered one of the most accurate classifier available (D´ıazUriarte and De Andres, 2006; Jiang et al., 2007).</S>
			<S sid ="34" ssid = "34">Finally, RF is reported to cope well with datasets where the number of features is larger than the number of observations (D´ıazUriarte and De Andres, 2006).</S>
			<S sid ="35" ssid = "35">In our dataset, the number of features is almost four times more than that of the observations.</S>
			<S sid ="36" ssid = "36">We represent pairs of terms using character gram features (i.e., first order features).</S>
			<S sid ="37" ssid = "37">Such shallow features have been proven effective in a number of NLP applications including: Named Entity Recognition (Klein et al., 2003), Multilingual Named Entity Transliteration (Klementiev and Roth, 2006; Freitag and Khadivi, 2007) and predicting authorship (Stamatatos, 2006).</S>
			<S sid ="38" ssid = "38">In addition, by selecting character n-grams instead of word n-grams, one avoids to segment words in Chinese which has been proven to be a challenging topic (Sproat and Emerson, 2003).</S>
			<S sid ="39" ssid = "39">We evaluate our proposed method on two datasets of biomedical terms (English-French and EnglishChinese) that contain equal numbers of positive and negative instances.</S>
			<S sid ="40" ssid = "40">RF achieves higher classification performance than baseline methods.</S>
			<S sid ="41" ssid = "41">To boost SVM’s performance further, we used a second order feature space to represent the data.</S>
			<S sid ="42" ssid = "42">It consists of pairs of character grams that co-occur in translation pairs.</S>
			<S sid ="43" ssid = "43">In the second order feature space, the performance of SVMs improved significantly.</S>
			<S sid ="44" ssid = "44">The rest of the paper is structured as follows.</S>
			<S sid ="45" ssid = "45">In Section 2, we present previous approaches in identifying translation equivalents of terms or named entities.</S>
			<S sid ="46" ssid = "46">In Section 3, we define the classification problem, we formulate the RF classifier and we discuss the first and second order feature space that we use to represent pairs of terms.</S>
			<S sid ="47" ssid = "47">In Section 4, we show that RF achieves superior classification performance.</S>
			<S sid ="48" ssid = "48">In Section 5, we overview our method and we discuss how it can be used to compile large-scale bilingual dictionaries of terms from comparable corpora.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="49" ssid = "1">In this section, we review previous approaches that exploit the internal structure of sequences to align terms or named entities across languages.</S>
			<S sid ="50" ssid = "2">(Klementiev and Roth, 2006; Freitag and Khadivi, 2007) use character gram features, similar to the feature space that we propose in this paper, to train discriminative, supervised models.</S>
			<S sid ="51" ssid = "3">Klementiev and Roth (2006) introduce a supervised Perceptron model for English and Russian named entities.</S>
			<S sid ="52" ssid = "4">They construct a character gram feature space as follows: firstly, they extract all distinct character grams from both source and target named entity.</S>
			<S sid ="53" ssid = "5">Then, they pair character grams of the source named entity with character grams of the corresponding target named entity into features.</S>
			<S sid ="54" ssid = "6">In or der to reduce the number of features, they link only those character grams whose position offsets in the source and target sequence differs by -1, 0 or 1.</S>
			<S sid ="55" ssid = "7">Freitag and Khadivi (2007) employ the same character gram feature space but they do not constraint the included character-grams to their relative position offsets in the source and target sequence.</S>
			<S sid ="56" ssid = "8">The boolean features are defined for every distinct character-grams observed in the data of length k or shorter.</S>
			<S sid ="57" ssid = "9">Using this feature space they train an Averaged Perceptron model, able to incorporate an arbitrary number of features in the input vectors, for English and Arabic named entities.</S>
			<S sid ="58" ssid = "10">The above character gram based methods mainly focused on aligning named entities of the general domain, i.e. person names, locations, organizations, etc., that are transliterated, i.e. present phonetic similarities, across languages.</S>
			<S sid ="59" ssid = "11">SMT-based approaches built on top of existing SMT frameworks to identify translation pairs of terms (Tsunakawa et al., 2008; Wu et al., 2008).</S>
			<S sid ="60" ssid = "12">Tsunakawa et al.</S>
			<S sid ="61" ssid = "13">(2008), align terms between a source language Ls and a target language Lt using a pivot language Lp.</S>
			<S sid ="62" ssid = "14">They assume that two bilingual dictionaries exist: from Ls to Lp and from Lp to Lt. Then, they train GIZA++ (Och and Ney, 2003) on both directions and they merge the resulting phrase tables into one table between Ls and Lt, using grow-diag-final heuristics (Koehn et al., 2007).</S>
			<S sid ="63" ssid = "15">Wu et al.</S>
			<S sid ="64" ssid = "16">(2008), use morphemes instead of words as translation units to train a phrase based SMT system for technical terms in English and Chinese.</S>
			<S sid ="65" ssid = "17">The use of shorter lexical fragments, e.g. lemmas, stems and suffixes, as translation units has reportedly reduced the Out-Of-Vocabulary problem (Virpioja et al., 2007; Popovic and Ney, 2004; Oflazer and ElKahlout, 2007).</S>
			<S sid ="66" ssid = "18">Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).</S>
			<S sid ="67" ssid = "19">For instance, person names are usually translated by transliteration (i.e., words exhibiting pronunciation similarities across languages, are likely to be mutual translations) while technical terms are likely to be translated by meaning (i.e., the same semantic units are used to generate the translation of the term in the target Lepage and Denoual (2005) presented an analog- ical learning machine translation system as part of the IWSLT task (Eck and Hori, 2005) that requires no training process and it is able to achieve state-of-the art performance.</S>
			<S sid ="68" ssid = "20">The core method of their system models relationships between sequences of characters, e.g., sentences, phrases or words, across languages using proportional analogies, i.e., [a : b = c : d], “a is to b as c is to d”, and is able to solve unknown analogical equations, i.e., [x : y = z :?]</S>
			<S sid ="69" ssid = "21">(Lepage, 1998).</S>
			<S sid ="70" ssid = "22">Analogical learning has been proven effective in translating unseen words (Langlais and Patry, 2007).</S>
			<S sid ="71" ssid = "23">Furthermore, analogical learning is reported to achieve a better precision but a lower recall than a phrase- based machine translation system when translating medical terms (Langlais et al., 2009).</S>
			<S sid ="72" ssid = "24">3 Methodology Let em = (e1, · · · , em) be an English term consisting of m translation units and f n = (f1, · · · , fn) a French or Chinese term consist ing of n units.</S>
			<S sid ="73" ssid = "25">As translation units, we con sider character grams.</S>
			<S sid ="74" ssid = "26">We define a function f : (em, f n) −→ {0, 1}: ( 1, if em translates into f n f (em, f n) = 0, otherwise The function can be learned by training a Random Forest (RF) classifier1.</S>
			<S sid ="75" ssid = "27">Let N be the number of training instances, |Ω| the total number of features, i.e. the number of dimensions of the feature space, |τ | a predefined number of random decision trees and |φ| a predefined number of random features.</S>
			<S sid ="76" ssid = "28">An RF classifier is defined as a collection of fully grown decision tree classifiers, δi(X ) (Breiman, 200 1): RF = {δ1(X ), · · · , δτ (X )}, X = (em, chn) (1) A pair of terms is classified as a translation pair if the majority of the trees is voting for this class label.</S>
			<S sid ="77" ssid = "29">Let I (δi(X )) be the vote of the ith tree in the forest and avj∈{0,1} the average number of votes for class labels 0 (translation) and 1 (non- translation).</S>
			<S sid ="78" ssid = "30">The function f of τ decision trees can be written as the majority function: f (em, chn) = Maj (I (δ1(X )), · · · , I (δτ (X )))language).</S>
			<S sid ="79" ssid = "31">The resulting hybrid systems were re = 1 I (δi(X )) + 1/2(−1) (2) ported to perform at least as well as existing SMT 2 τ systems (Feng et al., 2004).</S>
			<S sid ="80" ssid = "32">1 The WEKA.</S>
			<S sid ="81" ssid = "33">implementation (Hall et al., 2009) of RF was used for all experiments of this paper.</S>
			<S sid ="82" ssid = "34">The majority function returns 1 if the majority of I (δi(X )) is 1, or returns 0 if the majority of I (δi(X )) is 0.</S>
			<S sid ="83" ssid = "35">Adding or subtracting 1/2 controls whether a tie is resolved towards 1 or 0, respectively.</S>
			<S sid ="84" ssid = "36">In RF ties are resolved randomly.</S>
			<S sid ="85" ssid = "37">To rep resent this, the negative unit (−1) is raised to a randomly chosen positive integer r ∈ N+.</S>
			<S sid ="86" ssid = "38">We tuned the RF classifier using 140 random trees and |φ| = log2 |Ω| + 1 features as suggested in Breiman (Breiman, 2001).</S>
			<S sid ="87" ssid = "39">The RF mechanism that triggers term construction rules across languages lies in the decision trees.</S>
			<S sid ="88" ssid = "40">A RF grows a decision tree by selecting the most informative feature, i.e. corresponding to the lowest entropy, out of φ random features.</S>
			<S sid ="89" ssid = "41">For each selected feature, a node is created and this process is repeated for all φ random features of the unprunned decision trees.</S>
			<S sid ="90" ssid = "42">In other words, the process starts with the most informative feature and builds association rules between all random features.</S>
			<S sid ="91" ssid = "43">These are the construction rules that we are interested in.</S>
			<S sid ="92" ssid = "44">Figure 1 illustrates a path in one of the decision trees of an RF classifier taken from the experiments we conducted on the EnglishChinese dataset.</S>
			<S sid ="93" ssid = "45">In only one of thousands of branches of the forest, the classifier is able to partially trigger the construction rule of kinase, a type of enzyme, between English and Chinese.</S>
			<S sid ="94" ssid = "46">The translation rule correctly associates the English n-grams kin and as with their Chinese translation 激酶.</S>
			<S sid ="95" ssid = "47">In addition, the translation rule contains both positive and negative associations between features.</S>
			<S sid ="96" ssid = "48">The English n-grams ing and or are negatively correlated with the term kinase.</S>
			<SUBSECTION>3.1 Feature Engineering.</SUBSECTION>
			<S sid ="97" ssid = "49">Each pair of terms is represented as a feature vector of character n-grams.</S>
			<S sid ="98" ssid = "50">We further define two types of character n-gram features, namely first order and second order.</S>
			<S sid ="99" ssid = "51">First order character n- grams are boolean features that designate the occurrence of a corresponding character gram of predefined length in the input term.</S>
			<S sid ="100" ssid = "52">These features are monolingual, extracted separately from the source and target term.</S>
			<S sid ="101" ssid = "53">The RF classifier is shown to benefit from only monolingual features and achieves the best observed performance.</S>
			<S sid ="102" ssid = "54">In contrast, SVMs were shown not to perform well using the first order feature space because they cannot directly associate the source with the target character grams.</S>
			<S sid ="103" ssid = "55">To enhance the performance of SVMs, we constructed a second order feature space that contains associations between first order features.</S>
			<S sid ="104" ssid = "56">A second order feature is a tuple of a source and a target character gram that co-occur in one or more translation pairs.</S>
			<S sid ="105" ssid = "57">Table 2 illustrates an example.</S>
			<S sid ="106" ssid = "58">Second order character n-grams are multilingual features and are defined over true translation pairs.</S>
			<S sid ="107" ssid = "59">For this reason, we extract second order features from the training data only.</S>
			<S sid ="108" ssid = "60">In all experiments, the features were sorted in decreasing order of frequency of occurrence.</S>
			<S sid ="109" ssid = "61">We trained a RF and two SVM classifiers, namely linear-SVM and RBFSVM, using a gradually increasing number of features, always starting from the top of the list.</S>
			<S sid ="110" ssid = "62">SMT frameworks cannot be trained on an increasing number of features because each training instance needs to correspond to at least one known translation unit (i.e., first order features).</S>
			<S sid ="111" ssid = "63">Therefore, GIZA++ is trained on the complete set of translation units.</S>
			<S sid ="112" ssid = "64">4 Experiments.</S>
			<S sid ="113" ssid = "65">In this section, we discuss the employed datasets of biomedical terms in English-French and EnglishChinese and three baseline methods.</S>
			<S sid ="114" ssid = "66">We compare and discuss RF and SVMs trained on the first order and second order features.</S>
			<S sid ="115" ssid = "67">Finally, we report results of all classification methods evaluated on the same datasets.</S>
			<S sid ="116" ssid = "68">4.1 Datasets.</S>
			<S sid ="117" ssid = "69">For our experiments, we used an online bilingual dictionary2 for EnglishChinese terms and the UMLS metathesaurus3 for English-French terms.</S>
			<S sid ="118" ssid = "70">The former contains 31, 700 entries while the latter is a much larger dictionary containing 84, 000 entries.</S>
			<S sid ="119" ssid = "71">For training, we used the same number of instances for both language pairs (i.e., 21, 000 entries) in order not to bias the performance towards the larger English-French dataset.</S>
			<S sid ="120" ssid = "72">The remaining instances were used for testing (i.e., 10, 7000 and 63, 000 EnglishChinese and English-French respectively).</S>
			<S sid ="121" ssid = "73">In the case where a source term corresponded to more that one target terms according to the seed dictionary, we randomly selected only one translation.</S>
			<S sid ="122" ssid = "74">Negative instances were created by randomly matching non-translation pairs of terms.</S>
			<S sid ="123" ssid = "75">Since we are dealing with a balanced clas 2 www2.chkd.cnki.net/kns50/ 3 nlm.nih.gov/research/umls Figure 1: Example of a term construction rule as a branch in a decision tree.</S>
			<S sid ="124" ssid = "76">Input pair of English-French terms : (e1, e2, e3, f1, f2, f3) English first order French first order Second order φ1(e1, e2) φ1(e2, e3) φ1(f1, f2) φ1(f2, f3) φ1(e1e2, f1f2), φ1(e1e2, f2f3) φ1(e2e3, f1f2), φ1(e2e3, f2f3) Table 2: Example of first and second order features using a predefined n-gram size of 2.</S>
			<S sid ="125" ssid = "77">sification problem, we created as many negative instances as the positive ones in all our datasets.</S>
			<S sid ="126" ssid = "78">In all experiments we performed a 3-fold cross- validation.</S>
			<S sid ="127" ssid = "79">4.2 Baselines.</S>
			<S sid ="128" ssid = "80">We evaluated RF against three classification methods, namely SVMs, GIZA++ and a Levenshtein distance-based classifier.</S>
			<S sid ="129" ssid = "81">SVMs coordinate a hyperplane in the hyperspace defined by the features to best separate the positive and negative instances, i.e. aligned from nonaligned pairs.</S>
			<S sid ="130" ssid = "82">In contrast to RF, SVMs do not support building association rules between features, i.e., translation units, which in our task seems to be a deficiency.</S>
			<S sid ="131" ssid = "83">SVMs produce one final association rule, i.e. the classification boundary which separates positive from negative examples.</S>
			<S sid ="132" ssid = "84">Its ability to distinguish aligned from nonaligned pair of terms depends on how separable the two clusters are.</S>
			<S sid ="133" ssid = "85">We evaluated several settings for the open source implementation of the 5 IBM-models (Brown et al., 1993).</S>
			<S sid ="134" ssid = "86">GIZA++ is traditionally trained on a bilingual, parallel corpus of aligned sentences and estimates the probability P (s|t) of a source translation unit (typically a word), s, given a target unit t. To apply GIZA++ on our dataset, we consider the list of terms as parallel sentences.</S>
			<S sid ="135" ssid = "87">GIZA++, trained on a list of terms, estimates the alignment probability of EnglishChinese and English-French textual units, i.e. character n- grams.</S>
			<S sid ="136" ssid = "88">Each entry i, j in the translation table is the probability P (si|tj ), where si and tj are the source and target character n-grams in row i and column j, respectively.</S>
			<S sid ="137" ssid = "89">Further details about training a SMT toolkit for aligning technical terms can be found in (Tsunakawa et al., 2008; Freitag and Khadivi, 2007; Wu et al., 2008).</S>
			<S sid ="138" ssid = "90">After training GIZA++ we estimate the posterior probabil ity P (cf n|em) that a test, Chinese or French termcf n = {cf1, · · · , cfn} is aligned with a given En glish term em = {e1, · · · , em} as follows: SVM classifier.</S>
			<S sid ="139" ssid = "91">Apart from the default linear ker- n m nel function, we applied a radial basis function, i.e. RBFSVM.</S>
			<S sid ="140" ssid = "92">RBFSVM uses the kernel trick to project the instances in a higher dimensional space to better separate the two clusters.</S>
			<S sid ="141" ssid = "93">While tuning the SVM’s classification cost C, we observed optimal performance for a value of 100.</S>
			<S sid ="142" ssid = "94">Secondly, p(cf n|em) = n−m P (cfi|ej ) (3) i=1 j=1 A threshold ξ was defined to classify a pair of terms into translations or non-translations: ( 1, if p(cf n|em) ≥ ξ we seeded the association rules of translation units to the SVM classifier by creating a second or f (em, cf n) = 0, otherwise (4) der feature space, discussed in detail in section 3.1.</S>
			<S sid ="143" ssid = "95">We employed the LIBSVM implementation.</S>
			<S sid ="144" ssid = "96">(Chang and Lin, 2011) of SVMs using both the linear and RBF kernels.</S>
			<S sid ="145" ssid = "97">The second baseline method is GIZA++, an We experimented with different values of ξ (greedy search) and we selected a value that maximizes classification performance.</S>
			<S sid ="146" ssid = "98">In order to estimate how phonetically similar the two language pairs are, we employed a third base (a) English-French dataset (b) EnglishChinese dataset Figure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the first order dataset line method that uses the Edit/Levenshtein distance of pairs of terms to classify instances as translations or not.</S>
			<S sid ="147" ssid = "99">The Levenshtein distance is defined as the minimum edit operations, i.e., insertion, deletions and substitution, required to transform one sequence of characters to another.</S>
			<S sid ="148" ssid = "100">We cannot directly calculate the Levenshtein distance between EnglishChinese pairs of terms since the two languages are using different scripts.</S>
			<S sid ="149" ssid = "101">Therefore, before we applied the Levenshtein distance- based classifier, we converted the Chinese terms to their pinyin form, i.e., Romanization system of Chinese characters.</S>
			<S sid ="150" ssid = "102">As with GIZA++, we selected a threshold ξ that maximizes the performance of the classifier.</S>
			<S sid ="151" ssid = "103">4.3 Results.</S>
			<S sid ="152" ssid = "104">We hypothesise that a RF classifier is able to form association paths between first order features.</S>
			<S sid ="153" ssid = "105">We also have the theoretical intuition that SVM classifiers are not able to form such association paths.</S>
			<S sid ="154" ssid = "106">As a result, we expect limited performance on the first order feature set, because it does not contain any associations among character grams.</S>
			<S sid ="155" ssid = "107">Figure 2 shows the F-Score achieved by RF, linear- SVM, RBFSVM, GIZA++ and Levenshtein/Edit distance-based classifier on the English-French and EnglishChinese datasets.</S>
			<S sid ="156" ssid = "108">RF and SVMs are trained on an increasing number of features.</S>
			<S sid ="157" ssid = "109">The behaviour of the classifiers is approximately the same in both datasets.</S>
			<S sid ="158" ssid = "110">Performance is greater on the English-French dataset since English is more similar to French than to Chinese.</S>
			<S sid ="159" ssid = "111">We also observe that linear-SVM and RBFSVM do not behave consistently.</S>
			<S sid ="160" ssid = "112">RBFSVM’s performance quickly climbs to a maximum and after wards it declines while linear-SVM’s performance is constantly increasing until it balances to a very high error rate, almost corresponding to random classification.</S>
			<S sid ="161" ssid = "113">The linear-SVM classifier performs poorly using first order features only, indicating that this feature space is non-linearly separable, i.e. there exists no hyperplane that separates translation from non-translation instances.</S>
			<S sid ="162" ssid = "114">Contrary, RBFSVM is able to construct a higher dimensional space by applying the kernel trick so as to take full advantage of a small number of frequent and informative first order features.</S>
			<S sid ="163" ssid = "115">In this higher dimensional space of few but informative first order features, the RBFSVM classifier coordinates a hyperplane that effectively separates positive from negative instances.</S>
			<S sid ="164" ssid = "116">However, increasing the number of features introduces noise that affects the performance.</S>
			<S sid ="165" ssid = "117">The RF is able to profit from larger sets of first order features; thus, its performance is continuously increasing until it stabilises at 6, 000 features.</S>
			<S sid ="166" ssid = "118">The branches of the decision trees are shown to manage features correctly to construct most of the translation rules.</S>
			<S sid ="167" ssid = "119">Increasing the size of the feature space minimises the classification error, because more translation rules that generalize well on unseen data are constructed.</S>
			<S sid ="168" ssid = "120">The bilingual dictionary that we use for our experiments contains heterogeneous biomedical terms of diverse semantic categories.</S>
			<S sid ="169" ssid = "121">For example, our data-set contains common medical terms such as Intellectual Products (e.g. Pain Management, prise en charge de la douleur, 控 制 疼 痛) or complex biological concepts such as Enzymes (e.g. homogentisate 1,2dioxygenase, (a) English-French dataset (b) EnglishChinese dataset Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second order dataset English-French pairs EnglishChinese pairs P R F1 P R F1 GIZA++ Levenshtein Distance SV M -RBFsecond-order Linear-SV Msecond-order RFf irst-order 0.901 0.826 0.862 0.907 0.742 0.816 0.762 0.821 0.791 0.501 0.990 0.668 0.946 0.884 0.914 0.750 0.899 0.818 0.866 0.887 0.8763 0.765 0.893 0.824 0.962 0.874 0.916 0.779 0.940 0.851 Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance acide homogentisiqueoxydase, 尿 黑 酸1,2-双 氧酶).</S>
			<S sid ="170" ssid = "122">Therefore, we would expect poor perfor mance of the supervised methods using only a small portion of the total set of first order features due to the high diversity of the terms.</S>
			<S sid ="171" ssid = "123">For exam ple the morpheme ache/ mal/ 痛 is more frequent in Disease or Syndrome named entities rather than Enzyme named entities.</S>
			<S sid ="172" ssid = "124">However, the results indicate that RF can generalize well on heterogeneous terms.</S>
			<S sid ="173" ssid = "125">Figure 2 shows that the RF classifier outperforms SMT based methods, using only 1000 features.</S>
			<S sid ="174" ssid = "126">The Levenshtein distance-based classifier performs considerably better in the English-French dataset than in EnglishChinese.</S>
			<S sid ="175" ssid = "127">In fact, its best performance for the EnglishChinese dataset is achieved when classifying every pair of terms as a translation, i.e. 100% recall but 50% precision.</S>
			<S sid ="176" ssid = "128">In a second experiment, we attempted to explore whether the performance of SVMs can be improved by providing cross-language association features.</S>
			<S sid ="177" ssid = "129">We employed the second order feature set discussed in subsection 3.1.</S>
			<S sid ="178" ssid = "130">We used a constant number of 6, 000 first order features, the number of features that achieved maximum F-Score for RF in the previous experiment.</S>
			<S sid ="179" ssid = "131">Besides these first order features, we added an increasing number of second order ones.</S>
			<S sid ="180" ssid = "132">Figure 3 shows the F- Score curves of the RF, linear-SVM, RBFSVM, GIZA++ and Levenshtein distance using this feature space.</S>
			<S sid ="181" ssid = "133">We observe that second order features improved the performance of both SVMs considerably.</S>
			<S sid ="182" ssid = "134">In contrast to the previous experiment, the two SVMs present consistent bevaviour.</S>
			<S sid ="183" ssid = "135">Interestingly, the performance of the RF slightly decreased when using a small number of second order features.</S>
			<S sid ="184" ssid = "136">A possible explanation of this behaviour is that the second order associative features added noise, since the RF had already formed the association rules from first order features.</S>
			<S sid ="185" ssid = "137">In addition, for m English and n Chinese or French first order fea tures there were m × n possible combinations of second order features as explained in Subsection 3.1.</S>
			<S sid ="186" ssid = "138">Hence, there was a large number of second.</S>
			<S sid ="187" ssid = "139">order features that we excluded from the training process.</S>
			<S sid ="188" ssid = "140">Consequently, decision tree branches were populated with incomplete association rules while the RF was able to form these associations automatically.</S>
			<S sid ="189" ssid = "141">Nevertheless, as more second order features were added, more association rules were explored and the RF performance in creased.</S>
			<S sid ="190" ssid = "142">Table 3 summarises the highest performance achieved by the RF, SVMs, GIZA++ and Levenshtein distance all trained and tested on the same dataset.</S>
			<S sid ="191" ssid = "143">The resulting performance of the RF compared with GIZA++ is statistically significant (p &lt; 0.0001) in all experiments.</S>
			<S sid ="192" ssid = "144">Comparing the RF with the SVMs, we note that in the English- French dataset, the performance of the SVMRBF is approximately the same with the performance of our proposed method.</S>
			<S sid ="193" ssid = "145">However, this comes with a cost.</S>
			<S sid ="194" ssid = "146">Firstly, SVMs can possibly achieve a comparable performance to the RF when using multilingual, second order features.</S>
			<S sid ="195" ssid = "147">In contrast, our experiments show that RF benefit from monolingual, first order features only.</S>
			<S sid ="196" ssid = "148">Secondly, SVMs need a large number of additional multilingual features, (6.000 second order features or more) to perform similarly to RF.</S>
			<S sid ="197" ssid = "149">As a consequence, the resulting models of the SVM classifiers are more complex.</S>
			<S sid ="198" ssid = "150">We measured the average time needed by the two classifiers to decide for a single pair of terms.</S>
			<S sid ="199" ssid = "151">The RF is approximately 30 times faster than SVMs (on average 0.010 and 0.292 seconds, respectively).</S>
			<S sid ="200" ssid = "152">Finally, in the EnglishChinese dataset the RF performed significantly better than both SVMs.</S>
			<S sid ="201" ssid = "153">5 Discussion And Future Work.</S>
			<S sid ="202" ssid = "154">In this paper, we presented a novel classification method that uses Random Forest (RF) to recognise translations of biomedical terms across languages.</S>
			<S sid ="203" ssid = "155">Our approach is based on the hypothesis that in many languages, there exist some rules for combining textual units, e.g. n-grams, to form biomedical terms.</S>
			<S sid ="204" ssid = "156">Based on this assumption, we defined a first order feature space of character grams and demonstrated that an RF classifier is able to discover such cross language translation rules for terms.</S>
			<S sid ="205" ssid = "157">We experimented with two diverse language pairs: English-French and EnglishChinese.</S>
			<S sid ="206" ssid = "158">In the former case, pairs of terms exhibit high phonetic similarity while in the latter case they do not.</S>
			<S sid ="207" ssid = "159">Our results showed that the proposed method performs robustly in both cases and achieves a significantly better performance than GIZA++.</S>
			<S sid ="208" ssid = "160">We also evaluated Support Vector Machines (SVM) classifiers on the same first order feature space and showed that they fail to form translation rules in both language pairs, possibly because it cannot associate first order features with each other successfully.</S>
			<S sid ="209" ssid = "161">We attempted to boost the performance of the SVM classifier by adding association evidence of textual units to the features.</S>
			<S sid ="210" ssid = "162">We extracted second order features from the training data and we defined a new feature set consisting of both first order and second order features.</S>
			<S sid ="211" ssid = "163">In this feature space, the performance of the SVMs improved significantly.</S>
			<S sid ="212" ssid = "164">In addition to this, we observe from the reported experiments that RF achieves a better F-Score performance than GIZA++ in all datasets.</S>
			<S sid ="213" ssid = "165">Nonetheless, GIZA++ presents a better precision (but lower recall) in one dataset, i.e., English/Chinese.</S>
			<S sid ="214" ssid = "166">Based on this observation we plan to investigate the performance of a hybrid system combining RF with MT approaches.</S>
			<S sid ="215" ssid = "167">One trivial approach to apply the proposed method for compiling large-scale bilingual dictionaries of terms from comparable corpora would be to directly classify all possible pairs of terms into translations or non-translations.</S>
			<S sid ="216" ssid = "168">However, in comparable corpora, the size of the search space is quadratic to the input data.</S>
			<S sid ="217" ssid = "169">Therefore, the classification task is much more challenging since the distribution of positive and negative instances is highly skewed.</S>
			<S sid ="218" ssid = "170">To cope with the vast search space of comparable corpora, we plan to incorporate context-based approaches with the RF classification method.</S>
			<S sid ="219" ssid = "171">Context-based approaches, such as distributional vector similarity (Fung and McKe- own, 1997; Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008), can be used to limit the number of candidate translations by filtering out pairs of terms with low contextual similarity.</S>
			<S sid ="220" ssid = "172">Finally, the proposed method can be also used to online augment the phrase table of Statistical Machine Translation (SMT) in order to better handle the Out-of-Vocabulary problem i.e. inability to translate textual units that consist of one or more words and do not occur in the training data (Habash, 2008).</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="221" ssid = "173">The work described in this paper is partially funded by the European Community’s Seventh Framework Program (FP7/20072013) under grant agreement no. 318736 (OSSMETER).</S>
	</SECTION>
</PAPER>
