Semi-Supervised Lexicon Mining from Parenthetical Expressions 
in Monolingual Web Pages
Xianchao Wu†	Naoaki Okazaki†	Jun’ichi Tsujii†‡

†Computer Science, Graduate School of Information  Science and Technology, University  of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8656, Japan
‡School of Computer Science, University  of Manchester
National Centre for Text Mining (NaCTeM)
Manchester Interdisciplinary Biocentre, 131 Princess Street, Manchester M1 7DN, UK
{wxc,  okazaki, tsujii}@is.s.u-tokyo.ac.jp


Abstract

This paper presents a semi-supervised  learn- 
ing framework for mining Chinese-English 
lexicons from large amount of Chinese Web 
pages.    The issue  is motivated  by the ob- 
servation that many Chinese neologisms are 
accompanied by their English translations in 
the form of parenthesis. We  classify par- 
enthetical translations into bilingual abbrevi- 
ations, transliterations,  and translations. A 
frequency-based term recognition approach is 
applied for extracting bilingual abbreviations. 
A self-training algorithm is proposed for min- 
ing transliteration and translation lexicons. In 
which, we employ available lexicons in terms 
of morpheme levels, i.e., phoneme correspon- 
dences in transliteration  and grapheme (e.g., 
suffix, stem, and prefix) correspondences in 
translation.  The experimental results verified 
the effectiveness of our approaches.


1   Introduction

Bilingual lexicons, as  lexical or phrasal parallel 
corpora,  are widely used in applications of multi- 
lingual  language processing, such as statistical ma- 
chine translation (SMT) and cross-lingual informa- 
tion retrieval. However, it is a time-consuming  task 
for constructing  large-scale bilingual lexicons by 
hand. There are many facts cumber the manual de- 
velopment of bilingual  lexicons, such as the contin- 
uous emergence of neologisms (e.g., new technical 
terms, personal names, abbreviations, etc.), the dif- 
ficulty of keeping up with the neologisms for lexi- 
cographers, etc. In order to turn the facts to a better 
way, one of the simplest  strategies is to automati- 
cally mine large-scale lexicons from corpora such as 
the daily updated Web.



  Generally,  there are two kinds of corpora  used 
for automatic lexicon mining.  One is the purely 
monolingual corpora, wherein frequency-based 
expectation-maximization (EM, refer to (Dempster 
et al., 1977)) algorithms  and cognate clues play a 
central role (Koehn  and Knight, 2002). Haghighi 
et al.  (2008)  presented a generative  model  based 
on canonical correlation analysis, in which monolin- 
gual features  such as the context and orthographic 
substrings of words were taken into account. The 
other is multilingual parallel  and comparable cor- 
pora (e.g., Wikipedia1), wherein features such as co- 
occurrence frequency and context are popularly em- 
ployed (Cheng et al., 2004; Shao and Ng, 2004; Cao 
et al., 2007; Lin et al., 2008).
  In this paper, we focus on a special type of com- 
parable corpus, parenthetical translations.  The issue 
is motivated by the observation that Web pages and 
technical  papers written in Asian languages (e.g., 
Chinese, Japanese) sometimes annotate named enti- 
ties or technical terms with their translations in En- 
glish inside a pair of parentheses. This is considered 
to be a traditional  way to annotate new terms, per- 
sonal names or other named entities with their En- 
glish translations  expressed in brackets.  Formally, 
a parenthetical  translation  can be expressed by the 
following pattern,
           f1 f2 ... fJ  (e1 e2 ... eI ). 	(1) 
Here, f1  f2  ... fJ (f J ), the pre-parenthesis text, de-
notes the word sequence of some language  other 
than English; and e1 e2 ... eI (eI ), the in-parenthesis 
text, denotes the word sequence of English. We sep- 
arate parenthetical translations into three categories:

1 http://en.wikipedia.org/wiki/Main Page



424
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 424–432, 
Boulder, Colorado, June 2009. Qc 2009 Association for Computational Linguistics



Type
Examples  with translations in italic
Abbreviation
对  全球  气候  观测  系统 (GCOS)
to Global Climate Observing System (GCOS)
Transliteration
品牌  将  在  辛普顿- 特尔曼(Shipton-Tilman)
brand will be among Shipton-Tilman (Shipton-Tilman)
Translation
定时炸弹，  删除蝇(Cancelbots)
time bomb, Cancelbots (Cancelbots)
Mixture
在  香港  上课  的  英国  布拉福特  大学
(Bradford University)
the English Bradford University (Bradford University)
that holds lessons in Hongkong

Table 1: Parenthetical translation  categories and exam- 
ples extracted from Chinese Web pages. Mixture stands 
for the mixture of translation (University)  and translitera- 
tion (Bradford). ‘ ’ denotes the left boundary of f J .


bilingual abbreviation,  transliteration,  and transla- 
tion. Table 1 illustrates examples of these categories.
  We address several characteristics  of parenthetical 
translations that differ from traditional comparable 
corpora. The first is that they only appear in mono- 
lingual Web pages or documents,  and the context


approaches are restricted by the quality  and quantity 
of manually constructed training data, and unsuper- 
vised approaches are totally frequency-based with- 
out using any semantic clues. In contrast, we pro- 
pose a semi-supervised  framework  for mining par- 
enthetical translations. We apply a monolingual ab- 
breviation extraction approach to bilingual  abbrevia- 
tion extraction.  We construct an English-syllable to 
Chinese-pinyin transliteration model which is self- 
trained using phonemic  similarity measurements. 
We further employ our cascaded translation  model 
(Wu et al., 2008) which is self-trained  based on 
morpheme-level translation similarity.
  This paper is organized  as follows.  We briefly 
review the related work in the next section. Our 
system framework and self-training  algorithm is de- 
scribed in Section 3.   Bilingual abbreviation  ex- 
traction, self-trained transliteration models and cas- 
caded translation models are described in Section 4,
5, and 6, respectively. In Section 7, we evaluate our
mined lexicons by Wikipedia.  We conclude in Sec-


information of eI


is unknown. 
Second, frequency


tion 8 finally.


and word number of eI  are frequently  small. This
is because parenthetical  translations  are only used




2	Related Work


when the authors thought that f J


contained some


neologism(s) which deserved further explanation in 
another popular language (e.g., English).  Thus, tra- 
ditional context based approaches are not applicable 
and frequency  based approaches may yield low re- 
call while with high precision. Furthermore, cog- 
nate clues such as orthographic  features are not ap- 
plicable  between language pairs such as English  and 
Chinese.
  Parenthetical translation mining faces the follow- 
ing issues. First, we need  to distinguish  paren- 
thetical translations from parenthetical expressions, 
since parenthesis has many functions (e.g., defining 
abbreviations, elaborations, ellipsis, citations, anno- 
tations, etc.)  other than translation. Second, the 
left boundary  (denoted as   in Table 1) of the pre- 
parenthesis text need to be determined to get rid of 
the unrelated words. Third, we need further distin- 
guish different translation  types,  such as bilingual 
abbreviation, the mixture of translation and translit- 
eration, as shown in Table 1.
  In order to deal with these problems, supervised 
(Cao et al., 2007) and unsupervised (Li et al., 2008) 
methods have been proposed. However, supervised


Numerous  researchers have proposed  a variety of 
automatic  approaches to mine lexicons from the 
Web pages or other large-scale corpora. Shao and 
Ng (2004) presented a method to mine new transla- 
tions from Chinese and English news documents of 
the same period from different  news agencies, com- 
bining both transliteration  and context information. 
Kuo et al.  (2006) used active learning  and unsu- 
pervised learning for mining transliteration lexicon 
from the Web pages, in which an EM process was 
used for estimating the phonetic similarities between 
English syllables and Chinese characters.
  Cao et al.  (2007) split parenthetical translation 
mining task into two parts, transliteration detection 
and translation detection. They employed a translit- 
eration lexicon for constructing  a grapheme-based 
transliteration  model and annotated boundaries man- 
ually to train a classifier.   Lin et al. (2008) applied 
a frequency-based word alignment approach, Com- 
petitive Link (Melanmed, 2000), to determine the 
outer boundary (Section 7).
  On the other hand, there have been many semi- 
supervised  approaches  in  numerous applications


Chinese Web pages


Algorithm 1 self-training 
algorithm
Require: L, U  = {f J (eI )}, T , M
	
✄L, (labeled) train-


Parenthetical expression extraction{C(E)}	1 	1



S-MSRSeg
Chinese word segmentation{c…(e…)}

                 (Lin et al., 2008) 
Heuristic filtering{c…(e…)}

Section 4
Bilingual abbreviation mining


ing set; U , (unlabeled)  candidate set; T , test set; 
M, the
transliteration or translation model.
1: Lexicon = {}	✄ new mined lexicon
2: repeat
3:	N = {} ✄ new mined lexicon during one 
iteration
4:	train M on L
5:	evaluate M on T


Section 5


6:	for f J   I



Transliteration lexicon mining

Section 6


1 
(e1 ) 
∈ U 
do
7:	topN = {C t |decode eI  by M}
8:	N = N ∪ {(c, eI )|c ∈ f J ∧


Translation lexicon mining	1 	1



9:	end for


∃C t ∈ topN s.t. similarity{c, C t } ≥ θ}


Figure 1: The system framework of mining lexicons from
Chinese Web pages.


(Zhu, 2007), such as self-training   in word sense 
disambiguation (Yarowsky, 2005) and parsing (Mc- 
Closky et al., 2008). In this paper, we apply self- 
training to a new topic, lexicon mining.

3   System Framework and Self-Training
Algorithm

Figure 1 illustrates our system framework  for min-


10:	U = U − N
11:	L = unif ied(L ∪ N )
12:	Lexicon = unif ied(Lexicon ∪ N )
13: until |N | ≤ E
14: return Lexicon 	✄ the output


dates are used for translation lexicon mining.
  Algorithm 1 addresses the self-training algorithm 
for lexicon mining. The main part is a loop from 
Line 2 to Line 13. A given seed lexicon is taken 
as labeled  data and is split into training and testing
sets (L and T ). U ={f J (eI )}, stands for the (unla-


1	1


ing lexicons from Chinese Web pages.   First, par-
enthetical  expressions matching  Pattern 1 are ex- 
tracted.  Then, pre-parenthetical Chinese sequences 
are segmented into word  sequences by S-MSRSeg2 
(Gao et al., 2006). The initial parenthetical transla- 
tion corpus is constructed by applying the heuristic 
rules defined in (Lin et al., 2008)3. Based on this 
corpus, we mine three lexicons step by step, a bilin- 
gual abbreviation lexicon,  a transliteration  lexicon, 
and a translation  lexicon. The abbreviation candi- 
dates are extracted  firstly by using a heuristic  rule 
(Section 4.1). Then, the transliteration  candidates 
are selected by employing  a transliteration  model 
(Section 5.1). Specially, f J (eI ) is taken as a translit-


beled) parenthetical expression set. Initially, a trans- 
lation/transliteration model (M) is trained on L and
evaluated on T (Line 4 and 5). Then, the English 
phrase eI  of each unlabeled entry is decoded by M,
and the top-N outputs are stored in set            (Line
7∼8). A similarity function on c (a word substring 
of f J ) and a top-N output C 1   is employed to make
the decision of classification: the pair (c, eI ) will be 
selected as a new entry  if the similarity between c
and C 1   is no smaller than a threshold value θ (Line
8). After processing each entry in U , the new mined 
lexicon N  is deleted from U and unified with the 
current training  set L as the new training  set (Line
10 and 11). Also, N is added to the final lexicon


1	1


eration  candidate only if a  word ei  in eI


can be


(Line 12). When |N | is lower than a 
threshold, the
loop stops. Finally,  the algorithm returns the 
mined


transliterated. In addition,  a transliteration candidate
will also be considered as a translation  candidate if 
not all ei  can be transliterated (refer to the mixture 
example in Table1). Finally, after abbreviation filter- 
ing and transliteration filtering,  the remaining candi-

2 http://research.microsoft.com/research/downloads/details/
7a2bb7ee-35e6-40d7-a3f1-0b743a56b424/details.aspx
3 e.g., f J is predominantly in Chinese and eI   is predomi-



lexicon.
  One of the open problems in Algorithm 1 is how 
to append new mined entries into the existing  seed 
lexicon, considering  they have  different distribu- 
tions. One way is to design and estimate a weight 
function on the frequency of new mined entries. For 
simplicity,  we use a deficient strategy that takes the


1
nantly in English


1
w
e
i
g
h
t
s
 
o
f 
a
l
l 
n
e
w
 
m
i
n
e
d
 
e
n
t
r
i
e
s
 
t
o
 
b
e
 
o
n
e
.


4   Bilingual Abbreviation Extraction

4.1   Methodology
The method that we use for extracting  a bilingual 
abbreviation lexicon from parenthetical expressions 
is inspired by (Okzaki and Ananiadou, 2006). They 
used a term recognition approach to build a monolin- 
gual abbreviation dictionary from the Medical Liter- 
ature Analysis and Retrieval System Online (MED- 
LINE) abstracts, wherein acronym definitions  (e.g., 
ADM is short for adriamycin, adrenomedullin, etc.) 
are abundant. They reported 99% precision and 82-
95% recall.  Through locating  a textual fragment 
with an acronym and its expanded form in pattern

long form (short form),	(2)

they defined a heuristic formula to compute the long- 
form likelihood LH(c) for a candidate c:
(t)



No.
Chin
ese 
long
-
form 
cand
idat
es
LH
T/F
1
肿瘤 
相关 
抗原
Tum
or-
Asso
ciate
d 
Anti
gen
172.
5
T
2
硫 
代 
乙酰 
胺
thioa
ceta
mide
79.
9
T
3
胺
amin
e
33.
8
F
4
抗原
antig
en
24.
5
F
5
相关 
抗原
asso
ciate
d 
antig
en
21.
2
F
6
的 
肿瘤 
相关 
抗原
's 
Tum
or-
Asso
ciate
d 
Anti
gen
16.
5
F
7
总 
氨基
酸
total 
amin
o 
acid
16.
2
T

Table 2: Top-7 Chinese long-form  candidates for the En- 
glish acronym TAA, according to the LH score.


4.2	Experiment


LH(c) = freq(c) −      freq(t) ×	freq



(t) .


We  used  SogouT Internet Corpus 
Version 2.04,


t∈Tc


t∈Tc  freq



(3)


which contains about 13 
billion original Web 
pages
(mainly Chinese) in the 
form of 252 gigabyte 
.txt


Here, c is a long-form  candidate; freq(c) denotes the
frequency of co-occurrence of c with a short-form; 
and Tc is a set of nested long-form  candidates, each 
of which consists of a preceding  word followed by
the candidate c. Obviously, for t ∈ Tc, Equation 3
can be explained as:
       LH(c) = freq(c) − E[freq(t)].	(4) 
In this paper, we apply their method on the task
of bilingual abbreviation lexicon extraction.  Now,
the long-form  is a Chinese  word sequence and the 
short-form is an English acronym. We filter the par- 
enthetical expressions in the Web pages with several 
heuristic rules to meet the form of pattern 2 and to 
save the computing time:
• the short-form (eI ) should contain only one En-
glish word (I  = 1), and all letters in which
should be capital;

• similar with  (Lin  et  al.,  2008), the pre- 
parenthesis text is trimmed with: |c| ≥ 10 ×
|eI | + 6 when |eI | ≤ 6, and  |c| ≥ 2 × |eI | + 6,


files.  In addition, we used 55 gigabyte (.txt for- 
mat) Peking University  Chinese Paper Corpus. We 
constructed  a partially parallel corpus in the form 
of Pattern 1 from the union of the two corpora us- 
ing the heuristic rules defined in (Lin et al., 2008). 
We gained a partially parallel corpus which contains
12,444,264 entries.

  We extracted 107,856 distinct English acronyms. 
Limiting LH score ≥ 1.0 in Equation 3, we gained
2,020,012  Chinese  long-form candidates  for  the
107,856 English acronyms.  Table 2 illustrates the 
top-7 Chinese long-form  candidates of the English 
acronym TAA. Three candidates are correct (T) long- 
forms while the other 4 are wrong (F). Wrong can- 
didates from No. 3 to 5 are all subsequences of the 
correct candidate No. 1. No. 6 includes No. 1 while 
with a Chinese functional  word de in the left most 
side. These error types can be easily tackled with 
some filtering patterns, such as ‘remove the left most 
functional word in the long-form candidates’, ‘only 
keep the relatively  longer candidates with larger LH 
score’, etc.


1	1	1


otherwise.  |c| and  |eI | are measured in bytes.
We further trim the remaining pre-parenthesis
text by punctuations other than hyphens and 
dots, i.e., the right most punctuation and its left 
subsequence are discarded.
  

Since there does not yet exists a common  eval- 
uation  data set for the bilingual abbreviation lexi- 
con, we manually  evaluated a small sample of it.

4 http://www.sogou.com/labs/dl/t.html


Of the 107,856 English acronyms, we randomly se- 
lected 200 English  acronyms and their top-1 Chi- 
nese long-form  candidates for manually evaluating. 
We found, 92 candidates were correct including  3 
transliteration  examples.  Of the 108 wrong candi- 
dates, 96 candidates included the correct long-form 
with some redundant words on the left side (i.e., c = 
(word)+ correct long-form),  the other 12 candidates 
missed some words of the correct long-form or had 
some redundant words right before the left paren-
thesis (i.e., c = (word)∗ correct long-form (word)+
or c = (word)∗ subsequence of correct long-form
word)∗). We classified the redundant word right be-
fore the correct long-form  of each of the 96 candi- 
dates, de occupied 32, noun occupied 7, verb occu- 
pied 18, prepositions and conjunctions occupied the 
remaining ones.
In total, the abbreviation translation accuracy is
44.5%. We improved the accuracy to 60.5% with 
an additional  de filtering pattern. According to for- 
mer mentioned error analysis, the accuracy may fur- 
ther be improved if a Chinese part-of-speech tagger 
is employed and the non-nominal words in the long- 
form are removed beforehand.

5   Self-Training for Transliteration Models

In this section, we first describe and compare three 
transliteration models. Then, we select and train the 
best model following Algorithm 1 for lexicon min- 
ing. We investigate two things, the scalability of the 
self-trained model given different amount of initial 
training data, and the performance of several strate- 
gies for selecting new training samples.

5.1   Model description

We construct and compare  three forward translit- 
eration  models,  a phoneme-based  model (English 
phonemes to Chinese pinyins), a grapheme-based 
model (English syllables to Chinese  characters) 
and  a hybrid model (English syllables to Chinese 
pinyins).  Similar models  have been compared in 
(Oh et al., 2006) for English-to-Korean  and English- 
to-Japanese transliteration. All the three models are 
phrase-based, i.e., adjacent phonemes or graphemes 
are  allowable to form phrase-level transliteration 
units.   Building the correspondences  on phrase 
level can effectively  tackle the missing or redundant


phoneme/grapheme problem during transliteration. 
For example, when Aamodt is transliterated into a 
mo¯  te`5, a and d are missing. The problem can be 
easily solved when taking Aa and dt as single units 
for transliterating.
  Making use  of Moses (Koehn et al., 2007), a 
phrase-based  SMT system,  Matthews  (2007) has 
shown that the performance was comparable to re- 
cent state-of-the-art  work (Jiang et al., 2007) in 
English-to-Chinese   personal  name transliteration. 
Matthews (2007) took transliteration as translation 
at the surface level. Inspired by his idea, we also 
implemented our transliteration models employing 
Moses. The main difference is that, while Matthews 
(2007) tokenized the English names into individual 
letters before training in Moses, we split them into 
syllables using the heuristic rules described in (Jiang 
et al., 2007), such that one syllable only contains one 
vowel letter or a combination  of a consonant  and a 
vowel letter.
  English syllable sequences  are  used  in the 
grapheme-based  and hybrid models.    In the 
phoneme-based model, we transfer English  names 
into phonemes and Chinese characters into Pinyins 
in virtue of the CMU pronunciation dictionary6  and 
the LDC Chinese character-to-pinyin list7.
  In the mass,  the grapheme-based model is the 
most robust model, since no additional resources are 
needed. However, it suffers from the Chinese homo- 
phonic character problem. For instance, pinyin ai 
corresponds to numerous Chinese characters which 
are applicable  to personal names. The phoneme- 
based model is the most suitable model that reflects 
the essence of transliteration, while restricted by ad- 
ditional grapheme to phoneme dictionaries.   In or- 
der to eliminate the confusion of Chinese homo- 
phonic characters and alleviate  the dependency on 
additional  resources, we implement a hybrid  model 
that accepts English  syllables and Chinese pinyins 
as formats of the training data. This model is called 
hybrid, since English  syllables  are graphemes and 
Chinese pinyins are phonemes.


  5 The tones of Chinese pinyins are ignored in our translitera- 
tion models for simplicity.
6 http://www.speech.cs.cmu.edu/cgi-bin/cmudict
7 http://projects.ldc.upenn.edu/Chinese/docs/char2pinyin.txt




1.0
0.8
0.6
0.4
0.2
0.0



1.0
0.8
0.6
0.4
0.2
0.0



g
r
a
p
h
e
m
e
-
b
a
s
 
e
d
BLEU	WER	PER	EMatch





1	2	3	4	5	6	7	8 max_phras e_length

h
y
b
r
i
d
-
b
a
s
 
e
d
BLEU	WER	PER	EMatch





1	2	3	4	5	6	7	8 max_phrase_length




1.0
0.8
0.6
0.4
0.2
0.0



0.5
0.4
0.3
0.2
0.1
0.0



p
h
o
n
e
m
e
-
b
a
s
 
e
d
B
L
E
U
	
W
E
R
	
P
E
R
	
E
M
a
t
c
h





1
	
2
	
	
3
	
4
	
5
	
6
	
7
	
8
 
m
a
x
_
p
h
r
a
s
 
e
_
l
e
n
g
t
h

C
o
m
p
a
r
i
s
o
n
 
o
n
 
E
M
a
t
c
h
g
r
a
p
h
e
m
e
	
p
h
o
n
e
m
e
	
h
y
b
r
i
d






1
	
2
	
3
	
4
	
5
	
6
	
7
	
8
 
m
a
x
_
p
h
r
a
s
e
_
l
e
n
g
t
h



Figure 2: The performances of the transliteration models 
and their comparison on EMatch.




5.2   Experimental  model selection

Similar to (Jiang et al., 2007), the transliteration 
models were trained and tested on the LDC Chinese- 
English Named Entity Lists Version 1.08. The origi- 
nal list contains 572,213 English people names with 
Chinese transliterations.   We extracted 74,725 en- 
tries in which the English  names also appeared in 
the CMU pronunciation dictionary. We randomly 
selected 3,736 entries as an open testing set and the 
remaining entries as a training set9. The results were 
evaluated using the character/pinyin-based 4-gram 
BLEU score (Papineni et al., 2002), word error rate 
(WER), position independent word error rate (PER), 
and exact match (EMatch).
  Figure 2 reports the performances of the three 
models and the comparison  based on EMatch.  From 
the results, we can easily draw the conclusion that 
the hybrid model performs the best under the maxi- 
mal phrase length (mpl, the maximal phrase length 
allowed in Moses) from 1 to 8. The performances 
of the models  converge  at or right after mpl  =
4.  The pinyin-based WER of the hybrid model is
39.13%, comparable to the pinyin error rate 39.6%, 
reported in (Jiang et al., 2007)10.  Thus, our further

  8 Linguistic     Data     Consortium    catalog    number: 
LDC2005T34 (former catalog number: LDC2003E01)
9 Jiang et al.  (2007) selected 25,718 personal name pairs
from LDC2003E01 as the experiment data: 200 as development 
set, 200 as test set, and the remaining  entries as training  set.
10 It should be notified that we achieved this result by using


Table 3: The BLEU score of self-trained h4 translitera- 
tion models under four selection strategies.  nt (n=1..5) 
stands for the n-th iteration.


self-training  experiments are pursued on the hybrid 
model taking mpl to be 4 (short for h4, hereafter).

5.3	Experiments on the self-trained hybrid 
model
As former mentioned, we investigate the scalability 
of the self-trained h4 model by respectively using 5,
10, 20, 40, 60, 80, and 100 percent of initial training 
data, and the performances of using exact matching 
(em) or approximate matching (am, line 8 in Algo- 
rithm 1) on the top-1 and top-5 outputs (line 7 in Al- 
gorithm 1) for selecting new training samples. We 
used edit distance (ed) to measure the em and am 
similarities:
  ed(c, C 1) = 0 or < syllable number(C 1)/2.    (5) 
When applying Algorithm 1 for transliteration lexi-
con mining,  we decode each word in eI  respectively.
The algorithm terminated in five iterations when we 
set the terminal threshold E (Line 13 in Algorithm 1) 
to be 100.
  For simplicity, Table 3 only illustrates the BLEU 
score of h4 models under four selection strategies. 
From this table, we can draw the following conclu- 
sions. First, with fewer initial training data, the im-
provement is better. The best relative improvements


larger training set (70,989 vs. 25,718) and larger test set (3,736	 	


vs. 200) comparing with (Jiang et al., 2007), and we did not use


additional  Web resources as Jiang et al. (2007) did.



are 8.74%, 8.46%, 4.41%, 0.67%, 0.68%, 0.32%, 
and 1.39%, respectively.   Second, using top-5 and 
em for new training data selection performs the best 
among the four strategies. Compared under each it- 
eration, using top-5 is better than using top-1; em 
is better than am; and top-5 with am is a little bet- 
ter than top-1 with em. We mined 39,424, 42,466,
46,116, 47,057, 49,551, 49,622, and 50,313 distinct 
entries under the six types of initial data with top-5 
plus em strategy. The 50,313 entries are taken as the 
final transliteration lexicon for further comparison.

6   Self-Training for a Cascaded Translation
Model

We classify the parenthetical translation candidates 
by employing  a translation  model. In contrast to 
(Lin et al., 2008), wherein the lengthes of prefixes 
and suffixes of English words were assumed to be 
three bytes, we segment words into morphemes (se- 
quences of prefixes, stems, and suffixes) by Morfes- 
sor 0.9.211, an unsupervised language-independent 
morphological  analyzer (Creutz and Lagus, 2007). 
We  use the morpheme-level  translation  similarity 
explicitly in our cascaded translation  model (Wu et 
al., 2008), which makes  use of morpheme, word, 
and phrase level translation units. We train Moses 
to gain a phrase-level  translation  table. To gain a 
morpheme-level translation table, we run GIZA++ 
(Och and Ney, 2003) on both directions between En- 
glish morphemes and Chinese characters, and take 
the intersection of Viterbi alignments. The English- 
to-Chinese translation probabilities computed by 
GIZA++ are attached to each morpheme-character 
element in the intersection set.

6.1   Experiment

The Wanfang Chinese-English technical term dictio- 
nary12, which contains 525,259 entries in total, was 
used for training and testing. 10,000 entries were 
randomly  selected as the test set and the remaining 
as the training  set. Again, we investigated the scala- 
bility of the self-trained  cascaded translation  model 
by respectively using 20, 40, 60, 80, and 100 per- 
cent of initial training data. An aggressive similar-

11 http://www.cis.hut.fi/projects/morpho/
12 http://www.wanfangdata.com.cn/Search/ResourceBrowse
.aspx



%
0t
1t
2t
3t
4t
5t
20
.14
06
.11
96
.12
43
.12
39
.11
76
.11
79
40
.10
91
.12
24
.13
86
.13
45
.14
79
.14
66
60
.16
30
.16
24
.14
29
.17
14
.13
09
.13
98
80
.19
44
.17
83
.18
86
.18
70
.18
84
.18
73
10
0
.18
10
.18
14
.15
39
.19
81
.15
42
.19
44

Table 4: The BLEU score of self-trained  cascaded trans- 
lation model under five initial training sets.


ity measurement was used for selecting new training 
samples:
first char(c) = first char(C 1)  ∧ min{ed(c, C 1)}.
                                       (6) 
Here, we judge if the first characters of c and C 1
are similar or not.  c was gained by deleting zero 
or more characters from the left side of f J . When 
more than one c satisfied this condition, the c that
had the smallest edit distance with C 1  was selected.
When applying Algorithm 1 for translation lexicon 
mining, we took eI  as one input  for decoding instead 
of decoding each word respectively.  Only the top-1
output (C 1) was used for comparing. The algorithm
stopped in five iterations when we set the terminal 
threshold E to be 2000.
  For simplicity, Table 4 only illustrates the BLEU 
score of the cascaded translation  model under five 
initial training  sets. For the reason that there are fi- 
nite phonemes in English and Chinese while the se- 
mantic correspondences between the two languages 
tend to be infinite, Table 4 is harder to be analyzed 
than Table 3. When initially using 40%, 60%, and
100% training data for self-training, the results tend 
to be better at some iterations. We gain 35.6%,
5.2%, and 9.4% relative improvements, respectively. 
However, the results tend to be worse when 20% and
80% training data were used initially, with 11.6% 
and 3.0% minimal relative loss.  The best BLEU 
scores tend to be better when more initial training 
data are available. We mined 1,038,617, 1,025,606,
1,048,761, 1,056,311, and 1,060,936 distinct entries 
under the five  types of initial training data. The
1,060,936  entries are taken as the final translation 
lexicon for further comparison.

7   Wikipedia Evaluation

We  have mined three kinds of lexicons till  now, 
an abbreviation  lexicon containing 107,856 dis-


boundary inside f J  is determined when each ei  in







Table 5: The results of our lexicon and an unsupervised- 
mined lexicon (Lin  et al., 2008) evaluated  under 
Wikipedia title dictionary. Cov is short for coverage.


similar English acronyms with 2,020,012 Chinese 
long-form  candidates; a transliteration lexicon with
50,313 distinct entries; and a translation   lexicon 
with 1,060,936 distinct entries. The three lexicons 
are combined together as our final lexicon.
  Similar with (Lin et al., 2008), we compare our 
final mined lexicon with a dictionary  extracted from 
Wikipedia, the biggest multilingual  free-content en- 
cyclopedia on the Web. We extracted the titles of 
Chinese and English Wikipedia articles13  that are 
linked to each other. Since most titles contain less 
than five words, we take a linked title pair as a trans- 
lation entry without considering the word alignment 
relation between the words inside the titles. The re- 
sult lexicon contains 105,320 translation pairs be- 
tween 103,823 Chinese titles and 103,227 English 
titles. Obviously, only a small  percentage of titles 
have more than one translation.  Whenever there is 
more than one translation, we take the candidate en- 
try as correct  if and only if it matches one of the 
translations.
  Moreover, we compare our semi-supervised ap- 
proach with an unsupervised approach (Lin et al.,
2008).  Lin et al.   (2008) took ϕ2(fj , ei)  score
14(Gale and Church, 1991) with threshold  0.001 as
the word alignment probability in a word alignment 
algorithm, Competitive Link. Competitive Link tries 
to align an unlinked ei  with an unlinked fj by the 
condition that ϕ2(fj , ei) is the biggest. Lin et al. 
(2008) relaxed the unlinked constraints to allow con- 
secutive sequence of words on one side to be linked 
to the same  word on the other side15.  The left

  13 English  and Chinese Wikipedia  pages due to 2008.09.23 
are used here.
2
 

1 is aligned. After applying the modified Compet- 
itive Link on the partially parallel corpus which in- 
cludes 12,444,264 entries (Section 4.2), we obtained
2,628,366 distinct pairs.
  Table 5 shows the results of the two lexicons eval- 
uated under Wikipedia title dictionary. The coverage 
is measured by the percentage of titles which ap- 
pears in the mined lexicon. We then check whether 
the translation in the mined lexicon is an exact match 
of one of the translations in the Wikipedia lexicon. 
Through comparing the results, our mined lexicon is 
comparable with the lexicon mined in an unsuper- 
vised way. Since the selection is based on phone- 
mic and semantic clues instead of frequency,  a par- 
enthetical translation candidate will not be selected 
if the in-parenthetical English text is failed to be 
transliterated or translated.  This is one reason that 
explains why we earned a little lower coverage. An- 
other reason comes from the low coverage rate of 
seed lexicons  used for self-training, only 8.65% En- 
glish words in the partially  parallel corpus are cov- 
ered by the Wanfang dictionary.

8   Conclusion

We   have  proposed a  semi-supervised  learning 
framework for mining bilingual lexicons from par- 
enthetical  expressions in monolingual Web pages. 
We classified the parenthesis expressions into three 
categories: abbreviation, transliteration, and transla- 
tion. A set of heuristic  rules, a self-trained  hybrid 
transliteration model, and a self-trained   cascaded 
translation model were proposed for each category, 
respectively.
  We investigated the scalability of the self-trained 
transliteration  and translation  models by training 
them with different amount of data. The results shew 
the stability (transliteration) and feasibility  (transla- 
tion) of our proposals. Through employing the par- 
allel Wikipedia  article titles as a gold standard lex- 
icon, we gained the comparable results comparing 
our semi-supervised framework with our implemen-


(a+b)(a+c)(b+d)(c+d) , where a is the number


tation of Lin et al.  (2008)’s  unsupervised mining


of f J (eI ) containing both e


and f ; (a + b) is the number 
of


approach.


1 	1 	i	j
f J   I 	J   I
1 (e1 ) containing ei ; (a + c) is the number of f1 (e1 ) contain-


ing fj ; and d is the number of f J   I


i	ages, they only require that at 
least one of them be unlinked and



fj .


1 (e1 
) 
cont
ainin
g 
neith
er e


nor



that 
(sup
pos
e ei  
is 
unli
nke
d 
and 
fj 
is 
link
ed 
to 
ek ) 
non
e of 
the


15 Instead of requiring both ei and fj to have no previous link-


words between ei  and ek  be linked to any word other than fj .


Acknowledgments

This work was partially  supported by Grant-in-Aid 
for Specially  Promoted Research (MEXT, Japan) 
and Japanese/Chinese Machine Translation Project 
in Special Coordination  Funds for Promoting Sci- 
ence and Technology (MEXT, Japan). We thank 
the anonymous reviewers for their constructive com- 
ments.


References

Cao, Guihong,  Jianfeng Gao, and Jian-Yun  Nie.  2007.
A system to Mine Large-Scale Bilingual Dictionar- 
ies from Monolingual  Web Pages. In MT Summit XI. 
pages 57–64,  Copenhagen, Denmark.
Cheng, Pu-Jen, Yi-Cheng  Pan, Wen-Hsiang Lu, and Lee- 
Feng Chien. 2004. Creating Multilingual Translation 
Lexicons with Regional Variations Using Web Cor- 
pora.  In ACL 2004,  pages 534–541,  Barcelona, 
Spain.
Creutz, Mathias and Krista Lagus. 2007. Unsupervised 
Models for Morpheme Segmentation and Morphology 
Learning.	ACM Transactions  on Speech  and Lan- 
guage Processing, 4(1):Article 3.
Dempster, A. P., N. M. Laird and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the 
EM Algorithm. Journal of the Royal Statistical Soci- 
ety, 39:1–38.
Gale, W. and K. Church.  1991. Identifying  word corre- 
spondence in parallel text. In DARPA NLP Workshop. 
Gao, Jianfeng, Mu Li, Andi Wu, and Chang-Ning Huang.
2006. Chinese Word Segmentation and Named Entity
Recognition: A Pragmatic Approach. Computational
Linguistics, 31(4):531–574.
Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick, 
and Dan Klein  2008. Learning Bilingual Lexicons 
from Monolingual Corpora. In ACL-08:HLT. pages
771–779, Columbus, Ohio.
Jiang, Long, Ming Zhou, Lee-Feng Chien, and Cheng 
Niu. 2007. Named Entity Translation with Web Min- 
ing and Transliteration.   In IJCAI 2007. pages 1629–
1634, Hyderabad, India.
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran, Richard 
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con- 
stantin, and Evan Herbst. 2007. Moses: Open Source 
Toolkit for Statistical Machine Translation. In ACL
2007 Poster Session, pages 177–180.
Koehn, Philipp and Kevin  Knight.   2002.   Learning 
a translation  lexicon from monolingual corpora. In 
SIGLEX 2002, pages 9–16.


Kuo, Jin-Shea, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning Transliteration Lexicons from the Web. In
COLING-ACL 2006. pages 1129–1136.
Lin, Dekang, Shaojun Zhao, Benjamin Van Durme, and 
Marius Pas¸ca.   2008. Mining Parenthetical Transla- 
tions from the Web by Word Alignment.  In ACL-
  08:HLT, pages 994–1002,  Columbus,  Ohio. 
Matthews,  David.	2007.	Machine Transliteration of
Proper Names. A Thesis of Master. University of Ed-
inburgh.
McClosky,  David, Eugene Charniak, and Mark Johnson
2008. When is Self-Training Effective for Parsing? In 
Proceedings of the 22nd International  Conference on 
Computational Linguistics (Coling 2008),  pages 561–
568, manchester, UK.
Melamed, I. Dan. 2000. Models of Translational Equiv- 
alence  among Words.   Computational  Linguistics,
26(2):221–249.
Och, Franz Josef and Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29(1):19–51.
Oh, Jong-Hoon,  Key-Sun  Choi, and Hitoshi Isahara.
2006. A Comparison of Different Machine Translit- 
eration Models. Journal of Artifical Intelligence Re- 
search, 27:119–151.
Okazaki, Naoaki and Sophia Ananiadou. 2006. Building 
an Abbreviation  Dictionary Using a Term Recognition 
Approach. Bioinformatics, 22(22):3089–3095.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu. 2002. BLEU: a Method for Automatic Eval- 
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa- 
tional Linguistics (ACL).  pages 311–318, Philadel- 
phia.
Shao, Li and Hwee Tou Ng. 2004. Mining New Word 
Translations from Comparable Corpora. In Proceed- 
ings of the 20th International Conference on Com- 
putational Linguistics (COLING),   pages 618–624, 
Geneva, Switzerland.
Wu, Xianchao, Naoaki Okazaki, Takashi Tsunakawa, and 
Jun’ichi Tsujii. 2008. Improving English-to-Chinese 
Translation for Technical Terms Using Morphological 
Information. In Proceedings of the 8th Conference of 
the Association for Machine Translation in the Ameri- 
cas (AMTA),  pages 202–211,  Waikiki, Hawai’i.
Yarowsky, David. 1995.  Unsupervised  Word  Sense Dis- 
ambiguation Rivaling Supervised Methods. In Pro- 
ceedings of the 33rd annual meeting on Association 
for Computational Linguistics, pages 189–196,  Cam- 
bridge, Massachusetts.
Zhu, Xiaojin.  2007. Semi-Supervised Learning Litera- 
ture Survery. University of Wisconsin - Madison.

