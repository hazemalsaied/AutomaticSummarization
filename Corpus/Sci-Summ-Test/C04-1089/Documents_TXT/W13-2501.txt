Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Cross-lingual WSD for Translation Extraction
from Comparable Corpora
Marianna Apidianaki
LIMSI-CNRS
Rue John Von Neumann
BP 133, 91403
Orsay Cedex, France
marianna@limsi.fr
Nikola Ljubes?ic?
Dept. of Information Sciences
University of Zagreb
Ivana Luc?ic?a 3, HR-10000
Zagreb, Croatia
nljubesi@ffzg.hr
Darja Fis?er
Department of Translation
University of Ljubljana
As?kerc?eva 2, SI-1000
Ljubljana, Slovenia
darja.fiser@ff.uni-lj.si
Abstract
We propose a data-driven approach to en-
hance translation extraction from compa-
rable corpora. Instead of resorting to an
external dictionary, we translate source
vector features by using a cross-lingual
Word Sense Disambiguation method. The
candidate senses for a feature correspond
to sense clusters of its translations in a
parallel corpus and the context used for
disambiguation consists of the vector that
contains the feature. The translations
found in the disambiguation output con-
vey the sense of the features in the source
vector, while the use of translation clusters
permits to expand their translation with
several variants. As a consequence, the
translated vectors are less noisy and richer,
and allow for the extraction of higher qual-
ity lexicons compared to simpler methods.
1 Introduction
Large-scale comparable corpora are available in
many language pairs and are viewed as a source
of valuable information for multilingual applica-
tions. Identifying translation correspondences in
this type of corpora permits to construct bilingual
lexicons for low-resourced languages, and to com-
plement and reduce the sparseness of existing re-
sources (Munteanu and Marcu, 2005; Snover et
al., 2008). The main assumption behind transla-
tion extraction from comparable corpora is that a
source word and its translation appear in similar
contexts (Fung, 1998; Rapp, 1999). So, in order
to identify a translation correspondence between
the two languages, the contexts of the source word
and the candidate translation have to be compared.
For this comparison to take place, the same vector
space has to be produced, which means that the
vectors of the one language have to be translated
in the other language. This generally assumes the
availability of a bilingual dictionary which might
however not be the case for some language pairs
and domains. Moreover, the classic way in which
a dictionary is put into use, which consists in trans-
lating vector features by their first translation in
the dictionary, neglects semantics. We expect that
a method capable of identifying the correct sense
of the features and translating them accordingly
could contribute to producing cleaner vectors and
to extracting higher quality lexicons.
In this paper, we show how source vectors
can be translated into the target language by a
cross-lingual Word Sense Disambiguation (WSD)
method which exploits the output of data-driven
Word Sense Induction (WSI) (Apidianaki, 2009),
and demonstrate how feature disambiguation en-
hances the quality of the translations extracted
from the comparable corpus. This study extends
our previous work on the topic (Apidianaki et al,
2012) by applying the proposed methods to a com-
parable corpus of general language (built from
Wikipedia) and optimizing various parameters that
affect the quality of the extracted translations. We
expect the disambiguation to have a beneficial im-
pact on the results given that polysemy is a fre-
quent phenomenon in a general, mixed-domain
corpus. Our experiments are carried out on the
English-Slovene language pair but as the methods
are totally data-driven, the approach can be easily
applied to other languages.
The paper is organized as follows: In the next
section, we present some related work on bilin-
gual lexicon extraction from comparable corpora.
Section 3 presents the data used in our experiments
and Section 4 provides details on the approach and
the experimental setup. In Section 5, we report and
discuss the obtained results before concluding and
presenting some directions for future work.
1
2 Related work
The traditional approch to translation extraction
from comparable corpora and most of its exten-
sions (Fung, 1998; Rapp, 1999; Shao and Ng,
2004; Otero, 2007; Yu and Tsujii, 2009; Marsi
and Krahmer, 2010) presuppose the availability
of a bilingual lexicon for translating source vec-
tors into the target language. A translation can-
didate is generally considered as correct if it is
an appropriate translation for at least one sense
of the source word in the dictionary, which of-
ten corresponds to its most frequent sense. An
alternative consists in considering all translations
provided for a word in the dictionary but weight-
ing them by their frequency in the target lan-
guage (Prochasson et al, 2009; Hazem and Morin,
2012). The high quality of the exploited hand-
crafted resources, combined to the skewed distri-
bution of the translations corresponding to differ-
ent word senses, often lead to satisfying results.
Nevertheless, the applicability of the methods is
limited to languages and domains where bilingual
resources are available. Moreover, by promoting
the most frequent sense/translation, this approach
neglects polysemy. We believe that feature dis-
ambiguation can lead to the production of cleaner
vectors and, consequently, to higher quality re-
sults.
The need to bypass pre-existing dictionaries
has been addressed by Koehn and Knight (2002)
who built the initial seed dictionary automatically,
based on identical spelling features between En-
glish and German. Cognate detection has also
been used by Saralegi et al (2008) for extract-
ing word translations from English-Basque com-
parable corpora. The cognate and seed lexicon
approaches have been successfully combined by
Fis?er and Ljubes?ic? (2011) who showed that the re-
sults with an automatically created seed lexicon,
based on language similarity, can be as good as
with a pre-existing dictionary. But all these ap-
proaches work on closely-related languages and
cannot be used as successfully for language pairs
with little lexical overlap, such as English and
Slovene, which is the case in this experiment.
Regarding the translation of the source vectors,
we use contextual information to disambiguate
their features and translate them using clusters
of semantically similar translations in the target
language. A similar idea has been implemented
by Kaji (2003) who performed sense-based word
clustering to extract sets of synonymous transla-
tions from comparable corpora with the help of a
bilingual dictionary.
Using translation clusters permits to expand
feature translation and to suggest multiple seman-
tically correct translations. A similar approach has
been adopted by De?jean et al (2005) who expand
vector translation by using a bilingual thesaurus
instead of a lexicon. In contrast to their work, the
method proposed here does not rely on any exter-
nal knowledge source to determine word senses
or translation equivalents, and is thus fully data-
driven and language independent.
3 Resources
3.1 Comparable corpus
The comparable corpus from which the bilin-
gual lexicon will be extracted is a collection of
English (EN) and Slovene (SL) texts extracted
from Wikipedia. The February 2013 dumps of
Wikipedia articles were downloaded and cleaned
for both languages after which the English cor-
pus was tokenized, part-of-speech (PoS) tagged
and lemmatized with the TreeTagger (Schmid,
1994). The same pre-processing was applied to the
Slovene corpus with the ToTaLe analyzer (Erjavec
et al, 2010) which uses the TnT tagger (Brants,
2000) and was trained on MultextEast corpora.
The Wikipedia corpus contains about 1.5 billion
tokens for English and almost 24 million tokens
for Slovene.
In previous work, we applied our approach to a
specialized comparable corpus from the health do-
main (Apidianaki et al, 2012). The results were
encouraging, showing how translation clustering
and vector disambiguation help to improve the
quality of the translations extracted from the com-
parable corpus. We believe that the positive im-
pact of this approach will be more significant on
lexicon extraction from a general language com-
parable corpus, in which polysemy is more promi-
nent.
3.2 Parallel corpus
The parallel corpus used for clustering and word
sense induction consists of the Slovene-English
parts of Europarl (release v6) (Koehn, 2005) and
of JRC-Acquis (Steinberger et al, 2006) and
amounts to approximately 35M words per lan-
guage. A number of pre-processing steps are ap-
plied to the corpus prior to sense induction, such
2
Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.
as elimination of sentence pairs with a great dif-
ference in length, lemmatization and PoS tagging
with the TreeTagger (for English) and ToTaLe (for
Slovene) (Erjavec et al, 2010). Next, the cor-
pus is word-aligned with GIZA++ (Och and Ney,
2003) and two bilingual lexicons are extracted,
one for each translation direction (EN?SL/SL?
EN). To clean the lexicons from noisy alignments,
the translations are filtered on the basis of their
alignment score and PoS, keeping only transla-
tions that pertain to the same grammatical cate-
gory as the source word. We retain only intersect-
ing alignments and use for clustering translations
that translate a source word more than 10 times
in the training corpus. This threshold reduces
data sparseness issues that affect the clustering
and eliminates erroneous word alignments. The
filtered EN-SL lexicon contains entries for 6,384
nouns, 2,447 adjectives and 1,814 verbs having
more than three translations in the training corpus.
The parallel corpus, which contains EU texts, is
more specialized than the comparable corpus built
from Wikipedia. This is not the ideal scenario for
this experiment; domain adaptation is important
for the type of semantic processing we want to ap-
ply as there might be a shift in the senses present in
the two corpora. However, as EU texts often con-
tain a lot of general vocabulary, we expect that this
discrepancy will not strongly affect the quality of
the results.
3.3 Gold standard
We evaluate the quality of the bilingual lexicons
extracted from the comparable corpus by compar-
ing them to a gold standard lexicon, which was
built from the aligned English (Fellbaum, 1998)
and Slovene wordnets (Fis?er and Sagot, 2008). We
extracted all English synsets from the Base Con-
cept sets that belong to the Factotum domain and
contain literals with polysemy levels 1-5 and their
Slovene equivalents which have been validated by
a lexicographer. Of 1,589 such synsets, 200 were
randomly selected and used as a gold standard for
automatic evaluation of the method proposed in
this paper.
4 Experimental setup
4.1 Overview of the method
Figure 1 gives an overview of the way informa-
tion mined from the parallel training corpus is ex-
ploited for discovering translations of source (En-
glish) words in the comparable corpus. The par-
allel corpus serves to extract an English-Slovene
seed lexicon and source language context vec-
tors (Par vectors) for the Slovene translations of
English words. These vectors form the input to
the Word Sense Induction (WSI) method which
groups the translations of an English word into
clusters.
The clusters of semantically related Slovene
translations constitute the candidate senses which,
together with the Par vectors, are used for dis-
ambiguating and translating the vectors extracted
from the source (English) side of the comparable
corpus (Comp source). The translated vectors are
then compared to the ones extracted from the tar-
get language (Slovene) side of the comparable cor-
pus (Comp target) and the best translations are se-
lected, for a list of unknown words. All steps of
the proposed method illustrated in Figure 1 will
be detailed in the following sections.
4.2 Translation clustering
The translations of the English words in the lex-
icon built as described in 3.2 are clustered ac-
cording to their semantic proximity using a cross-
lingual Word Sense Induction method (Apidi-
anaki, 2008). For each translation Ti of a word
w, a vector is built from the content word co-
3
Language POS Source word Slovene sense clusters
EN?SL
Nouns
sphere
{krogla} (geometrical shape)
{sfera, podroc?je} (area)
address
{obravnava, res?evanje, obravnavanje} (dealing with)
{naslov} (postal address)
portion
{kos} (piece)
{obrok, porcija} (serving)
{delez?} (share)
figure
{s?tevilka, podatek, znesek} (amount)
{slika} (image)
{osebnost} (person)
Verbs
seal
{tesniti} (to be water-/airtight)
{zapreti, zapec?atiti} (to close an envelope or some other container)
weigh
{pretehtati} (consider possibilities)
{tehtati, stehtati} (check weight)
educate
{pouc?iti} (give information)
{izobraz?evati, izobraziti} (give education)
consume
{potros?iti} (spend money/goods)
{uz?ivati, zauz?iti} (eat/drink)
Adjs
mature
{zrel, odrasel} (adult)
{zorjen, zrel} (ripe)
minor
{nepomemben} (not very important)
{mladoleten, majhen} (under 18 years old)
juvenile
{nedorasel} (not adult/biologically mature yet)
{mladoleten, mladoletnis?ki} (not 18/legally adult yet)
remote
{odmaknjen, odroc?en} (far away and not easily accessible)
{oddaljen daljinski} (controlled from a distance (e.g. remote control))
Table 1: Entries from the English-Slovene sense cluster inventory.
occurrences of w in the parallel sentences where it
is translated by Ti. Let N be the number of features
retained for each Ti from the corresponding source
contexts. Each feature Fj (1 ? j ? N) receives a
total weight with a translation Ti, tw(Fj,Ti), de-
fined as the product of the feature?s global weight,
gw(Fj), and its local weight with that translation,
lw(Fj,Ti). The global weight of a feature Fj is a
function of the number Ni of translations (Ti?s) to
which Fj is related, and of the probabilities (pi j)
that Fj co-occurs with instances of w translated by
each of the Ti?s:
gw(Fj) = 1?
?Ti pi j log(pi j)
Ni
(1)
Each pi j is computed as the ratio of the co-
occurrence frequency of Fj with w when translated
as Ti to the total number of features seen with Ti:
pi j =
cooc frequency(Fj,Ti)
N
(2)
The local weight lw(Fj,Ti) between Fj and Ti di-
rectly depends on their co-occurrence frequency:
lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (3)
The pairwise similarity of the translations is cal-
culated using the Weighted Jaccard Coefficient
(Grefenstette, 1994).
WJ(Tm,Tn) =
? j min(tw(Tm,Fj), tw(Tn,Fj))
? j max(tw(Tm,Fj), tw(Tn,Fj))
(4)
The similarity score of each translation pair is
compared to a threshold locally defined for each w
using an iterative procedure. The threshold (T ) for
a word w is initially set to the mean of the scores
(above 0) of its translation pairs. The set of trans-
lation pairs of w is then divided into two sets (G1
and G2) according to whether they exceed, or are
inferior to, the threshold. The average of scores of
the translation pairs in each set is computed (m1
and m2) and a new threshold is calculated that is
the average of m1 and m2 (T = (m1+m2)/2). The
new threshold serves to separate again the transla-
tion pairs into two sets, a new threshold is calcu-
lated and the procedure is repeated until conver-
gence.
The semantically similar translations of w are
grouped into clusters. Translation pairs with a
score above the threshold form initial clusters that
4
might be further enriched provided that there exist
additional strongly related translations. Cluster-
ing stops when all translations of w are clustered
and all their relations have been checked. An im-
portant feature of the algorithm is that it performs
soft clustering, so translations can be found in dif-
ferent clusters. The final clusters are characterized
by global connectivity, i.e. all their elements are
linked by pertinent relations.
Table 1 gives examples of clusters obtained for
English words of different PoS with clear sense
distinctions in the parallel corpus. For each En-
glish word, we provide the obtained clusters of
Slovene translations including a description of the
sense described by each cluster. For instance, the
translations for the adjective minor from the train-
ing corpus (nepomemben, mladoleten and majhen)
are grouped into two clusters describing its two
senses: {nepomemben} - ?not very important?
and {mladoleten, majhen} - ?under 18 years old?.
The resulting cluster inventory contains 13,352
clusters in total, for 8,892 words. 2,585 of the
words (1,518 nouns, 554 verbs and 513 adjectives)
have more than one cluster.
In the next section, we explain how the clus-
ters and the corresponding translation vectors are
used for disambiguating the source language vec-
tors extracted from the comparable corpus.
4.3 Cross-lingual vector comparison
4.3.1 Vector building
We build context vectors in the two languages for
nouns occurring at least 50 times in the compa-
rable corpus. The frequency threshold is impor-
tant for the lexicon extraction approach to produce
good results. As features we use three content
words to the left and to the right of the retained
nouns, stopping at the sentence boundary, without
taking into account their position. Log-likelihood
is used to calculate feature weights.
In the reported experiments we focus on the
1,000 strongest features. A portion of these fea-
tures is disambiguated for each headword, de-
pending on the availability of clustering informa-
tion. We observed that disambiguating a smaller
amount of features yielded similar results and in-
cluding additional features did not improve the re-
sults.
4.3.2 Vector translation and disambiguation
Translation correspondences between the two lan-
guages of the comparable corpus are identified by
comparing the source language vectors, built as
described in Section 4.3.1, to the ones of the candi-
date translations. This comparison serves to quan-
tify the similarity of the source and target words
represented by the vectors and the highest ranked
pairs are retained.
For the comparison to take place, the source
vectors have to be translated in the target language.
In most previous work, the vectors were translated
using external seed dictionaries: the first transla-
tion proposed for a word in the dictionary was
used to translate all instances of the word in the
vectors irrespective of their sense. Here, we re-
place the external dictionary with the output of
a data-driven cross-lingual WSD method (Apidi-
anaki, 2009) which renders the method knowledge
light and adaptable to other language pairs.
The translation clusters obtained during WSI
(cf. Section 4.2) describe the senses of the En-
glish words in the parallel corpus. We exploit this
sense inventory for disambiguating the features in
the English vectors extracted from the comparable
corpus. More precisely, we ask the WSD method
to select among the available clusters the one that
correctly translates in Slovene the sense of the En-
glish features in the vectors built from the compa-
rable corpus. The selection is performed by com-
paring information from the context of a feature,
which corresponds to the rest of the vector where
the feature appears, to the source language vectors
of the translations which served to their cluster-
ing. Inside the vectors, the features are ordered
according to their score, calculated as described in
Section 4.3.1. Feature weights filter out the weak
features, i.e. features with a score below the ex-
perimentally set threshold of 0.01. The retained
features are then considered as a bag of words.
On the clusters? side, the information used for
disambiguation is found in the source language
vectors that revealed the similarity of the transla-
tions. If common features (CFs) exist between the
context of a feature and the vectors of the transla-
tions in a cluster, a score is calculated correspond-
ing to the mean of the weights of the CFs with the
clustered translations, where weights correspond
to the total weights (tw?s) computed between fea-
tures and translations during WSI. In formula 5,
CFj is the set of CFs and NCF is the number of
translations Ti characterized by a CF.
wsd score =
?
NCF
i=1 ? j w(Ti,CFj)
NCF ? |CFj|
(5)
5
PoS Feature Assigned Cluster MFT
Nouns
party {oseba, stran, pogodbenica, stranka} stranka
matter {zadeva, vpras?anje} zadeva
Verbs
settle {urediti, res?iti, res?evati} res?iti
follow {upos?tevati, spremljati, slediti} slediti
Adjs
alternative {nadomesten, alternativen} alternativen
involved {vkljuc?en, vpleten} vkljuc?en
Table 2: Disambiguation results.
The cluster that receives the highest score is se-
lected and assigned to the feature as a sense tag.
The features are also tagged with their most fre-
quent translation (MFT) in the parallel corpus,
which sometimes already exists in the cluster se-
lected during WSD.
In Table 2, we present examples of disam-
biguated features of different PoS from the vec-
tor of the word transition. The context used for
disambiguation consists of the other strong fea-
tures in the vector and the cluster that best de-
scribes the sense of the features in this context
is selected. In the last column, we provide the
MFT of the feature in the parallel corpus. In the
examples shown here the MFT translation already
exists in the cluster selected by the WSD method
but this is not always the case. As we will show
in the Evaluation section, the configuration where
the MFT from the cluster assigned during disam-
biguation is selected (called CLMFT) gives better
results than MFT, which shows that the MFT in
the selected cluster is not always the most frequent
alignment for the word in the parallel corpus. Fur-
thermore, the clusters provide supplementary ma-
terial (i.e. multiple semantically correct transla-
tions) for comparing the vectors in the target lan-
guage and improving the baseline results. Still,
MFT remains a very powerful heuristic due to the
skewed distribution of word senses and transla-
tions.
4.4 Vector comparison
The translation clusters proposed during WSD for
the features in the vectors built from the source
side of the comparable corpus serve to translate the
vectors in the target language. In our experiments,
we compare three different ways of translating the
source language features.
1. by keeping the most frequent transla-
tion/alignment of the feature in the parallel
corpus (MFT);
2. by keeping the most frequent translation from
the cluster assigned to the feature during dis-
ambiguation (CLMFT); and
3. by using the same cluster as in the second ap-
proach, but producing features for all transla-
tions in the cluster with the same weight (CL).
The first approach (MFT) serves as the base-
line since, instead of the sense clustering and
WSD results, it just uses the most frequent
sense/alignment heuristic. In the first batch of ex-
periments, we noticed that the results of the CL and
CLMFT approaches heavily depend on the part-of-
speech of the features. So, we divided the CL and
CLMFT approaches into three sub-approaches:
1. translate only nouns, verbs or adjectives with
the clusters and other features with the MFT
approach (CLMFT N, CLMFT V, CLMFT A);
2. translate nouns and adjectives with the clus-
ters and verbs with the MFT approach
(CLMFT NA); and
3. translate nouns and verbs with the clus-
ters and adjectives with the MFT approach
(CLMFT NV).
The distance between the translated source and
the target-language vectors is computed by the
Dice metric. By comparing the translated source
vectors to the target language ones, we obtain a
ranked list of candidate translations for each gold
standard entry.
5 Evaluation
5.1 Metrics
The final result of our method consists in ranked
lists of translation candidates for gold standard en-
tries. We evaluate this output by the mean recipro-
cal rank (MRR) measure which takes into account
6
the rank of the first good translation found for each
entry. Formally, MRR is defined as
MRR =
1
|Q|
|Q|
?
i=1
1
ranki
(6)
where |Q| is the length of the query, i.e. the num-
ber of gold standard entries we compute transla-
tion candidates for, and ranki is the position of the
first correct translation in the candidate list.
5.2 Results
Table 4 shows the translation extraction results
for different configurations. The MFT score is
used as the baseline. We observe that disam-
biguating all features in the vectors (CL) yields
lower results than the baseline compared to se-
lecting only the most frequent translation from the
cluster which slightly outperforms the MFT base-
line. In the CLMFT N, CLMFT NA, CLMFT NV
configurations we disambiguate noun features,
nouns and adjectives, and nouns and verbs, respec-
tively, and translate words of other PoS using the
MFT. In CLMFT N, for instance, nouns are dis-
ambiguated while verbs and adjectives are trans-
lated by the word to which they were most fre-
quently aligned in the parallel corpus. The three
configurations where nouns are disambiguated
(CLMFT N, CLMFT NA, CLMFT NV) give better
results compared to those addressing verbs or ad-
jectives alone. Interestingly, disambiguating only
adjectives gives worse results than disambiguating
only verbs, but the combination of nouns and ad-
jectives outperforms the combination of nouns and
verbs.
In CLMFT, features of all PoS are disambiguated
but we only keep the most frequent translation in
the cluster and ignore the other translations. This
setting gives much better results than CL, where
the whole cluster is used, which highlights two
facts: first, that disambiguation is beneficial for
translation extraction and, second, that the noise
present in the automatically built clusters harms
the quality of the translations extracted from the
comparable corpus. The better score obtained for
CLMFT compared to MFT also shows that, in many
cases, the most frequent translation in the cluster
does not coincide with the most frequent align-
ment of the word in the parallel corpus. So, disam-
biguation helps to select a more appropriate trans-
lation than the MFT approach. This improvement
compared to the baseline shows again that WSD is
MRR
MFT 0.0685
CLMFT 0.0807
CL 0.0434
CLMFT N 0.0817
CLMFT A 0.07
CLMFT V 0.0714
CLMFT NA 0.0842
CLMFT NV 0.08048
Table 3: Results of the experiment.
MRR diff p-value
MFT CLMFT 0.0122 0.1830
MFT CL 0.0251 0.0410
CLMFT CL 0.0373 0.0120
MFT CLMFT NA 0.0157 0.4296
MFT CLMFT NV 0.0120 0.5195
Table 4: Comparison of different configurations.
useful in this setting.
In Table 4, the results for different configura-
tions are compared. The statistical significance of
the difference in the results was calculated by ap-
proximate randomization (1,000 repetitions). We
observe that the differences between the CL and
MFT configurations and the CL and CLMFT ones,
are statistically significant. This confirms that tak-
ing most frequent translations, disambiguated or
not, works better than exploiting all the informa-
tion in the clusters. The remainder of the dif-
ferences in the results are not statistically signif-
icant. One could wonder why the p-values are that
high in case of the MFT setting on one side and
CLMFT NA and CLMFT NV settings on the other
side although the differences in the results are not
that high. The most probable explanation is that
there is a low intersection in correct results and
errors. Because of that, flipping the results be-
tween the two systems ? as performed in approx-
imate randomization ? often generates differences
higher than the initial difference on the original re-
sults.
5.3 Qualitative analysis
Manual evaluation of the results shows that the
procedure can deal with concrete words much bet-
ter than with abstract ones. For example, the cor-
rect translation of the headword enquiry is the
third highest-ranked translation. The results are
7
also much better with monosemous and domain-
specific terms (e.g. the correct translation for cat-
aclysm is the top-ranking candidate). On the other
hand, general and polysemous expressions that
can appear in a wide range of contexts are a much
tougher nut to crack. For example, the correct
translation candidate for word role, which can be
used in a variety of contexts as well as metaphor-
ically, is in the tenth position, whereas no correct
translation was found for transition. However, it
must be noted that even if the correct translation is
not found in the results, the output of our method
is in most cases a very coherent and solid descrip-
tion of the semantic field of the headword in ques-
tion. This means that the list can still be useful for
lexicographers to illicit the correct translation that
is missing, or organize the vocabulary in terms of
their relational-semantic principles.
We have also performed an error analysis in
cases where the correct translation could not be
found among the candidates, which consisted of
checking the 30 strongest disambiguated features
of an erroneously translated headword. We ob-
served cases where the strongest features in the
vectors are either very abstract and generic or too
heterogeneous for our method to be able to per-
form well. This was the case with the headwords
characterisation, antecedent and thread. In cases
where the strongest features represented the con-
cept clearly but the correct translation was not
found, we examined cluster, WSD and MFT qual-
ity, as suggested by the parallel corpus. The main
source of errors in these cases is the noise in the
clusters which is often due to pre-processing er-
rors, especially in the event of multi-word expres-
sions. It seems that clustering is also problematic
for abstract or generic words, where senses might
be lumped together. The WSD step, on the other
hand, does not seem to introduce noise to the pro-
cedure as it is correct in almost all the cases we
have examined.
6 Discussion and conclusion
We have shown how cross-lingual WSD can be
applied to bilingual lexicon extraction from com-
parable corpora. The disambiguation of source
language features using translation clusters con-
stitutes the main contribution of this work and
presents several advantages. First, the method per-
forms disambiguation by using sense descriptions
derived from the data, which clearly differentiates
our method from the approaches based on external
lexicons and extends its applicability to resource-
poor languages. The translation clusters acquired
through WSI serve to disambiguate the features in
the source language context vectors and to pro-
duce less noisy translated vectors. An additional
advantage is that the sense clusters often contain
more than one translation and, therefore, provide
supplementary material for the comparison of the
vectors in the target language.
The results show that data-driven semantic anal-
ysis can help to circumvent the need for an exter-
nal seed dictionary, traditionally considered as a
prerequisite for translation extraction from paral-
lel corpora. Moreover, it is clear that disambiguat-
ing the vectors improves the quality of the ex-
tracted lexicons and manages to beat the simpler,
but yet powerful, most frequent translation heuris-
tic. These encouraging results pave the way to-
wards pure data-driven methods for bilingual lex-
icon extraction. This knowledge-light approach
can be applied to languages and domains that do
not dispose of large-scale seed dictionaries but for
which parallel corpora are available.
An avenue that we intend to explore in future
work is to extract translations corresponding to
different senses of the headwords. Up to now,
research on translation extraction has most of-
ten aimed the identification of one good trans-
lation for a source word in the comparable cor-
pus. This has also been the case because most
works have focused on identifying translations
for specialized terms that do not convey differ-
ent senses. However, words in a general lan-
guage corpus like Wikipedia can be polysemous
and it is important to identify translations corre-
sponding to their different senses. Moreover, pol-
ysemy makes the translation extraction procedure
more difficult, as features corresponding to differ-
ent senses are mingled in the same vector. A way
to discover translations corresponding to different
word senses would be to apply a monolingual WSI
method on the source side of the comparable cor-
pus which would group the closely related usages
of the headwords together, and to then build vec-
tors for each usage group hopefully describing a
distinct sense. Using the generated sets of vectors
separately will allow to extract translations corre-
sponding to different senses of the source words.
8
References
Marianna Apidianaki, Nikola Ljubes?ic?, and Darja
Fis?er. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15, Ljubljana, Slovenia.
Marianna Apidianaki. 2008. Translation-oriented
sense induction based on parallel corpora. In Pro-
ceedings of the 6th International Conference on
Language Resources and Evaluation (LREC-08),
pages 3269?3275, Marrakech, Morocco.
Marianna Apidianaki. 2009. Data-driven Semantic
Analysis for Multilingual WSD and Lexical Selec-
tion in Translation. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-09),
pages 77?85, Athens, Greece.
Thorsten Brants. 2000. TnT - A Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing (ANLP-2000), Seat-
tle, WA.
Herve? De?jean, Eric Gaussier, Jean-Michel Renders,
and Fatiha Sadat. 2005. Automatic processing of
multilingual medical terminology: applications to
thesaurus enrichment and cross-language informa-
tion retrieval. Artificial Intelligence in Medicine,
33(2):111?124, February.
Tomaz? Erjavec, Darja Fis?er, Simon Krek, and Nina
Ledinek. 2010. The JOS Linguistically Tagged
Corpus of Slovene. In Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC?10), Valletta, Malta.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Darja Fis?er and Nikola Ljubes?ic?. 2011. Bilingual lexi-
con extraction from comparable corpora for closely
related languages. In Proceedings of the Inter-
national Conference Recent Advances in Natural
Language Processing 2011, pages 125?131, Hissar,
Bulgaria. RANLP 2011 Organising Committee.
Darja Fis?er and Beno??t Sagot. 2008. Combining mul-
tiple resources to build reliable wordnets. In TSD
2008 - Text Speech and Dialogue, Lecture Notes in
Computer Science, Brno, Czech Republic. Springer.
Pascale Fung. 1998. Machine translation and the in-
formation soup, third conference of the association
for machine translation in the americas, amta ?98,
langhorne, pa, usa, october 28-31, 1998, proceed-
ings. In AMTA, volume 1529 of Lecture Notes in
Computer Science. Springer.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer Academic Pub-
lishers, Norwell, MA.
Amir Hazem and Emmanuel Morin. 2012. Ica for
bilingual lexicon extraction from comparable cor-
pora. In Proceedings of the 5th Workshop on Build-
ing and Using Comparable Corpora (BUCC), Istan-
bul, Turkey.
Hiroyuki Kaji. 2003. Word sense acquisition from
bilingual comparable corpora. In HLT-NAACL.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
MT Summit X, pages 79?86, Phuket, Thailand.
Erwin Marsi and Emiel Krahmer. 2010. Automatic
analysis of semantic similarity in comparable text
through syntactic tree matching. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 752?760,
Beijing, China, August. Coling 2010 Organizing
Committee.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational Lin-
guistics, 31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51.
Pablo Gamallo Otero. 2007. Learning bilingual lexi-
cons from comparable english and spanish corpora.
In Proceedings of MT Summit XI, pages 191?198.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Machine Translation Summit 2009, page 8.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 519?526, College Park, Maryland, USA,
June. Association for Computational Linguistics.
Xabier Saralegi, In?aki San Vicente, and Antton Gur-
rutxaga. 2008. Automatic extraction of bilingual
terms from comparable corpora in a popular sci-
ence domain. In Proceedings of the Building and
using Comparable Corpora workshop, 6th Interna-
tional Conference on Language Resources and Eval-
uations (LREC), Marrakech, Morocco.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings
of the International Conference on New Methods
in Language Processing, pages 44?49, Manchester,
UK.
9
Li Shao and Hwee Tou Ng. 2004. Mining new
word translations from comparable corpora. In Pro-
ceedings of Coling 2004, pages 618?624, Geneva,
Switzerland, Aug 23?Aug 27. COLING.
Matthew G. Snover, Bonnie J. Dorr, and Richard M.
Schwartz. 2008. Language and translation model
adaptation using comparable corpora. In EMNLP,
pages 857?866.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Toma Erjavec, and Dan Tufi. 2006.
The jrc-acquis: A multilingual aligned parallel cor-
pus with 20+ languages. In In Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC?2006), pages 2142?2147.
Kun Yu and Junichi Tsujii. 2009. Extracting bilin-
gual dictionary from comparable corpora with de-
pendency heterogeneity. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 121?124, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
10
