Mining Name Translations from Entity Graph Mapping∗


Gae-won You†	Seung-won Hwang†	Young-In Song‡	Long Jiang‡	Zaiqing Nie‡

†Pohang University of Science and Technology, Pohang, Republic of Korea
{gwyou,swhwang}@postech.ac.kr

‡Microsoft Research Asia, Beijing, China
{yosong,longj,znie}@microsoft.com






Abstract

This paper studies the problem of mining en- 
tity translation, specifically, mining English 
and  Chinese  name  pairs.    Existing  efforts 
can be categorized into (a) a transliteration- 
based approach leveraging phonetic similar- 
ity and (b) a corpus-based approach exploiting 
bilingual co-occurrences, each of which suf- 
fers from inaccuracy and scarcity respectively. 
In clear contrast, we use unleveraged re- 
sources of monolingual entity co-occurrences, 
crawled from entity search engines, repre- 
sented as two entity-relationship graphs ex- 
tracted from two language corpora respec- 
tively. Our problem is then abstracted as find- 
ing correct mappings across two graphs.  To 
achieve this goal, we propose a holistic ap- 
proach, of exploiting both transliteration sim- 
ilarity and monolingual co-occurrences. This 
approach, building upon monolingual corpora, 
complements existing corpus-based work, re- 
quiring scarce resources of parallel or compa- 
rable corpus, while significantly boosting the 
accuracy of transliteration-based work.   We 
validate our proposed system using real-life 
datasets.


1   Introduction

Entity translation aims at mapping the entity names 
(e.g., people, locations, and organizations) in source 
language into their corresponding names in target 
language. While high quality entity translation is es- 
sential in cross-lingual information access and trans-


lation, it is non-trivial to achieve, due to the chal- 
lenge that entity translation, though typically bear- 
ing pronunciation similarity, can also be arbitrary, 
e.g.,  Jackie Chan  and  fiX :it  (pronounced Cheng 
Long). Existing efforts to address these challenges 
can be categorized into transliteration- and corpus- 
based approaches. Transliteration-based approaches 
(Wan and Verspoor, 1998; Knight and Graehl, 1998) 
identify translations based on pronunciation similar- 
ity, while corpus-based approaches mine bilingual 
co-occurrences of translation pairs obtained from 
parallel (Kupiec, 1993; Feng et al., 2004) or compa- 
rable (Fung and Yee, 1998) corpora, or alternatively 
mined from bilingual sentences (Lin et al., 2008; 
Jiang et al., 2009). These two approaches have com- 
plementary strength– transliteration-based similar- 
ity can be computed for any name pair but cannot 
mine translations of little (or none) phonetic simi- 
larity. Corpus-based similarity can support arbitrary 
translations, but require highly scarce resources of 
bilingual co-occurrences, obtained from parallel or 
comparable bilingual corpora.
  In this paper, we propose a holistic approach, 
leveraging both transliteration- and corpus-based 
similarity.   Our key contribution is to replace the 
use of scarce resources of bilingual co-occurrences 
with the use of untapped and significantly larger 
resources of monolingual co-occurrences for trans- 
lation.  In particular, we extract monolingual co- 
occurrences of entities from English and Chinese 
Web corpora, which are readily available from en- 
tity search engines such as PeopleEntityCube1, de- 
ployed by Microsoft Research Asia.  Such engine


∗This work was done when the first two authors visited Mi-	 	


crosoft Research Asia.


1 
http://people.entitycube.
com




430


Proceedings of the 2010 Conference on Empirical  Methods in Natural  Language Processing, pages 430–439, 
MIT, Massachusetts, USA, 9-11 October 2010. Qc 2010 Association for Computational Linguistics


automatically extracts people names from text and 
their co-occurrences to retrieve related entities based 
on co-occurrences. To illustrate, Figure 1(a) demon- 
strates the query result for “Bill Gates,” retrieving 
and visualizing the “entity-relationship graph” of re- 
lated people names that frequently co-occur with 
Bill in English corpus. Similarly, entity-relationship 
graphs can be built over other language corpora, as 
Figure 1(b) demonstrates the corresponding results 
for the same query, from Renlifang2 on Chinese Web 
corpus. From this point on, for the sake of simplic- 
ity, we refer to English and Chinese graphs, simply





0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0




Tail
Head











Ours 	Google 	Engkoo


as Ge and Gc respectively. Though we illustrate with 
English-Chinese pairs in the paper, our method can 
be easily adapted to other language pairs.
  In particular, we propose a novel approach of ab- 
stracting entity translation as a graph matching prob- 
lem of two graphs Ge and Gc in Figures 1(a) and (b). 
Specifically, the similarity between two nodes ve 
and vc in Ge  and Gc  is initialized as their transliter- 
ation similarity, which is iteratively refined based on 
relational similarity obtained from monolingual co- 
occurrences. To illustrate this, an English news ar- 
ticle mentioning “Bill Gates” and “Melinda Gates” 
evidences a relationship between the two entities, 
which can be quantified from their co-occurrences 
in the entire English Web corpus.   Similarly, we 
can mine Chinese news articles to obtain the re-
lationships between “t )];·Jli Vi” and “1'1 li\ it·Jli
Vi”. Once these two bilingual graphs of people and
their relationships are harvested, entity translation 
can leverage these parallel relationships to further 
evidence the mapping between translation pairs, as 
Figure 1(c) illustrates.
  To highlight the advantage of our proposed ap- 
proach, we compare our results with commercial 
machine translators (1) Engkoo3  developed in Mi- 
crosoft Research Asia and (2) Google Translator4. 
In particular, Figure 2 reports the precision for two 
groups– “heads” that belong to top-100 popular peo- 
ple (determined by the number of hits), among ran- 
domly sampled 304 people names5 from six graph 
pairs of size 1,000 each, and the remaining “tails”. 
Commercial translators such as Google, leveraging

2 http://renlifang.msra.cn
3 http://www.engkoo.com
4 http://translate.google.com
5 See Section 4 for the sampling process.


Figure 2: Comparison for Head and Tail datasets


bilingual co-occurrences that are scarce for tails, 
show significantly lower precision for tails.  Mean- 
while,  our  work,  depending  solely  on  monolin- 
gual co-occurrences, shows high precision, for both 
heads and tails.
  Our focus is to boost translation accuracy for 
long tails with non-trivial Web occurrences in each 
monolingual corpus, but not with much bilingual co- 
occurrences, e.g., researchers publishing actively in 
two languages but not famous enough to be featured 
in multi-lingual Wikipedia entries or news articles. 
As existing translators are already highly accurate 
for popular heads, this focus well addresses the re- 
maining challenges for entity translation.
  To summarize, we believe that this paper has the 
following contributions:

• We  abstract  entity  translation  problem  as 
a graph mapping between entity-relationship 
graphs in two languages.

• We   develop   an   effective   matching   algo- 
rithm leveraging both pronunciation and co- 
occurrence similarity.  This holistic approach 
complements  existing  approaches  and  en-
hances the translation coverage and accuracy.

• We validate the effectiveness of our approach 
using various real-life datasets.

  The rest of this paper is organized as follows. Sec- 
tion 2 reviews existing work. Section 3 then devel- 
ops our framework. Section 4 reports experimental 
results and Section 5 concludes our work.


 
(a) English PeopleEntityCube Ge 	(b) Chinese Renlifang Gc





















(c) Abstracting translation as graph mapping

Figure 1: Illustration of entity-relationship graphs




2   Related Work

In this section, we first survey related efforts, cate- 
gorized into transliteration-based and corpus-based 
approaches. Our approach leveraging both is com- 
plementary to these efforts.

2.1   Transliteration-based Approaches
Many name translations are loosely based on 
phonetic similarity, which naturally inspires 
transliteration-based translation of finding the 
translation with the closest pronunciation similarity, 
using either rule-based (Wan and Verspoor, 1998) or 
statistical (Knight and Graehl, 1998; Li et al., 2004)


approaches.  However, people are free to designate 
arbitrary bilingual names of little (or none) pho- 
netic similarity, for which the transliteration-based 
approach is not effective.

2.2   Corpus-based Approaches

Corpus-based approach can mine arbitrary transla- 
tion pairs, by mining bilingual co-occurrences from 
parallel and comparable bilingual corpora.   Using 
parallel corpora (Kupiec, 1993; Feng et al., 2004), 
e.g., bilingual Wikipedia entries on the same per- 
son, renders high accuracy but suffers from high 
scarcity. To alleviate such scarcity, (Fung and Yee,


1998; Shao and Ng, 2004) explore a more vast re- 
source of comparable corpora, which share no par- 
allel document- or sentence-alignments as in paral- 
lel corpora but describe similar contents in two lan- 
guages, e.g., news articles on the same event.  Al- 
ternatively, (Lin et al., 2008) extracts bilingual co- 
occurrences from bilingual sentences, such as an- 
notating terms with their corresponding translations 
in English inside parentheses.  Similarly, (Jiang et 
al., 2009) identifies potential translation pairs from 
bilingual sentences using lexical pattern analysis.

2.3   Holistic Approaches

The complementary strength of the above two ap- 
proaches  naturally  calls  for  a  holistic  approach, 
such  as  recent  work  combining  transliteration- 
and corpus-based similarity mining bilingual co- 
occurrences using general search engines.  Specifi- 
cally, (Al-Onaizan and Knight, 2002) uses translit- 
eration to generate candidates and then web corpora 
to identify translations.  Later, (Jiang et al., 2007) 
enhances to use transliteration to guide web mining.
  Our work is also a holistic approach, but leverag- 
ing significantly larger corpora, specifically by ex- 
ploiting monolingual co-occurrences.  Such expan- 
sion enables to translate “long-tail” people entities 
with non-trivial Web occurrences in each monolin- 
gual corpus, but not much bilingual co-occurrences. 
Specifically, we initialize name pair similarity using 
transliteration-based approach, and iteratively rein- 
forces base similarity using relational similarity.

3   Our Framework

Given two graphs Ge  = (Ve, Ee ) and Gc  = (Vc , Ec ) 
harvested from English and Chinese corpora respec- 
tively, our goal is to find translation pairs, or a set S


lation similarities Rij by exploiting the mono- 
lingual co-occurrences

3. Matching extraction: extracting the matching 
pairs from the final translation similarities Rij

3.1   Initialization with Transliteration

We initialize the translation similarity Rij as the 
transliteration similarity. This section explains how 
to get the transliteration similarity between English 
and Chinese names using an unsupervised approach.
  Formally,    let   an   English   name   Ne          = 
(e1, e2, · · · , en)  and   a   Chinese   name   Nc        = 
(c1 , c2, · · · , cm ) be given, where ei  is an English
word and Ne   is a sequence of the words, and ci 
is a Chinese character and Nc   is a  sequence of 
the characters.   Our goal is to compute a score 
indicating the similarity between the pronunciations 
of the two names.
We first convert Nc  into its Pinyin representation
P Yc  = (s1, s2, · · · , sm ), where si is the Pinyin rep-
resentation of ci .   Pinyin is the romanization rep-
resentation of pronunciation of Chinese character. 
For example, the Pinyin representation of Ne   = 
(“Barack”, “Obama”) is P Yc   =(“ba”,  “la”, “ke”, 
“ao”, “ba”, “ma”).  The Pinyin representations of 
Chinese characters can be easily obtained from Chi- 
nese character pronunciation dictionary. In our ex- 
periments, we use an in-house dictionary, which 
contains pronunciations of 20, 774 Chinese charac- 
ters. For the Chinese characters having multiple pro- 
nunciations, we only use the most popular one.
  Calculation of transliteration similarity between 
Ne  and Nc  is now transformed to calculation of pro- 
nunciation similarity between Ne  and P Yc . Because 
letters in Chinese Pinyins and English strings are 
pronounced similarly, we can further approximate


of matching node pairs such that S ⊆ Ve × Vc. Let
R be a |Ve |-by-|Vc| matrix where each Rij denotes


pronunciation similarity between Ne


and P Yc


us-


the similarity between two nodes i ∈ Ve and j ∈ Vc.
Overall, with the matrix R, our approach consists
of the following three steps, as we will discuss in the 
following three sections respectively:

1. Initialization: computing base translation sim- 
ilarities Rij between two entity nodes using 
transliteration similarity

2. Reinforcement model: reinforcing the trans-


ing their spelling similarity.  In this paper, we use
Edit Distance (ED) to measure the spelling similar- 
ity.  Moreover, since words in Ne  are transliterated 
into characters in P Yc independently, it is more ac- 
curate to compute the ED between Ne  and P Yc , i.e., 
EDname (Ne , P Yc ),  as the sum of the EDs of all 
component transliteration pairs, i.e., every ei in Ne 
and its corresponding transliteration (si ) in P Yc. In 
other words, we need to first align all sj ’s in P Yc 
with corresponding ei in Ne  based on whether they


are translations of each other.  Then based on the 
alignment, we can calculate EDname (Ne , P Yc) us- 
ing the following formula.

EDname (Ne , P Yc) = ∑ ED(ei , esi ) 	(1)
i

where esi is a string generated by concatenating all 
si ’s that are aligned to ei  and ED(ei , esi ) is the 
Edit Distance between ei  and esi , i.e., the mini- 
mum number of edit operations (including inser-


3.2   Reinforcement Model
From the initial similarity, we model our problem as 
an iterative approach that iteratively reinforces the 
similarity Rij of the nodes i and j from the matching 
similarities of their neighbor nodes u and v.
  The basic intuition is built on exploiting the sim- 
ilarity between monolingual co-occurrences of two 
different languages.  In particular, we assume two 
entities with strong relationship co-occur frequently 
in both corpora. In order to express this intuition, we
formally define an iterative reinforcement model as


tion, deletion and substitution) needed to transform
ei into esi .  Because an English word usually con-


follows. Let Rt


denote the similarity of nodes i and


sists of multiple syllables but every Chinese charac- 
ter consists of only one syllable, when aligning ei ’s


j at t-th iteration:

t
t+1	∑	uv 	0


with sj ’s, we add the constraint that each ei is al- 
lowed to be aligned with 0 to 4 si ’s but each si can


Rij	= λ



(u,v)k ∈Bt (i,j,θ)


2k    + (1 − λ)Rij	(3)


only be aligned with 0 to 1 ei .  To get the align-
ment between P Yc  and Ne  which has the minimal


The model is  expressed as  a  linear combination
of (a) the relational similarity ∑ Rt	k



EDname (Ne , P Yc ),  we use a Dynamic Program-



transliteration similarity R0


uv /2


and (b)



ming based algorithm as defined in the following 
formula:


ij .  (λ is the coefficient
for interpolating two similarities.)
  In the relational similarity, Bt (i, j, θ) is an or- 
dered set of the best matching pairs between neigh-


EDname (N 1,i , P Y 1,j ) = min(


bor nodes of i and ones of j 
such that


(u, v)


e 	c
k


EDname (N 1,i−1


1,j


e	, P Yc      ) + Len(ei ),


Bt (i, j, θ), Rt 	≥ θ, where (u, v)k   is 
the match-


EDname (N 1,i


1,j−1


e   , P Yc 	) + Len(sj ),


ing pair with k-th highest similarity 
score. We con-


EDname (N 1,i−1


1,j−1


e	, P Yc 	) + ED(ei , sj ),


sider (u, v) with similarity over some 
threshold θ,


EDname (N 1,i−1


1,j−2


j−1,j


e	, P Yc 	) + ED(ei , P Yc 	),


or Rt 	≥ θ, as a matching pair.  In this 
neighbor


EDname (N 1,i−1


1,j−3


j−2,j


e	, P Yc 	) + ED(ei , P Yc 	),


matching process, if many-to-many matches 
exist,


EDname (N 1,i−1


1,j−4


j−3,j


e	, P Yc 	) + ED(ei , P Yc 	))

where,   given  a  sequence  X   =   (x1 , x2, · · ·)
such that xi  is a word,  X i,j  is the subsequence
(xi , xi+1, · · · , xj ) of X and Len(X ) is the number
of letters except spaces in the sequence X . For in-
stance, the minimal Edit Distance between the En- 
glish name “Barack Obama” and the Chinese Pinyin 
representation “ba la ke ao ba ma” is 4,  as the
best alignment is: “Barack” ↔ “ba la ke” (ED: 3),


we select only one with the greatest matching score.
Figure 3 describes such matching process more for- 
mally. N (i) and N (j) are the sets of neighbor nodes 
of i and j,  respectively, and H  is a priority queue 
sorting pairs in the decreasing order of similarity 
scores.
  Meanwhile, note that,  in order to express that 
the confidence for matching (i, j) progressively con- 
verges  as  the  number  of  matched  neighbors  in-
creases,  we  empirically use  decaying  coefficient


“Obama” ↔ “ao ba ma” (ED: 1). Finally the translit-
eration similarity between Nc  and Ne  is calculated


1/2k  for Rt


, because ∑∞


1/2k  = 1.


using the following formula.

Sim (N  , N  ) = 1	EDname (Ne , P Yc) 	(2)
Len(P Y ) + Len(N )


3.3	Matching Extraction

After the convergence of the above model, we get 
the |Ve |-by-|Vc |  similarity matrix R∞.   From this


c 	e 	matrix, we extract one-to-one matches maximizing
the overall similarity.


For  example,  Simtl (“Barack  Obama”,  “\3 5I
�·�\3�”) is 1 −     4      = 0.826.
  

More formally,  this  problem can  be  stated as 
the maximum weighted bipartite matching (West,



1. 	Bt (i, j, θ) ← {}
  

First,  we crawled Ge    
=  (Ve , Ee)  and Gc    = 
(V , E )  from English 
and  Chinese 
EntityCubes.


c 	c
2. 	∀u ∈ N (i), ∀v ∈ N (j) : H.push(u, v; R    )
uv	Specifically, we built a graph pairs (G  , G ) expand-


3. 	while H


is not empty do 	e 	c


4. 	(u, v; s) ← H.pop()
5. 	if s < θ then
6. 	break
7. 	end if
8. 	if neither u nor v are matched yet then


ing from a “seed pair” of nodes se  ∈ Ve and sc  ∈ 
Vc
until the number of nodes for each graph 
becomes
1,0006.  More specifically, when we build a 
graph Ge  by expanding from se , we use a 
queue Q.  We first initialize Q by pushing the 
seed node se .  We


9. 	Bt (i, j, θ) ← Bt (i, j, θ) ∪ {(u, v)}


then iteratively pop a node ve


from Q, save ve


into


10.
11.
12.


end 
if
end 
while 
retur
n Bt (i, 
j, θ)


Ve , 
and 
push 
its 
neig
hbor 
node
s in 
decr
easin
g 
order 
of 
co-
occu
rrenc
e 
score
s 
with 
ve . 
Simi
larly, 
we 
can 
get 
Gc  
from 
a 
coun
terpa
rt 
seed 
node 
vc .  
By 
usin
g 
this 
proc
edur
e, we 
built 
six 
grap
h 
pairs 
from 
six 
dif-


Figure 3: How to get the ordered set Bt (i, j, θ)


2000)– Given two groups of entities Ve and Vc from 
the two graphs Ge  and Gc , we can build a weighted
bipartite graph is G = (V, E), where V  = Ve ∪ Vc
and E is a set of edges (u, v) with weight R∞ . To
filter out null alignment, we construct only the edges


ferent seed pairs.  In particular, the six seed nodes 
are English names and its corresponding Chinese 
names representing a wide range of occupation do- 
mains (e.g., ‘Barack Obama,’ ‘Bill Gates,’ ‘Britney 
Spears,’ ‘Bruno Senna,’ ‘Chris Paul,’ and ‘Eminem’) 
as Table 1 depicts. Meanwhile, though we demon- 
strate the effectiveness of the proposed method for


with weight R∞


≥ δ.  From this 
bipartite graph,


mining name 
translations in 
Chinese and 
English



the maximum weighted bipartite matching problem 
finds a set of pairwise non-adjacent edges S  ⊆ E


languages, the method can be easily adapted to other
language pairs.


such that ∑ 	∞
∈


is the maximum.	Well-


known algorithms include Hungarian algorithm with 
time complexity of O(|V |2 log |V | + |V ||E|) (West,
2000).
  In this paper, to speed up processing, we consider 
a greedy alternative with the following steps– (1) 
choose the pair with the highest similarity score, (2) 
remove the corresponding row and column from the 
matrix, and (3) repeat (1) and (2) until their match- 
ing scores are over a specific threshold δ.

4   Experiments

This section reports our experimental results to eval- 
uate our proposed approach. First, we report our ex- 
perimental setting in Section 4.1. Second, we vali- 
date the effectiveness and the scalability of our ap- 
proach over a real-life dataset in Section 4.2.

4.1   Experimental Settings

This section describes (1) how we collect the En- 
glish and Chinese EntityCube datasets, (2) how to 
build ground-truth test datasets for evaluating our 
framework, and (3) how to set up three parameters 
λ, θ, and δ.


Table 1: Summary for graphs and test datasets obtained
from each seed pair

i
|Ve 
|, 
|Vc 
|
|Ti 
|
En
gli
sh 
Na
me
C
h
i
n
e
s
e 
N
a
m
e
1
1
,
0
0
0
51
Ba
rac
k 
Ob
am
a
\
3
5I
�
·
�
\
3
�
2
1
,
0
0
0
52
B
i
l
l 
G
a
t
e
s
t
)
]
;
·
J
l
i
V
i
3
1
,
0
0
0
40
Bri
tne
y 
Sp
ear
s
   
·  
)]; 
4
1
,
0
0
0
53
Br
un
o 
Se
nn
a
   
·  
5
1
,
0
0
0
51
C
h
r
i
s 
P
a
u
l
�
  
·  
6
1
,
0
0
0
57
E
m
i
n
e
m

    

  Second, we manually searched for about 50 
“ground-truth” matched translations for each graph 
pair to build test datasets Ti , by randomly selecting 
nodes within two hops7 from the seed pair (se, sc ), 
since nodes outside two hops may include nodes 
whose neighbors are not fully crawled. More specif- 
ically, due to our crawling process expanding to add 
neighbors from the seed, the nodes close to the seed 
have all the neighbors they would have in the full 
graph, while those far from the node may not. In or- 
der to pick the nodes that well represent the actual

  6 Note, this is just a default setting, which we later increase 
for scalability evaluation in Figure 6.
7 Note that the numbers of nodes within two hops in Ge and
Gc  are 327 and 399 on average respectively.


neighbors, we built test datasets among those within 
two hops.  However, this crawling is used for the 
evaluation sake only, and thus does not suggest the 
bias in our proposed framework. Table 1 describes 
the size of such test dataset for each graph pair.
  Lastly, we set up the three parameters λ, θ, and 
δ using 6-fold cross validation with 6 test datasets 
Ti ’s  of  the  graphs.    More  specifically, for  each 
dataset Ti , we decide λi and θi  such that average 
MRR for the other 5 test datasets is maximized. 
(About MRR, see more details of Equation (4) in 
Section 4.2.)  We then decide δi  such that average 
F1-score is maximized. Figure 4 shows the average 
MRR for λi and θi  with default values θ  = 0.66 
and λ = 0.2. Based on these results, we set λi with
values {0.2, 0.15, 0.2, 0.15, 0.2, 0.15} that optimize
MRR in datasets T1, . . . T6,  and similarly θi  with
{0.67, 0.65, 0.67, 0.67, 0.65, 0.67}.  We also set δi
with values {0.63, 0.63, 0.61, 0.61, 0.61, 0.61} opti-
mizing F1-score with the same default values λ =
0.2 and θ  = 0.66.  We can observe the variances 
of optimal parameter setting values are low, which 
suggests the robustness of our framework.

4.2   Experimental Results
This section reports our experimental results using 
the evaluation datasets explained in previous sec- 
tion.   For each graph pair, we evaluated the ef- 
fectiveness of (1) reinforcement model using MRR 
measure in Section 4.2.1 and (2) overall framework 
using precision, recall,  and F1 measures in Sec- 
tion 4.2.2.  We also validated (3) scalability of our 
framework over larger scale of graphs (with up to 
five thousand nodes) in Section 4.2.3. (In all experi- 
mental results, Bold numbers indicate the best per- 
formance for each metric.)

4.2.1   Effectiveness of reinforcement model
  We  evaluated  the  reinforcement  model  over 
MRR (Voorhees, 2001), the average of the recipro- 
cal ranks of the query results as the following for- 
mula:


MRRs for two matrices R0  and R∞, we can validate
the effectiveness of the reinforcement model.
• Baseline matrix (R0):  using only the translit- 
eration similarity score, i.e., without reinforce- 
ment
• Reinforced matrix (R∞):  using the reinforced 
similarity score obtained from Equation (3)



Table 2: MRR of baseline and reinforced matrices


S
e
t
M
R
R

Ba
sel
in
e 
R0
Re
inf
or
ce
d 
R
∞
T
1
0
.
6
9
6
4
0
.
8
3
7
7
T
2
0
.
6
2
1
3
0
.
7
5
8
1
T
3
0
.
7
0
9
5
0
.
7
9
8
9
T
4
0
.
8
1
5
9
0
.
8
3
7
8
T
5
0
.
6
9
8
4
0
.
8
1
5
8
T
6
0
.
5
9
8
2
0
.
8
0
1
1
Av
er
ag
e
0
.
6
9
0
0
0
.
8
0
8
2

We empirically observed that the iterative model 
converges within 5 iterations. In all experiments, we 
used 5 iterations for the reinforcement.
  Table 2 summarizes our experimental results. As 
these figures show, MRR scores significantly in- 
crease after applying our reinforcement model ex- 
cept for the set T4  (on average from 69% to 81%), 
which indirectly shows the effectiveness of our rein- 
forcement model.

4.2.2   Effectiveness of overall framework
  Based on  the  reinforced matrix,  we  evaluated 
the effectiveness of our overall matching framework 
using the following three measures–(1) precision: 
how accurately the method returns matching pairs, 
(2) recall: how many the method returns correct 
matching pairs, and (3) F1-score:  the harmonic 
mean of precision and recall. We compared our ap- 
proach with a baseline, mapping two graphs with 
only transliteration similarity.


MRR =


1   ∑   1


(4)	0


|Q| q∈Q rankq


• Baseline: in matching extraction, 
using R   as



Each q is a ground-truth matched pair (u, v) such 
that u ∈ Ve and v ∈ Vc , and rankq is the rank of the
similarity score of Ruv  among all Ruk ’s such that
k  ∈ Vc .  Q is a set of such queries. By comparing


the similarity matrix by bypassing the rein-
forcement step
• Ours:  using R∞,  the similarity matrix con- 
verged by Equation (3)


0.85


0.84


0.74



0.84

0.83

0.82

0.81



1
2	0.82
3
 	  4	0.8
5



0
.
7
3

0
.
7
2
1
	
1
2
	
0
.
7
1
	
2


0.8

0.79

0.78

0.77



 	  6



0.1
	
0.15
	
0.2
	
0.25
	
0.3
(
 
=
0
.
6
6
)


0.78

0.76

0.74


3

4

5

6

0.61
	0
.63
	0
.65
	0
.67
	0
.69
(
 
=
0
.
2
)



0.7

0.69

0.68


3

4

5

6

0.57	0.59	0.61	0.63	0.65
(
 
=
0
.
2
,
  
=
0
.
6
6
)



Figure 4: Parameter setup for λ, θ, and δ




In addition, we compared ours with the machine 
translators of Engkoo and Google. Table 3 summa- 
rizes our experimental results.
  As this table shows, our approach results in the 
highest precision (about 80% on average) without 
compromising the best recall of Google, i.e., 61% 
of Google vs.  63% of ours.  Overall, our approach 
outperforms others in all three measures.
  Meanwhile, in order to validate the translation ac- 
curacy over popular head and long-tail, as discussed 
in Section 1, we separated the test data into two 
groups and analyzed the effectiveness separately. 
Figure 5 plots the number of hits returned for the 
names from Google search engine. According to the 
distribution, we separate the test data into top-100 
popular people with the highest hits and the remain- 
ing, denoted head and tail, respectively.


4.2.3   Scalability
  To validate the scalability of our approach, we 
evaluated the effectiveness of our approach over the 
number of nodes in two graphs. We built larger six 
graph pairs (Ge, Gc ) by expanding them from the 
seed pairs further until the number of nodes reaches
5,000. Figure 6 shows the number of matched trans- 
lations according to such increase. Overall, the num- 
ber of matched pairs linearly increases as the num- 
ber of nodes increases, which suggests scalability. 
The ratio of node overlap in two graphs is about be- 
tween 7% and 9% of total node size.

350

300

250

200



150
8
10
100

7
10 	50
1000 	2000 	3000 	4000 	5000
|V | and |V |
e	c
6
10
Figure 6: Matched translations over |Ve | and |Vc |



5
10


4
10
0 	50 	100 	150 	200 	250 	300 	350
Number of names

Figure 5: Distribution over number of hits



  Table 4 shows the effectiveness with both 
datasets, respectively. As difference of the effective- 
ness between tail and head (denoted diff ) with re- 
spect to three measures shows, our approach shows 
stably high precision, for both heads and tails.




5   Conclusion

This paper abstracted name translation problem as a 
matching problem of two entity-relationship graphs. 
This novel approach complements existing name 
translation work,  by not requiring rare resources 
of parallel or comparable corpus yet outperforming 
the state-of-the-art. More specifically, we combine 
bilingual phonetic similarity and monolingual Web 
co-occurrence similarity, to compute a holistic no- 
tion of entity similarity. To achieve this goal, we de-


Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti

Se
t
P
r
e
c
i
s
i
o
n
R
e
c
a
l
l
F
1
-
s
c
o
r
e

En
gko
o
Go
ogl
e
Bas
elin
e
O
ur
s
En
gko
o
Go
ogl
e
Bas
elin
e
O
ur
s
Eng
koo
Go
ogl
e
Bas
elin
e
O
ur
s
T
1
0.5
26
3
0.4
510
0.5
26
3
0.8
974
0.3
92
2
0.4
510
0.1
96
1
0.6
863
0.4
49
4
0.4
510
0.2
85
7
0.7
778
T
2
0.7
55
1
0.
75
0.7
14
3
0.8
056
0.7
11
5
0.
75
0.2
88
5
0.5
577
0.7
32
7
0.
75
0.4
11
0
0.6
591
T
3
0.5
83
3
0.7
925
0.5
55
6
0.7
949
0.5
28
3
0.7
925
0.1
88
7
0.5
849
0.5
54
5
0.7
925
0.2
81
7
0.6
739
T
4
0
.
5
0.
45
0.7
36
8
0.7
353
0.
42
5
0.
45
0.
3
5
0.6
25
0.4
59
5
0.
45
0.4
74
6
0.6
757
T
5
0.6
11
1
0.3
137
0
.
5
0.7
234
0.4
31
4
0.3
137
0.1
76
5
0.6
667
0.5
05
7
0.3
137
0.2
60
9
0.6
939
T
6
0.5
63
6
0.8
947
0
.
6
0.8
605
0.5
43
8
0.8
947
0.1
05
3
0.6
491
0.5
53
6
0.8
947
0.1
79
1
0.
74
AV
G
0.5
89
9
0.6
086
0.6
05
5
0.8
028
0.5
05
4
0.6
086
0.2
17
5
0.6
283
0.5
42
6
0.6
086
0.3
15
5
0.7
034


Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets


M
et
ho
d
P
r
e
c
i
s
i
o
n
R
e
c
a
l
l
F
1
-
s
c
o
r
e

h
e
a
d
t
a
i
l
d
i
f
f
he
ad
t
a
i
l
d
i
f
f
h
e
a
d
t
a
i
l
d
i
f
f
En
gk
oo
0.
60
82
0.
58
54
0.
02
29
0.
59
0.
47
06
0.
11
94
0.
59
90
0.
52
17
0.
07
72
G
oo
gl
e
0
.
7
5
0.
55
88
0.
19
12
0.
75
0.
55
88
0.
19
12
0
.
7
5
0.
55
88
0.
19
12
O
u
r
s
0.
84
62
0.
78
12
0.
06
49
0.
66
0.
61
27
0.
04
73
0.
74
16
0.
68
68
0.
05
48




veloped a graph alignment algorithm that iteratively 
reinforces the matching similarity exploiting rela- 
tional similarity and then extracts correct matches. 
Our evaluation results empirically validated the ac- 
curacy of our algorithm over real-life datasets, and 
showed the effectiveness on our proposed perspec- 
tive.

Acknowledgments

This work was supported by Microsoft Research 
Asia NLP theme funding and MKE (Ministry of 
Knowledge Economy), Korea, under the ITRC (In- 
formation Technology Research Center) support 
program supervised by the IITA (Institute for In- 
formation Technology Advancement) (IITA-2009- 
C1090-0902-0045).


References

Yaser Al-Onaizan and Kevin Knight.   2002.   Trans- 
lating Named Entities Using Monolingual and Bilin- 
gual Resources.   In Proceedings of the 40th Annual 
Meeting on Association for Computational Linguistics 
(ACL’02), pages 400–408. Association for Computa- 
tional Linguistics.
Donghui Feng,  Yajuan  Lu¨ ,  and  Ming  Zhou.     2004.
A New Approach for English-Chinese Named En- 
tity Alignment.  In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP’04), pages 372–379. Association for Com- 
putational Linguistics.


Pascale Fung and Lo Yuen Yee.   1998.   An IR Ap- 
proach for Translating New Words from Nonparal- 
lel,Comparable Texts.  In Proceedings of the 17th In- 
ternational Conference on Computational Linguistics 
(COLING’98), pages 414–420. Association for Com- 
putational Linguistics.
Long Jiang, Ming Zhou, Lee feng Chien, and Cheng Niu.
2007. Named Entity Translation with Web Mining and 
Transliteration.  In Proceedings of the 20th Interna- 
tional Joint Conference on Artificial Intelligence (IJ- 
CAI’07), pages 1629–1634. Morgan Kaufmann Pub- 
lishers Inc.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and 
Qingsheng Zhu.  2009.  Mining Bilingual Data from 
the Web with Adaptively Learnt Patterns. In Proceed- 
ings of the 47th Annual Meeting of the Association for 
Computational Linguistics (ACL’09), pages 870–878. 
Association for Computational Linguistics.
Kevin  Knight  and  Jonathan  Graehl.      1998.      Ma- 
chine  Transliteration.     Computational  Linguistics,
24(4):599–612.
Julian Kupiec.  1993.  An Algorithm for finding Noun 
Phrase Correspondences in Bilingual Corpora. In Pro- 
ceedings of the 31th Annual Meeting of the Association 
for Computational Linguistics (ACL’93), pages 17–22. 
Association for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian.   2004.   A Joint 
Source-Channel Model for Machine Transliteration. 
In Proceedings of the 42nd Annual Meeting on Associ- 
ation for Computational Linguistics (ACL’04), pages
  159–166. Association for Computational Linguistics. 
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca.  2008.  Mining Parenthetical Transla-


tions from the Web by Word Alignment. In Proceed- 
ings of the 46th Annual Meeting of the Association 
for Computational Linguistics (ACL’08), pages 994–
1002. Association for Computational Linguistics.
Li Shao and Hwee Tou Ng.  2004.  Mining New Word 
Translations from Comparable Corpora.  In Proceed- 
ings of the 20th International Conference on Computa- 
tional Linguistics (COLING’04), pages 618–624. As- 
sociation for Computational Linguistics.
Ellen M. Voorhees.  2001.  The trec question answering 
track. Natural Language Engineering, 7(4):361–378.
Stephen Wan and Cornelia Maria Verspoor. 1998. Auto- 
matic English-Chinese Name Transliteration for De- 
velopment of Multilingual Resources.   In Proceed- 
ings of the 17th International Conference on Compu- 
tational Linguistics (COLING’98), pages 1352–1356. 
Association for Computational Linguistics.
Douglas Brent West.  2000.  Introduction to Graph The- 
ory. Prentice Hall, second edition.

