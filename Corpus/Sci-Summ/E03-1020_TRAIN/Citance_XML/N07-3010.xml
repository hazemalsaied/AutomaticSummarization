<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In the past, NLP has always been based on the explicit or implicit use of linguistic knowledge.</S>
		<S sid ="2" ssid = "2">In classical computer linguistic applications explicit rule based approaches prevail, while machine learning algorithms use implicit knowledge for generating linguistic knowledge.</S>
		<S sid ="3" ssid = "3">The question behind this work is: how far can we go in NLP without assuming explicit or implicit linguistic knowledge?</S>
		<S sid ="4" ssid = "4">How much efforts in annotation and resource building are needed for what level of sophistication in text processing?</S>
		<S sid ="5" ssid = "5">This work tries to answer the question by experimenting with algorithms that do not presume any linguistic knowledge in the system.</S>
		<S sid ="6" ssid = "6">The claim is that the knowledge needed can largely be acquired by knowledge-free and unsupervised methods.</S>
		<S sid ="7" ssid = "7">Here, graph models are employed for representing language data.</S>
		<S sid ="8" ssid = "8">A new graph clustering method finds related lexical units, which form word sets on various levels of homogeneity.</S>
		<S sid ="9" ssid = "9">This is exemplified and evaluated on language separation and unsupervised part-of-speech tagging, further applications are discussed.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="10" ssid = "10">1.1 Unsupervised and Knowledge-Free A frequent remark on work dealing with unsupervised methods in NLP is the question: “Why not.</S>
			<S sid ="11" ssid = "11">take linguistic knowledge into account?” While for English, annotated corpora, classification examples, sets of rules and lexical semantic word nets of high coverage do exist, this does not reflect the situation for most of even the major world languages.</S>
			<S sid ="12" ssid = "12">Further, as e.g. Lin (1997) notes, handmade and generic resources often do not fit the application domain, whereas resources created from and for the target data will not suffer from these discrepancies.</S>
			<S sid ="13" ssid = "13">Shifting the workload from creating resources manually to developing generic methods, a one-size-fits-all solution needing only minimal adaptation to new domains and other languages comes into reach.</S>
			<S sid ="14" ssid = "14">1.2 Graph Models The interest in incorporating graph models into NLP arose quite recently, and there is still a high potential exploiting this combination (cf.</S>
			<S sid ="15" ssid = "15">Widdows, 2005).</S>
			<S sid ="16" ssid = "16">An important parallelism between human language and network models is the small world structure of lexical networks both built manually and automatically (Steyvers and Tenenbaum, 2005), providing explanation for power-law distributions like Zipf’s law and others, see Biemann (2007).</S>
			<S sid ="17" ssid = "17">For many problems in NLP, a graph representation is an intuitive, natural and direct way to represent the data..</S>
			<S sid ="18" ssid = "18">The pure vector space model (cf.</S>
			<S sid ="19" ssid = "19">Schütze, 1993) is not suited to highly skewed distributions omnipresent in natural language.</S>
			<S sid ="20" ssid = "20">Computationally expensive, sometimes lossy transformations have to be applied for effectiveness and efficiency in processing.</S>
			<S sid ="21" ssid = "21">Graph models are a veritable alternative, as the equivalent of zero-entries in the vector representation are neither represented nor have to 37 be processed, rendering dimensionality reduction techniques unnecessary while still retaining the exact information.</S>
			<S sid ="22" ssid = "22">1.3 Roadmap For the entirety of this research, nothing more is required as input data than plain, tokenized text, separated into sentences.</S>
			<S sid ="23" ssid = "23">This is surely quite a bit of knowledge that is provided to the system, but unsupervised word boundary and sentence boundary detection is left for future work.</S>
			<S sid ="24" ssid = "24">Three steps are undertaken to identify similar words on different levels of homogeneity: same language, same part-of-speech, or same distributional properties.</S>
			<S sid ="25" ssid = "25">Figure 1 shows a coarse overview of the processing steps discussed in this work..</S>
			<S sid ="26" ssid = "26">Figure 1: Coarse overview: From multilingual input to typed relations and instances</S>
	</SECTION>
	<SECTION title="Methods in Unsupervised Processing. " number = "2">
			<S sid ="27" ssid = "1">Having at hand neither explicit nor implicit knowledge, but in turn the goal of identifying structure of equivalent function, the only possibility that is left in unsupervised and knowledge-free processing is statistics and clustering.</S>
			<S sid ="28" ssid = "2">2.1 Co-occurrence Statistics As a building block, co-occurrence statistics are used in several components of the system described here.</S>
			<S sid ="29" ssid = "3">A significance measure for co-occurrence is a means to distinguish between observations that are there by chance and effects that take place due to an underlying structure.</S>
			<S sid ="30" ssid = "4">Throughout, the likelihood ratio (Dunning, 1993) is used as significance measure because of its stable performance in various evaluations, yet many more measures are possible.</S>
			<S sid ="31" ssid = "5">Dependent on the context range in co-occurrence calculation, they will.</S>
			<S sid ="32" ssid = "6">be called sentence-based or neighbor-based co-occurrences in the remainder of this paper.</S>
			<S sid ="33" ssid = "7">The entirety of all co-occurrences of a corpus is called its co-occurrence graph.</S>
			<S sid ="34" ssid = "8">Edges are weighted by co-occurrence significance; often a threshold on edge weight is applied.</S>
			<S sid ="35" ssid = "9">2.2 Graph Clustering For clustering graphs, a plethora of algorithms exist that are motivated from a graph-theoretic viewpoint, but often optimize NP-complete measures (cf.</S>
			<S sid ="36" ssid = "10">Šíma and Schaeffer, 2005), making them non-applicable to lexical data that is naturally represented in graphs with millions of vertices.</S>
			<S sid ="37" ssid = "11">In Biemann and Teresniak (2005) and more detailed in Biemann (2006a), the Chinese Whispers (CW) Graph Clustering algorithm is described, which is a randomized algorithm with edge-linear run-time.</S>
			<S sid ="38" ssid = "12">The core idea is that vertices retain class labels which are inherited along the edges: In an update step, a vertex gets assigned the predominant label in its neighborhood.</S>
			<S sid ="39" ssid = "13">For initialization, all vertices get different labels, and after a handful of update steps per vertex, almost no changes in the labeling are observed – especially small world graphs converge fast.</S>
			<S sid ="40" ssid = "14">CW can be viewed as a more efficient modification and simplification of Markov Chain Clustering (van Dongen, 2000), which requires full matrix multiplications..</S>
			<S sid ="41" ssid = "15">CW is parameter-free, non-deterministic and finds the number of clusters automatically – a feature that is welcome in NLP, where the number of desired clusters (e.g. in word sense induction) is often unknown.</S>
	</SECTION>
	<SECTION title="Results. " number = "3">
			<S sid ="42" ssid = "1">3.1 Language Separation Clustering the sentence-based co-occurrence graph of a multilingual corpus with CW, a language separator with almost perfect performance is implemented in the following way: The clusters represent languages; a sentence gets assigned the label of the cluster with the highest lexical overlap between sentence and cluster.</S>
			<S sid ="43" ssid = "2">The method is evaluated in (Biemann and Teresniak, 2005) by sorting monolingual material that has been artificially mixed together.</S>
			<S sid ="44" ssid = "3">Dependent on similarities of languages, the method works almost error-free from about 1001,000 sentences per language on.</S>
			<S sid ="45" ssid = "4">For.</S>
			<S sid ="46" ssid = "5">38 languages with different encoding, it is possible to un-mix corpora of size factors up to 10,000 for the monolingual parts.</S>
			<S sid ="47" ssid = "6">In a nutshell, comparable scores to supervised language identifiers are reached without training.</S>
			<S sid ="48" ssid = "7">Notice that the number of languages in a multilingual chunk of text is unknown.</S>
			<S sid ="49" ssid = "8">This prohibits any clustering method that needs the number of clusters to be specified beforehand.</S>
			<S sid ="50" ssid = "9">3.2 Unsupervised POS Tagging Unlike in standard POS tagging, there is neither a set of predefined categories, nor annotation in a text.</S>
			<S sid ="51" ssid = "10">As POS tagging is not a system for its own sake, but serves as a preprocessing step for systems building upon it, the names and the number of categories are very often not important..</S>
			<S sid ="52" ssid = "11">The system presented in Biemann (2006b) uses CW clustering on graphs constructed by distributional similarity to induce a lexicon of supposedly non-ambiguous words w.r.t. POS by selecting only safe bets and excluding questionable cases from the lexicon.</S>
			<S sid ="53" ssid = "12">In this implementation, two clusterings are combined, one for high and medium frequency words, the other collecting medium and low frequency words.</S>
			<S sid ="54" ssid = "13">High and medium frequency words are clustered by similarity of their stop word context feature vectors: a graph is built, including only words that are involved in highly similar pairs.</S>
			<S sid ="55" ssid = "14">Clustering this graph of typically 5,000 vertices results in several hundred clusters, which are further used as POS categories.</S>
			<S sid ="56" ssid = "15">To extend the lexicon, words of medium and low frequency are clustered using a graph that encodes similarity of neighbor-based co-occurrences.</S>
			<S sid ="57" ssid = "16">Both clusterings are mapped by overlapping elements into a lexicon that provides POS information for some 50,000 words.</S>
			<S sid ="58" ssid = "17">For obtaining a clustering on datasets of this size, an effective algorithm like CW is crucial.</S>
			<S sid ="59" ssid = "18">Using this lexicon, a trigram tagger with a morphological extension is trained, which assigns a tag to every token in the corpus.</S>
			<S sid ="60" ssid = "19">The tagsets obtained with this method are usually more fine-grained than standard tagsets and reflect syntactic as well as semantic similarity.</S>
			<S sid ="61" ssid = "20">Figure 2 demonstrates the domain-dependence on the tagset for MEDLINE: distinguishing e.g. illnesses and error probabilities already in the tagset might be a valuable feature for relation extraction tasks.</S>
			<S sid ="62" ssid = "21">Size Sample words 1613 colds, apnea, aspergilloma, ACS, breathlessness, lesions, perforations, ...</S>
			<S sid ="63" ssid = "22">1383 proven, supplied, engineered, distin guished, constrained, omitted, … 589 dually, circumferentially, chronically, rarely, spectrally, satisfactorily, ...</S>
			<S sid ="64" ssid = "23">124 1min, two-week, 4min, 2-day, … 6 P&lt;0.001, P&lt;0.01, p&lt;0.001, p&lt;0.01, ...</S>
			<S sid ="65" ssid = "24">Figure 2: Some examples for MEDLINE tagset: Number of lex.</S>
			<S sid ="66" ssid = "25">entries per tag and sample words.</S>
			<S sid ="67" ssid = "26">In Biemann (2006b), the tagger output was directly compared to supervised taggers for English, German and Finnish via information-theoretic measures.</S>
			<S sid ="68" ssid = "27">While it is possible to compare the contribution of different components of a system relatively along this scale, it only gives a poor impression on the utility of the unsupervised tag-ger’s output.</S>
			<S sid ="69" ssid = "28">Therefore, the tagger was evaluated indirectly in machine learning tasks, where POS tags are used as features.</S>
			<S sid ="70" ssid = "29">Biemann et al.</S>
			<S sid ="71" ssid = "30">(2007) report that for standard Named Entity Recognition, Word Sense Disambiguation and Chunking tasks, using unsupervised POS tags as features helps about as much as supervised tagging: Overall, almost no significant differences between results could be observed, supporting the initial claim.</S>
			<S sid ="72" ssid = "31">3.3 Word Sense Induction (WSI) Co-occurrences are a widely used data source for WSI.</S>
			<S sid ="73" ssid = "32">The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these).</S>
			<S sid ="74" ssid = "33">Clustering this graph with CW and regarding clusters as senses, this method yields comparable results to Bordag (2006), tested using the unsupervised evaluation framework presented there.</S>
			<S sid ="75" ssid = "34">More detailed results are reported in Biemann (2006a)..</S>
	</SECTION>
	<SECTION title="Further Work. " number = "4">
			<S sid ="76" ssid = "1">4.1 Word Sense Disambiguation (WSD) The encouraging results in WSI enable support in automatic WSD systems.</S>
			<S sid ="77" ssid = "2">As described by Agirre et al.</S>
			<S sid ="78" ssid = "3">(2006), better performance can be expected if the WSI component distinguishes between a large number of so-called micro-senses.</S>
			<S sid ="79" ssid = "4">This illustrates a. 39 principle of unsupervised NLP: It is not important to reproduce word senses found by introspection; rather, it is important that different usages of a word can be reliably distinguished, even if the corresponding WordNet sense is split into several sub-senses.</S>
			<S sid ="80" ssid = "5">4.2 Distributional Thesaurus with Relations It is well understood that distributional similarity reflects semantic similarity and can be used to automatically construct a distributional thesaurus for frequent words (Lin, 1997; inter al).</S>
			<S sid ="81" ssid = "6">Until now, most works aiming at semantic similarity rely on a parser that extracts dependency relations.</S>
			<S sid ="82" ssid = "7">The claim here again is that similarity on parser output might be replaced by similarity on a pattern basis, (cf.</S>
			<S sid ="83" ssid = "8">Davidov and Rappoport 2006).</S>
			<S sid ="84" ssid = "9">For class-based generalization in these patterns, the system described in section 3.2 might prove useful.</S>
			<S sid ="85" ssid = "10">Preliminary experiments revealed that similarity on significantly co-occurring patterns is able to produce very promising similarity rankings.</S>
			<S sid ="86" ssid = "11">A clustering of these with CW leads to thesaurus entries comparable to thesauri like Roget’s..</S>
			<S sid ="87" ssid = "12">Clustering not only words based on similarity of patterns, but also patterns based on similarity of words enables us to identify clusters of patterns with different relations they manifest.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5"></SECTION>
</PAPER>
