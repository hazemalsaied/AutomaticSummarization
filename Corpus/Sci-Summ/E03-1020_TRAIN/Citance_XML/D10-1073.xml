<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Graph-based methods have gained attention inmany areas of Natural Language Processing(NLP) including Word Sense Disambiguation(WSD), text summarization, keyword extraction and others.</S>
		<S sid ="2" ssid = "2">Most of the work in these areas formulate their problem in a graph-basedsetting and apply unsupervised graph clustering to obtain a set of clusters.</S>
		<S sid ="3" ssid = "3">Recent studiessuggest that graphs often exhibit a hierarchical structure that goes beyond simple flat clustering.</S>
		<S sid ="4" ssid = "4">This paper presents an unsupervisedmethod for inferring the hierarchical grouping of the senses of a polysemous word.</S>
		<S sid ="5" ssid = "5">Theinferred hierarchical structures are applied tothe problem of word sense disambiguation,where we show that our method performs significantly better than traditional graph-basedmethods and agglomerative clustering yielding improvements over state-of-the-art WSDsystems based on sense induction.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">A number of NLP problems can be cast into a graph-based framework, in which entities are representedas vertices in a graph and relations between them aredepicted by weighted or unweighted edges.</S>
			<S sid ="7" ssid = "7">For instance, in unsupervised WSD a number of methods(Widdows and Dorow, 2002; Ve´ronis, 2004; Agirreet al., 2006) have constructed word cooccurrencegraphs for a target polysemous word and appliedgraph-clustering to obtain the clusters (senses) ofthat word.Similarly in text summarization, Mihalcea (2004)developed a method, in which sentences are rep resented as vertices in a graph and edges betweenthem are drawn according to their common tokensor words of a given POS category, e.g. nouns.Graph-based ranking algorithms, such as PageRank(Brin and Page, 1998), were then applied in orderto determine the significance of sentences.</S>
			<S sid ="8" ssid = "8">In thesame vein, graph-based methods have been appliedto other problems such as determining semantic similarity of text (Ramage et al., 2009).</S>
			<S sid ="9" ssid = "9">Recent studies (Clauset et al., 2006; Clauset etal., 2008) suggest that graphs exhibit a hierarchical structure (e.g. a binary tree), in which verticesare divided into groups that are further subdividedinto groups of groups, and so on, until we reach theleaves.</S>
			<S sid ="10" ssid = "10">This hierarchical structure provides additional information as opposed to flat clustering byexplicitly including organisation at all scales of agraph (Clauset et al., 2008).</S>
			<S sid ="11" ssid = "11">In this paper, we presentan unsupervised method for inferring the hierarchical structure (binary tree) of a graph, in which vertices are the contexts of a polysemous word andedges represent the similarity between contexts.</S>
			<S sid ="12" ssid = "12">Themethod that we use to infer that hierarchical structure is the Hierarchical Random Graphs (HRGs) algorithm due to Clauset et al.</S>
			<S sid ="13" ssid = "13">(2008).</S>
			<S sid ="14" ssid = "14">The binary tree produced by our method groupsthe contexts of a polysemous word at differentheights of the tree.</S>
			<S sid ="15" ssid = "15">Thus, it induces the senses ofthat word at different levels of sense granularity.</S>
			<S sid ="16" ssid = "16">Toevaluate our method, we apply it to the problem ofnoun sense disambiguation showing that inferringthe hierarchical structure using HRGs provides additional information from the observed graph leading to improved WSD performance compared to: (1) 745 Figure 1: Stages of the proposed method.</S>
			<S sid ="17" ssid = "17">simple flat clustering, and (2) traditional agglomerative clustering.</S>
			<S sid ="18" ssid = "18">Finally, we compare our results withstate-of-the-art sense induction systems and showthat our method yields improvements.</S>
			<S sid ="19" ssid = "19">Figure 1shows the different stages of the proposed methodthat we describe in the following sections.</S>
	</SECTION>
	<SECTION title="Related work. " number = "2">
			<S sid ="20" ssid = "1">Typically, graph-based methods, when applied tounsupervised sense disambiguation represent eachword wi co-occurring with the target word tw as avertex.</S>
			<S sid ="21" ssid = "2">Two vertices are connected via an edge ifthey co-occur in one or more contexts of tw.</S>
			<S sid ="22" ssid = "3">Oncethe co-occurrence graph of tw has been constructed,different graph clustering algorithms are applied toinduce the senses.</S>
			<S sid ="23" ssid = "4">Each cluster (induced sense) consists of a set of words that are semantically related tothe particular sense.</S>
			<S sid ="24" ssid = "5">Figure 2 shows an example ofa graph for the target word paper that appears withtwo different senses scholarly article and newspaper.</S>
			<S sid ="25" ssid = "6">Ve´ronis (2004) has shown that cooccurrencegraphs are small-world networks that contain highlydense subgraphs representing the different clusters(senses) of the target word (Ve´ronis, 2004).</S>
			<S sid ="26" ssid = "7">To identify these dense regions Ve´ronis’s algorithm iteratively finds their hubs, where a hub is a vertex with avery high degree.</S>
			<S sid ="27" ssid = "8">The degree of a vertex is defined tobe the number of edges incident to that vertex.</S>
			<S sid ="28" ssid = "9">Theidentified hub is then deleted along with its directneighbours from the graph producing a new cluster.</S>
			<S sid ="29" ssid = "10">For example, in Figure 2 the highest degree vertex, news, is the first hub, which would be deletedalong with its direct neighbours.</S>
			<S sid ="30" ssid = "11">The deleted region corresponds to the newspaper sense of the target word paper.</S>
			<S sid ="31" ssid = "12">Ve´ronis (2004) further processedthe identified clusters (senses), in order to assign therest of graph vertices to the identified clusters by utilising the minimum spanning tree of the originalgraph.</S>
			<S sid ="32" ssid = "13">In Agirre et al.</S>
			<S sid ="33" ssid = "14">(2006), the algorithm of Ve´ronis(2004) is analysed and assessed on the SensEval3dataset (Snyder and Palmer, 2004), after optimising its parameters on the SensEval2 dataset (Edmonds and Dorow, 2001).</S>
			<S sid ="34" ssid = "15">The results show that theWSD F-Score outperforms the Most Frequent Sense(MFS) baseline by approximately 10%, while inducing a large number of clusters (with averages of 60to 70).</S>
			<S sid ="35" ssid = "16">Another graph-based method is presented in(Dorow and Widdows, 2003).</S>
			<S sid ="36" ssid = "17">They extract onlynoun neighbours that appear in conjunctions or dis-junctions with the target word.</S>
			<S sid ="37" ssid = "18">Additionally, theyextract second-order co-occurrences.</S>
			<S sid ="38" ssid = "19">Nouns are represented as vertices, while edges between verticesare drawn, if their associated nouns co-occur in conjunctions or disjunctions more than a given number of times.</S>
			<S sid ="39" ssid = "20">This co-occurrence frequency is alsoused to weight the edges.</S>
			<S sid ="40" ssid = "21">The resulting graph isthen pruned by removing the target word and vertices with a low degree.</S>
			<S sid ="41" ssid = "22">Finally, the MCL algorithm(Dongen, 2000) is used to cluster the graph and produce a set of clusters (senses) each one consisting ofa set of contextually related words.</S>
			<S sid ="42" ssid = "23">Chinese Whispers (CW) (Biemann, 2006) is aparameterfree1 graph clustering method that hasbeen applied in sense induction to cluster the co-occurrence graph of a target word (Biemann, 2006),as well as a graph of collocations related to the target word (Klapaftis and Manandhar, 2008).</S>
			<S sid ="43" ssid = "24">Theevaluation of the collocational-graph method in theSemEval2007 sense induction task (Agirre andSoroa, 2007) showed promising results.</S>
			<S sid ="44" ssid = "25">All the described methods for sense induction ap1One needs to specify only the number of iterations.</S>
			<S sid ="45" ssid = "26">The number of clusters is generated automatically.</S>
			<S sid ="46" ssid = "27">746 Figure 2: Graph of words for the target word paper.Numbers inside vertices correspond to their degree.</S>
			<S sid ="47" ssid = "28">Figure 3: Running example of graph creation ply flat graph clustering methods to derive the clusters (senses) of a target word.</S>
			<S sid ="48" ssid = "29">As a result, they neglect the fact that their constructed graphs often exhibit a hierarchical structure that is useful in severaltasks including word sense disambiguation.</S>
	</SECTION>
	<SECTION title="Building a graph of contexts. " number = "3">
			<S sid ="49" ssid = "1">This section describes the process of creating agraph of contexts for a polysemous target word.</S>
			<S sid ="50" ssid = "2">Figure 3 provides a running example of the differentstages of our method.</S>
			<S sid ="51" ssid = "3">In the example, the targetword paper appears with the scholarly article sensein the contexts A, B, and with the newspaper sensein the contexts C and D. 3.1 Corpus preprocessing.</S>
			<S sid ="52" ssid = "4">Let bc denote the base corpus consisting of the contexts containing the target word tw.</S>
			<S sid ="53" ssid = "5">In our work, a context is defined as a paragraph2 containing thetarget word.</S>
			<S sid ="54" ssid = "6">The aim of this stage is to capture nouns contextually related to tw.</S>
			<S sid ="55" ssid = "7">Initially, the target word is removed from bc and part-of-speech tagging is appliedto each context.</S>
			<S sid ="56" ssid = "8">Following the work in (Ve´ronis,2004; Agirre et al., 2006) only nouns are kept andlemmatised.</S>
			<S sid ="57" ssid = "9">In the next step, the distribution of eachnoun in the base corpus is compared to the distribution of the same noun in a reference corpus3 using the log-likelihood ratio (G2) (Dunning, 1993).Nouns with a G2 below a pre-specified threshold(parameter p1) are removed from each paragraph ofthe base corpus.</S>
			<S sid ="58" ssid = "10">The upper left part of Figure 3shows the words kept as a result of this stage.</S>
			<S sid ="59" ssid = "11">3.2 Graph creationGraph vertices: To create the graph of vertices, werepresent each context ci as a vertex in a graph G.Graph edges: Edges between the vertices of thegraph are drawn based on their similarity, definedin Equation 1, where simcl(ci, cj) is the collocational weight of contexts ci, cj and simwd(ci, cj)is their bag-of-words weight.</S>
			<S sid ="60" ssid = "12">If the edge weightW (ci, cj) is above a prespecified threshold (parameter p3), then an edge is drawn between the corresponding vertices in the graph..</S>
			<S sid ="61" ssid = "13">W (ci, cj) =12(simcl(ci, cj) + simwd(ci, cj)) (1) Collocational weight: The limited polysemy of collocations can be exploited to compute the similaritybetween contexts ci and cj . In our setting, a collocation is a juxtaposition of two nouns within the samecontext.</S>
			<S sid ="62" ssid = "14">Thus, given a context ci, each of its nounsis combined with any other noun yielding a total of(N2 )collocations for a context with N nouns.</S>
			<S sid ="63" ssid = "15">Each collocation, clij is weighted using the log-likelihoodratio (G2) (Dunning, 1993) and is filtered out if theG2 is below a prespecified threshold (parameter p2).At the end of this process, each context ci of tw isassociated with a vector of collocations (vi).</S>
			<S sid ="64" ssid = "16">Theupper right part of Figure 3 shows the collocationsassociated with each context of our example.</S>
			<S sid ="65" ssid = "17">2Our definition of context is equivalent to an instance of thetarget word in the SemEval2007 sense induction task dataset(Agirre and Soroa, 2007).</S>
			<S sid ="66" ssid = "18">3The British National Corpus, 2001, Distributed by OxfordUniversity Computing Services.</S>
			<S sid ="67" ssid = "19">747 Given two contexts ci and cj , we calculate theircollocational weight using the Jaccard coefficienton the collocational vectors, i.e. simcl(ci, cj) =|vinvj ||vi?vj | . The selection of Jaccard is based on the workof Weeds et al.</S>
			<S sid ="68" ssid = "20">(2004), who analyzed the variationin a word’s distributionally nearest neighbours withrespect to a variety of similarity measures.</S>
			<S sid ="69" ssid = "21">Theiranalysis showed that there are three classes of measures, i.e. those selecting distributionally more general neighbours (e.g. cosine), those selecting distributionally less general neighbours (e.g. AMCRM-Precision (Weeds et al., 2004)) and those without abias towards the distributional generality of a neigh-bour (e.g. Jaccard).</S>
			<S sid ="70" ssid = "22">In our setting, we are interestedin calculating the similarity between two contextswithout any bias.</S>
			<S sid ="71" ssid = "23">We selected Jaccard, since the restof that class’s measures are based on pointwise mutual information that assigns high weights to infrequent events.Bag-of-words weight: Estimating context similarity using collocations may provide reliable estimatesregarding the existence of an edge in the graph, however, it also suffers from data sparsity.</S>
			<S sid ="72" ssid = "24">For this reason, we also employ a bag-of-words model.</S>
			<S sid ="73" ssid = "25">Specifically, each context ci is associated with a vector githat contains the nouns kept as result of the corpuspreprocessing stage.</S>
			<S sid ="74" ssid = "26">The upper left part of Figure3 shows the words associated with each context ofour example.</S>
			<S sid ="75" ssid = "27">Given two contexts ci and cj , we calculate their bag-of-words weight using the Jaccardcoefficient on the word vectors, i.e. simwd(ci, cj) =|gingj ||gi?gj | . The collocational weight and bag-of-wordsweight are averaged to derive the edge weight between two contexts as defined in Equation 1.</S>
			<S sid ="76" ssid = "28">Theresulting graph of our running example is shown onthe bottom of Figure 3.</S>
			<S sid ="77" ssid = "29">This graph is the input to thehierarchical random graphs method (Clauset et al.,2008) described in the next section.</S>
	</SECTION>
	<SECTION title="Hierarchical Random Graphs for senseinduction. " number = "4">
			<S sid ="78" ssid = "1">In this section, we describe the process of inferringthe hierarchical structure of the graph of contextsusing hierarchical random graphs (Clauset et al.,2008).</S>
			<S sid ="79" ssid = "2">Figure 4: Two dendrograms for the graph in Figure 3.</S>
			<S sid ="80" ssid = "3">4.1 The Hierarchical Random Graph model.</S>
			<S sid ="81" ssid = "4">A dendrogram is a binary tree with n leaves andn1 parents.</S>
			<S sid ="82" ssid = "5">Figure 4 shows an example of twodendrograms with 4 leaves and 3 parents.</S>
			<S sid ="83" ssid = "6">Given aset of n contexts that we need to arrange hierarchically, let us denote by G = (V,E) the graph of contexts, where V = {v0, v1 . . .</S>
			<S sid ="84" ssid = "7">vn} is the set of vertices, E = {e0, e1 . . .</S>
			<S sid ="85" ssid = "8">em} is the set of edges andek = {vi, vj}.</S>
			<S sid ="86" ssid = "9">Given an undirected graph G, each of its n vertices is a leaf in a dendrogram, while the internalnodes of that dendrogram indicate the hierarchicalrelationships among the leaves.</S>
			<S sid ="87" ssid = "10">We denote this or-ganisation byD = {D1, D2, . . .</S>
			<S sid ="88" ssid = "11">Dn1}, where eachDk is an internal node.</S>
			<S sid ="89" ssid = "12">Every pair of nodes (vi, vj)is associated with a unique Dk, which is their lowest common ancestor in the tree.</S>
			<S sid ="90" ssid = "13">In this manner Dpartitions the edges that exist in G. The primary assumption in the hierarchical random graph model is that edges in G exist independently, but with a probability that is not identicallydistributed.</S>
			<S sid ="91" ssid = "14">In particular, the probability that an edge{vi, vj} exists in G is given by a parameter ?k associated with Dk, the lowest common ancestor of viand vj in D. In this manner, the topological structure D and the vector of probabilities ~?</S>
			<S sid ="92" ssid = "15">define theHRG given by H(D, ~?)</S>
			<S sid ="93" ssid = "16">(Clauset et al., 2008).</S>
			<S sid ="94" ssid = "17">748 4.2 HRG parameterisationAssuming a uniform prior over all HRGs, the targetis to identify the parameters of D and ~?, so that thechosen HRG is statistically similar to G. Let Dk bean internal node of dendrogram D and f(Dk) be thenumber of edges between the vertices of the subtreesof the subtree rooted at Dk that actually exist in G.For example, in Figure 4(A), f(D2) = 1, becausethere is one edge in G connecting vertices B and C.Let l(Dk) be the number of leaves in the left subtreeof Dk, and r(Dk) be the number of leaves in theright subtree.</S>
			<S sid ="95" ssid = "18">For example in Figure 4(A), l(D2) =2 and r(D2) = 2.</S>
			<S sid ="96" ssid = "19">The likelihood of the hierarchicalrandom graph (D, ~?)</S>
			<S sid ="97" ssid = "20">is defined in Equation 2, whereA(Dk) = l(Dk)r(Dk)- f(Dk)..</S>
			<S sid ="98" ssid = "21">L(D, ~?)</S>
			<S sid ="99" ssid = "22">=?Dk?D ?f(Dk)k (1- ?k)A(Dk) (2) The probabilities ?k that maximise the likelihoodof a dendrogram D can be easily estimated usingthe method of MLE i.e ?k = f(Dk)l(Dk)r(Dk) . Substituting this into Equation 2 yields Equation 3.</S>
			<S sid ="100" ssid = "23">Fornumerical reasons, it is more convenient to workwith the logarithm of the likelihood which is definedin Equation 4, where h(?k) = -?k log ?k - (1 -?k) log (1- ?k)..</S>
			<S sid ="101" ssid = "24">L(D) =?Dk?D [??kk (1- ?k)1-?k ]l(Dk)r(Dk) (3) logL(D) = -?Dk?D h(?k)l(Dk)r(Dk) (4) As can be observed, each term -l(Dk)r(Dk)h(?k)is maximised when ?k approaches 0 or 1.</S>
			<S sid ="102" ssid = "25">Thismeans that high-likelihood dendrograms partitionvertices into subtrees, such that the connectionsamong their vertices in the observed graph are eithervery rare or very common (Clauset et al., 2008).</S>
			<S sid ="103" ssid = "26">Forexample, consider the two dendrograms in Figures4(A) and 4(B).</S>
			<S sid ="104" ssid = "27">We observe that 4(A) is more likelythan 4(B), since it provides a better division of thenetwork leaves.</S>
			<S sid ="105" ssid = "28">Particularly, the likelihood of 4(A)is L(D1) = (11 · (11)1) · (11 · (11)1) · (0.251 ·(10.25)3) = 0.105, while the likelihood of 4(B)is L(D2) = (00 · (10)1) · (11 · (11)1) · (0.52 ·(10.5)2) = 0.062.</S>
			<S sid ="106" ssid = "29">4.2.1 MCMC samplingFinding the values of ?k using the MLE method is straightforward.</S>
			<S sid ="107" ssid = "30">However, this is not the casefor maximising the likelihood function over thespace of all possible dendrograms.</S>
			<S sid ="108" ssid = "31">Given a graphwith n vertices, i.e. n leaves in each dendrogram,the total number of different dendrograms is super-exponential ((2n3)!!</S>
			<S sid ="109" ssid = "32">˜ v2(2n)n-1e-n) (Clausetet al., 2006).</S>
			<S sid ="110" ssid = "33">To deal with this problem, we use a MarkovChain Monte Carlo (MCMC) method that samplesdendrograms from the space of dendrogram models with probability proportional to their likelihood.Each time MCMC samples a dendrogram with anew highest likelihood, that dendrogram is stored.Hence, our goal is to choose the highest likelihooddendrogram once MCMC has converged.</S>
			<S sid ="111" ssid = "34">Following the work in (Clauset et al., 2008),we pick a set of transitions between dendrograms,where a transition is a rearrangement of the sub-trees of a dendrogram.</S>
			<S sid ="112" ssid = "35">In particular, given a currentdendrogram Dcurr, each internal node Dk of Dcurris associated with three subtrees of Dcurr.</S>
			<S sid ="113" ssid = "36">For instance, in Figure 5A, the subtrees st1 and st2 arederived from the two children of Dk and the thirdst3 from its sibling.</S>
			<S sid ="114" ssid = "37">Given a current dendrogram,Dcurr, the algorithm proceeds as follows: 1.</S>
			<S sid ="115" ssid = "38">Choose an internal node, Dk ? Dcurr uniformly..</S>
			<S sid ="116" ssid = "39">2.</S>
			<S sid ="117" ssid = "40">Generate two possible new configurations ofthe subtrees of Dk (See Figure 5)..</S>
			<S sid ="118" ssid = "41">3.</S>
			<S sid ="119" ssid = "42">Choose one of the configurations uniformly togenerate a new dendrogram, Dnext..</S>
			<S sid ="120" ssid = "43">4.</S>
			<S sid ="121" ssid = "44">Accept or reject Dnext according toMetropolisHastings (MH) rule..</S>
	</SECTION>
	<SECTION title="If transition is accepted, then Dcurr = Dnext.. " number = "5">
			<S sid ="122" ssid = "1">According to MH rule (Newman and Barkema,1999), a transition is accepted if logL(Dnext) =logL(Dcurr); otherwise the transition is acceptedwith probability L(Dnext)L(Dcurr) . These transitions definean ergodic Markov chain, hence its stationary distribution can be reached (Clauset et al., 2008).</S>
			<S sid ="123" ssid = "2">749 Figure 5: (A) current configuration for internal node Dk and its associated subtrees (B) first alternative configuration,(C) second alternative configuration.</S>
			<S sid ="124" ssid = "3">Note that swapping st1, st2 in (A) results in an equivalent tree.</S>
			<S sid ="125" ssid = "4">Hence, thisconfiguration is excluded.</S>
			<S sid ="126" ssid = "5">In our experiments, we noticed that the algorithmconverged relatively quickly.</S>
			<S sid ="127" ssid = "6">The same behaviour(roughly O(n2) steps) was also noticed in Clauset etal.</S>
			<S sid ="128" ssid = "7">(2008), when considering graphs with thousandsof vertices.</S>
			<S sid ="129" ssid = "8">5 HRGs for sense disambiguation.</S>
			<S sid ="130" ssid = "9">5.1 Sense mappingThe output of HRG learning is a dendrogramD withn leaves (contexts) and n-1 internal nodes.</S>
			<S sid ="131" ssid = "10">To perform sense disambiguation, we mapped the internalnodes to gold standard senses using a sense-taggedcorpus.</S>
			<S sid ="132" ssid = "11">Such a sense-tagged corpus is needed wheninduced word senses need to be mapped to a goldstandard sense inventory..</S>
			<S sid ="133" ssid = "12">Instead of using a hard mapping from the den-drogram internal nodes to the Gold Standard (GS)senses, we use a soft probabilistic mapping and calculateP (sk|Di), i.e the probability of sense sk givennode Di.</S>
			<S sid ="134" ssid = "13">Let F (Di) be the set of training contextsgrouped by internal node Di.</S>
			<S sid ="135" ssid = "14">Let F &apos;(sk) be the setof training contexts that are tagged with sense sk.Then the conditional probability, P (sk|Di), is defined in Equation 5.</S>
			<S sid ="136" ssid = "15">P (sk|Di) = |F (Di) n F&apos;(sk)| |F (Di)| (5) Table 1 provides a sense-tagged corpus for therunning example of Figure 3.</S>
			<S sid ="137" ssid = "16">Using this corpusand the tree in Figure 4(A), P (s1|D2) = 23 andP (s2|D2) = 13 . In Figure 4(A) the rest of the calculated conditional probabilities are given.</S>
			<S sid ="138" ssid = "17">5.2 Sense taggingFor evaluation we compared the proposed methodagainst the current state-of-the-art sense induction.</S>
			<S sid ="139" ssid = "18">GS sense Context ID Context wordss1 A journal, scholar, observation science, papers1 B scholar, scholar, author, publication, papers2 D times, guardian, journalist, paper Table 1: Sense-tagged corpus for the example in Figure 3 systems in the WSD task.</S>
			<S sid ="140" ssid = "19">We followed the settingof SemEval2007 sense induction task (Agirre andSoroa, 2007).</S>
			<S sid ="141" ssid = "20">In this setting, the base corpus (bc)(Section 3.1) for a target word consists both of thetraining and testing corpus.</S>
			<S sid ="142" ssid = "21">As a result, a testingcontext cj of tw is a leaf in the generated dendro-gram.</S>
			<S sid ="143" ssid = "22">The process of disambiguating cj is straightforward exploiting the structural information provided by HRGs.</S>
			<S sid ="144" ssid = "23">w(sk, cj) =?</S>
			<S sid ="145" ssid = "24">Di?H(cj)P (sk|Di) · ?i (6) w(s*, cj) = argmax sk(w(sk, cj)) (7) Let H(cj) denote the set of parents for context cj .Then, the weight assigned to sense sk is the sum ofweighted scores provided by each identified parent.This is shown in Equation 6, where ?i is the probability associated with each internal nodeDi from thehierarchical random graph (see Figure 4(A)).</S>
			<S sid ="146" ssid = "25">Thisprobability reflects the discriminating ability of internal nodes.</S>
			<S sid ="147" ssid = "26">Finally, the highest weight determines the winning sense for context cj (Equation 7).</S>
			<S sid ="148" ssid = "27">In our example (Figure 4(A)), w(s1, C) = (0 ·1+ 23 ·0.25) =0.16 andw(s2, C) = (1·1+ 13 ·0.25) = 1.08.</S>
			<S sid ="149" ssid = "28">Hence,s2 is the winning sense.</S>
			<S sid ="150" ssid = "29">750 Parameter RangeG2 word threshold (p1) 15,25,35,45G2 collocation threshold (p2) 10,15,20Edge similarity threshold (p3) 0.05,0.09,0.13 Table 2: Parameter values used in the evaluation.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "6">
			<S sid ="151" ssid = "1">6.1 Evaluation setting &amp; baselinesWe evaluate our method on the nouns of theSemEval2007 word sense induction task (Agirreand Soroa, 2007) under the second evaluation settingof that task, i.e. supervised evaluation.</S>
			<S sid ="152" ssid = "2">Specifically,we use the standard WSD measures of precision andrecall in order to produce their harmonic mean (F-Score).</S>
			<S sid ="153" ssid = "3">The official scoring software of that task hasbeen used in our evaluation.</S>
			<S sid ="154" ssid = "4">Note that the unsupervised measures of that task are not directly applicable to our induced hierarchies, since they focus onassessing flat clustering methods..</S>
			<S sid ="155" ssid = "5">The first aim of our evaluation is to test whetherinferring the hierarchical structure of the constructedgraphs improves WSD performance.</S>
			<S sid ="156" ssid = "6">For that reasonour first baseline, Chinese Whispers Unweightedversion (CWU), takes as input the same unweightedgraph of contexts as HRGs in order to produce aflat clustering.</S>
			<S sid ="157" ssid = "7">The set of produced clusters is thenmapped to GS senses using the training dataset andperformance is then measured on the testing dataset.We followed the same sense mapping method as inthe SemEval2007 sense induction task (Agirre andSoroa, 2007).</S>
			<S sid ="158" ssid = "8">Our second baseline, Chinese Whispers Weightedversion (CWW), is similar to the previous one, withthe difference that the edges of the input graphare weighted using Equation 1.</S>
			<S sid ="159" ssid = "9">For clustering thegraphs of CWU and CWW we employ, ChineseWhispers4 (Biemann, 2006).</S>
			<S sid ="160" ssid = "10">The second aim of our evaluation is to assesswhether the hierarchical structure inferred by HRGsis more informative than the hierarchical structure inferred by traditional Hierarchical Clustering(HAC).</S>
			<S sid ="161" ssid = "11">Hence, our third baseline, takes as input asimilarity matrix of the graph vertices and performsbottom-up clustering with average-linkage, whichhas already been used in WSI in (Pantel and Lin, 4The number of iterations for CW was set to 200.</S>
			<S sid ="162" ssid = "12">2003) and was shown to have superior or similar performance to single-linkage and complete-linkage inthe related problem of learning a taxonomy of senses(Klapaftis and Manandhar, 2010).</S>
			<S sid ="163" ssid = "13">To calculate the similarity matrix of vertices wefollow a process similar to the one used in Section 4.2 for calculating the probability of an internal node.</S>
			<S sid ="164" ssid = "14">The similarity between two vertices iscalculated according to the degree of connected-ness among their direct neighbours.</S>
			<S sid ="165" ssid = "15">Specifically,we would like to assign high similarity to pairs ofvertices, whose neighbours are close to forming aclique.</S>
			<S sid ="166" ssid = "16">Given two vertices (contexts) ci and cj , letN(ci, cj) be the set of their neighbours andK(ci, cj)be the set of edges between the vertices inN(ci, cj).The maximum number of edges that could exist between vertices in N(ci, cj) is (|N(ci,cj)|2 ).</S>
			<S sid ="167" ssid = "17">Thus, the similarity of ci, cj is set equal to the number ofedges that actually exist in that neighbourhood divided by the total number of edges that could exist( |K(ci,cj)|(|N(ci,cj)| 2)).</S>
			<S sid ="168" ssid = "18">The disambiguation process using the HAC treeis identical to the one presented in Section 5.2 withthe only difference that the internal probability, ?i,in Equation 6 does not exist for HAC.</S>
			<S sid ="169" ssid = "19">Hence, we replaced it with the factor 1|H(Di)| , whereH(Di) is theset of children of internal node Di.</S>
			<S sid ="170" ssid = "20">This factor provides lower weights for nodes high in the tree, sincetheir discriminating ability will possibly be lower.</S>
			<S sid ="171" ssid = "21">6.2 Results &amp; discussion.</S>
			<S sid ="172" ssid = "22">Table 2 shows the parameter values used in the evaluation.</S>
			<S sid ="173" ssid = "23">Figure 6(A) shows the performance of theproposed method against the baselines for p3 = 0.05and different p1 and p2 values.</S>
			<S sid ="174" ssid = "24">Figure 6(B) illustrates the results of the same experiment usingp3 = 0.09.</S>
			<S sid ="175" ssid = "25">In both figures, we observe that HRGsoutperform the CWU baseline under all parametercombinations.</S>
			<S sid ="176" ssid = "26">In particular, all of the 12 performance differences for p3 = 0.09 are statisticallysignificant using McNemar’s test at 95% confidencelevel, while for p3 = 0.05 only 2 out of the 12 performance differences were not judged as significantfrom the test.</S>
			<S sid ="177" ssid = "27">The picture is the same for p3 = 0.13, whereCWU performs significantly worse than for p3 = 751 Figure 6: Performance analysis of HRGs, CWU, CWW &amp; HAC for different parameter combinations (Table 2).</S>
			<S sid ="178" ssid = "28">(A)All combinations of p1, p2 and p3 = 0.05.</S>
			<S sid ="179" ssid = "29">(B) All combinations of p1, p2 and p3 = 0.09.</S>
			<S sid ="180" ssid = "30">0.05 and p3 = 0.09.</S>
			<S sid ="181" ssid = "31">Specifically, the largest performance difference between HRGs and CWU is 9.4%at p1 = 25, p2 = 10 and p3 = 0.13.</S>
			<S sid ="182" ssid = "32">Setting the vertex similarity threshold (p3) equal to 0.13 leads tomore sparse and disconnected graphs, which causesChinese Whispers to produce a large number of clusters.</S>
			<S sid ="183" ssid = "33">This leads to sparsity problems and unreliablemapping of clusters to GS senses due to the lack ofadequate training data.</S>
			<S sid ="184" ssid = "34">In contrast, HRGs suffer lessat this high threshold, although their performancewhen p3&lt;0.13 is better.</S>
			<S sid ="185" ssid = "35">This picture does not change for the weighted version of Chinese Whispers (CWW) which performsworse than CWU.</S>
			<S sid ="186" ssid = "36">This is because CWW producesa smaller number of clusters than CWU that con-flate the target word senses.</S>
			<S sid ="187" ssid = "37">It seems that usingweighted edges creates a bias towards the MFS, ineffect missing rare senses of a target word.</S>
			<S sid ="188" ssid = "38">Thismeans that a number of words in the bag-of-wordscontext vectors and collocations in the collocationalcontext vectors (Section 3.2) are associated to morethan one sense of the target word and most stronglyassociated to the MFS.</S>
			<S sid ="189" ssid = "39">As a result, increasing the p1threshold to 25 and 35 leads to a higher performancefor CWW, since many of these words and collocations are filtered out.Overall, the comparison of HRGs against theCWU and CWW baselines has shown that inferringthe hierarchical structure of observed graphs leadsto improved WSD performance as opposed to usingflat clustering.</S>
			<S sid ="190" ssid = "40">This is because HRGs are able to in fer both the hierarchical structure of the graph andinclude the probabilities, ?k, associated with eachinternal node.</S>
			<S sid ="191" ssid = "41">These probabilities reflect the discriminating ability of each node, offering information missed by flat clustering.</S>
			<S sid ="192" ssid = "42">In Figures 6(A) and 6(B) we observe that HRGsperform significantly better than HAC.</S>
			<S sid ="193" ssid = "43">In particular,all of their performance differences are statisticallysignificant for these parameter values.</S>
			<S sid ="194" ssid = "44">The largestperformance difference is 6.0% at p1 = 45, p2 = 10and p3 = 0.05.</S>
			<S sid ="195" ssid = "45">However, this picture is not the samewhen considering a higher context similarity threshold (p3 = 0.13) as Figure 7 shows.</S>
			<S sid ="196" ssid = "46">In particular,HRGs and HAC perform similarly for p3 = 0.13,while the majority of performance differences arenot statistically significant.</S>
			<S sid ="197" ssid = "47">The similar behaviour of HRGs and HAC at thisthreshold is caused both by the worse performanceof HRGs and the improved performance of HAC asopposed to lower p3 values.</S>
			<S sid ="198" ssid = "48">As it has been mentioned, setting p3 = 0.13 leads to sparse and disconnected graphs.</S>
			<S sid ="199" ssid = "49">Additionally, the likelihood function (Equation 3) is maximised when the probability, ?k, of an internal node, Dk, approaches 0 or 1.This creates a bias towards dendrograms, in which alarge number of internal nodes have zero probability.</S>
			<S sid ="200" ssid = "50">These dendrograms might be a good-fit to theobserved graph, but not to the GS.</S>
			<S sid ="201" ssid = "51">In contrast, HAC is less affected, because it neverconsiders creating an internal node, when the maximum similarity among any pair of two candidate 752 Figure 7: Performance of HRGs and HAC for differentparameter combinations (Table 2).</S>
			<S sid ="202" ssid = "52">All combinations ofp1, p2 and p3 = 0.13.</S>
			<S sid ="203" ssid = "53">subtrees is zero.</S>
			<S sid ="204" ssid = "54">Additionally, our experiments showthat HAC is unable to deal with noise when considering sparse graphs (p3&lt;0.13).</S>
			<S sid ="205" ssid = "55">For that reason,the F-Score of HAC increases as the edge similaritythreshold decreases.</S>
			<S sid ="206" ssid = "56">To further investigate this issue and test whetherHAC is able to achieve a higher F-Score than HRGsin higher p3 values, we executed two more experiments for HAC and HRGs increasing p3 to 0.17 and0.21 respectively.</S>
			<S sid ="207" ssid = "57">In the first case we observed thatthe performance of HAC remained relatively stablecompared to p3 = 0.13, while in the second case theperformance of HAC decreased as Figure 7 shows.In both cases, HAC performed significantly betterthan HRGs.</S>
			<S sid ="208" ssid = "58">Overall, the comparison of HRGs against HAChas shown that HRGs perform significantly betterthan HAC when considering connected or less sparsegraphs (p3&lt;0.13).</S>
			<S sid ="209" ssid = "59">This is due to the fact that HACcreates dendrograms, in which connections withinthe clusters are dense, while connections betweenthe clusters are sparse, i.e. it only considers assortative structures.</S>
			<S sid ="210" ssid = "60">In contrast, HRGs also consider dis-assortative dendrograms, i.e. dendrograms in whichvertices are less likely to be connected on smallscales than on large ones, as well as mixtures ofassortative and disassortative (Clauset et al., 2008).This is achieved by allowing the probability ?k ofa node k to vary arbitrarily throughout the dendro-gram.HAC performs similarly or better than HRGs for largely disconnected and sparse graphs, becauseHRGs become biased towards disassortative treeswhich are not a good fit to the GS (Figure 7).</S>
			<S sid ="211" ssid = "61">Despite that, our evaluation has also shown that the bestperformance of HAC (F-Score = 86.0% at p1 = 15,p2 = 10, p3 = 0.13) is significantly lower thanthe best performance of HRGs (F-Score = 87.6% atp1 = 35, p2 = 10, p3 = 0.09).</S>
			<S sid ="212" ssid = "62">6.3 Comparison to state-of-the-art methodsTable 3 compares the best performing parametercombination of our method against state-of-the-artmethods.</S>
			<S sid ="213" ssid = "63">Table 3 also includes the best performanceof our baselines, i.e HAC, CWU and CWW..</S>
			<S sid ="214" ssid = "64">Brody &amp; Lapata (2009) presented a sense induction method that is related to Latent Dirichlet Allocation (Blei et al., 2003).</S>
			<S sid ="215" ssid = "65">In their work, theymodel the target word instances as samples from amultinomial distribution over senses which are successively characterized as distributions over words(Brody and Lapata, 2009).</S>
			<S sid ="216" ssid = "66">A significant advantageof their method is the inclusion of more than onelayer in the LDA setting, where each layer corresponds to a different feature type e.g. dependencyrelations, bigrams, etc. The inclusion of differentfeature types as separate models in the sense induction process can easily be modeled in our setting, by inferring a different hierarchy of target wordinstances according to each feature type, and thencombining all of them to a consensus tree.</S>
			<S sid ="217" ssid = "67">In thiswork, we have focused on extracting a single hierarchy combining word co-occurrence and bigram features.</S>
			<S sid ="218" ssid = "68">Niu et al.</S>
			<S sid ="219" ssid = "69">(2007) developed a vector-basedmethod that performs sense induction by grouping the contexts of a target word using three typesof features, i.e. POS tags of neighbouring words,word co-occurrences and local collocations.</S>
			<S sid ="220" ssid = "70">The sequential information bottleneck algorithm (Slonimet al., 2002) is applied for clustering.</S>
			<S sid ="221" ssid = "71">HRGs performslightly better than the methods of Brody &amp; Lap-ata (2009) and Niu et al.</S>
			<S sid ="222" ssid = "72">(2007), although the differences are not significant (McNemar’s test at 95%confidence level).Klapaftis &amp; Manandhar (2008) developed agraph-based sense induction method, in which vertices correspond to collocations related to the target word and edges between vertices are drawn ac 753 System Performance (%)HRGs 87.6(Brody and Lapata, 2009) 87.3(Niu et al., 2007) 86.8(Klapaftis and Manandhar, 2008) 86.4HAC 86.0CWU 85.1CWW 84.7(Pedersen, 2007) 84.5MFS 80.9 Table 3: HRGs against recent methods &amp; baselines.</S>
			<S sid ="223" ssid = "73">cording to the co-occurrence frequency of the corresponding collocations.</S>
			<S sid ="224" ssid = "74">The constructed graph issmoothed to identify more edges between verticesand then clustered using Chinese Whispers (Biemann, 2006).</S>
			<S sid ="225" ssid = "75">This method is related to the basicinputs of our presented method.</S>
			<S sid ="226" ssid = "76">Despite that, it isa flat clustering method that ignores the hierarchicalstructure exhibited by observed graphs.</S>
			<S sid ="227" ssid = "77">The previous section has shown that inferring the hierarchicalstructure of graphs leads to superior WSD performance.</S>
			<S sid ="228" ssid = "78">Pedersen (2007) presented SenseClusters, avector-based method that clusters second order co-occurrence vectors using k-means, where k is automatically determined using the Adapted Gap Statistic (Pedersen and Kulkarni, 2006).</S>
			<S sid ="229" ssid = "79">As can be observed, HRGs perform significantly better than themethods of Pedersen (2007) and Klapaftis &amp; Man-andhar (2008) (McNemar’s test at 95% confidencelevel).</S>
			<S sid ="230" ssid = "80">Finally, Table 3 shows that the best performingparameter combination of HRGs achieves a significantly higher F-Score than the best performing parameter combination of HAC, CWU and CWW.</S>
			<S sid ="231" ssid = "81">Furthermore, HRGs outperform the most frequent sensebaseline by 6.7%.</S>
	</SECTION>
	<SECTION title="Conclusion &amp; future work. " number = "7">
			<S sid ="232" ssid = "1">We presented an unsupervised method for inferringthe hierarchical grouping of the senses of a polysemous word.</S>
			<S sid ="233" ssid = "2">Our method creates a graph, in whichvertices correspond to contexts of a polysemous target word and edges between them are drawn according to their similarity.</S>
			<S sid ="234" ssid = "3">The hierarchical randomgraphs algorithm (Clauset et al., 2008) was applied to the constructed graph in order to infer its hierarchical structure, i.e. binary tree.</S>
			<S sid ="235" ssid = "4">The learned tree provides an induction of thesenses of a given word at different levels of sensegranularity and was applied to the problem of WSD.The WSD process mapped the tree’s internal nodesto GS senses using a sense tagged corpus, and thentagged new instances by exploiting the structural information provided by the tree.</S>
			<S sid ="236" ssid = "5">Our experimental results have shown that ourgraphs exhibit hierarchical organisation that canbe captured by HRGs, in effect providing improved WSD performance compared to flat clustering.</S>
			<S sid ="237" ssid = "6">Additionally, our comparison against hierarchical agglomerative clustering with average-linkagehas shown that HRGs perform significantly betterthan HAC when the graphs do not suffer from sparsity (disconnected graphs).</S>
			<S sid ="238" ssid = "7">The comparison withstate-of-the-art sense induction systems has shownthat our method yields improvements.</S>
			<S sid ="239" ssid = "8">Our future work focuses on using different featuretypes, e.g. dependency relations, second-order co-occurrences, named entities and others to constructour undirected graphs and then applying HRGs, inorder to measure the impact of each feature typeon the induced hierarchical structures within a WSDsetting.</S>
			<S sid ="240" ssid = "9">Moreover, following the work in (Clauset etal., 2008), we are also working on using MCMC inorder to sample more than one dendrogram at equilibrium, and then combine them to a consensus tree.This consensus tree might be able to express a largeramount of topological features of the initial undirected graph.</S>
			<S sid ="241" ssid = "10">Finally in terms of evaluation, our future workalso focuses on evaluating HRGs using a fine-grained sense inventory, extending the evaluation onthe SemEval2010 WSI task dataset (Manandhar etal., 2010) as well as applying HRGs to other relatedtasks such as taxonomy learning.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="242" ssid = "11">This work is supported by the European Commission via the EU FP7 INDECT project, GrantNo.218086, Research area: SEC2007-1.201 Intelligent Urban Environment Observation System.</S>
			<S sid ="243" ssid = "12">Theauthors would like to thank the anonymous reviewers for their useful comments.</S>
			<S sid ="244" ssid = "13">754</S>
	</SECTION>
</PAPER>
