<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Word Sense Induction (WSI) is an unsupervised approach for learning the multiple senses of a word.</S>
		<S sid ="2" ssid = "2">Graph-based approaches to WSI frequently represent word co-occurrence as a graph and use the statistical properties of the graph to identify the senses.</S>
		<S sid ="3" ssid = "3">We reinterpret graph-based WSI as community detection, a well studied problem in network science.</S>
		<S sid ="4" ssid = "4">The relations in the co-occurrence graph give rise to word communities, which distinguish senses.</S>
		<S sid ="5" ssid = "5">Our results show competitive performance on the SemEval2010 WSI Task.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Many words have several distinct meanings.</S>
			<S sid ="7" ssid = "7">For example, “law” may refer to legislation, a rule, or police depending on the context.</S>
			<S sid ="8" ssid = "8">Word Sense Induction (WSI) discovers the different senses of a word, such as “law,” by examining its contextual uses.</S>
			<S sid ="9" ssid = "9">By deriving the senses of a word directly from a corpus, WSI is able to identify specialized, topical meanings in domains such as medicine or law, which predefined sense inventories may not include.</S>
			<S sid ="10" ssid = "10">aWe consider graph-based approaches to WSI, which typically construct a graph from word occurrences or collocations.</S>
			<S sid ="11" ssid = "11">The core problem is how to identify sense-specific information within the graph in order to perform sense induction.</S>
			<S sid ="12" ssid = "12">Current approaches have used clustering (Dorow and Widdows, 2003; Klapaftis and Manandhar, 2008) or statistical graph models (Klapaftis and Manandhar, 2010) to identify sense-specific subgraphs.</S>
			<S sid ="13" ssid = "13">We reinterpret the challenge of identifying sense- specific information in a co-occurrence graph as one of community detection, where a community is de fined as a group of connected nodes that are more connected to each other than to the rest of the graph (Fortunato, 2010).</S>
			<S sid ="14" ssid = "14">Within the co occurrence graph, we hypothesize that communities identify sense- specific contexts for each of the terms.</S>
			<S sid ="15" ssid = "15">Community detection identifies groups of contextual cues that constrain each of the words in a community to a single sense.</S>
			<S sid ="16" ssid = "16">To test our hypothesis, we require a community detection algorithm with two key properties: (1) a word may belong to multiple, overlapping communities, which is necessary for discovering multiple senses, and (2) the community detection may be hierarchically tuned, which corresponds to sense granularity.</S>
			<S sid ="17" ssid = "17">Therefore, we adapt a recent, state of the art approach, Link Clustering (Ahn et al., 2010).</S>
			<S sid ="18" ssid = "18">Our initial study suggests that community detection offers competitive performance and sense quality.</S>
	</SECTION>
	<SECTION title="Word Sense Induction. " number = "2">
			<S sid ="19" ssid = "1">A co-occurrence graph is fundamental to our approach; terms are represented as nodes and an edge between two nodes indicates the terms’ co- occurrence, with a weight proportional to frequency.</S>
			<S sid ="20" ssid = "2">While prior work has focused on clustering the nodes to induce senses, using Link Clustering (Ahn et al., 2010), we cluster the edges, which is equivalent to grouping the word collocations to identify sense-specific contexts.</S>
			<S sid ="21" ssid = "3">We summarize our approach as four steps: (1) selecting the contextual cues, (2) building a co occurrence graph, (3) performing community detection on the graph, and (4) sense labeling new contexts using the discovered communities.</S>
			<S sid ="22" ssid = "4">Context Refinement Representing the co- occurrence graph for all terms in a context is 24 Proceedings of the TextGraphs6 Workshop, pages 24–28, Portland, Oregon, USA, 1924 June 2011.</S>
			<S sid ="23" ssid = "5">Qc 2011 Association for Computational Linguistics prohibitively expensive.</S>
			<S sid ="24" ssid = "6">Moreover, often only a subset of the terms in a context constrain the sense of an ambiguous word.</S>
			<S sid ="25" ssid = "7">Therefore, we refine a word’s context to include only a subset of the terms present.</S>
			<S sid ="26" ssid = "8">Following previous work (Ve´ronis, 2004), we select only nouns in the context.</S>
			<S sid ="27" ssid = "9">Early experiments indicated that including infrequent terms in the co-occurrence graph yielded poor performance, which we attribute to having too few connecting edges to identify meaningful community structure.</S>
			<S sid ="28" ssid = "10">Therefore, we include only those nouns occurring in the most frequent 5000 tokens, which are likely to be representative the largest communities in which a term takes part.</S>
			<S sid ="29" ssid = "11">Last, we include all the nouns and verbs used in the SemEval 2010 WSI Task (Manandhar et al., 2010), which are used in our evaluation.</S>
			<S sid ="30" ssid = "12">The selected context terms are then stemmed using the Porter stemmer.</S>
			<S sid ="31" ssid = "13">Building the Co-occurrence Graph The graph is iteratively constructed by adding edges between the terms from a context.</S>
			<S sid ="32" ssid = "14">For each pairwise combination of terms, an edge is added and its weight is increased by 1.</S>
			<S sid ="33" ssid = "15">This step effectively embeds a clique if it did not exist before, connecting all of the context’s words within the graph.</S>
			<S sid ="34" ssid = "16">Once all contexts have been seen, the graph is then pruned to remove all edges with weight below a threshold τ = 25.</S>
			<S sid ="35" ssid = "17">This step removes edges form infrequent collo-.</S>
			<S sid ="36" ssid = "18">cations, which may not contribute sufficient graph structure for community detection, and as a practical consideration, greatly speeds up the community Figure 1: A portion of the local co-occurrence graph for “mouse” from the SemEval2010 Task 14 corpus ties we selected the approach of Ahn et al.</S>
			<S sid ="37" ssid = "19">(2010), summarized next, which performs well for overlapping community structure.</S>
			<S sid ="38" ssid = "20">First, the edges are clustered using an unweighted similarity function based on the neighbors of two edges, ei,j and ei,k: sim(ei,j, ei,k) = nj∩nk , where ∪ni denotes the node i and its neighbors.</S>
			<S sid ="39" ssid = "21">This similarity reflects the percentage of terms that co occur in common with the term for nodes j and k, independent of the terms that co-occur with the shared term for i. For example, in Figure 1, the similarity for the edges connecting “mouse” with “user” and “software,” 2 , measures the overlap in the neighbors detection process.</S>
			<S sid ="40" ssid = "22">However, we note that parameter 5 of “user” and“software” independent of the neigh was largely unoptimized and future work may see a benefit from accounting for edge weight.</S>
			<S sid ="41" ssid = "23">Community Detection Within the co-occurrence graph, communities may have partial overlap.</S>
			<S sid ="42" ssid = "24">For example, Figure 1 illustrates a part of the local graph for “mouse.” Two clear senses emerge from the neighbors: one for the input device and another for the animal.</S>
			<S sid ="43" ssid = "25">However, the terms that correspond to one sense also co-occur with terms corresponding to the other sense, e.g., “information,” which hinders finding communities directly from disconnected components in the local neighborhood.</S>
			<S sid ="44" ssid = "26">Finding sense-specific communities requires recognizing bors for “mouse,” such as “cell” and “size.” Using this similarity function, the edges are agglomeratively clustered into a dendrogram.</S>
			<S sid ="45" ssid = "27">We use the single-link criteria which iteratively merges the two clusters connected by the edge pair with the highest similarity.</S>
			<S sid ="46" ssid = "28">The dendrogram may then be cut at different levels to reveal different cluster granularities; cuts near the bottom of the dendrogram create a larger number of small groups of collocations, whereas cuts near the top create fewer, larger groups of collocations.</S>
			<S sid ="47" ssid = "29">To select the specific partitioning of the dendrogram into clusters, we select the solution that maximizes the partition density, which Ahnthat the co-occurring terms may be shared by mul et al.</S>
			<S sid ="48" ssid = "30">(2010) define as D = 2 ), m mc−(nc−1) c− c−tiple communities.</S>
			<S sid ="49" ssid = "31">Therefore, to identify communi- where M is the number of edges in the graph, c de notes a specific cluster, and nc and mc are the number of nodes and edges in cluster c, respectively.</S>
			<S sid ="50" ssid = "32">The final set of communities is derived from these partitions: a node is a member of each community in which one of its edges occurs.</S>
			<S sid ="51" ssid = "33">Last, we remove all communities of size 3 and below, which we interpret as having too few semantic constraints to reliably disambiguate each of its terms.</S>
			<S sid ="52" ssid = "34">Sense Induction from Communities Each term in a community is treated as having a specific sense, with one sense per community.</S>
			<S sid ="53" ssid = "35">To label a contextual usage, we identify the community that best maps to the context.</S>
			<S sid ="54" ssid = "36">For a given context, made of the set of SPD FScore 61.1 (3) V-Meas.</S>
			<S sid ="55" ssid = "37">3.6 (18) S80/20 57.64 (18) S60/40 57.64 (16) SV 56.16 (9) 8.7 (6) 57.90 (18) 57.36 (17) SF 63.4 (1) 0 (26) 56.18 (21) 56.20 (21) BestF 63.3 (1) 0 (26) 58.69 (14) 58.24 (13) BestV 26.7 (25) 16.2 (1) 58.34 (16) 57.27 (17) BestS 49.8 (15) 15.7 (2) 62.44 (1) 61.96 (1) MFS 63.4 0 58.67 58.95 Table 1: Performance results on the SemEval2010 WSI Task, with rank shown in parentheses.</S>
			<S sid ="56" ssid = "38">Reference scores of the best submitted systems are shown in the bottom.</S>
			<S sid ="57" ssid = "39">words W , we score each community i, consisting of words C, using the Jaccard index weighted by community size: score(C , W ) = C |Ci∩W | . This |Ci∪W | similarity function favors mapping contexts to larger communities, which we interpret as having more semantic constraints.</S>
			<S sid ="58" ssid = "40">The final sense labeling consists of the scores for all overlapping communities.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "3">
			<S sid ="59" ssid = "1">We use the SemEval2 Task 14 evaluation (Manand 120 90 60 30 0 1.00 0.80 0.60 0.40 0.20 0.00 375 300 225 150 75 0 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 har et al., 2010) to measure the quality of induced senses.</S>
			<S sid ="60" ssid = "2">We summarize the evaluation as follows.</S>
			<S sid ="61" ssid = "3">Systems are provided with an unlabeled training corpus consisting of 879,807 multi-sentence contexts for 100 polysemous words, comprised of 50 nouns and 50 verbs.</S>
			<S sid ="62" ssid = "4">Systems induce sense representations for target words from the training corpus and then use those representations to label the senses of the target words in unseen contexts from a test corpus.</S>
			<S sid ="63" ssid = "5">We use the entire multi-sentence context for building the co-occurrence graph.</S>
			<S sid ="64" ssid = "6">The induced sense labeling is scored using two unsupervised and one supervised methods.</S>
			<S sid ="65" ssid = "7">The unsupervised scores consists of two contrasting measures: the paired FScore (Artiles et al., 2009) and the V-Measure (Rosenberg and Hirschberg, 2007).</S>
			<S sid ="66" ssid = "8">Briefly, the V-Measure rates the homogeneity and completeness of a clustering solution.</S>
			<S sid ="67" ssid = "9">Solutions that have word clusters formed from one gold-standard sense are homogeneous; completeness measures the degree to which a gold-standard sense’s instances are assigned to a single cluster.</S>
			<S sid ="68" ssid = "10">The paired FScore reflects the overlap of the solution and the gold standard in cluster assignments for all pairwise combi Merge Steps Prior to Cutting the Dendrogram (in thousands) Figure 2: V-Measure and paired FScore results for different partitionings of the dendrogram.</S>
			<S sid ="69" ssid = "11">The dashed vertical line indicates SP D nation of instances.</S>
			<S sid ="70" ssid = "12">The supervised evaluation transforms the induced sense clusters of a portion of the corpus into a word sense classifier, which is then tested on the remaining corpus.</S>
			<S sid ="71" ssid = "13">An 80/20 train-test split, S80/20, and 60/40 split, S60/40, are both used.</S>
			<S sid ="72" ssid = "14">Results As a first measure of the quality of the induced senses, we evaluated both the solution that maximized the partition density, referred to as SP D, and an additional 5,000 solutions, evenly distributed among the possible dendrogram partitionings.</S>
			<S sid ="73" ssid = "15">Figure 2 shows the score distribution for V Measure and paired FScore.</S>
			<S sid ="74" ssid = "16">Table 1 lists the scores and rank for SP D and the solutions that optimize the V-Measure, SV , and FScore, SF , among the 26 participating Task-14 systems.</S>
			<S sid ="75" ssid = "17">For comparison, we include the highest performing systems on each measure and the Most Frequent Sense (MFS) baseline.</S>
			<S sid ="76" ssid = "18">Discussion Optimizing the partition density results in high performance only for the FScore; however, optimizing for the V-Measure yields competitive performance on both measures.</S>
			<S sid ="77" ssid = "19">The behavior is encouraging as most approaches submitted to Task 14 favor only one measure.</S>
			<S sid ="78" ssid = "20">Figure 2 indicates a relationship between the V- Measure and community memberships.</S>
			<S sid ="79" ssid = "21">Therefore, using SV , we calculated the Pearson correlation between a term’s scores and the number of community memberships within a single solution.</S>
			<S sid ="80" ssid = "22">The correlation with the paired FScore, r = -0.167, was not statistically significant at p &lt; .05, while correlation with the V-Measure, r = 0.417 is significant with p &lt; 1.6e5.</S>
			<S sid ="81" ssid = "23">This suggests that at a specific community granularity, additional communities enable the WSI mapping process to make better sense distinctions between contexts.</S>
			<S sid ="82" ssid = "24">However, we note that V-Measure begins to drop as the average community membership increases in solutions after SV , as shown in Figure 2.</S>
			<S sid ="83" ssid = "25">We suspect that as the agglomerative merge process continues, communities representing different senses become merged, leading to a loss of purity.</S>
			<S sid ="84" ssid = "26">The lower performance of SP D and the impact of community memberships raises the important question of how to best select the communities.</S>
			<S sid ="85" ssid = "27">While co-occurrence graphs have been shown to exhibit small-world network patterns (Ve´ronis, 2004), optimizing for the general criterion of partition density that has performed well on such networks does not result in communities that map well to sense-specific contexts.</S>
			<S sid ="86" ssid = "28">We believe that this behavior is due to impact of the sense inventory; selecting a community solution purely based on the graph’s structure may not capture the correct sense distinctions, either having communities with too few members to distinguish between senses or too many members, which conflates senses.</S>
			<S sid ="87" ssid = "29">However, a promising future direction is to examine whether the there exist features of the graph structure that would allow for recognizing the specific community solutions that correspond directly to different sense granularities without the need for an external evaluation metric.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "4">
			<S sid ="88" ssid = "1">We highlight those related works with connections to community detection.</S>
			<S sid ="89" ssid = "2">Ve´ronis (2004) demon strated that word co-occurrence graphs follow a small-world network pattern.</S>
			<S sid ="90" ssid = "3">In his scheme, word senses are discovered by iteratively deleting the more connected portions of the subgraph to reveal the different senses’ network structure.</S>
			<S sid ="91" ssid = "4">Our work capitalizes on this intuition of discovering sense- related subgraphs, but leverages formalized methods for community detection to identify them.</S>
			<S sid ="92" ssid = "5">Dorow and Widdows (2003) identify sense- related subgraphs in a similar method to community detection for local region of the co occurrence graph.</S>
			<S sid ="93" ssid = "6">They use a random walk approach to identify regions of the graph that are sense-specific.</S>
			<S sid ="94" ssid = "7">Though not identical, we note that the random walk model has been successfully applied to community detection (Rosvall et al., 2009).</S>
			<S sid ="95" ssid = "8">Furthermore, Dorow and Widdows (2003) performs graph clustering on a per- word basis; in contrast, the proposed approach identifies communities for the entire graph, effectively performing an all-word WSI.</S>
			<S sid ="96" ssid = "9">Klapaftis and Manandhar (2010) capture hierarchical relations between collocations using a Hierarchical Random Graph model where nodes are collocations and edges indicate their co occurrence, which improved performance over non-hierarchical models.</S>
			<S sid ="97" ssid = "10">Our community detection approach also captures the hierarchical structure of the collocation graph, but uses a much simpler graphical representa tion that for n terms requires O(n) nodes and O(n2) edges, compared to O(n2) nodes and O(n3) edges for the above approach, which allows it to build the collocation graph from a larger set of terms.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "5">
			<S sid ="98" ssid = "1">We have proposed a new graph-based method for WSI based on finding sense-specific word communities within a co-occurrence graph, which are then identify distinguish senses in new contexts.</S>
			<S sid ="99" ssid = "2">An initial analysis using the SemEval 2010 WSI task demonstrates competitive.</S>
			<S sid ="100" ssid = "3">performance.</S>
			<S sid ="101" ssid = "4">Future research will address two potential avenues: (1) the impact of word frequency on community size and memberships and (2) identifying both graph properties and semantic relations within hierarchical communities that distinguish between sense granulari- ties.</S>
			<S sid ="102" ssid = "5">Software for the WSI model and for Link Clustering is available as a part of the S-Space Package (Jurgens and Stevens, 2010).</S>
	</SECTION>
</PAPER>
