<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We investigate generalizations of the all-subtrees &quot;DOP&quot; approach to unsupervisedparsing.</S>
		<S sid ="2" ssid = "2">Unsupervised DOP models assignall possible binary trees to a set of sentencesand next use (a large random subset of) allsubtrees from these binary trees to computethe most probable parse trees.</S>
		<S sid ="3" ssid = "3">We will testboth a relative frequency estimator forunsupervised DOP and a maximumlikelihood estimator which is known to bestatistically consistent.</S>
		<S sid ="4" ssid = "4">We report state-of-the-art results on English (WSJ), German(NEGRA) and Chinese (CTB) data.</S>
		<S sid ="5" ssid = "5">To thebest of our knowledge this is the first paperwhich tests a maximum likelihood estimatorfor DOP on the Wall Street Journal, leadingto the surprising result that an unsupervisedparsing model beats a widely usedsupervised model (a treebank PCFG).</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">The problem of bootstrapping syntactic structurefrom unlabeled data has regained considerableinterest.</S>
			<S sid ="7" ssid = "7">While supervised parsers suffer fromshortage of hand-annotated data, unsupervisedparsers operate with unlabeled raw data of whichunlimited quantities are available.</S>
			<S sid ="8" ssid = "8">During the lastfew years there has been steady progress in the field.Where van Zaanen (2000) achieved 39.2%unlabeled f-score on ATIS word strings, Clark(2001) reports 42.0% on the same data, and Kleinand Manning (2002) obtain 51.2% f-score on ATISpart-of-speech strings using a constituent-context model called CCM.</S>
			<S sid ="9" ssid = "9">On Penn Wall Street Journal p-o-s-strings = 10 (WSJ10), Klein and Manning(2002) report 71.1% unlabeled f-score with CCM.And the hybrid approach of Klein and Manning(2004), which combines constituency anddependency models, yields 77.6% f-score.</S>
			<S sid ="10" ssid = "10">Bod (2006) shows that a further improvement on the WSJ10 can be achieved by an unsupervised generalization of the all-subtrees approachknown as Data-Oriented Parsing (DOP).</S>
			<S sid ="11" ssid = "11">Thisunsupervised DOP model, coined U-DOP, firstassigns all possible unlabeled binary trees to a set ofsentences and next uses all subtrees from (a largesubset of) these trees to compute the most probableparse trees.</S>
			<S sid ="12" ssid = "12">Bod (2006) reports that U-DOP notonly outperforms previous unsupervised parsers butthat its performance is as good as a binarized supervised parser (i.e. a treebank PCFG) on the WSJ.</S>
			<S sid ="13" ssid = "13">A possible drawback of U-DOP, however,is the statistical inconsistency of its estimator(Johnson 2002) which is inherited from the DOP1model (Bod 1998).</S>
			<S sid ="14" ssid = "14">That is, even with unlimitedtraining data, U-DOP&apos;s estimator is not guaranteedto converge to the correct weight distribution.Johnson (2002: 76) argues in favor of a maximumlikelihood estimator for DOP which is statisticallyconsistent.</S>
			<S sid ="15" ssid = "15">As it happens, in Bod (2000) we alreadydeveloped such a DOP model, termed MLDOP,which reestimates the subtree probabilities by amaximum likelihood procedure based onExpectation-Maximization.</S>
			<S sid ="16" ssid = "16">Although cross-validation is needed to avoid overlearning, MLDOPoutperforms DOP1 on the OVIS corpus (Bod2000).</S>
			<S sid ="17" ssid = "17">This raises the question whether we cancreate an unsupervised DOP model which is also 865 statistically consistent.</S>
			<S sid ="18" ssid = "18">In this paper we will showthat an unsupervised version of MLDOP can beconstructed along the lines of U-DOP.</S>
			<S sid ="19" ssid = "19">We will startout by summarizing DOP, U-DOP and MLDOP,and next create a new unsupervised model calledUMLDOP.</S>
			<S sid ="20" ssid = "20">We report that UMLDOP not onlyobtains higher parse accuracy than U-DOP on threedifferent domains, but that it also achieves this withfewer subtrees than U-DOP.</S>
			<S sid ="21" ssid = "21">To the best of ourknowledge, this paper presents the firstunsupervised parser that outperforms a widely usedsupervised parser on the WSJ, i.e. a treebankPCFG.</S>
			<S sid ="22" ssid = "22">We will raise the question whether the endof supervised parsing is in sight.</S>
	</SECTION>
	<SECTION title="DOP. " number = "2">
			<S sid ="23" ssid = "1">The key idea of DOP is this: given an annotatedcorpus, use all subtrees, regardless of size, to parsenew sentences.</S>
			<S sid ="24" ssid = "2">The DOP1 model in Bod (1998)computes the probabilities of parse trees andsentences from the relative frequencies of thesubtrees.</S>
			<S sid ="25" ssid = "3">Although it is now known that DOP1&apos;srelative frequency estimator is statisticallyinconsistent (Johnson 2002), the model yieldsexcellent empirical results and has been used instate-of-the-art systems.</S>
			<S sid ="26" ssid = "4">Let&apos;s illustrate DOP1 with asimple example.</S>
			<S sid ="27" ssid = "5">Assume a corpus consisting ofonly two trees, as given in figure 1.</S>
			<S sid ="28" ssid = "6">NP VP S NP Mary V likes John NP VP S NPVPeter hates Susan Figure 1.</S>
			<S sid ="29" ssid = "7">A corpus of two trees New sentences may be derived by combiningfragments, i.e. subtrees, from this corpus, by meansof a node-substitution operation indicated as °.Node-substitution identifies the leftmostnonterminal frontier node of one subtree with theroot node of a second subtree (i.e., the secondsubtree is substituted on the leftmost nonterminalfrontier node of the first subtree).</S>
			<S sid ="30" ssid = "8">Thus a newsentence such as Mary likes Susan can be derived by combining subtrees from this corpus, shown infigure 2.</S>
			<S sid ="31" ssid = "9">NP VP S NPV likes NP Mary NP Susan NP VP S NPMary V likes Susan =° ° Figure 2.</S>
			<S sid ="32" ssid = "10">A derivation for Mary likes Susan Other derivations may yield the same tree, e.g.: NP VP S NPV NP Mary NP VP S NPMary V likes Susan = Susan V likes ° ° Figure 3.</S>
			<S sid ="33" ssid = "11">Another derivation yielding same tree DOP1 computes the probability of a subtree t as theprobability of selecting t among all corpus subtreesthat can be substituted on the same node as t. Thisprobability is computed as the number ofoccurrences of t in the corpus, | t |, divided by thetotal number of occurrences of all subtrees t&apos; withthe same root label as t .1 Let r(t) return the root labelof t. Then we may write: P(t) = | t |S t&apos;: r(t&apos;)= r(t) | t&apos; | The probability of a derivation t1°...°tn is computedby the product of the probabilities of its subtrees t i: P(t1°...°tn) = ?i P(ti)As we have seen, there may be several distinctderivations that generate the same parse tree.</S>
			<S sid ="34" ssid = "12">Theprobability of a parse tree T is the sum of the 1 This subtree probability is redressed by a simple.</S>
			<S sid ="35" ssid = "13">correction factor discussed in Goodman (2003: 136)and Bod (2003).</S>
			<S sid ="36" ssid = "14">866 probabilities of its distinct derivations.</S>
			<S sid ="37" ssid = "15">Let tid be theith subtree in the derivation d that produces tree T ,then the probability of T is given by P(T) = Sd?i P(tid)Thus DOP1 considers counts of subtrees of a widerange of sizes: everything from counts of single-level rules to entire trees is taken into account tocompute the most probable parse tree of a sentence.A disadvantage of the approach may be that anextremely large number of subtrees (andderivations) must be considered.</S>
			<S sid ="38" ssid = "16">Fortunately thereexists a compact isomorphic PCFG-reduction ofDOP1 whose size is linear rather than exponential inthe size of the training set (Goodman 2003).Moreover, Collins and Duffy (2002) show how atree kernel can be applied to DOP1&apos;s all-subtreesrepresentation.</S>
			<S sid ="39" ssid = "17">The currently most successfulversion of DOP1 uses a PCFG-reduction of themodel with an n-best parsing algorithm (Bod 2003).</S>
	</SECTION>
	<SECTION title="U-DOP" number = "3">
			<S sid ="40" ssid = "1">U-DOP extends DOP1 to unsupervised parsing(Bod 2006).</S>
			<S sid ="41" ssid = "2">Its key idea is to assign all unlabeledbinary trees to a set of sentences and to next use (inprinciple) all subtrees from these binary trees toparse new sentences.</S>
			<S sid ="42" ssid = "3">U-DOP thus proposes one ofthe richest possible models in bootstrapping trees.Previous models like Klein and Manning&apos;s (2002,2005) CCM model limit the dependencies to&quot;contiguous subsequences of a sentence&quot;.</S>
			<S sid ="43" ssid = "4">Thismeans that CCM neglects dependencies that arenon-contiguous such as between more and than in&quot;BA carried more people than cargo&quot;.</S>
			<S sid ="44" ssid = "5">Instead, U-DOP&apos;s all-subtrees approach captures bothcontiguous and non-contiguous lexical dependencies.</S>
			<S sid ="45" ssid = "6">As with most other unsupervised parsingmodels, U-DOP induces trees for p-o-s stringsrather than for word strings.</S>
			<S sid ="46" ssid = "7">The extension to wordstrings is straightforward as there exist highlyaccurate unsupervised part-of-speech taggers (e.g.Schütze 1995) which can be directly combined withunsupervised parsers.</S>
			<S sid ="47" ssid = "8">To give an illustration of U-DOP, considerthe WSJ p-o-s string NNS VBD JJ NNS whichmay correspond for instance to the sentenceInvestors suffered heavy losses . U-DOP starts by assigning all possible binary trees to this string,where each root node is labeled S and each internalnode is labeled X. Thus NNS VBD JJ NNS has atotal of five binary trees shown in figure 4 -- wherefor readability we add words as well.</S>
			<S sid ="48" ssid = "9">NNS VBD JJ NNS Investors suffered heavy losses XX S NNS VBD JJ NNSInvestors suffered heavy losses X X S NNS VBD JJ NNSInvestors suffered heavy losses XX S NNS VBD JJ NNSInvestors suffered heavy losses X X S NNS VBD JJ NNS Investors suffered heavy losses XX S Figure 4.</S>
			<S sid ="49" ssid = "10">All binary trees for NNS VBD JJ NNS(Investors suffered heavy losses) While we can efficiently represent the set of allbinary trees of a string by means of a chart, we needto unpack the chart if we want to extract subtreesfrom this set of binary trees.</S>
			<S sid ="50" ssid = "11">And since the totalnumber of binary trees for the small WSJ10 isalmost 12 million, it is doubtful whether we canapply the unrestricted U-DOP model to such acorpus.</S>
			<S sid ="51" ssid = "12">U-DOP therefore randomly samples a largesubset from the total number of parse trees from thechart (see Bod 2006) and next converts the subtreesfrom these parse trees into a PCFG-reduction(Goodman 2003).</S>
			<S sid ="52" ssid = "13">Since the computation of themost probable parse tree is NP-complete (Sima&apos;an1996), U-DOP estimates the most probable treefrom the 100 most probable derivations usingViterbi n-best parsing.</S>
			<S sid ="53" ssid = "14">We could also have used themore efficient k-best hypergraph parsing techniqueby Huang and Chiang (2005), but we have not yetincorporated this into our implementation.</S>
			<S sid ="54" ssid = "15">To give an example of the dependencies thatUDOP can take into account, consider thefollowing subtrees in figure 5 from the trees in 867 figure 4 (where we again add words for readability).These subtrees show that U-DOP takes into accountboth contiguous and non-contiguous substrings.</S>
			<S sid ="55" ssid = "16">NNS VBD Investors suffered X X S VBD suffered X X NNS NNS Investors losses X X S JJ NNS heavy losses XX S JJ NNS heavy losses X NNS VBD Investors suffered X VBD JJ suffered heavy X Figure 5.</S>
			<S sid ="56" ssid = "17">Some subtrees from trees in figure 4 Of course, if we only had the sentence Investorssuffered heavy losses in our corpus, there would beno difference in probability between the five parsetrees in figure 4.</S>
			<S sid ="57" ssid = "18">However, if we also have adifferent sentence where JJ NNS ( heavy losses)appears in a different context, e.g. in Heavy losseswere reported , its covering subtree gets a relativelyhigher frequency and the parse tree where heavylosses occurs as a constituent gets a higher totalprobability.</S>
	</SECTION>
	<SECTION title="ML-DOP. " number = "4">
			<S sid ="58" ssid = "1">MLDOP (Bod 2000) extends DOP with amaximum likelihood reestimation technique basedon the expectation-maximization (EM) algorithm(Dempster et al. 1977) which is known to bestatistically consistent (Shao 1999).</S>
			<S sid ="59" ssid = "2">MLDOPreestimates DOP&apos;s subtree probabilities in aniterative way until the changes become negligible.The following exposition of MLDOP is heavilybased on previous work by Bod (2000) andMagerman (1993).</S>
			<S sid ="60" ssid = "3">It is important to realize that there is animplicit assumption in DOP that all possiblederivations of a parse tree contribute equally to the total probability of the parse tree.</S>
			<S sid ="61" ssid = "4">This is equivalentto saying that there is a hidden component to themodel, and that DOP can be trained using an EMalgorithm to determine the maximum likelihoodestimate for the training data.</S>
			<S sid ="62" ssid = "5">The EM algorithm forthis MLDOP model is related to the Inside-Outsidealgorithm for context-free grammars, but thereestimation formula is complicated by the presenceof subtrees of depth greater than 1.</S>
			<S sid ="63" ssid = "6">To derive thereestimation formula, it is useful to consider thestate space of all possible derivations of a tree.</S>
			<S sid ="64" ssid = "7">The derivations of a parse tree T can beviewed as a state trellis, where each state contains apartially constructed tree in the course of a leftmostderivation of T. st denotes a state containing the treet which is a subtree of T . The state trellis is definedas follows.</S>
			<S sid ="65" ssid = "8">The initial state, s0, is a tree with depth zero,consisting of simply a root node labeled with S. Thefinal state, sT, is the given parse tree T. A state st is connected forward to all statesstf such that tf = t ° t&apos;, for some t&apos; . Here theappropriate t&apos; is defined to be tf - t. A state s t is connected backward to all statesstb such that t = tb ° t&apos;, for some t&apos; . Again, t&apos; isdefined to be t - tb.</S>
			<S sid ="66" ssid = "9">The construction of the state lattice andassignment of transition probabilities according tothe MLDOP model is called the forward pass.</S>
			<S sid ="67" ssid = "10">Theprobability of a given state, P (s), is referred to asa(s).</S>
			<S sid ="68" ssid = "11">The forward probability of a state s t iscomputed recursively a(st) = S a(st ) P(t - tb).bs tb The backward probability of a state, referred to asß(s), is calculated according to the followingrecursive formula: ß(st) = S ß(st ) P(tf - t)ffst where the backward probability of the goal state isset equal to the forward probability of the goal state,ß(sT) = a(sT).</S>
			<S sid ="69" ssid = "12">The update formula for the count of asubtree t is (where r(t) is the root label of t): 868 ct(t) = S ß(st )a(st )P(t | r(t))f b a(sgoal)st :?st ,tb°t=tfb f The updated probability distribution, P&apos;(t | r(t)), isdefined to be P&apos; (t | r(t)) = ct(t)ct(r(t)) where ct(r(t)) is defined as ct(r(t)) = S ct(t&apos;)t &apos;: r( t&apos;)=r( t) In practice, MLDOP starts out by assigning thesame relative frequencies to the subtrees as DOP1,which are next reestimated by the formulas above.We may in principle start out with any initialparameters, including random initializations, butsince ML estimation is known to be very sensitiveto the initialization of the parameters, it is convenientto start with parameters that are known to performwell.</S>
			<S sid ="70" ssid = "13">To avoid overtraining, MLDOP uses thesubtrees from one half of the training set to betrained on the other half, and vice versa.</S>
			<S sid ="71" ssid = "14">This cross-training is important since otherwise UMLDOPwould assign the training set trees their empiricalfrequencies and assign zero weight to all othersubtrees (cf.</S>
			<S sid ="72" ssid = "15">Prescher et al. 2004).</S>
			<S sid ="73" ssid = "16">The updatedprobabilities are iteratively reestimated until thedecrease in cross-entropy becomes negligible.Unfortunately, no compact PCFG-reduction of MLDOP is known.</S>
			<S sid ="74" ssid = "17">As a consequence, parsing withMLDOP is very costly and the model has hithertonever been tested on corpora larger than OVIS(Bonnema et al. 1997).</S>
			<S sid ="75" ssid = "18">Yet, we will show that byclever pruning we can extend our experiments notonly to the WSJ, but also to the German NEGRAand the Chinese CTB.</S>
			<S sid ="76" ssid = "19">(Zollmann and Sima&apos;an 2005propose a different consistent estimator for DOP,which we cannot go into here).</S>
	</SECTION>
	<SECTION title="UML-DOP. " number = "5">
			<S sid ="77" ssid = "1">Analogous to U-DOP, UMLDOP is anunsupervised generalization of MLDOP: it firstassigns all unlabeled binary trees to a set of sentences and next extracts a large (random) set ofsubtrees from this tree set.</S>
			<S sid ="78" ssid = "2">It then reestimates theinitial probabilities of these subtrees by MLDOP onthe sentences from a held-out part of the tree set.The training is carried out by dividing the tree setinto two equal parts, and reestimating the subtreesfrom one part on the other.</S>
			<S sid ="79" ssid = "3">As initial probabilitieswe use the subtrees&apos; relative frequencies as describedin section 2 (smoothed by Good-Turing -- see Bod1998), though it would also be interesting to seehow the model works with other initial parameters,in particular with the usage frequencies proposed byZuidema (2006).</S>
			<S sid ="80" ssid = "4">As with U-DOP, the total number ofsubtrees that can be extracted from the binary treeset is too large to be fully taken into account.Together with the high computational cost ofreestimation we propose even more drastic pruningthan we did in Bod (2006) for U-DOP.</S>
			<S sid ="81" ssid = "5">That is,while for sentences = 7 words we use all binarytrees, for each sentence = 8 words we randomlysample a fixed number of 128 trees (whicheffectively favors more frequent trees).</S>
			<S sid ="82" ssid = "6">The resultingset of trees is referred to as the binary tree set.</S>
			<S sid ="83" ssid = "7">Next,we randomly extract for each subtree-depth a fixednumber of subtrees, where the depth of subtree isthe longest path from root to any leaf.</S>
			<S sid ="84" ssid = "8">This hasroughly the same effect as the correction factor usedin Bod (2003, 2006).</S>
			<S sid ="85" ssid = "9">That is, for each particulardepth we sample subtrees by first randomlyselecting a node in a random tree from the binarytree set after which we select random expansionsfrom that node until a subtree of the particular depthis obtained.</S>
			<S sid ="86" ssid = "10">For our experiments in section 6, werepeated this procedure 200,000 times for eachdepth.</S>
			<S sid ="87" ssid = "11">The resulting subtrees are then given to MLDOP&apos;s reestimation procedure.</S>
			<S sid ="88" ssid = "12">Finally, thereestimated subtrees are used to compute the mostprobable parse trees for all sentences using Viterbin-best, as described in section 3, where the mostprobable parse is estimated from the 100 mostprobable derivations.</S>
			<S sid ="89" ssid = "13">A potential criticism of (U)MLDOP is thatsince we use DOP1&apos;s relative frequencies as initialparameters, MLDOP may only find a localmaximum nearest to DOP1&apos;s estimator.</S>
			<S sid ="90" ssid = "14">But this isof course a criticism against any iterative MLapproach: it is not guaranteed that the globalmaximum is found (cf.</S>
			<S sid ="91" ssid = "15">Manning and Schütze 1999:401).</S>
			<S sid ="92" ssid = "16">Nevertheless we will see that our reestimation 869 procedure leads to significantly better accuracycompared to U-DOP (the latter would be equal toUMLDOP under 0 iterations).</S>
			<S sid ="93" ssid = "17">Moreover, incontrast to U-DOP, UMLDOP can be theoreticallymotivated: it maximizes the likelihood of the datausing the statistically consistent EM algorithm.</S>
	</SECTION>
	<SECTION title="Experiments: Can we beat supervised byunsupervised parsing?. " number = "6">
			<S sid ="94" ssid = "1">To compare UMLDOP to U-DOP, we started outwith the WSJ10 corpus, which contains 7422sentences = 10 words after removing emptyelements and punctuation.</S>
			<S sid ="95" ssid = "2">We used the sameevaluation metrics for unlabeled precision (UP) andunlabeled recall (UR) as defined in Klein (2005: 2122).</S>
			<S sid ="96" ssid = "3">Klein&apos;s definitions differ slightly from thestandard PARSEVAL metrics: multiplicity ofbrackets is ignored, brackets of span one are ignoredand the bracket labels are ignored.</S>
			<S sid ="97" ssid = "4">The two metricsof UP and UR are combined by the unlabeled f -score F1 which is defined as the harmonic mean ofUP and UR: F1 = 2*UP*UR/(UP+UR).</S>
			<S sid ="98" ssid = "5">For the WSJ10, we obtained a binary treeset of 5.68 * 105 trees, by extracting the binary treesas described in section 5.</S>
			<S sid ="99" ssid = "6">From this binary tree setwe sampled 200,000 subtrees for each subtree-depth.</S>
			<S sid ="100" ssid = "7">This resulted in a total set of roughly 1.7 *1 0 6 subtrees that were reestimated by ourmaximum-likelihood procedure.</S>
			<S sid ="101" ssid = "8">The decrease incross-entropy became negligible after 14 iterations(for both halfs of WSJ10).</S>
			<S sid ="102" ssid = "9">After computing themost probable parse trees, UMLDOP achieved anf-score of 82.9% which is a 20.5% error reductioncompared to U-DOP&apos;s f-score of 78.5% on thesame data (Bod 2006).</S>
			<S sid ="103" ssid = "10">We next tested UMLDOP on twoadditional domains which were also used in Kleinand Manning (2004) and Bod (2006): the GermanNEGRA10 (Skut et al. 1997) and the ChineseCTB10 (Xue et al. 2002) both containing 2200+sentences = 10 words after removing punctuation.Table 1 shows the results of UMLDOP comparedto U-DOP, the CCM model by Klein and Manning(2002), the DMV dependency learning model byKlein and Manning (2004) as well as theircombined model DMV+CCM.</S>
			<S sid ="104" ssid = "11">Table 1 shows that UMLDOP scores betterthan U-DOP and Klein and Manning&apos;s models in allcases.</S>
			<S sid ="105" ssid = "12">It thus pays off to not only use subtrees rather than substrings (as in CCM) but to also reestimatethe subtrees&apos; probabilities by a maximum-likelihoodprocedure rather than using their (smoothed) relativefrequencies (as in U-DOP).</S>
			<S sid ="106" ssid = "13">Note that UMLDOPachieves these improved results with fewer subtreesthan U-DOP, due to UMLDOP&apos;s more drasticpruning of subtrees.</S>
			<S sid ="107" ssid = "14">It is also noteworthy that UMLDOP, like U-DOP, does not employ a separate classfor non-constituents, so-called distituents, whileCCM and CCM+DMV do.</S>
			<S sid ="108" ssid = "15">(Interestingly, the top10 most frequently learned constituents by UMLDOP were exactly the same as by U-DOP -- see therelevant table in Bod 2006).</S>
			<S sid ="109" ssid = "16">Model English German Chinese(WSJ10) (NEGRA10) (CTB10)CCM 71.9 61.6 45.0DMV 52.1 49.5 46.7DMV+CCM 77.6 63.9 43.3UDOP 78.5 65.4 46.6UMLDOP 82.9 67.0 47.2 Table 1.</S>
			<S sid ="110" ssid = "17">F-scores of UMLDOP compared toprevious models on the same data We were also interested in testing UMLDOP onlonger sentences.</S>
			<S sid ="111" ssid = "18">We therefore included all WSJsentences up to 40 words after removing emptyelements and punctuation (WSJ40) and againsampled 200,000 subtrees for each depth, using thesame method as before.</S>
			<S sid ="112" ssid = "19">Furthermore, we comparedUMLDOP against a supervised binarized PCFG,i.e. a treebank PCFG whose simple relativefrequency estimator corresponds to maximumlikelihood (Chi and Geman 1998), and which weshall refer to as &quot;MLPCFG&quot;.</S>
			<S sid ="113" ssid = "20">To this end, we useda random 90%/10% division of WSJ40 into atraining set and a test set.</S>
			<S sid ="114" ssid = "21">The MLPCFG had thusaccess to the Penn WSJ trees in the training set,while UMLDOP had to bootstrap all structure fromthe flat strings from the training set to next parse the10% test set -- clearly a much more challengingtask.</S>
			<S sid ="115" ssid = "22">Table 2 gives the results in terms of f-scores.The table shows that UMLDOP scoresbetter than U-DOP, also for WSJ40.</S>
			<S sid ="116" ssid = "23">Our results onWSJ10 are somewhat lower than in table 1 due tothe use of a smaller training set of 90% of the data.But the most surprising result is that UMLDOP&apos;s f 870 score is higher than the supervised binarized tree-bank PCFG (MLPCFG) for both WSJ10 andWSJ40.</S>
			<S sid ="117" ssid = "24">In order to check whether this difference isstatistically significant, we additionally tested on 10different 90/10 divisions of the WSJ40 (which werethe same splits as in Bod 2006).</S>
			<S sid ="118" ssid = "25">For these splits,UMLDOP achieved an average f-score of 66.9%,while MLPCFG obtained an average f-score of64.7%.</S>
			<S sid ="119" ssid = "26">The difference in accuracy between UMLDOP and MLPCFG was statistically significantaccording to paired t-testing (p=0.05).</S>
			<S sid ="120" ssid = "27">To the best ofour knowledge this means that we have shown forthe first time that an unsupervised parsing model(UMLDOP) outperforms a widely used supervisedparsing model (a treebank PCFG) on the WSJ40 . Model WSJ10 WSJ40 U-DOP 78.1 63.9UMLDOP 82.5 66.4MLPCFG 81.5 64.6 Table 2.</S>
			<S sid ="121" ssid = "28">F-scores of U-DOP, UMLDOP and asupervised treebank PCFG (MLPCFG) for a random 90/10 split of WSJ10 and WSJ40.</S>
			<S sid ="122" ssid = "29">We should keep in mind that (1) a treebank PCFGis not state-of-the-art: its performance is mediocrecompared to e.g. Bod (2003) or McClosky et al.(2006), and (2) that our treebank PCFG is binarizedas in Klein and Manning (2005) to make resultscomparable.</S>
			<S sid ="123" ssid = "30">To be sure, the unbinarized version ofthe treebank PCFG obtains 89.0% average f-scoreon WSJ10 and 72.3% average f-score on WSJ40.Remember that the Penn Treebank annotations areoften exceedingly flat, and many branches have aritylarger than two.</S>
			<S sid ="124" ssid = "31">It would be interesting to see howUMLDOP performs if we also accept ternary (andwider) branches -- though the total number ofpossible trees that can be assigned to strings wouldthen further explode.</S>
			<S sid ="125" ssid = "32">UMLDOP&apos;s performance still remainsbehind that of supervised (binarized) DOP parsers,such as DOP1, which achieved 81.9% average f-score on the 10 WSJ40 splits, and MLDOP, whichperformed slightly better with 82.1% average f-score.</S>
			<S sid ="126" ssid = "33">And if DOP1 and MLDOP are notbinarized, their average f-scores are respectively90.1% and 90.5% on WSJ40.</S>
			<S sid ="127" ssid = "34">However, DOP1 and MLDOP heavily depend on annotated data whereasUMLDOP only needs unannotated data.</S>
			<S sid ="128" ssid = "35">It wouldthus be interesting to see how close UMLDOP canget to MLDOP&apos;s performance if we enlarge theamount of training data.</S>
	</SECTION>
	<SECTION title="Conclusion: Is the end of supervisedparsing in sight?. " number = "7">
			<S sid ="129" ssid = "1">Now that we have outperformed a well-knownsupervised parser by an unsupervised one, we mayraise the question as to whether the end ofsupervised NLP comes in sight.</S>
			<S sid ="130" ssid = "2">All supervisedparsers are reaching an asymptote and furtherimprovement does not seem to come from morehand-annotated data but by adding unsupervised orsemi-unsupervised techniques (cf.</S>
			<S sid ="131" ssid = "3">McClosky et al.2006).</S>
			<S sid ="132" ssid = "4">Thus if we modify our question as: does theexclusively supervised approach to parsing come toan end, we believe that the answer is certainly yes.</S>
			<S sid ="133" ssid = "5">Yet we should neither rule out thepossibility that entirely unsupervised methods willin fact surpass semi-supervised methods.</S>
			<S sid ="134" ssid = "6">The mainproblem is how to quantitatively compare thesedifferent parsers, as any evaluation on hand-annotated data (like the Penn treebank) willunreasonably favor semi-supervised parsers.</S>
			<S sid ="135" ssid = "7">Thereis thus is a quest for designing an annotation-independent evaluation scheme.</S>
			<S sid ="136" ssid = "8">Since parsers arebecoming increasingly important in applications likesyntax-based machine translation and structurallanguage models for speech recognition, one way togo would be to compare these different parsingmethods by isolating their contribution in improvinga concrete NLP system, rather than by testing themagainst gold standard annotations which areinherently theory-dependent.</S>
			<S sid ="137" ssid = "9">The initially disappointing results ofinducing trees entirely from raw text was not somuch due to the difficulty of the bootstrappingproblem per se , but to (1) the poverty of the initialmodels and (2) the difficulty of finding theory-independent evaluation criteria.</S>
			<S sid ="138" ssid = "10">The time has cometo fully reappraise unsupervised parsing modelswhich should be trained on massive amounts ofdata, and be evaluated in a concrete application.</S>
			<S sid ="139" ssid = "11">There is a final question as to how far theDOP approach to unsupervised parsing can bestretched.</S>
			<S sid ="140" ssid = "12">In principle we can assign all possiblesyntactic categories, semantic roles, argument 871 structures etc. to a set of given sentences and let thestatistics decide which assignments are most usefulin parsing new sentences.</S>
			<S sid ="141" ssid = "13">Whether such a massivelymaximalist approach is feasible can only beanswered by empirical investigation in due time.</S>
	</SECTION>
	<SECTION title="Acknowledgements"></SECTION>
</PAPER>
