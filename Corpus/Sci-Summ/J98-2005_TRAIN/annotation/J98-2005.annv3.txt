Citance Number: 1 | Reference Article:  J98-2005.xml | Citing Article:  P06-1109.xml | Citation Marker Offset:  ['112'] | Citation Marker:  Chi and Geman 1998 | Citation Offset:  ['112'] | Citation Text: <S sid ="112" ssid = "19">a treebank PCFG whose simple relativefrequency estimator corresponds to maximumlikelihood (Chi and Geman 1998), and which weshall refer to as &quot;MLPCFG&quot;.</S> | Reference Offset:  ['16', '17'] | Reference Text:  <S sid ="16" ssid = "16">If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?</S><S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 2 | Reference Article:  J98-2005.xml | Citing Article:  N06-1043.xml | Citation Marker Offset:  ['85'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['85'] | Citation Text:  <S sid ="85" ssid = "43">Theline of our argument below follows a proof providedin (Chi and Geman, 1998) for the maximum like lihood estimator based on finite tree distributions.</S> | Reference Offset:  ['17'] | Reference Text:  <S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 3 | Reference Article:  J98-2005.xml | Citing Article:  N06-1043.xml | Citation Marker Offset:  ['159'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['159'] | Citation Text:  <S sid ="159" ssid = "14">Such maximization provides the estimator (see for instance (Chi and Geman, 1998)) pG(A? a) =f(A? a, T ) f(A, T ).</S> | Reference Offset:  ['45'] | Reference Text:  <S sid ="45" ssid = "9">(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.</S> | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 4 | Reference Article:  J98-2005.xml | Citing Article:  N06-1043.xml | Citation Marker Offset:  ['167'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['167'] | Citation Text:  <S sid ="167" ssid = "22">This resulthas been firstly shown in (Chaudhuri et al., 1983) and later, with a different proof technique, in (Chi and Geman, 1998).</S> | Reference Offset:  ['16', '17'] | Reference Text:  <S sid ="16" ssid = "16">If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?</S><S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 5 | Reference Article:  J98-2005.xml | Citing Article:  J01-2004.xml | Citation Marker Offset:  ['72'] | Citation Marker:  1998 | Citation Offset:  ['72'] | Citation Text: <S sid ="72" ssid = "37">Chi and Geman (1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.</S> | Reference Offset:  ['17'] | Reference Text:  <S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 6 | Reference Article:  J98-2005.xml | Citing Article:  N03-1027.xml | Citation Marker Offset:  ['56'] | Citation Marker:  1998 | Citation Offset:  ['56'] | Citation Text:  <S sid ="56" ssid = "19">Chi and Geman (1998) proved that this con­dition is met if the rule probabilities are estimated using relative frequency estimation from a corpus.</S> | Reference Offset:  ['45'] | Reference Text:  <S sid ="45" ssid = "9">(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.</S> | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 7 | Reference Article:  J98-2005.xml | Citing Article:  J98-4004.xml | Citation Marker Offset:  ['34'] | Citation Marker:  Chi and Geman 1998 | Citation Offset:  ['34'] | Citation Text: 	<S sid ="34" ssid = "3">PCFGs estimated from tree banks using the relative frequency estimator always satisfy (Chi and Geman 1998).</S> | Reference Offset:  ['17'] | Reference Text:  <S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 8 | Reference Article:  J98-2005.xml | Citing Article:  P01-1017.xml | Citation Marker Offset:  ['79'] | Citation Marker:  5 | Citation Offset:  ['79'] | Citation Text: <S sid ="79" ssid = "32">When a PCFG probability distribu­tion is estimated from training data (in our case the Penn tree-bank) PCFGs de.ne a tight (sum­ming to one) probability distribution over strings [5], thus making them appropriate for language models.</S> | Reference Offset:  ['17'] | Reference Text:  <S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 9 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['9'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['9'] | Citation Text: <S sid ="9" ssid = "9">In later work by (Sa´nchez and Benedi´, 1997)and (Chi and Geman, 1998), the result was in dependently extended to expectation maximization,which is an unsupervised method exploited to estimate probabilistic context-free grammars by find ing local maxima of the likelihood of a sample of unannotated sentences.</S> | Reference Offset:  ['70'] | Reference Text:  <S sid ="70" ssid = "34">Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 10 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['10'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['10'] | Citation Text:  <S sid ="10" ssid = "10">the proof in (Chi and Ge man, 1998) is based on a simpler counting argument.Both these proofs assume restrictions on the un derlying context-free grammars.</S> | Reference Offset:  ['45'] | Reference Text:  <S sid ="45" ssid = "9">(8~fl)ea ~(B --~/3) = ~=lf(B --~/3;cai) (3) c~ s.t. H &lt;B-~)e~ ~i=lf(B ---+o4cai) The maximum-likelihood estimator is the natural, &quot;relative frequency,&quot; estimator.</S> | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 11 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['11'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['11'] | Citation Text:  <S sid ="11" ssid = "11">More specifically, in (Chi and Geman, 1998) empty rules and unaryrules are not allowed, thus excluding infinite ambi guity, that is, the possibility that some string in the input sample has an infinite number of derivations inthe grammar.</S> | Reference Offset:  ['0'] | Reference Text:  <S sid ="0">Estimation of Probabilistic Context-Free Grammars</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 12 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['58'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['58'] | Citation Text:  <S sid ="58" ssid = "7">d?D pG(d)f(d,D), (6) subject to the properness conditions?a pG(A ?a) = 1 for eachA ? N . The maximization problemabove has a unique solution, provided by the estima tor (see for instance (Chi and Geman, 1998)) pG(A? a) =f(A? a,D) f(A,D).</S> | Reference Offset:  ['70'] | Reference Text:  <S sid ="70" ssid = "34">Dempster, Laird, and Rubin [1977] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization.</S> | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 13 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['64'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['64'] | Citation Text:  <S sid ="64" ssid = "13">The above maximization prob lem provides a system of R nonlinear equations(see (Chi and Geman, 1998)) pG(A? a) =?w?C f(w, C) · EpG(d w) f(A? a, d)? </S> | Reference Offset:  ['65'] | Reference Text:  <S sid ="65" ssid = "29">Letting ~y denote {w Efk Y(w) = Y}, the likelihood of the corpus becomes n H E H P(A--&apos;~oL)f(A~;~)&quot; i=1 ~OE~y(~i) (A---~o~)ER And the maximum-likelihood equation becomes + p(B fl) Ei=l EwEfly(wi,I-I(A--.)cR p(A -~ a)f(A-~&quot;;~) = 0 fT(B ~ /3) = ~iL1 Ep~f(B ~ fl;w)lw E ~y(~,)] (4) ,~s,,, ~Ei=IEpV(&quot; a;w)lw E ~Y(o~,)] E(B_~,E B ~ where E~ is expectation under fi and where &quot;]w E~-~y(wi)&quot; means &quot;conditioned on 0.2E ~-~Y(wi)&apos;&quot; There is no hope for a closed form solution, but (4) does suggest an iteration scheme, which, as it turns out, &quot;climbs&quot; the likelihood surface (though there are no guarantees about approaching a global maximum): Let P0 be an arbitrary assignment respecting (1).</S> | Discourse Facet:  Method_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 14 | Reference Article:  J98-2005.xml | Citing Article:  N06-1044.xml | Citation Marker Offset:  ['178'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['178'] | Citation Text:  <S sid ="178" ssid = "27">This solves a problem that was left open in the literature (Chi and Geman,1998) </S> | Reference Offset:  ['16', '17'] | Reference Text:  <S sid ="16" ssid = "16">If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?</S><S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 15 | Reference Article:  J98-2005.xml | Citing Article:  J99-1004.xml | Citation Marker Offset:  ['33'] | Citation Marker:  1998 | Citation Offset:  ['33'] | Citation Text:  <S sid ="33" ssid = "33">A suffi­ cient condition for proper assignment is established by Chi and Geman (1998), who prove that production probabilities estimated by the maximum-likelihood (ML) esti­ mation procedure (or relative frequency estimation procedure, as it is called in com­ putational linguistics) always impose proper PCFG distributions.</S> | Reference Offset:  ['17'] | Reference Text:  <S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 16 | Reference Article:  J98-2005.xml | Citing Article:  J99-1004.xml | Citation Marker Offset:  ['97'] | Citation Marker:  1998 | Citation Offset:  ['97'] | Citation Text:  <S sid ="97" ssid = "97">Chi and Geman (1998) proved the properness of PCFG distributions imposed by esti­ mated production probabilities</S> | Reference Offset:  ['16', '17'] | Reference Text:  <S sid ="16" ssid = "16">If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?</S><S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 17 | Reference Article:  J98-2005.xml | Citing Article:  J99-1004.xml | Citation Marker Offset:  ['170'] | Citation Marker:  1998 | Citation Offset:  ['170'] | Citation Text:  <S sid ="170" ssid = "170">This simple estimator, as shown by Chi and Geman (1998), assigns proper production prob­ abilities for PCFGs.</S> | Reference Offset:  ['1'] | Reference Text:  <S sid ="1" ssid = "1">The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution: the probability of all finite parse trees is less than one.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 18 | Reference Article:  J98-2005.xml | Citing Article:  J99-1004.xml | Citation Marker Offset:  ['186'] | Citation Marker:  1998 | Citation Offset:  ['186'] | Citation Text:  <S sid ="186" ssid = "186">Proof The proof is almost identical to the one given by Chi and Ceman (1998).</S> | Reference Offset:  ['0'] | Reference Text:  <S sid="0">Estimation of Probabilistic Context-Free Grammars</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 19 | Reference Article:  J98-2005.xml | Citing Article:  J99-1004.xml | Citation Marker Offset:  ['177'] | Citation Marker:  Chi and Geman, 1998 | Citation Offset:  ['177'] | Citation Text:  <S sid ="177" ssid = "177"> impose proper probability distributions on D (Chi and Geman 1998).</S> | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">We show here that estimated production probabilities always yield proper distributions.</S> | Discourse Facet:  Results_Citation | Annotator:  Ankur Khanna, NUS |

Citance Number: 20 | Reference Article:  J98-2005.xml | Citing Article:  P13-1102.xml | Citation Marker Offset:  ['21'] | Citation Marker:  1998 | Citation Offset:  ['21'] | Citation Text:  <S sid ="21" ssid = "21">we follow Chi and Geman (1998)in calling them “non-tight” to avoid confusionwith the consistency of statistical estimators).</S> | Reference Offset:  ['0'] | Reference Text:  <S sid="0">Estimation of Probabilistic Context-Free Grammars</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


Citance Number: 21 | Reference Article:  J98-2005.xml | Citing Article:  P13-1102.xml | Citation Marker Offset:  ['23'] | Citation Marker:  1998 | Citation Offset:  ['23'] | Citation Text:  <S sid ="23" ssid = "23">Chi and Geman(1998) studied the question for Maximum Likelihood (ML) estimation, and showed that ML es 1033 timates are always tight for both the supervisedcase (where the input consists of parse trees) andthe unsupervised case (where the input consists ofyields or terminal strings).</S> | Reference Offset:  ['16', '17'] | Reference Text:  <S sid ="16" ssid = "16">If the corpus is unparsed then there is an iterative approach to maximum-likelihood estimation (the EM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: do we get actual probabilities or do the estimated PCFG&apos;s assign some mass to infinite trees?</S><S sid ="17" ssid = "17">We will show that in both cases the estimated probability is tight.</S> | Discourse Facet:  Aim_Citation | Annotator:  Ankur Khanna, NUS |


