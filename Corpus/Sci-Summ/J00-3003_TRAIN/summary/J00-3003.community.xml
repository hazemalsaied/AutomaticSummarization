<COMMUNITY>
<S sid ="22" ssid = "22">While there is hardly consensus on exactly how discourse structure should be described, some agreement exists that a useful first level of analysis involves the identification of dialogue acts (DAs).</S><S sid ="27" ssid = "27">Thus, DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic, semantic, and syntactic criteria.</S>
<S sid ="32" ssid = "32">Interactional dominance (Linell 1990) might be measured more accurately using DA distributions than with simpler techniques, and could serve as an indicator of the type or genre of discourse at hand.</S><S sid ="33" ssid = "33">In all these cases, DA labels would enrich the available input for higher-level processing of the spoken words.</S>
<S sid ="38" ssid = "38">Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS, OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me, I&apos;m in the legal department.</S>
<S sid ="64" ssid = "64">&lt;.1% Hey thanks a lot &lt;.1% The goal of this article is twofold: On the one hand, we aim to present a comprehensive framework for modeling and automatic classification of DAs, founded on well-known statistical methods.</S>
<S sid ="66" ssid = "66">For example, our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work, such as Nagata and Morimoto (1993, 1994) and Woszczyna and Waibel (1994) (see Section 7).</S>
<S sid ="67" ssid = "67">However, our framework generalizes earlier models, giving us a clean probabilistic approach for performing DA classification from unreliable words and nonlexical evidence.</S>
<S sid ="68" ssid = "68">For the speech recognition task, our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialogue structure, as well as on nonlexical information correlated with DA identity.</S><S sid ="71" ssid = "71">Second, we present results obtained with this approach on a large, widely available corpus of spontaneous conversational speech.</S>
<S sid ="81" ssid = "1">The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech (Godfrey, Holliman, and McDaniel 1992) distributed by the Linguistic Data Consortium.</S>
<S sid ="188" ssid = "11">The computation of likelihoods P(EIU ) depends on the types of evidence used.</S><S sid ="192" ssid = "15">Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch, duration, energy, etc., of the speech signal; the associated likelihoods are P(F I U).</S>
<S sid ="214" ssid = "37">The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and Juang 1986).</S>
<S sid ="215" ssid = "38">The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(Eil Ui).</S>
<S sid ="358" ssid = "89">We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here.</S>
<S sid ="392" ssid = "123">Table 9 Combined utterance classification accuracies (chance = 35%).</S>
<S sid ="394" ssid = "125">Discourse Grammar Accuracy (%) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy (in %) for individual and combined models for two subtasks, using uniform priors (chance = 50%).</S>
<S sid ="155" ssid = "75">A backchannel is a short utterance that plays discourse-structuring roles, e.g., indicating that the speaker should go on talking.</S><S sid ="156" ssid = "76">These are usually referred to in the conversation analysis literature as &quot;continuers&quot; and have been studied extensively (Jefferson 1984; Schegloff 1982; Yngve 1970).</S>
</COMMUNITY>