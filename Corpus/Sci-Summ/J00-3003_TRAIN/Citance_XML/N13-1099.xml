<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">We present a method of improving the performance of dialog act tagging in identifying minority classes by using per-class feature optimization and a method of choosing the classbased not on confidence, but on a cascade ofclassifiers.</S>
		<S sid ="2" ssid = "2">We show that it gives a minority class F-measure error reduction of 22.8%,while also reducing the error for other classesand the overall error by about 10%.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="3" ssid = "3">In this paper, we discuss dialog act tagging, thetask of assigning a dialog act to an utterance, wherea dialog act (DA) is a high-level categorization ofthe pragmatic meaning of the utterance.</S>
			<S sid ="4" ssid = "4">Our data isemail.</S>
			<S sid ="5" ssid = "5">Our starting point is the tagger described in(Hu et al., 2009), which uses a standard multiclassclassifier based on support vector machines (SVMs).While the performance of this system is pretty goodas measured by accuracy, it performs badly on theDA REQUEST-ACTION, which is a rare class.</S>
			<S sid ="6" ssid = "6">Multi-class SVMs are typically implemented as a set ofSVMs, one per class, with the overall choice of classbeing determined by the SVM with the highest confidence (“one-against-all”).</S>
			<S sid ="7" ssid = "7">Multi-class SVMs aretypically packaged as a single system, whose innerworkings are ignored by the NLP researcher.</S>
			<S sid ="8" ssid = "8">In thispaper we show that, for our problem of DA classification, we can boost the performance of the rareclasses (while maintaining the overall performance)by performing feature optimization separately foreach individual classifier.</S>
			<S sid ="9" ssid = "9">But we also show that we can achieve an all-around error reduction by altering the method by which the multi-class classifiercombines the individual SVMs.</S>
			<S sid ="10" ssid = "10">This new methodof combination is a simple cascade: we run the individual classifiers in ascending order of frequencyof the classes in the training corpus; the first classifier to classify the data point positively determinesthe choice of the overall classifier.</S>
			<S sid ="11" ssid = "11">If no classifierclassifies the data point positively, we use the usualconfidence-based method.</S>
			<S sid ="12" ssid = "12">This new method obtainsa 22.8% error reduction for the minority class, andaround 10% error reduction for the other classes andfor the overall classifier.</S>
			<S sid ="13" ssid = "13">This paper is structured as follows.</S>
			<S sid ="14" ssid = "14">We start outby discussing related work (Section 2).</S>
			<S sid ="15" ssid = "15">We thenpresent our data in Section 3, and in Section 4 wepresent the experiments with our systems and the results.</S>
			<S sid ="16" ssid = "16">We report the results of an extrinsic evaluationin Section 5, and conclude.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="17" ssid = "1">Dialog act (DA) annotations and tagging, inspiredby the speech act theory of Austin (1975) and Searle(1976), have been used in the NLP community to understand and model dialog.</S>
			<S sid ="18" ssid = "2">Initial work was done onspoken interactions (see for example (Stolcke et al.,2000)).</S>
			<S sid ="19" ssid = "3">Recently, studies have explored dialog acttagging in written interactions such as emails (Cohen et al., 2004), forums (Kim et al., 2006; Kim etal., 2010b), instant messaging (Kim et al., 2010a)and Twitter (Zhang et al., 2012).</S>
			<S sid ="20" ssid = "4">Most DA taggingsystems for written interactions use a message/postlevel tagging scheme, and allow multiple tags foreach message/post.</S>
			<S sid ="21" ssid = "5">In such a tagging scheme, indi 802 vidual binary classifiers for each tag are independentof one another.</S>
			<S sid ="22" ssid = "6">However, recent studies have foundmerit in segmenting each message into functionalunits and assigning a single DA to each segment (Huet al., 2009).</S>
			<S sid ="23" ssid = "7">Our work falls in this paradigm (wechoose a single DA for smaller textual units).</S>
			<S sid ="24" ssid = "8">Webuild on the work by (Hu et al., 2009); we improvetheir dialog act predicting performance on minorityclasses using per-class feature optimization.</S>
	</SECTION>
	<SECTION title="Data. " number = "3">
			<S sid ="25" ssid = "1">In this study, we use the email corpus presented in(Hu et al., 2009), which is manually annotated forDA tags.</S>
			<S sid ="26" ssid = "2">The corpus contains 122 email threadswith a total of 360 messages and 20,740 word tokens.</S>
			<S sid ="27" ssid = "3">This set of email threads is chosen from a version of the Enron email corpus with some missingmessages restored from other emails in which theywere quoted (Yeh and Harnly, 2006; Agarwal et al.,2012).</S>
			<S sid ="28" ssid = "4">Most emails are concerned with exchanginginformation, scheduling meetings, or solving problems, but there are also purely social emails.</S>
			<S sid ="29" ssid = "5">Dialog Act Tag Count (%)REQUEST-ACTION (R-A) 35 (2.5%)REQUEST-INFORMATION (R-I) 151 (10.7%)CONVENTIONAL (CONV) 357 (25.4%)INFORM (INF) 853 (60.7%)Total # of DFUs 1406 Table 1: Annotation statistics Each message in the thread is segmented into Dialog Functional Units (DFUs).</S>
			<S sid ="30" ssid = "6">A DFU is a contiguous span within an email message which hasa coherent communicative intention.</S>
			<S sid ="31" ssid = "7">Each DFUis assigned a single DA label which is one of thefollowing: REQUEST-ACTION (R-A), REQUEST-INFORMATION (R-I), CONVENTIONAL (CONV)and INFORM (INF).</S>
			<S sid ="32" ssid = "8">There are three other DA labels— INFORM-OFFLINE, COMMIT, and NODA for nodialog act — which occurred 5 or fewer times in thecorpus.</S>
			<S sid ="33" ssid = "9">We ignore these DA labels in this paper.</S>
			<S sid ="34" ssid = "10">Thecorpus also contains links between the DFUs, but wedo not use those annotations in this study.</S>
			<S sid ="35" ssid = "11">Table 1presents the distribution of DA labels in our corpus.We now describe each of the DAs we consider in ourexperiments.</S>
			<S sid ="36" ssid = "12">In a REQUEST-ACTION, the writer signalsher desire that the reader perform some non-communicative act, i.e., an act that cannot in itselfbe part of the dialogue.</S>
			<S sid ="37" ssid = "13">For example, a writer canask the reader to write a report or make coffee.</S>
			<S sid ="38" ssid = "14">In a REQUEST-INFORMATION, the writer signalsher desire that the reader perform a specific communicative act, namely that he provide information(either facts or opinion).</S>
			<S sid ="39" ssid = "15">In an INFORM, the writer conveys information, ormore precisely, the writer signals that her desire thatthe reader adopt a certain belief.</S>
			<S sid ="40" ssid = "16">It covers many different types of information that can be conveyed including answers to questions, beliefs (committed ornot), attitudes, and elaborations on prior DAs.</S>
			<S sid ="41" ssid = "17">A CONVENTIONAL dialog act does not signal anyspecific communicative intention on the part of thewriter, but rather it helps structure and thus facilitatethe communication.</S>
			<S sid ="42" ssid = "18">Examples include greetings, introductions, expressions of gratitude, etc.</S>
	</SECTION>
	<SECTION title="System. " number = "4">
			<S sid ="43" ssid = "1">We developed four systems for our experiments: abaseline (BAS) system which is close to the systemdescribed in (Hu et al., 2009), and three variants ofour novel divide and conquer (DAC) system.</S>
			<S sid ="44" ssid = "2">Features used in both systems are extracted as explainedin Section 4.2.</S>
			<S sid ="45" ssid = "3">Section 4.3 describes the baselinesystem, the basic DAC system, and two variationsof the DAC system.</S>
			<S sid ="46" ssid = "4">4.1 Experimental FrameworkIn all our experiments, we use linear kernel Support Vector Machines (SVM).</S>
			<S sid ="47" ssid = "5">However, across thesystems, there are differences in how we use them.Our framework was built with the ClearTK toolkit(Ogren et al., 2008) with its wrapper for SVMLight(Joachims, 1999).</S>
			<S sid ="48" ssid = "6">The ClearTK wrapper internallyshifts the prediction threshold based on posteriorprobabilistic scores calculated using the algorithmof Lin et al.</S>
			<S sid ="49" ssid = "7">(2007).</S>
			<S sid ="50" ssid = "8">We report results from 5foldcross validation performed on the entire corpus..</S>
			<S sid ="51" ssid = "9">4.2 Feature EngineeringIn developing our system, we classified our featuresinto three categories: lexical, verbal and message-.</S>
			<S sid ="52" ssid = "10">803 level.</S>
			<S sid ="53" ssid = "11">Lexical features consists of n-grams of words,n-grams of POS tags, mixed n-grams of closed classwords and POS tags (Prabhakaran et al., 2012), aswell as a small set of specialized features — Start-POS/Lemma (POS tag and lemma of the first word),LastPOS/Lemma (POS tag and lemma of the lastword), MDCount (number of modal verbs in theDFU) and QuestionMark (is there a question markin the DFU).</S>
			<S sid ="54" ssid = "12">We used the POS tags produced by theOpenNLP POS tagger.</S>
			<S sid ="55" ssid = "13">Verbal features capture theposition and identity of the first verb in the DFU.</S>
			<S sid ="56" ssid = "14">Finally, message-level features capture aspects of thelocation of the DFU in the message and of the message in the thread (relative position and size).</S>
			<S sid ="57" ssid = "15">Inoptimizing each system, we first performed an exhaustive search across all combinations of featureswithin each category.</S>
			<S sid ="58" ssid = "16">For the lexical n-gram features we varied the n-gram window from 1 to 5.</S>
			<S sid ="59" ssid = "17">Thisstep gave us the best performing feature combinationwithin each category.</S>
			<S sid ="60" ssid = "18">In a second step, we found thebest combination of categories, using the previouslydetermined features for each category.</S>
			<S sid ="61" ssid = "19">In this paper, we do not report best performing feature setsfor each configuration, due to lack of space.</S>
			<S sid ="62" ssid = "20">4.3 Experiments.</S>
			<S sid ="63" ssid = "21">Baseline (BAS) System This system uses theClearTK built-in one-versus-all multiclass SVM inprediction.</S>
			<S sid ="64" ssid = "22">Internally, the multi-class SVM buildsa set of binary classifiers, one for each dialog act.For a given test instance, the classifier that obtainsthe highest probability score determines the overallprediction.</S>
			<S sid ="65" ssid = "23">We performed feature optimization onthe whole multiclass classifier (as described in Section 4.2), i.e., the same set of features was availableto all component classifiers.</S>
			<S sid ="66" ssid = "24">We optimized for system accuracy.</S>
			<S sid ="67" ssid = "25">Table 2 shows results using this system.</S>
			<S sid ="68" ssid = "26">In this and all tables, we give the performanceof the system on the four DAs, using precision, recall, and F-measure.</S>
			<S sid ="69" ssid = "27">The DAs are listed in ascending order of frequency in the corpus (least frequentDA first).</S>
			<S sid ="70" ssid = "28">We also give an overall accuracy evaluation.</S>
			<S sid ="71" ssid = "29">As we can see, detecting REQUEST-ACTION ismuch harder than detecting the other DAs.</S>
			<S sid ="72" ssid = "30">Basic Divide and Conquer (DAC) System Likethe BAS system, the DAC system also builds a binary classifier for each dialog act separately, and the Prec.</S>
			<S sid ="73" ssid = "31">Rec.</S>
			<S sid ="74" ssid = "32">F-meas.R-A 57.9 31.4 40.7R-I 91.5 78.2 84.3CONV 92.0 95.8 93.8INF 91.6 95.1 93.3Accuracy 91.3 Table 2: Results for baseline (BAS) system (standardmulticlass SVM) component classifier with highest probability scoredetermines the overall prediction.</S>
			<S sid ="75" ssid = "33">The crucial difference in the DAC system is that the feature optimization is performed for each component classifierseparately.</S>
			<S sid ="76" ssid = "34">Each component classifier is optimizedfor F-measure.</S>
			<S sid ="77" ssid = "35">Table 3 shows results using this system.</S>
			<S sid ="78" ssid = "36">Prec.</S>
			<S sid ="79" ssid = "37">Recall F-meas.</S>
			<S sid ="80" ssid = "38">ERR-A 66.7 40.0 50.0 15.6R-I 91.5 78.2 84.3 0.0CONV 93.9 94.1 94.0 2.6INF 91.4 96.1 93.7 5.7Accuracy 91.7 4.9 Table 3: Results for basic DAC system (per-class featureoptimization followed by maximum confidence basedchoice); “ER” refers to error reduction in percent overstandard multiclass SVM (Table 2) Minority Preference (DACMP) System This system is exactly the same as the basic DAC systemexcept for one crucial difference: overall classification is biased towards a specified minority class.</S>
			<S sid ="81" ssid = "39">Ifthe minority class binary classifier predicts true, thissystem chooses the minority class as the predictedclass.</S>
			<S sid ="82" ssid = "40">In cases where the minority class classifierpredicts false, it backs off to the basic DAC systemafter removing the minority class classifier from theconfidence tally.</S>
			<S sid ="83" ssid = "41">Table 4 shows our results usingREQUEST-ACTION as the minority class.</S>
			<S sid ="84" ssid = "42">Cascading Minority Preference (DACCMP) SystemThis system is similar to the Minority PreferenceSystem; however, instead of a single supplied minority class, the system accepts an ordered list ofclasses.</S>
			<S sid ="85" ssid = "43">The classifier then works, in order, throughthis list; whenever any classifier in the list predicts 804 Prec.</S>
			<S sid ="86" ssid = "44">Recall F-meas.</S>
			<S sid ="87" ssid = "45">ERR-A 66.7 45.7 54.2 22.8R-I 91.5 78.2 84.3 0.0CONV 93.9 94.1 94.0 2.6INF 91.6 96.0 93.8 6.5Accuracy 91.8 5.7 Table 4: Results for minority-preference DAC system —DACMP (first consult REQUEST-ACTION tagger, then default to choice by maximum confidence); “ER” refers toerror reduction in percent over standard multiclass SVM(Table 2) true, for a given instance, it then assigns this classas the predicted class.</S>
			<S sid ="88" ssid = "46">The subsequent classifiers inthe list are not run.</S>
			<S sid ="89" ssid = "47">If all classifiers predict false, weback off to the basic DAC system, i.e., the component classifier with highest probability score determines the overall prediction.</S>
			<S sid ="90" ssid = "48">We ordered the list ofclasses in the ascending order of their frequencies inthe training data.</S>
			<S sid ="91" ssid = "49">This ordering is driven by the observation that the less frequent classes are also hardto predict correctly.</S>
			<S sid ="92" ssid = "50">Table 5 shows our results usingthe ordered list: (REQUEST-ACTION, REQUEST-INFORMATION, CONVENTIONAL, INFORM).</S>
			<S sid ="93" ssid = "51">Prec.</S>
			<S sid ="94" ssid = "52">Recall F-meas.</S>
			<S sid ="95" ssid = "53">ERR-A 66.7 45.7 54.2 22.8R-I 91.0 80.8 85.6 8.4CONV 93.7 95.3 94.5 10.1INF 92.4 95.8 94.0 10.0Accuracy 92.2 10.6 Table 5: Results for cascading minority-preference DACsystem — DACCMP (consult classifiers in reverse orderof frequency of class); “ER” refers to error reduction inpercent over standard multiclass SVM (Table 2) 4.4 DiscussionAs shown in Table 3, the basic DAC system obtaineda 15.6% F-measure error reduction for the minority class REQUEST-ACTION over the BAS system.It also improves performance of two other classes— CONVENTIONAL and INFORM, and obtaines a4.9% error reduction on overall accuracy.</S>
			<S sid ="96" ssid = "54">Recallhere that the only difference between the DAC system and the BAS system is the per-class feature optimization and therefore this must be the reason for.</S>
			<S sid ="97" ssid = "55">this boost in performance.</S>
			<S sid ="98" ssid = "56">When we turn to DACMP,we see that the performance on the minority classREQUEST-ACTION is further enhanced, with an F-measure error reduction of 22.8%; the overall accuracy improves slightly with an error reduction of5.7%.</S>
			<S sid ="99" ssid = "57">Finally, DACCMP further improves the performance.</S>
			<S sid ="100" ssid = "58">Since the method of choosing the minority class REQUEST-ACTION does not change overDACMP, the F-measure error reduction remains thesame.</S>
			<S sid ="101" ssid = "59">However, now all three other classes also improve their performance, and we obtain a 10.6% error reduction on overall accuracy over the baselinesystem.</S>
			<S sid ="102" ssid = "60">Following (Guyon et al., 2002), we performed aposthoc analysis by inspecting the feature weightsof the best performing models created for each individual classifier in the DAC system.</S>
			<S sid ="103" ssid = "61">Table 6 listssome interesting features chosen during feature optimization for the individual SVMs.</S>
			<S sid ="104" ssid = "62">We selected themfrom the top 25 features in terms of absolute valueof feature weights.</S>
			<S sid ="105" ssid = "63">Some features help distinguish different DA categories.</S>
			<S sid ="106" ssid = "64">For example, the feature QuestionMarkis the feature with the highest negative weight forINFORM, but has the highest positive weight forREQUEST-INFORMATION.</S>
			<S sid ="107" ssid = "65">Features like fyi and period (.)</S>
			<S sid ="108" ssid = "66">have high positive weights for INFORMand high negative weights for CONVENTIONAL.Some other features are important only for certainclasses.</S>
			<S sid ="109" ssid = "67">For e.g., please and VB NN are importantfor REQUEST-ACTION, but not so for other classes.Overall, the most discriminating features for bothINFORM and CONVENTIONAL are mostly wordngrams, while those for REQUEST-ACTION andREQUEST-INFORMATION are mostly POS ngrams.This shows why our approach of per-class featureoptimization is important to boost the classificationperformance.</S>
			<S sid ="110" ssid = "68">Another interesting observation is that the leastfrequent category, REQUEST-ACTION, has the leaststrong indicators (as measured by feature weights).Presumably this is because there is much less training data for this class.</S>
			<S sid ="111" ssid = "69">This explains why our cascading classifiers approach giving priority to the leastfrequent categories worked better than a simple confidence based approach, since the simple approachdrowns out the less confident classifiers.</S>
			<S sid ="112" ssid = "70">805 REQUEST-ACTION REQUEST-INFORMATION CONVENTIONAL INFORMplease (0.9) QuestionMark (6.6) StartPOS NNP (2.7) QuestionMark (-3.0)VB NN (0.7) BOS PRP (-1.2) thanks (2.3) thanks (-2.2)you VB (0.3) WRB (1.0) .</S>
			<S sid ="113" ssid = "71">(-2.0) .</S>
			<S sid ="114" ssid = "72">(2.2)PRP (-0.3) PRP VBP (-0.9) fyi (-2.0) fyi (1.9)MD PRP VB (0.3) BOS MD (0.8) , (0.9) you (-1.0)will (-0.2) BOS DT (-0.7) QuestionMark (-0.8) can you (-0.9) Table 6: Post-hoc analysis on the models built by the DAC system: some of the top features with correspondingfeature weights in parentheses, for each individual tagger.</S>
			<S sid ="115" ssid = "73">(POS tags are capitalized; BOS stands for Beginning OfSentence)</S>
	</SECTION>
	<SECTION title="Extrinsic Evaluation. " number = "5">
			<S sid ="116" ssid = "1">In this section, we perform an extrinsic evaluationfor the dialog act tagger presented in Section 4 byapplying it to the task of identifying Overt Displaysof Power (ODP) in emails, proposed by Prabhakaranet al.</S>
			<S sid ="117" ssid = "2">(2012).</S>
			<S sid ="118" ssid = "3">The task is to identify utterances wherethe linguistic form introduces additional constraintson its responses, beyond those introduced by thegeneral dialog act.</S>
			<S sid ="119" ssid = "4">The dialog act features werefound to be useful and the best performing systemobtained an F-measure of 65.8 using gold dialogact tags.</S>
			<S sid ="120" ssid = "5">For our extrinsic evaluation, we retrainedthe ODP tagger using dialog act tags predicted byour BAS and DACCMP systems instead of gold dialog acts.</S>
			<S sid ="121" ssid = "6">ODP tagger uses the same dataset as oursfor training.</S>
			<S sid ="122" ssid = "7">In the cross validation step, we madesure that the test folds for ODP were excluded fromtraining the taggers to obtain DA tags.</S>
			<S sid ="123" ssid = "8">At each ODPcross validation step, we trained a BAS or DACCMPtagger using ODP’s training folds for that step andused tags produced by that tagger for both trainingand testing the ODP tagger for that step.</S>
			<S sid ="124" ssid = "9">Table 7 liststhe results obtained.</S>
			<S sid ="125" ssid = "10">Prec.</S>
			<S sid ="126" ssid = "11">Rec.</S>
			<S sid ="127" ssid = "12">F-meas.No-DA 55.7 45.4 50.0GoldDA 75.8 58.1 65.8BASDA 60.6 46.5 52.6DACCMPDA 67.2 45.4 54.2 Table 7: Results for ODP system using various sourcesof DA tagsUsing BAS tagged DA, the F-measure of ODPsystem reduced by 13.2 points to 52.6 from usinggold dialog acts (F=65.8).</S>
			<S sid ="128" ssid = "13">Using DACCMP, the F measure improved over BAS by 1.6 points to 54.2.This constitutes an error reduction of 12.1%, taking the system using gold DA tags as the reference.This improvement is noteworthy, given the fact thatthe overall error reduction obtained by DACCMP overBAS in the DA tagging was around 10.6%.</S>
			<S sid ="129" ssid = "14">Also, theDACCMP-based ODP system obtained an error reduction of about 26.6% over a system that does not usethe DA features at all (F=50.0).</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="130" ssid = "1">We presented a method of improving the performance of dialog act tagging in identifying minorityclasses by using per-class feature optimization andchoosing the class based on a cascade of classifiers.We showed that it gives a minority class F-measureerror reduction of 22.8% while also reducing the error on other classes and the overall error by around10%.</S>
			<S sid ="131" ssid = "2">We also presented an extrinsic evaluation ofthis technique on detecting Overt Displays of Powerin dialog, where we achieve an error reduction of12.1% over using the standard multiclass SVM togenerate dialog act tags.</S>
	</SECTION>
	<SECTION title="Acknowledgements">
			<S sid ="132" ssid = "3">This work is supported, in part, by the Johns Hopkins Human Language Technology Center of Excellence.</S>
			<S sid ="133" ssid = "4">Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflect theviews of the sponsor.</S>
			<S sid ="134" ssid = "5">While working on this project,the first author Adinoyi Omuya was affiliated withthe Center for Computational Learning Systems atColumbia University.</S>
			<S sid ="135" ssid = "6">We thank several anonymousreviewers for their constructive feedback.</S>
			<S sid ="136" ssid = "7">806</S>
	</SECTION>
</PAPER>
