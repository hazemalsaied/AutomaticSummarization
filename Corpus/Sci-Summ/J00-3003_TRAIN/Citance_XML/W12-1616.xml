<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this work we study the effectiveness ofspeaker adaptation for dialogue act recognition in multiparty meetings.</S>
		<S sid ="2" ssid = "2">First, we analyzeidiosyncracy in dialogue verbal acts by qualitatively studying the differences and conflictsamong speakers and by quantitively comparing speaker-specific models.</S>
		<S sid ="3" ssid = "3">Based on theseobservations, we propose a new approach fordialogue act recognition based on reweighteddomain adaptation which effectively balancethe influence of speaker specific and otherspeakers’ data.</S>
		<S sid ="4" ssid = "4">Our experiments on a real-world meeting dataset show that with evenonly 200 speaker-specific annotated dialogueacts, the performances on dialogue act recognition are significantly improved when compared to several baseline algorithms.</S>
		<S sid ="5" ssid = "5">To ourknowledge, this work is the first 1 to tackle thispromising research direction of speaker adaptation for dialogue act recogntion.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">By representing a higher level intention of utterancesduring human conversation, dialogue act labels arebeing used to enrich the information provided byspoken words (Stolcke et al., 2000).</S>
			<S sid ="7" ssid = "7">Dialogue actrecognition is a preliminary step towards deep dialogue understanding.</S>
			<S sid ="8" ssid = "8">It plays a key role in the design of dialogue systems.</S>
			<S sid ="9" ssid = "9">Besides, Fernandez et al.(2008) find certain dialogue acts are important cuesfor detecting decisions in Multi-party dialogue.</S>
			<S sid ="10" ssid = "10">In 1This paper is an extended version of a poster presented atSemDial 2011, with new experiments and deeper analysis.</S>
			<S sid ="11" ssid = "11">Ranganath et al.</S>
			<S sid ="12" ssid = "12">(2009), dialogue acts are used asimportant features for flirt detection.</S>
			<S sid ="13" ssid = "13">Automatic dialogue act recognition is still an active research topic.</S>
			<S sid ="14" ssid = "14">The conventional approach is totrain one generic classifier using a large corpus ofannotated utterances.</S>
			<S sid ="15" ssid = "15">One aspect that makes it sochallenging is that people can express the same idea(or speech act) using a very different set of spokenwords.</S>
			<S sid ="16" ssid = "16">Even more, people can mean different thingswith the exact same spoken words.</S>
			<S sid ="17" ssid = "17">These idiosyncratic differences in dialogue acts make the learningof generic classifiers extremely challenging.</S>
			<S sid ="18" ssid = "18">Luckily, in many applications such as face-to-face meetings or tele-immersion, we have access to archivesof previous interactions with the same participants.From these archives, a small subset of spoken utterances can be efficiently annotated.</S>
			<S sid ="19" ssid = "19">As we will latershow in our experiments, even a small number of annotated utterances can make a significant difference.</S>
			<S sid ="20" ssid = "20">In this paper, we propose a new approach fordialogue act recognition based on reweighted domain adaptation which effectively balance the influence of speaker specific and other speakers’ data.By treating each speaker as one domain, we pointout the connection between training speaker specific dialogue act classifier and supervised domainadaptation problem.</S>
			<S sid ="21" ssid = "21">We analyze idiosyncracy indialogue verbal acts by qualitatively studying thedifferences and conflicts among speakers and byquantitively comparing speaker-specific models.</S>
			<S sid ="22" ssid = "22">Wepresent an extensive set of experiments studying theeffect of speaker adaptation on dialogue act recogntion in multi-party meetings using the ICSIMRDAdataset (Shriberg, 2004).</S>
			<S sid ="23" ssid = "23">118 The following section presents related work on dialogue act recognition and domain adaptation.</S>
			<S sid ="24" ssid = "24">Section 3 describes the ICSIMRDA (Shriberg, 2004)dataset which is used in all our experiments.</S>
			<S sid ="25" ssid = "25">Section 4 analyze idiosyncracy in dialogue acts, bothqualitatively and quantitatively.</S>
			<S sid ="26" ssid = "26">Section 5 explains our reweighting-based speaker adaptation algorithm.</S>
			<S sid ="27" ssid = "27">Section 6 contains all experiments to provethe applicability of speaker adaptation to dialogueact recognition.</S>
			<S sid ="28" ssid = "28">Finally, inspired by the promisingresults, Section 8 describes some future directions.</S>
	</SECTION>
	<SECTION title="Previous Work. " number = "2">
			<S sid ="29" ssid = "1">Automatic dialogue act recognition has been an important problem in the past decades.</S>
			<S sid ="30" ssid = "2">Different dialogue act labeling standards and datasets have beenprovided, including Switchboard-DAMSL (Stolckeet al., 2000), ICSIMRDA (Shriberg, 2004) andAMI (Carletta, 2007).</S>
			<S sid ="31" ssid = "3">Stolcke et al (2000) is oneof the first work using machine learning technique(HMM) to automatically segment and recognize dialogue acts.</S>
			<S sid ="32" ssid = "4">Rangarajan et al.</S>
			<S sid ="33" ssid = "5">(2009) demonstratedwell-designed prosodic n-gram features are veryhelpful for Dialogue Act recognition in MaximumEntropy model.</S>
			<S sid ="34" ssid = "6">And Ang et al (2005) exploredjoint segmentation and dialogue act classification forspeech from ICSI.</S>
			<S sid ="35" ssid = "7">Domain adaptation is a popular problem in natural language processing community due to the sparsity of labeled data.</S>
			<S sid ="36" ssid = "8">Jiang (Jiang, 2007) breaksthe analysis of domain adaptation problem into dis-tributional differences in instances and classification functions between source and target data.</S>
			<S sid ="37" ssid = "9">InDaume’s work (2007) several domain adaptation algorithms are described.</S>
			<S sid ="38" ssid = "10">Our speaker adaptation algorithm is inspired by the reweighting-based adaptation algorithm introduced in this paper.</S>
			<S sid ="39" ssid = "11">Recently, dialogue act adaptation has been gettinga lot of attention.</S>
			<S sid ="40" ssid = "12">Tur et al.</S>
			<S sid ="41" ssid = "13">(2006) successfully useSwitchboardDAMSL to help dialogue act recognition in ICSIMRDA.</S>
			<S sid ="42" ssid = "14">Promising results have beenobtained by using a regression model to combine themodel weights obtained by training on Switchboard-DAMSL and ICSIMRDA respectively.</S>
			<S sid ="43" ssid = "15">Followingthe work by Tur et al.</S>
			<S sid ="44" ssid = "16">(2006), Guz et al.</S>
			<S sid ="45" ssid = "17">(2009) further studied the effectiveness of dialogue act domainadaptation in cascaded dialogue act segmentation and recognition system, their results prove adaptation in the intermediate step (segmentation) are alsovery helpful for the final output (recognition).</S>
			<S sid ="46" ssid = "18">Jeonget al (2009) use semi-supervised boosting algorithmto leverage labeled data from Switchboard-DAMSLand ICSIMRDA to help dialogue act recognition inemail and forums.</S>
			<S sid ="47" ssid = "19">Margolis et.al (2010) use a structural correspondence learning technique to adapt dialogue act recognition on automatic translated Spanish genre with the help of Switchboard-DAMSL andICSIMRDA.</S>
			<S sid ="48" ssid = "20">Kolar et al.</S>
			<S sid ="49" ssid = "21">(2007) explores the difference among speakers for dialogue act segmentation in ICSIMRDA dataset.</S>
			<S sid ="50" ssid = "22">Similar to the approachtaken in Tur et al.</S>
			<S sid ="51" ssid = "23">(2006), adaptation is performedthrough the combination of generic speaker independent Language Model and other speakers’ Language Model.</S>
			<S sid ="52" ssid = "24">Significant improvements have beenobtained for most of the selected speakers.</S>
			<S sid ="53" ssid = "25">All these previous papers focused on adapting dialogue act models between domains and did notaddress the person-specific adaptation.</S>
			<S sid ="54" ssid = "26">The onlyexception was Kolar et al.</S>
			<S sid ="55" ssid = "27">(2007) who exploredspeaker-specific dialogue act segmentation.</S>
			<S sid ="56" ssid = "28">To ourknowledge, this paper is the first work to analyze theeffectiveness of speaker adaptation for dialogue actrecognition.</S>
	</SECTION>
	<SECTION title="ICSI-MRDA Corpus. " number = "3">
			<S sid ="57" ssid = "1">Different Dialogue Act labeling standards anddatasets have been provided in recent years, including Switchboard-DAMSL (Stolcke et al., 2000),ICSIMRDA (Shriberg, 2004) and AMI (Carletta,2007).</S>
			<S sid ="58" ssid = "2">ICSIMRDA is the dataset for our experiments because many of its meetings contain thesame speakers, thus making it more suitable for ourspeaker adaptation study.</S>
			<S sid ="59" ssid = "3">The tagset in ICSIMRDAis adapted from DAMSL standard (damsl, 1997) byallowing multiple tags per dialogue act.</S>
			<S sid ="60" ssid = "4">Each dialogue act in ICSIMRDA has one general tag andmultiple specific tags.</S>
			<S sid ="61" ssid = "5">ICSIMRDA consists of 75 meetings, eachroughly an hour long.</S>
			<S sid ="62" ssid = "6">There are five categories ofmeetings (three of which we are actively using inour experiments) : Bed is about the discussion ofnatural language processing and neural theories oflanguage, Bmr is for the discussion on ICSI meetingcorpus, Bro is on speech recognition topics and Bns 119 ID Tag Type Nb.</S>
			<S sid ="63" ssid = "7">Meetings Nb.</S>
			<S sid ="64" ssid = "8">DAs1 mn015 Bed 15 62282 me010 Bed 11 53093 me013 Bmr 25 97534 mn017 Bmr 15 40595 fe016 Bmr 18 55006 me018 Bro 20 42637 me013 Bro 22 11928 Table 1: The 7 speakers from ICSIMRDA dataset usedin our experiments.</S>
			<S sid ="65" ssid = "9">The table lists: the Speaker ID, original speaker tag, the type of meeting selected for thisspeaker, the number of meetings this speaker participatedand the total number of dialogue acts by this speaker.</S>
			<S sid ="66" ssid = "10">is about network and architecture.</S>
			<S sid ="67" ssid = "11">The last categoryis varies which contains all other topics.</S>
			<S sid ="68" ssid = "12">From these 75 meetings, there are 53 uniquespeakers in total, and an average of about 6 speakersper meeting.</S>
			<S sid ="69" ssid = "13">7 speakers2 having more than 4, 000dialogue acts are selected for our adaptation experiments.</S>
			<S sid ="70" ssid = "14">Table 1 shows the details of our 7 selectedspeakers.</S>
			<S sid ="71" ssid = "15">From the word transcriptions, we created an extended list of linguistic features per utterance.</S>
			<S sid ="72" ssid = "16">From the 7 selected speakers, we computed 14653 unigram features, 158884 bigram features and 400025 trigram features.</S>
			<S sid ="73" ssid = "17">Following the work of Shriberg et al.</S>
			<S sid ="74" ssid = "18">(2004), weuse the 5 general tags in our experiments: • Disruption indicates the current Dialogue Actis interrupted.</S>
			<S sid ="75" ssid = "19">• Back Channel are utterances which are notmade directly by a speaker as a response anddo not function in a way that elicits a responseeither.</S>
			<S sid ="76" ssid = "20">• Floor Mechanism are dialogue acts for grabbing or maintaining the floor.</S>
			<S sid ="77" ssid = "21">• Question is for eliciting listener feed back.• And finally, unless an utterance is completely indecipherable or else can be further describedby a general tag, then its default status is Statement.</S>
			<S sid ="78" ssid = "22">Our dataset consisted of 47040 dialogue acts.</S>
			<S sid ="79" ssid = "23">Thedistribution of Dialogue Act is shown in Table 2.</S>
			<S sid ="80" ssid = "24">2speaker me013 is split into me013Bmr and me013Bro toavoid the difference introduced by meeting types.</S>
			<S sid ="81" ssid = "25">Tag proportionDisruption 14.73% Back Channel 10.20%Floor Mechanism 12.40% Question 7.20%Statement 55.46% Table 2: Distribution of dialogue acts in our dataset.</S>
	</SECTION>
	<SECTION title="Idiosyncrasy in Dialogue Acts. " number = "4">
			<S sid ="82" ssid = "1">Our goal is to create a dialogue act recognition algorithm that can adapt to specific speakers.</S>
			<S sid ="83" ssid = "2">Someimportant questions must be studied before creating such algorithm.</S>
			<S sid ="84" ssid = "3">The first obvious one is: dospeakers really differ in their choice of words andassociated dialogue acts?</S>
			<S sid ="85" ssid = "4">Do we really see a variability on how people express their dialogue intent?</S>
			<S sid ="86" ssid = "5">If the answers are yes, then we will expectthat learning a dialogue act recognizer from speaker-specific utterances should always outperform a recognizer learned from someone else data.</S>
			<S sid ="87" ssid = "6">Section 4.1presents a comparative experiment addressing thesequestions.</S>
			<S sid ="88" ssid = "7">To better understand the results from this comparative experiment, we also performed a qualitative analysis presented in Section 4.2 where welook more closely at the differences between speakers.</S>
			<S sid ="89" ssid = "8">These two qualitative and quantitative analysisare building block for our adaptation algorithm presented in Section 5.</S>
			<S sid ="90" ssid = "9">4.1 Speaker-Specific Recognizers.</S>
			<S sid ="91" ssid = "10">An important assumption when performing speakeradaptation (or more generally domain adaptation)is that data coming from the same speaker shouldbe similar than data coming from another person.In other words, a recognizer trained on a speakershould perform better (when tested on the same person) than a recognizer trained on another speaker.We designed an experiment to test this hypothesis.</S>
			<S sid ="92" ssid = "11">We learned 7 speaker-specific recognizers, onefor each speaker (see Table 1).</S>
			<S sid ="93" ssid = "12">We then tested allthese recognizers on new utterances from the same7 speakers.</S>
			<S sid ="94" ssid = "13">We looked the recognition performancewhen (1) the recognizer was trained on the sameperson and (2) when the recognizer was trained ona different person.</S>
			<S sid ="95" ssid = "14">This experiments quantitatively 120 Figure 1: Effect of same-speaker data on dialogue actrecognition.</S>
			<S sid ="96" ssid = "15">We compare two approaches: (1) when arecognizer is trained on the same person and tested onnew utterances from the same person, and (2) when therecognizer was trained on another speaker (same test set).We vary the amount of training data to be 200, 500,1000, 1500 and 2000 dialogue acts.</S>
			<S sid ="97" ssid = "16">In all cases, usingspeaker-specific recognizer outperforms recognizer fromother speakers.</S>
			<S sid ="98" ssid = "17">analyze the the difference among speakers.</S>
			<S sid ="99" ssid = "18">The experimental methodology used in this experiment isthe same as the other experiments described in thispaper (see Section 6).</S>
			<S sid ="100" ssid = "19">We use the Maximum Entropy model(MaxEnt) for all dialogue act recognizers (Ratnaparkhi, 1996).</S>
			<S sid ="101" ssid = "20">Please refer to Section 6.2for more details about the experimental methodology.</S>
			<S sid ="102" ssid = "21">Figure 1 compares the average performanceswhen testing on the same speaker or on some otherspeaker.</S>
			<S sid ="103" ssid = "22">We vary the number of training data foreach speaker to be 200, 500, 1000, 1500 and 2000dialogue acts.</S>
			<S sid ="104" ssid = "23">For all five cases, the recognizerstrained on the same speaker outperforms the average performance when using a recognizer from another person.</S>
			<S sid ="105" ssid = "24">Thus speaker specific dialogue actsadaptation fits the assumption of domain adaptationproblems.</S>
			<S sid ="106" ssid = "25">4.2 Speakers Differences.</S>
			<S sid ="107" ssid = "26">To better understand the problem, we look moreclosely at the differences among speakers and theiruse of dialogue acts.</S>
			<S sid ="108" ssid = "27">We analyze the probleminduced by speaker idiosyncrasy in dialogue acts.During our qualitative analysis of the ICSIMRDAdataset, we identified three major differences explaining the performances observed in the previous sections: dialogue act conflicts, word distributionand dialogue act label distribution.</S>
			<S sid ="109" ssid = "28">We describethese three differences with some examples: Conflicts: These differences happen when twospeakers intended to express different meaningswhile speaking the exact same utterance.</S>
			<S sid ="110" ssid = "29">To exam-plify these conflicts, we computed mutual information between a specific utterance and all dialogue actlabels.</S>
			<S sid ="111" ssid = "30">We find interesting examples where for ex-emple the word right is the most important cue fordialogue act question when spoken by me013Bmr,while right is also an important cue for dialogue actback-channel for speaker me010-Bed.</S>
			<S sid ="112" ssid = "31">These examples suggest that conflicts exist among speakers andsimply trying to learn one generic model may notbe able to handle these conflicts.</S>
			<S sid ="113" ssid = "32">The generic modelwill learn what most people mean with this utterance, which may be the wrong prediction for ourspecific speaker.</S>
			<S sid ="114" ssid = "33">Word distribution: People have their own vocabulary.</S>
			<S sid ="115" ssid = "34">Although many words are the same, how often one person use each word will vary.</S>
			<S sid ="116" ssid = "35">Although wemay not have direct conflict here, the problem canalso be serious.</S>
			<S sid ="117" ssid = "36">The learning algorithm may misleadingly focus on optimizing the weights for certainwords which are not important(e.g., words that occur more often in other speakers’ dialogue acts thanhis/her own) while underestimating the importantwords for this speaker.</S>
			<S sid ="118" ssid = "37">This observation suggeststhat our adaptation should take into account worddistribution.</S>
			<S sid ="119" ssid = "38">Label Distribution: Another interesting observation is to look at the distribution of dialogue act labels for different speakers.</S>
			<S sid ="120" ssid = "39">Table 2 shows the average distribution over all 7 speakers.</S>
			<S sid ="121" ssid = "40">When lookingmore closely at each speaker, we find some interesting differences.</S>
			<S sid ="122" ssid = "41">For example, speaker 1 made statements 61% of the time while speaker 4 made 49% ofthe time.</S>
			<S sid ="123" ssid = "42">While this difference may not look significant, these changes can definitely affect the recognition performance.</S>
			<S sid ="124" ssid = "43">So the adaptation model shouldalso take into account the dialogue act label distribution.</S>
	</SECTION>
	<SECTION title="Reweighted Speaker Adaptation. " number = "5">
			<S sid ="125" ssid = "1">Based on the observations described in the previoussections, we implement a simple reweighting-based 121 domain adaptation algorithm mentioned in (Daume,2007) based on Maximum Entropy model (MaxEnt)(Ratnaparkhi, 1996).</S>
			<S sid ="126" ssid = "2">MaxEnt model is a popularand efficient discriminative model which can effectively accommodate large numbers of features.</S>
			<S sid ="127" ssid = "3">Allthe unigram, bigram and trigram features are usedas input to the maxEnt model, the output is the dialogue act label.</S>
			<S sid ="128" ssid = "4">MaxEnt model maximizes the logconditional likelihood of all samples: Loss = N?1 log(p(yn|xn)) (1) where N is the number of samples for the trainingdata.</S>
			<S sid ="129" ssid = "5">xn represents the feature of the nth sample andyn is the label.</S>
			<S sid ="130" ssid = "6">The conditional likelihood is definedas p(y|x) = exp(?i ?ifi(x, y))/Z(x) (2) where Z(x) is the normalization factor and fi(x, y)are the n-gram features described in Section 3.</S>
			<S sid ="131" ssid = "7">When applied to our problem of speaker adaptation, the reweighting adaptation model can be formally defined as Loss = wS?</S>
			<S sid ="132" ssid = "8">n=1 log(p(yn|xn))+O? m=1 log(p(ym|xm))(3) where S is the number of labeled speaker-specificdialogue acts, O is the number for other speakers’labeled dialogue acts.</S>
			<S sid ="133" ssid = "9">For each speaker, we trainone speaker-specific classifier by varying the distribution of training data.</S>
			<S sid ="134" ssid = "10">We reweight the importanceof speaker specific dialogue acts versus other speakers’ labeled dialogue acts in the training data.</S>
			<S sid ="135" ssid = "11">Theoptimal weight parameter w is automatically estimated through validation.</S>
			<S sid ="136" ssid = "12">It is worth mentioning a specific instance of thereweighting adaptation algorithm.</S>
			<S sid ="137" ssid = "13">When w is set to1, the reweighting adaptation algorithm is equivalentto simply training a MaxEnt model by putting thespeaker-specific and generic data samples togetheras training data.</S>
			<S sid ="138" ssid = "14">In our experiments, we will compare the reweighting adaptation approach with thissimpler approach, referred as constant adaptation.</S>
	</SECTION>
	<SECTION title="Experiments. " number = "6">
			<S sid ="139" ssid = "1">Our goal is to get one model specifically adaptedfor each speaker.</S>
			<S sid ="140" ssid = "2">We first describes 4 different approaches to be compared in the experiments, andsection 6.2 explains our experimental methodology.</S>
			<S sid ="141" ssid = "3">6.1 4 ApproachesIn these experiments, we compare our approach,called reweighted adaptation, with three moreconventional approaches: speaker-specific only,Generic and Constant adaptation.</S>
			<S sid ="142" ssid = "4">• Speaker Specific Only For this approach, wetrain the dialogue act recognizer using trainingsentences from the same speaker used duringtesting.</S>
			<S sid ="143" ssid = "5">• Generic In this case, we train the dialogue actrecognizer using utterances from all speakersother than the speaker used during testing.</S>
			<S sid ="144" ssid = "6">• Constant Adaptation For this approach, wetrain the dialogue act recognizer using allspeakers, including the speaker who will laterbe used for testing.</S>
			<S sid ="145" ssid = "7">All utterances have thesame weight in this case.</S>
			<S sid ="146" ssid = "8">• Reweighted Adaptation This is our proposedapproach.</S>
			<S sid ="147" ssid = "9">As described in Section 5, we trainour dialogue act recognizer using all speakersbut reweight the utterances from the speakerwho will later be used for testing.</S>
			<S sid ="148" ssid = "10">6.2 MethodologyIn all the following experiments we use MaxEntmodels as defined in Section 5.</S>
			<S sid ="149" ssid = "11">L2 regularizationis used for MaxEnt to avoid overfitting.</S>
			<S sid ="150" ssid = "12">The optimalregularization parameter was automatically selectedduring validation.</S>
			<S sid ="151" ssid = "13">The following regularization parameters were used: 0.01, 0.1, 1, 10 , 100, 1000 and0 (no regularization).</S>
			<S sid ="152" ssid = "14">All the unigram, bigram andtrigram features are used in the maxEnt model.</S>
			<S sid ="153" ssid = "15">Thelabels are the five dialogue act tags described in Section 3..</S>
			<S sid ="154" ssid = "16">All experiments were performed using holdouttesting and holdout validation.</S>
			<S sid ="155" ssid = "17">Both validationand test sets consisted of 1000 dialogue acts.</S>
			<S sid ="156" ssid = "18">Thetraining sets contained only utterances from meetings that were not in the validation set of test set.</S>
			<S sid ="157" ssid = "19">122 Train Data 200 500 1000 1500 2000Speaker-specific Only 64.07 65.99 68.51 69.99 71.06Constant adaptation model 76.81 76.96 77.00 77.23 77.53Our reweighted adaptation model 78.17 78.29 78.67 78.74 78.47 Table 3: Average results among all 7 speakers when trainwith different combinations of speaker specific data andother speakers’ data.</S>
			<S sid ="158" ssid = "20">The number of speaker specific datais varied from 200, 500, 1000, 1500 to 2000.</S>
			<S sid ="159" ssid = "21">In many of our experiments, we analyzed the effect of training set size on the recognition performance.</S>
			<S sid ="160" ssid = "22">The speaker-specific data size varied from200, 500, 1000, 1500 and 2000 dialogue acts respectively.</S>
			<S sid ="161" ssid = "23">When training our reweighting adaptation algorithm described in Section 5, we used the following weights: 10, 30, 50, 75, and 100.</S>
			<S sid ="162" ssid = "24">The optimalweight factor was selected automatically during validation.</S>
	</SECTION>
	<SECTION title="Results. " number = "7">
			<S sid ="163" ssid = "1">In this section we present our approaches to studythe importance of speaker adaptation for dialogueact recognition.</S>
			<S sid ="164" ssid = "2">All following results are calculatedbased on the overall tag accuracies.</S>
			<S sid ="165" ssid = "3">We designedthree series of experiments for this study:• Generic Recognizer (Section 7.1)• Sparsity in speaker-specific data (Section 7.2)• Effectiveness of Constant Adaptation (Sec tion 7.3) • Performance of the reweighting algorithm(Section 7.4) 7.1 Generic RecognizerThe first result we get is on average, for each speakerwhen we use all other speaker’s data for training,then test on speaker- specific test data.</S>
			<S sid ="166" ssid = "4">The performance of this generic recognizer is 76.76% is thebaseline we try to improve when adding speaker-specific data into consideration.</S>
			<S sid ="167" ssid = "5">3.</S>
			<S sid ="168" ssid = "6">3The performance of our generic model is comparable to theresults from Ang et al (2005) when you take into considerationthat we used only 47,040 dialogue acts in our experiments (i.e.,dialogue acts from our 7 speakers) which is a small fractioncompared with Ang et al (2005) . 7.2 Sparsity of speaker-specific data.</S>
			<S sid ="169" ssid = "7">A second result is the performance when only using speaker-specific data.</S>
			<S sid ="170" ssid = "8">The row Speaker SpecificOnly in Table 3 shows the average results amongall speakers when for each speaker, we train using only data from the same speaker.</S>
			<S sid ="171" ssid = "9">The numberof speaker-specific training data we tried are 200,500, 1000, 1500, and 2000 respectively.</S>
			<S sid ="172" ssid = "10">Even with2000 speaker-specific dialogue acts for training, thebest accuracy is 71.06% which is lower than 76.76%when using generic recognizer.</S>
			<S sid ="173" ssid = "11">Given the challengein getting 2000 speaker-specific annotated dialogueacts, we are looking at a different approach wherewe need less speaker-specific data.</S>
			<S sid ="174" ssid = "12">7.3 Results of Constant Adaptation.</S>
			<S sid ="175" ssid = "13">The most straightforward way to combine otherspeakers’ data is to directly add them with speaker-specific data as train.</S>
			<S sid ="176" ssid = "14">We refer to this approachas constant adaptation.</S>
			<S sid ="177" ssid = "15">The row Constant Adaptation in Table 3 shows the average results amongall speakers when for each speaker, we combinethe speaker-specific data directly with the all otherspeaker’s data.</S>
			<S sid ="178" ssid = "16">In our experiments, we varied theamount of speaker-specific data included to be 200,500, 1000, 1500, and 2000 respectively.</S>
			<S sid ="179" ssid = "17">For all7 speakers, the performance can always been improved by including speaker-specific data with allother speakers’ data for training.</S>
			<S sid ="180" ssid = "18">Furthermore, themore speaker specific data added, the better performance we get.</S>
			<S sid ="181" ssid = "19">7.4 Results of Reweighting Algorithm.</S>
			<S sid ="182" ssid = "20">Finally, in this section we describe the results fora simple adaptation algorithm based on reweighting, as described in Section 5.</S>
			<S sid ="183" ssid = "21">Following the samemethodology as previous experiments, we vary theamount of speaker-specific data to be 200, 500,1000, 1500 and 2000.</S>
			<S sid ="184" ssid = "22">The best reweighting factor isselected through validation on speaker-specific validation data described in section 6.2.</S>
			<S sid ="185" ssid = "23">The results ofall 7 speakers from Reweighting algorithm when wevary the amount of speaker-specific data are shownin Figure 3.We analyze the influence of the weighting factoron our speaker adaptation by plotting the recognition performance for different weights.</S>
			<S sid ="186" ssid = "24">Figure 4 il 123 Figure 2: The average results among all 7 speakers whentrain with different combinations of speaker specific dataand other speakers’ data are displayed.</S>
			<S sid ="187" ssid = "25">In both Constantadaptation and Reweighted adaptation models the number of speaker specific data are varied from 200, 500,1000, 1500 to 2000.</S>
			<S sid ="188" ssid = "26">In Generic model, only all otherspeakers’ data are used for training data.</S>
			<S sid ="189" ssid = "27">Figure 3: Reweighting algorithm for all 7 IndividualSpeakers when varying the amount of training data to be0, 200, 500, 1000, 1500 and 2000.</S>
			<S sid ="190" ssid = "28">lustrates the influence of the weight factor on threespeaker adaptation cases: None, 500 and 2000.</S>
			<S sid ="191" ssid = "29">Inthis case, None represent the Constant Adaptation.We observe the following trend: with more speaker-specific data, the optimal reweighting factor is alsolower.</S>
			<S sid ="192" ssid = "30">This confirms that our reweighting algorithmfinds the right balance between speaker-specific dataand generic data.</S>
			<S sid ="193" ssid = "31">Figure 2 and the row Reweighted Adaptationfrom Table 3 shows the effectiveness of reweighting algorithm.</S>
			<S sid ="194" ssid = "32">Results shows that even this simple algorithm can efficiently balance the influenceof speaker specific data and other speakers’ data and 0 20 40 60 80 1000.765 0.77 0.775 0.78 0.785 0.79 None5002000 Figure 4: Average results of Reweighting among all 7speakers when the amount of speaker specific data is 0,500, 2000 give significantly improved results.</S>
			<S sid ="195" ssid = "33">And most surprisingly, even with only 200 speaker specific datathe reweighting algorithm can give very promisingresults.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "8">
			<S sid ="196" ssid = "1">In this work we analyze the effectiveness of speakeradaptation for dialogue act recognition.</S>
			<S sid ="197" ssid = "2">A simplereweighting algorithm is shown to give promisingimprovement on several baseline algorithms evenwith only 200 speaker-specific dialogue acts.</S>
			<S sid ="198" ssid = "3">Thispaper is a first step toward automatic adaptation fordialogue act recognition.</S>
			<S sid ="199" ssid = "4">Inspired by the promisingresults from the simple reweighting algorithm, weplan to evaluate other domain adaptation techniquessuch as Daume’s feature-based approach (2007).</S>
			<S sid ="200" ssid = "5">Itwill also be interesting to consider the unlabeleddata from each speaker when performing dialogueact recognition.</S>
	</SECTION>
	<SECTION title="Acknowledgments">
			<S sid ="201" ssid = "6">This material is based upon work supported bythe National Science Foundation under Grant No.1118018 and the U.S. Army Research, Development, and Engineering Command (RDECOM).</S>
			<S sid ="202" ssid = "7">Thecontent does not necessarily reflect the position orthe policy of the Government, and no official endorsement should be inferred.</S>
			<S sid ="203" ssid = "8">124</S>
	</SECTION>
</PAPER>
