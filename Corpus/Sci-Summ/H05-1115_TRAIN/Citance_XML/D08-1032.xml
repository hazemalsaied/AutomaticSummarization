<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Complex questions that require inferencingand synthesizing information from multipledocuments can be seen as a kind of topic-oriented, informative multi-document summarization.</S>
		<S sid ="2" ssid = "2">In this paper, we have experimentedwith one empirical and two unsupervisedstatistical machine learning techniques: k-means and Expectation Maximization (EM),for computing relative importance of the sentences.</S>
		<S sid ="3" ssid = "3">However, the performance of these approaches depends entirely on the feature setused and the weighting of these features.</S>
		<S sid ="4" ssid = "4">Weextracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic andshallow-semantic) for each of the documentsentences in order to measure its importanceand relevancy to the user query.</S>
		<S sid ="5" ssid = "5">We used alocal search technique to learn the weights ofthe features.</S>
		<S sid ="6" ssid = "6">For all our methods of generatingsummaries, we have shown the effects of syntactic and shallow-semantic features over thebag of words (BOW) features.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="7" ssid = "7">After having made substantial headway in factoidand list questions, researchers have turned their attention to more complex information needs that cannot be answered by simply extracting named entities (persons, organizations, locations, dates, etc.)from documents.</S>
			<S sid ="8" ssid = "8">For example, the question: “Describe the aftereffects of cyclone SidrNov 2007 inBangladesh” requires inferencing and synthesizinginformation from multiple documents.</S>
			<S sid ="9" ssid = "9">This information synthesis in NLP can be seen as a kind of topic-oriented, informative multi-document summarization, where the goal is to produce a single text asa compressed version of a set of documents with aminimum loss of relevant information.</S>
			<S sid ="10" ssid = "10">In this paper, we experimented with one empirical and two well-known unsupervised statistical machine learning techniques: k-means and EMand evaluated their performance in generating topic-oriented summaries.</S>
			<S sid ="11" ssid = "11">However, the performance ofthese approaches depends entirely on the feature setused and the weighting of these features.</S>
			<S sid ="12" ssid = "12">We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, treekernel based syntactic and shallow-semantic) foreach of the document sentences in order to measureits importance and relevancy to the user query.</S>
			<S sid ="13" ssid = "13">Wehave used a gradient descent local search techniqueto learn the weights of the features.</S>
			<S sid ="14" ssid = "14">Traditionally,information extraction techniques are based on theBOW approach augmented by language modeling.But when the task requires the use of more complex semantics, the approaches based on only BOWare often inadequate to perform fine-level textualanalysis.</S>
			<S sid ="15" ssid = "15">Some improvements on BOW are givenby the use of dependency trees and syntactic parsetrees (Hirao et al., 2004), (Punyakanok et al., 2004),(Zhang and Lee, 2003), but these, too are not adequate when dealing with complex questions whoseanswers are expressed by long and articulated sentences or even paragraphs.</S>
			<S sid ="16" ssid = "16">Shallow semantic representations, bearing a more compact information,could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007).</S>
			<S sid ="17" ssid = "17">Attempting an application of 304 syntactic and semantic information to complex QAhence seems natural, as pinpointing the answer to aquestion relies on a deep understanding of the semantics of both.</S>
			<S sid ="18" ssid = "18">In more complex tasks such ascomputing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions), to our knowledge no study uses treekernel functions to encode syntactic/semantic information.</S>
			<S sid ="19" ssid = "19">For all our methods of generating summaries (i.e. empirical, k-means and EM), we haveshown the effects of syntactic and shallow-semanticfeatures over the BOW features.</S>
			<S sid ="20" ssid = "20">This paper is organized as follows: Section 2 focuses on the related work, Section 3 describes howthe features are extracted, Section 4 discusses thescoring approaches, Section 5 discusses how we remove the redundant sentences before adding themto the summary, Section 6 describes our experimental study.</S>
			<S sid ="21" ssid = "21">We conclude and give future directions inSection 7.</S>
	</SECTION>
	<SECTION title="Related Work. " number = "2">
			<S sid ="22" ssid = "1">Researchers all over the world working on query-based summarization are trying different directions to see which methods provide the best results.</S>
			<S sid ="23" ssid = "2">The LexRank method addressed in (Erkanand Radev, 2004) was very successful in genericmulti-document summarization.</S>
			<S sid ="24" ssid = "3">A topic-sensitiveLexRank is proposed in (Otterbacher et al., 2005).As in LexRank, the set of sentences in a documentcluster is represented as a graph, where nodes aresentences and links between the nodes are inducedby a similarity relation between the sentences.</S>
			<S sid ="25" ssid = "4">Thenthe system ranked the sentences according to a random walk model defined in terms of both the inter-sentence similarities and the similarities of the sentences to the topic description or question.</S>
			<S sid ="26" ssid = "5">The summarization methods based on lexicalchain first extract the nouns, compound nouns andnamed entities as candidate words (Li et al., 2007).Then using WordNet, the systems find the semanticsimilarity between the nouns and compound nouns.After that, lexical chains are built in two steps: 1)Building single document strong chains while dis-ambiguating the senses of the words and, 2) building multi-chain by merging the strongest chains of the single documents into one chain.</S>
			<S sid ="27" ssid = "6">The systemsrank sentences using a formula that involves a) thelexical chain, b) keywords from query and c) namedentities.</S>
			<S sid ="28" ssid = "7">(Harabagiu et al., 2006) introduce a new paradigmfor processing complex questions that relies on acombination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Sum-marization (MDS) techniques.</S>
			<S sid ="29" ssid = "8">The question decomposition procedure operates on a Marcov chain, byfollowing a random walk with mixture model on abipartite graph of relations established between concepts related to the topic of a complex question andsubquestions derived from topic-relevant passagesthat manifest these relations.</S>
			<S sid ="30" ssid = "9">Decomposed questionsare then submitted to a state-of-the-art QA systemin order to retrieve a set of passages that can later bemerged into a comprehensive answer by a MDS system.</S>
			<S sid ="31" ssid = "10">They show that question decompositions usingthis method can significantly enhance the relevanceand comprehensiveness of summary-length answersto complex questions.</S>
			<S sid ="32" ssid = "11">There are approaches that are based on probabilistic models (Pingali et al., 2007) (Toutanova et al.,2007).</S>
			<S sid ="33" ssid = "12">(Pingali et al., 2007) rank the sentences basedon a mixture model where each component of themodel is a statistical model: Score(s) = a×QIScore(s)+(1-a)×QFocus(s,Q) Where, Score(s) is the score for sentence s. Query-independent score (QIScore) and query-dependent score(QFocus) are calculated based on probabilistic models.(Toutanova et al., 2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)Model Frequency.</S>
			<S sid ="34" ssid = "13">The scoring function is learned by fitting weights for a set of feature functions of sentencesin the document set and is trained to optimize a sentencepair-wise ranking criterion.</S>
			<S sid ="35" ssid = "14">The scoring function is further adapted to apply to summaries rather than sentencesand to take into account redundancy among sentences.</S>
			<S sid ="36" ssid = "15">There are approaches in “Recognizing Textual Entail-ment”, “Sentence Alignment” and “Question Answering”that use syntactic and/or semantic information in order tomeasure the similarity between two textual units.</S>
			<S sid ="37" ssid = "16">(MacCartney et al., 2006) use typed dependency graphs (sameas dependency trees) to represent the text and the hypothesis.</S>
			<S sid ="38" ssid = "17">Then they try to find a good partial alignment between the typed dependency graphs representing the hypothesis and the text in a search space of O((m + 1)n) 305 where hypothesis graph contains n nodes and a text graphcontains m nodes.</S>
			<S sid ="39" ssid = "18">(Hirao et al., 2004) represent the sentences using Dependency Tree Path (DTP) to incorporatesyntactic information.</S>
			<S sid ="40" ssid = "19">They apply String SubsequenceKernel (SSK) to measure the similarity between the DTPsof two sentences.</S>
			<S sid ="41" ssid = "20">They also introduce Extended StringSubsequence Kernel (ESK) to incorporate semantics inDTPs.</S>
			<S sid ="42" ssid = "21">(Kouylekov and Magnini, 2005) use the tree editdistance algorithms on the dependency trees of the textand the hypothesis to recognize the textual entailment.According to this approach, a text T entails a hypothesisH if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such thatwe can obtain H with an overall cost below a certainthreshold.</S>
			<S sid ="43" ssid = "22">(Punyakanok et al., 2004) represent the question and the sentence containing answer with their dependency trees.</S>
			<S sid ="44" ssid = "23">They add semantic information (i.e. namedentity, synonyms and other related words) in the dependency trees.</S>
			<S sid ="45" ssid = "24">They apply the approximate tree matchingin order to decide how similar any given pair of trees are.They also use the edit distance as the matching criteria inthe approximate tree matching.</S>
			<S sid ="46" ssid = "25">All these methods showthe improvement over the BOW scoring methods.</S>
			<S sid ="47" ssid = "26">Our Basic Element (BE)-based feature used the dependency tree to extract the BEs (i.e. head-modifier-relation)and ranked the BEs based on their log-likelihood ratios.For syntactic feature, we extracted the syntactic trees forthe sentence as well as for the query using the Charniakparser and measured the similarity between the two treesusing the tree kernel function.</S>
			<S sid ="48" ssid = "27">We used the ASSERT semantic role labeler system to parse the sentence as wellas the query semantically and used the shallow semantic tree kernel to measure the similarity between the twoshallow-semantic trees.</S>
			<S sid ="49" ssid = "28">3 Feature ExtractionThe sentences in the document collection are analyzedin various levels and each of the document-sentences isrepresented as a vector of feature-values.</S>
			<S sid ="50" ssid = "29">The featurescan be divided into several categories:.</S>
			<SUBSECTION>3.1 Lexical Features3.1.1 N-gram Overlap.</SUBSECTION>
			<S sid ="51" ssid = "30">N-gram overlap measures the overlapping word sequences between the candidate sentence and the querysentence.</S>
			<S sid ="52" ssid = "31">With the view to measure the N-gram(N=1,2,3,4) overlap scores, a query pool and a sentencepool are created.</S>
			<S sid ="53" ssid = "32">In order to create the query (or sentence)pool, we took the query (or document) sentence and created a set of related sentences by replacing its importantwords1 by their first-sense synonyms.</S>
			<S sid ="54" ssid = "33">For example given 1hence forth important words are the nouns, verbs, adverbsand adjectives a stemmed document-sentence: “John write a poem”, thesentence pool contains: “John compose a poem”, “Johnwrite a verse form” along with the given sentence.</S>
			<S sid ="55" ssid = "34">Wemeasured the recall based n-gram scores for a sentence Pusing the following formula: n-gramScore(P) = maxi(maxj N-gram(si, qj)) N-gram(S,Q) =?</S>
			<S sid ="56" ssid = "35">gramn?SCountmatch(gramn)?</S>
			<S sid ="57" ssid = "36">gramn?SCount(gramn) Where, n stands for the length of the n-gram (n =1, 2, 3, 4) and Countmatch (gramn) is the numberof n-grams co-occurring in the query and the candidate sentence, qj is the jth sentence in the querypool and si is the ith sentence in the sentence poolof sentence P . 3.1.2 LCS, WLCS and Skip-Bigram A sequence W = [w1, w2, ..., wn] is a subsequence of another sequence X = [x1, x2, ..., xm], ifthere exists a strict increasing sequence [i1, i2, ..., ik]of indices of X such that for all j =1, 2, ..., k we have xij = wj . Given two sequences,S1 and S2, the Longest Common Subsequence(LCS) of S1 and S2 is a common subsequence withmaximum length.</S>
			<S sid ="58" ssid = "37">The longer the LCS of two sentences is, the more similar the two sentences are.</S>
			<S sid ="59" ssid = "38">The basic LCS has a problem that it does not differentiate LCSes of different spatial relations withintheir embedding sequences (Lin, 2004).</S>
			<S sid ="60" ssid = "39">To improvethe basic LCS method, we can remember the lengthof consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS.</S>
			<S sid ="61" ssid = "40">We call this weighted LCS (WLCS)and use k to indicate the length of the current consecutive matches ending at words xi and yj . Giventwo sentences X and Y, the WLCS score of X andY can be computed using the similar dynamic programming procedure as stated in (Lin, 2004).</S>
			<S sid ="62" ssid = "41">Wecomputed the LCS and WLCS-based F-measure following (Lin, 2004) using both the query pool and thesentence pool as in the previous section.</S>
			<S sid ="63" ssid = "42">Skip-bigram is any pair of words in their sentenceorder, allowing for arbitrary gaps.</S>
			<S sid ="64" ssid = "43">Skip-bigram measures the overlap of skip-bigrams between a candidate sentence and a query sentence.</S>
			<S sid ="65" ssid = "44">Following (Lin,2004), we computed the skip bi-gram score usingboth the sentence pool and the query pool.</S>
			<S sid ="66" ssid = "45">306 3.1.3 Head and Head Related-words Overlap The number of head words common in betweentwo sentences can indicate how much they are relevant to each other.</S>
			<S sid ="67" ssid = "46">In order to extract the headsfrom the sentence (or query), the sentence (or query)is parsed by Minipar 2 and from the dependencytree we extract the heads which we call exact headwords.</S>
			<S sid ="68" ssid = "47">For example, the head word of the sentence:“John eats rice” is “eat”.</S>
			<S sid ="69" ssid = "48">We take the synonyms, hyponyms and hyper-nyms3 of both the query-head words and thesentence-head words and form a set of words whichwe call head-related words.</S>
			<S sid ="70" ssid = "49">We measured the exacthead score and the head-related score as follows: ExactHeadScore = ?w1?HeadSet Countmatch(w1)?w1?HeadSet Count(w1) HeadRelatedScore = ?w1?HeadRelSet Countmatch(w1)?w1?HeadRelSet Count(w1) Where HeadSet is the set of head words in the sentence and Countmatch is the number of matchesbetween the HeadSet of the query and the sentence.</S>
			<S sid ="71" ssid = "50">HeadRelSet is the set of synonyms, hyponyms and hypernyms of head words in the sentence and Countmatch is the number of matchesbetween the head-related words of the query and thesentence.</S>
			<SUBSECTION>3.2 Lexical Semantic Features.</SUBSECTION>
			<S sid ="72" ssid = "51">We form a set of words which we call QueryRelatedWords by taking the important words from thequery, their first-sense synonyms, the nouns’ hypernyms/hyponyms and important words from thenouns’ gloss definitions.</S>
			<S sid ="73" ssid = "52">Synonym overlap measure is the overlap between the list of synonyms of the important wordsextracted from the candidate sentence and theQueryRelatedWords.</S>
			<S sid ="74" ssid = "53">Hypernym/hyponym overlapmeasure is the overlap between the list of hypernymsand hyponyms of the nouns extracted from the sentence and the QueryRelatedWords, and gloss overlapmeasure is the overlap between the list of importantwords that are extracted from the gloss definitionsof the nouns of the sentence and the QueryRelated-Words.2http://www.cs.ualberta.ca/ lindek/minipar.htm3hypernym and hyponym levels are restricted to 2 and 3 re spectively 3.3 Statistical Similarity Measures.</S>
			<S sid ="75" ssid = "54">Statistical similarity measures are based on thecooccurance of similar words in a corpus.</S>
			<S sid ="76" ssid = "55">Wehave used two statistical similarity measures:1.</S>
			<S sid ="77" ssid = "56">Dependency-based similarity measure and 2.Proximity-based similarity measure.</S>
			<S sid ="78" ssid = "57">Dependency-based similarity measure uses thedependency relations among words in order to measure the similarity.</S>
			<S sid ="79" ssid = "58">It extracts the dependency triplesthen uses statistical approach to measure the similarity.</S>
			<S sid ="80" ssid = "59">Proximity-based similarity measure is computedbased on the linear proximity relationship betweenwords only.</S>
			<S sid ="81" ssid = "60">It uses the information theoretic definition of similarity to measure the similarity.</S>
			<S sid ="82" ssid = "61">We used the data provided by Dr. Dekang Lin4.Using the data, one can retrieve most similar wordsfor a given word.</S>
			<S sid ="83" ssid = "62">The similar words are grouped intoclusters.</S>
			<S sid ="84" ssid = "63">Note that, for a word there can be more thanone cluster.</S>
			<S sid ="85" ssid = "64">Each cluster represents the sense of theword and its similar words for that sense.</S>
			<S sid ="86" ssid = "65">For each query word, we extract all of its clusters from the data.</S>
			<S sid ="87" ssid = "66">Now, in order to determine theright cluster for a query word, we measure the overlap score between the QueryRelatedWords and theclusters of words.</S>
			<S sid ="88" ssid = "67">The hypothesis is that, the clusterthat has more words common with the QueryRelatedWords is the right cluster.</S>
			<S sid ="89" ssid = "68">We chose the cluster fora word which has the highest overlap score.</S>
			<S sid ="90" ssid = "69">Once we get the clusters for the query words, wemeasured the overlap between the cluster words andthe sentence words as follows: Measure = ?w1?SenWords Countmatch(w1)?w1?SenWords Count(w1) Where, SenWords is the set of important words extracted from the sentence and Countmatch is the numberof matches between the sentence words and the clustersof similar words of the query words.</S>
			<SUBSECTION>3.4 Graph-based Similarity MeasureIn LexRank (Erkan and Radev, 2004), the concept ofgraph-based centrality is used to rank a set of sentences,in producing generic multi-document summaries.</SUBSECTION>
			<S sid ="91" ssid = "70">A similarity graph is produced for the sentences in the document collection.</S>
			<S sid ="92" ssid = "71">In the graph, each node represents asentence.</S>
			<S sid ="93" ssid = "72">The edges between the nodes measure the cosine similarity between the respective pair of sentences.The degree of a given node is an indication of how muchimportant the sentence is. Once the similarity graph is. 4http://www.cs.ualberta.ca/ lindek/downloads.htm 307 constructed, the sentences are then ranked according totheir eigenvector centrality.</S>
			<S sid ="94" ssid = "73">To apply LexRank to query-focused context, a topic-sensitive version of LexRank isproposed in (Otterbacher et al., 2005).</S>
			<S sid ="95" ssid = "74">We followed asimilar approach in order to calculate this feature.</S>
			<S sid ="96" ssid = "75">Thescore of a sentence is determined by a mixture model ofthe relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences.</S>
			<SUBSECTION>3.5 Syntactic and Semantic Features:So far, we have included the features of type Bag ofWords (BOW).</SUBSECTION>
			<S sid ="97" ssid = "76">The task like query-based summarizationthat requires the use of more complex syntactic and semantics, the approaches with only BOW are often inadequate to perform fine-level textual analysis.</S>
			<S sid ="98" ssid = "77">We extractedthree features that incorporate syntactic/semantic information..</S>
			<SUBSECTION>3.5.1 Basic Element (BE) Overlap MeasureThe “head-modifier-relation” triples, extracted from the dependency trees are considered as BEs in our experiment.</SUBSECTION>
			<S sid ="99" ssid = "78">The triples encode some syntactic/semantic information and one can quite easily decide whether any twounits match or not- considerably more easily than withlonger units (Zhou et al., 2005).</S>
			<S sid ="100" ssid = "79">We used the BE packagedistributed by ISI5 to extract the BEs for the sentences.</S>
			<S sid ="101" ssid = "80">Once we get the BEs for a sentence, we computed theLikelihood Ratio (LR) for each BE following (Zhou etal., 2005).</S>
			<S sid ="102" ssid = "81">Sorting BEs according to their LR scores produced a BE-ranked list.</S>
			<S sid ="103" ssid = "82">Our goal is to generate a summary that will answer the user questions.</S>
			<S sid ="104" ssid = "83">The rankedlist of BEs in this way contains important BEs at the topwhich may or may not be relevant to the user questions.We filter those BEs by checking whether they contain anyword which is a query word or a QueryRelatedWords (defined in Section 3.2).</S>
			<S sid ="105" ssid = "84">The score of a sentence is the sumof its BE scores divided by the number of BEs in the sentence.3.5.2 Syntactic FeatureEncoding syntactic structure is easier and straight for ward.</S>
			<S sid ="106" ssid = "85">Given a sentence (or query), we first parse it intoa syntactic tree using a syntactic parser (i.e. Charniakparser) and then we calculate the similarity between thetwo trees using the tree kernel defined in (Collins andDuffy, 2001).</S>
			<SUBSECTION>3.5.3 Shallow-semantic FeatureThough introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequatewhen dealing with complex questions whose answersare expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs.</SUBSECTION>
			<S sid ="107" ssid = "86">Shallow semantic representations, bearing amore compact information, could prevent the sparsenessof deep structural approaches and the weakness of BOWmodels (Moschitti et al., 2007).</S>
			<S sid ="108" ssid = "87">Initiatives such as PropBank (PB) (Kingsbury andPalmer, 2002) have made possible the design of accurateautomatic Semantic Role Labeling (SRL) systems likeASSERT (Hacioglu et al., 2003).</S>
			<S sid ="109" ssid = "88">For example, considerthe PB annotation: [ARG0 all][TARGET use][ARG1 the frenchfranc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against othersemantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italianlira][ARG2 as their currency] In order to calculate the semantic similarity betweenthe sentences, we first represent the annotated sentenceusing the tree structures like Figure 1 which we call Semantic Tree (ST).</S>
			<S sid ="110" ssid = "89">In the semantic tree, arguments are replaced with the most important word-often referred to asthe semantic head.</S>
			<S sid ="111" ssid = "90">The sentences may contain one or more subordinateclauses.</S>
			<S sid ="112" ssid = "91">For example the sentence, “the Vatican, locatedwholly within Italy uses the Italian lira as their currency.”gives the STs as in Figure 2.</S>
			<S sid ="113" ssid = "92">As we can see in Figure 2(A), when an argument node corresponds to an entire subordinate clause, we label its leaf with ST, e.g.the leaf of ARG0.</S>
			<S sid ="114" ssid = "93">Such ST node is actually the root ofthe subordinate clause in Figure 2(B).</S>
			<S sid ="115" ssid = "94">If taken separately,such STs do not express the whole meaning of the sentence, hence it is more accurate to define a single structure encoding the dependency between the two predicatesas in Figure 2(C).</S>
			<S sid ="116" ssid = "95">We refer to this kind of nested STs asSTNs.</S>
			<S sid ="117" ssid = "96">Note that, the tree kernel (TK) function defined in(Collins and Duffy, 2001) computes the number of common subtrees between two trees.</S>
			<S sid ="118" ssid = "97">Such subtrees are subject to the constraint that their nodes are taken with allor none of the children they have in the original tree.</S>
			<S sid ="119" ssid = "98">308 Figure 2: Two STs composing a STN Though, this definition of subtrees makes the TK function appropriate for syntactic trees but at the same timemakes it not well suited for the semantic trees (ST) defined above.</S>
			<S sid ="120" ssid = "99">For instance, although the two STs of Figure 1 share most of the subtrees rooted in the ST node,the kernel defined above computes only one match (STARG0 TARGET ARG1 ARG2) which is not useful.</S>
			<S sid ="121" ssid = "100">The critical aspect of the TK function is that the productions of two evaluated nodes have to be identical toallow the match of further descendants.</S>
			<S sid ="122" ssid = "101">This means thatcommon substructures cannot be composed by a nodewith only some of its children as an effective ST representation would require.</S>
			<S sid ="123" ssid = "102">(Moschitti et al., 2007) solve thisproblem by designing the Shallow Semantic Tree Kernel(SSTK) which allows to match portions of a ST. We followed the similar approach to compute the SSTK.</S>
			<S sid ="124" ssid = "103">4 Ranking SentencesIn this section, we describe the scoring techniques in detail..</S>
			<S sid ="125" ssid = "104">4.1 Learning Feature-weights: A Local SearchStrategy.</S>
			<S sid ="126" ssid = "105">In order to fine-tune the weights of the features, we useda local search technique with simulated annealing to findthe global maximum.</S>
			<S sid ="127" ssid = "106">Initially, we set all the feature-weights, w1, · · · , wn, as equal values (i.e. 0.5) (see Algorithm 1).</S>
			<S sid ="128" ssid = "107">Based on the current weights we score thesentences and generate summaries accordingly.</S>
			<S sid ="129" ssid = "108">We evaluate the summaries using the automatic evaluation toolROUGE (Lin, 2004) (described in Section 6) and theROUGE value works as the feedback to our learningloop.</S>
			<S sid ="130" ssid = "109">Our learning system tries to maximize the ROUGEscore in every step by changing the weights individuallyby a specific step size (i.e. 0.01).</S>
			<S sid ="131" ssid = "110">That means, to learnweight wi, we change the value of wi keeping all otherweight values (wj?j 6=i) stagnant.</S>
			<S sid ="132" ssid = "111">For each weight wi,the algorithm achieves the local maximum of ROUGEvalue.</S>
			<S sid ="133" ssid = "112">In order to find the global maximum we ran this algorithm multiple times with different random choicesof initial values (i.e. simulated annealing).</S>
			<S sid ="134" ssid = "113">Input: Stepsize l, Weight Initial Value vOutput: A vector ~w of learned weightsInitialize the weight values wi to v.for i?</S>
			<S sid ="135" ssid = "114">1 to n do rg1 = rg2 = prev = 0while (true) do scoreSentences(~w)generateSummaries()rg2 = evaluateROUGE()if rg1 = rg2 then prev = wiwi+ = lrg1 = rg2 elsebreak endend endreturn ~w Algorithm 1: Tuning weights using Local Searchtechnique Once we have learned the feature-weights, our empirical method computes the final scores for the sentencesusing the formula: scorei = ~xi.</S>
			<S sid ="136" ssid = "115">~w (1) Where, ~xi is the feature vector for i-th sentence, ~w isthe weight vector and scorei is the score of i-th sentence.</S>
			<S sid ="137" ssid = "116">4.2 K-means LearningWe start with a set of initial cluster centers and go throughseveral iterations of assigning each object to the clusterwhose center is closest.</S>
			<S sid ="138" ssid = "117">After all objects have been assigned, we recompute the center of each cluster as thecentroid or mean (µ) of its members.</S>
			<S sid ="139" ssid = "118">Once we have learned the means of the clusters usingthe k-means algorithm, our next task is to rank the sentences according to a probability model.</S>
			<S sid ="140" ssid = "119">We have usedBayesian model in order to do so.</S>
			<S sid ="141" ssid = "120">Bayes’ law says: P (qk|~x,T) = p(~x|qk,T)P (qk|T)?Kk=1 p(~x|qk,T)p(qk|T) (2) where qk is a class, ~x is a feature vector representing a sentence and T is the parameter set of allclass models.</S>
			<S sid ="142" ssid = "121">We set the weights of the clusters asequiprobable (i.e. P (qk|T) = 1/K).</S>
			<S sid ="143" ssid = "122">We calculated 309 p(x|qk,T) using the gaussian probability distribution.</S>
			<S sid ="144" ssid = "123">The gaussian probability density function (pdf)for the d-dimensional random variable ~x is given by: p(µ,S)(~x) =e-12 (~x-µ)TS-1(~x-µ)v 2pidv det(S)(3) where µ, the mean vector and S, the covariancematrix are the parameters of the gaussian distribution.</S>
			<S sid ="145" ssid = "124">We get the means (µ) from the k-means algorithm and we calculate the covariance matrix usingthe unbiased covariance estimation: Sˆ =1 N - 1N?i=1 (xj - µj)(xi - µi)T (4) 4.3 EM LearningEM is an iterative two step procedure:1.</S>
			<S sid ="146" ssid = "125">Expectation-step and 2.</S>
			<S sid ="147" ssid = "126">Maximization-step.In the expectation step, we compute expected valuesfor the hidden variables hi,j which are cluster membership probabilities.</S>
			<S sid ="148" ssid = "127">Given the current parameters,we compute how likely an object belongs to anyof the clusters.</S>
			<S sid ="149" ssid = "128">The maximization step computesthe most likely parameters of the model given thecluster membership probabilities.</S>
			<S sid ="150" ssid = "129">The data-pointsare considered to be generated by a mixture modelof k-gaussians of the form:.</S>
			<S sid ="151" ssid = "130">P (~x) =k?i=1 P (C = i)P (~x|µi,Si) (5) Where the total likelihood of model T with kcomponents given the observed data points, X =x1, · · · , xn is: L(T|X) =n?i=1 k?j=1 P (C = j)P (xi|Tj) =n?i=1 k?j=1 wjP (xi|µj ,Sj) ?n?i=1 logk?j=1 wjP (xi|µj ,Sj) where P is the probability density function (i.e.eq 3).</S>
			<S sid ="152" ssid = "131">µj and Sj are the mean and covariance matrix of component j, respectively.</S>
			<S sid ="153" ssid = "132">Each component contributes a proportion, wj , of the total population,such that: ?Kj=1wj = 1.</S>
			<S sid ="154" ssid = "133">However, a significant problem with the EM algorithm is that it converges to a local maximumof the likelihood function and hence the quality ofthe result depends on the initialization.</S>
			<S sid ="155" ssid = "134">In orderto get good results from using random starting values, we can run the EM algorithm several timesand choose the initial configuration for which weget the maximum log likelihood among all configurations.</S>
			<S sid ="156" ssid = "135">Choosing the best one among severalruns is very computer intensive process.</S>
			<S sid ="157" ssid = "136">So, to improve the outcome of the EM algorithm on gaussian mixture models it is necessary to find a bettermethod of estimating initial means for the components.</S>
			<S sid ="158" ssid = "137">To achieve this aim we explored the widelyused “k-means” algorithm as a cluster (means) finding method.</S>
			<S sid ="159" ssid = "138">That means, the means found by k-means clustering above will be utilized as the initialmeans for EM and we calculate the initial covariance matrices using the unbiased covariance estimation procedure (eq:4).</S>
			<S sid ="160" ssid = "139">Once the sentences are clustered by EM algorithm, we filter out the sentences which arenot query-relevant by checking their probabilities,P (qr|xi,T) where, qr denotes the cluster “query-relevant”.</S>
			<S sid ="161" ssid = "140">If for a sentence xi, P (qr|xi,T) &gt; 0.5then xi is considered to be query-relevant.</S>
			<S sid ="162" ssid = "141">Our next task is to rank the query-relevant sentences in order to include them in the summary.</S>
			<S sid ="163" ssid = "142">Thiscan be done easily by multiplying the feature vector~xi with the weight vector ~w that we learned by thelocal search technique (eq:1).</S>
			<S sid ="164" ssid = "143">5 Redundancy Checking.</S>
			<S sid ="165" ssid = "144">When many of the competing sentences are includedin the summary, the issue of information overlap between parts of the output comes up, and a mechanism for addressing redundancy is needed.</S>
			<S sid ="166" ssid = "145">Therefore, our summarization systems employ a final levelof analysis: before being added to the final output,the sentences deemed to be important are comparedto each other and only those that are not too similar to other candidates are included in the final answer or summary.</S>
			<S sid ="167" ssid = "146">Following (Zhou et al., 2005), wemodeled this by BE overlap between an intermediate summary and a to-be-added candidate summary 310 sentence.</S>
			<S sid ="168" ssid = "147">We call this overlap ratio R, where R isbetween 0 and 1 inclusively.</S>
			<S sid ="169" ssid = "148">Setting R = 0.7 meansthat a candidate summary sentence, s, can be addedto an intermediate summary, S, if the sentence has aBE overlap ratio less than or equal to 0.7.</S>
			<S sid ="170" ssid = "149">6 Experimental Evaluation.</S>
			<S sid ="171" ssid = "150">6.1 Evaluation Setup.</S>
			<S sid ="172" ssid = "151">We used the main task of Document UnderstandingConference (DUC) 2007 for evaluation.</S>
			<S sid ="173" ssid = "152">The taskwas: “Given a complex question (topic description)and a collection of relevant documents, the task is tosynthesize a fluent, well-organized 250-word summary of the documents that answers the question(s)in the topic.” NIST assessors developed topics of interest tothem and choose a set of 25 documents relevant(document cluster) to each topic.</S>
			<S sid ="174" ssid = "153">Each topic and itsdocument cluster were given to 4 different NIST assessors.</S>
			<S sid ="175" ssid = "154">The assessor created a 250-word summaryof the document cluster that satisfies the informationneed expressed in the topic statement.</S>
			<S sid ="176" ssid = "155">These multiple “reference summaries” are used in the evaluationof summary content.</S>
			<S sid ="177" ssid = "156">We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, whichhas been widely adopted by DUC for automaticsummarization evaluation.</S>
			<S sid ="178" ssid = "157">It measures summaryquality by counting overlapping units such as then-grams (ROUGE-N), word sequences (ROUGE-Land ROUGE-W) and word pairs (ROUGES andROUGESU) between the candidate summary andthe reference summary.</S>
			<S sid ="179" ssid = "158">ROUGE parameters wereset as the same as DUC 2007 evaluation setup.</S>
			<S sid ="180" ssid = "159">One purpose of our experiments is to study theimpact of different features for complex questionanswering task.</S>
			<S sid ="181" ssid = "160">To accomplish this, we generatedsummaries for the topics of DUC 2007 by each ofour seven systems defined as below: The LEX system generates summaries based ononly lexical features: n-gram (n=1,2,3,4), LCS,WLCS, skip bi-gram, head, head synonym.</S>
			<S sid ="182" ssid = "161">TheLSEM system considers only lexical semanticfeatures: synonym, hypernym/hyponym, gloss,dependency-based and proximity-based similarity.The COS system generates summary based on thegraph-based method.</S>
			<S sid ="183" ssid = "162">The SYS1 system considers all the features except the BE, syntactic and semantic features.</S>
			<S sid ="184" ssid = "163">The SYS2 system considers all the features except the syntactic and semantic features.</S>
			<S sid ="185" ssid = "164">TheSYS3 considers all the features except the semanticand the ALL6 system generates summaries takingall the features into account.</S>
			<S sid ="186" ssid = "165">6.2 Evaluation Results.</S>
			<S sid ="187" ssid = "166">Table 17 to Table 3, Table 4 to Table 6 and Table 7 toTable 9 show the evaluation measures for k-means,EM and empirical approaches respectively.</S>
			<S sid ="188" ssid = "167">As Table 1 shows, in k-means, SYS2 gets 021%, SYS3gets 432% and ALL gets 336% improvement inROUGE2 scores over the SYS1 system.</S>
			<S sid ="189" ssid = "168">We get bestROUGE-W (Table 2) scores for SYS2 (i.e. including BE) but SYS3 and ALL do not perform well inthis case.</S>
			<S sid ="190" ssid = "169">SYS2 improves the ROUGE-W F-score by1% over SYS1.</S>
			<S sid ="191" ssid = "170">We do not get any improvement inROUGESU (Table 3) scores when we include anykind of syntactic/semantic structures.</S>
			<S sid ="192" ssid = "171">The case is different for EM and empirical approaches.</S>
			<S sid ="193" ssid = "172">Here, in every case we get a significantamount of improvement when we include the syntactic and/or semantic features.</S>
			<S sid ="194" ssid = "173">For EM (Table 4 toTable 6), the ratio of improvement in F-scores overSYS1 is: 13% for SYS2, 315% for SYS3 and 224% for ALL.</S>
			<S sid ="195" ssid = "174">In our empirical approach (Table 7to Table 9), SYS2, SYS3 and ALL improve the F-scores by 311%, 715% and 819% over SYS1 respectively.</S>
			<S sid ="196" ssid = "175">These results clearly indicate the positiveimpact of the syntactic/semantic features for complex question answering task.</S>
			<S sid ="197" ssid = "176">Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.074 0.077 0.086 0.075 0.075 0.078 0.077 P 0.081 0.084 0.093 0.081 0.098 0.107 0.110 F 0.078 0.080 0.089 0.078 0.085 0.090 0.090 Table 1: ROUGE-2 measures in k-means learningTable 10 shows the F-scores of the ROUGE measures for one baseline system, the best system inDUC 2007 and our three scoring techniques considering all features.</S>
			<S sid ="198" ssid = "177">The baseline system gener 6SYS2, SYS3 and ALL systems show the impact of BE,syntactic and semantic features respectively 7R stands for Recall, P stands for Precision and F stands forF-score 311 Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.098 0.097 0.101 0.099 0.101 0.097 0.097P 0.195 0.194 0.200 0.237 0.233 0.241 0.237F 0.130 0.129 0.134 0.140 0.141 0.139 0.138 Table 2: ROUGE-W measures in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.131 0.127 0.139 0.136 0.135 0.135 0.135P 0.155 0.152 0.162 0.176 0.171 0.174 0.174F 0.142 0.139 0.150 0.153 0.151 0.152 0.152 Table 3: ROUGESU in k-means learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.089 0.080 0.087 0.085 0.085 0.089 0.091P 0.096 0.087 0.094 0.092 0.095 0.116 0.138F 0.092 0.083 0.090 0.088 0.090 0.101 0.109 Table 4: ROUGE-2 measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.103 0.096 0.101 0.102 0.101 0.102 0.101P 0.205 0.193 0.200 0.203 0.218 0.222 0.223F 0.137 0.128 0.134 0.136 0.138 0.139 0.139 Table 5: ROUGE-W measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.146 0.128 0.138 0.143 0.144 0.145 0.144P 0.171 0.153 0.162 0.168 0.177 0.186 0.185F 0.157 0.140 0.149 0.154 0.159 0.163 0.162 Table 6: ROUGESU measures in EM learning Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.086 0.080 0.087 0.087 0.090 0.095 0.099P 0.093 0.087 0.094 0.094 0.112 0.115 0.116F 0.089 0.083 0.090 0.090 0.100 0.104 0.107 Table 7: ROUGE-2 in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.102 0.096 0.101 0.102 0.102 0.104 0.105P 0.203 0.193 0.200 0.204 0.239 0.246 0.247F 0.135 0.128 0.134 0.137 0.143 0.147 0.148 Table 8: ROUGE-W in empirical approach Score LEX LSEM COS SYS1 SYS2 SYS3 ALL R 0.144 0.129 0.138 0.145 0.146 0.149 0.150P 0.169 0.153 0.162 0.171 0.182 0.195 0.197F 0.155 0.140 0.150 0.157 0.162 0.169 0.170 Table 9: ROUGESU in empirical approach ates summaries by returning all the leading sentences (up to 250 words) in the &lt;TEXT &gt; field ofthe most recent document(s).</S>
			<S sid ="199" ssid = "178">It shows that the empirical approach outperforms the other two learningtechniques and EM performs better than k-means algorithm.</S>
			<S sid ="200" ssid = "179">EM improves the F-scores over k-meansby 0.722.5%.</S>
			<S sid ="201" ssid = "180">Empirical approach improves the F-scores over k-means and EM by 5.920.2% and 3.56.5% respectively.</S>
			<S sid ="202" ssid = "181">Comparing with the DUC 2007participants our systems achieve top scores and forsome ROUGE measures there is no statistically significant difference between our system and the bestDUC 2007 system.</S>
			<S sid ="203" ssid = "182">System ROUGE-1 ROUGE-2 ROUGE-W ROUGESU Baseline 0.335 0.065 0.114 0.113 Best 0.438 0.122 0.153 0.174 k-means 0.390 0.090 0.138 0.152 EM 0.399 0.109 0.139 0.162 Empirical 0.413 0.107 0.148 0.170 Table 10: F-measures for different systems 7 Conclusion and Future Work.</S>
			<S sid ="204" ssid = "183">Our experiments show the following: (a) our approaches achieve promising results, (b) empiricalapproach outperforms the other two learning andEM performs better than the k-means algorithm forthis particular task, and (c) our systems achieve better results when we include BE, syntactic and semantic features.</S>
			<S sid ="205" ssid = "184">In future, we have the plan to decompose the complex questions into several simple questions beforemeasuring the similarity between the document sentence and the query sentence.</S>
			<S sid ="206" ssid = "185">We expect that by decomposing complex questions into the sets of sub-questions that they entail, systems can improve theaverage quality of answers returned and achieve better coverage for the question as a whole.</S>
			<S sid ="207" ssid = "186">312</S>
	</SECTION>
</PAPER>
