<PAPER>
<S sid="0">Using Random Walks for Question-focused Sentence Retrieval</S>
	<ABSTRACT>
		<S sid="1" ssid="1">We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time . </S>
		<S sid="2" ssid="2">Annotators generated a list of questions central to understanding each story in our corpus . </S>
		<S sid="3" ssid="3">Because of the dynamic nature of the stories , many questions are time-sensitive ( e.g.&#147;How many victims have been found ? &#148; ) Judges found sentences providing an answer to each question . </S>
		<S sid="4" ssid="4">To address the sentence retrieval problem , we apply stochastic graph-based method for comparing the relative importance of the textual units , which was previously used successfully for generic summarization . </S>
		<S sid="5" ssid="5">Currently , we present a topic-sensitive version of our method and hypothesize that it outperform a competitive baseline , which compares the similarity of each sentence to the input question via IDFweightedword overlap . </S>
		<S sid="6" ssid="6">In our experiments , the method achieves a TRDR score that is significantly higher than that of the baseline . </S>
	</ABSTRACT>
	<SECTION number="1" title="Introduction">
			<S sid="7" ssid="7">Recent work has motivated the need for systems that support &#147;Information Synthesis&#148; tasks , in which user seeks a global understanding of a topic story Amigo et al. , 2004 ) . </S>
			<S sid="8" ssid="8">In contrast to the classical question answering setting ( e.g . TREC-style Q &amp; A ( Voorhees and Tice , 2000 ) ) , in which the user presents a single question and the system returns corresponding answer ( or a set of likely answers ) , in this case the user has a more complex information . </S>
			<S sid="9" ssid="9">Similarly , when reading about a complex news story such as an emergency situation , users might seek answers to a set of questions in order to understand it better . </S>
			<S sid="10" ssid="10">For example , Figure 1 shows the interface to our Web-based news summarization system which a user has queried for information about Hurricane Isabel . </S>
			<S sid="11" ssid="11">Understanding such stories is challenging for a number of reasons . </S>
			<S sid="12" ssid="12">In particular , complex stories contain many sub-events ( e.g . devastation of the hurricane , the relief effort , etc . ) Inaddition , while some facts surrounding the situation not change ( such as &#147;Which area did the hurricane first hit ? &#148; ) , others may change with time ( &#147;Howmany people have been left homeless ? &#148; ) . </S>
			<S sid="13" ssid="13">Therefore , we are working towards developing a system for question answering from clusters of complex stories published over time . </S>
			<S sid="14" ssid="14">As can be seen at the bottom of Figure 1 , we plan to add a component to our current system that allows users to ask questions as they read a story . </S>
			<S sid="15" ssid="15">They may then choose to receive a precise answer or a question-focused summary . </S>
			<S sid="16" ssid="16">Currently , we address the question-focused sentence retrieval task . </S>
			<S sid="17" ssid="17">While passage retrieval ( PR ) clearly not a new problem ( e.g . </S>
			<S sid="18" ssid="18">( Robertson et al.,1992 Salton et al. , 1993 ) ) , it remains important and yet often overlooked . </S>
			<S sid="19" ssid="19">As noted by ( Gaizauskas et al.,2004 , while PR is the crucial first step for question answering Q &amp; A research has typically not empha 9 1 5 Hurricane Isabel 's outer bands moving onshore produced on 09/18 6:18 AM 2 % SummaryThe North Carolina coast braced for a weakened but still potent Hurricane Isabel while already rain-soaked areas as faraway as Pennsylvania prepared for possibly ruinous flooding . </S>
			<S sid="20" ssid="20">( 2:3 ) A hurricane warning was in effect from CapeFear in southern North Carolina to the VirginiaMaryland line , and tropical storm warnings extended from South Carolinato New Jersey . </S>
			<S sid="21" ssid="21">( 2:14 While the outer edge of the hurricane approached the North Carolina coast Wednesday , the center of the storm was still 4 0 0 miles south-southeast of Cape Hatteras , N.C. , late Wednesday morning . </S>
			<S sid="22" ssid="22">( 3:10 BBC NEWS World AmericasHurricane Isabel prompts US shutdown ( 4:1 ) Ask us : What states have been affected by the hurricane so far ? </S>
			<S sid="23" ssid="23">Around 200,000 people in coastal areas of North Carolina and Virginia were ordered to evacuate or risk getting trapped flooding from storm surges up to 11 feet . </S>
			<S sid="24" ssid="24">( 5:8 ) The storm was expected to hit with its full fury today , slamming into the North Carolina coast with 105mph winds and 45-foot wave crests , before moving through Virginia and bashing the capital with gusts of about 60 mph . </S>
			<S sid="25" ssid="25">( 7:6 ) Figure 1 : Question tracking interface to a summarization system . </S>
			<S sid="26" ssid="26">sized it . </S>
			<S sid="27" ssid="27">The specific problem we consider differs from the classic task of PR for a Q &amp; A system interesting ways , due to the time-sensitive nature of the stories in our corpus . </S>
			<S sid="28" ssid="28">For example , one challenges that the answer to a users question may be updated and reworded over time by journalists in order to keep a running story fresh , or because the themselves change . </S>
			<S sid="29" ssid="29">Therefore , there is often more than one correct answer to a question.We aim to develop a method for sentence retrieval that goes beyond finding sentences that are similar to a single query . </S>
			<S sid="30" ssid="30">To this end , we propose to use a stochastic , graph-based method . </S>
			<S sid="31" ssid="31">Recently , graph-based methods have proved useful for number of NLP and IR tasks such as documentre-ranking in ad hoc IR ( Kurland and Lee , 2005 ) and analyzing sentiments in text ( Pang and Lee,2004 ) . </S>
			<S sid="32" ssid="32">In ( Erkan and Radev , 2004 ) , we introduce method and successfully applied it generic multi-document summarization . </S>
			<S sid="33" ssid="33">Presently , we introduce topic-sensitive LexRank in creating sentence retrieval system . </S>
			<S sid="34" ssid="34">We evaluate its performance against a competitive baseline , which considers the similarity between each sentence and the question using IDF-weighed word overlap ) . </S>
			<S sid="35" ssid="35">We demonstrate that LexRank significantly improvesquestion-focused sentence selection over the baseline . </S>
	</SECTION>
	<SECTION number="2" title="Formal description of the problem. ">
			<S sid="36" ssid="1">Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the method . </S>
			<S sid="37" ssid="2">In contrast to previous PR systems such as Okapi ( Robertson et al. , 1992 ) , which ranks documents for relevancy and then proceeds to find paragraphs related to a question , we address featherbrained problem of finding sentences containing answers . </S>
			<S sid="38" ssid="3">In addition , the input to our system isa set of documents relevant to the topic of the query the user has already identified ( e.g . via a search engine . </S>
			<S sid="39" ssid="4">Our system does not rank the input documents , nor is it restricted in terms of the number of sentences that may be selected from the same document . </S>
			<S sid="40" ssid="5">The output of our system , a ranked list of sentences relevant to the users question , can be subsequently used as input to an answer selection system in order to find specific answers from the extracted sentences . </S>
			<S sid="41" ssid="6">Alternatively , the sentences cane returned to the user as a question-focused summary . </S>
			<S sid="42" ssid="7">This is similar to snippet ( Wu eta 2004 ) . </S>
			<S sid="43" ssid="8">However , in our system answers are extracted from a set of multiple documents rather than on a document-by-document basis . </S>
	</SECTION>
	<SECTION number="3" title="Our approach: topic-sensitive LexRank. ">
			<S sid="44" ssid="1">3.1 The LexRank method . </S>
			<S sid="45" ssid="2">In ( Erkan and Radev , 2004 ) , the concept of graph-based centrality was used to rank a set of sentences , in producing generic multi-document summaries . </S>
			<S sid="46" ssid="3">To apply LexRank , a similarity graph is produce the sentences in an input document set . </S>
			<S sid="47" ssid="4">In the graph each node represents a sentence . </S>
			<S sid="48" ssid="5">There are edges between nodes for which the cosine similarity between the respective pair of sentences exceeds given threshold . </S>
			<S sid="49" ssid="6">The degree of a given node Isa indication of how much information the respective sentence has in common with other sentences . </S>
			<S sid="50" ssid="7">Therefore , sentences that contain the most salient information in the document set should be very centrality the graph.Figure 2 shows an example of a similarity graph for a set of five input sentences , using a cosine similarity threshold of 0.15 . </S>
			<S sid="51" ssid="8">Once the similarity graph constructed the sentences are then ranked according to their eigenvector centrality . </S>
			<S sid="52" ssid="9">As previously mentioned , the original LexRank method performed welling the context of generic summarization . </S>
			<S sid="53" ssid="10">Below , we describe a topic-sensitive version of LexRank , which is more appropriate for the question-focusedsentence retrieval problem . </S>
			<S sid="54" ssid="11">In the new approach , the 916 score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences . </S>
			<S sid="55" ssid="12">3.2 Relevance to the question . </S>
			<S sid="56" ssid="13">In topic-sensitive LexRank , we first stem all of the sentences in a set of articles and compute word IDFsby the following formula : ides log ( N + 1 0.5 + sfw ) ( 1 ) whereN is the total number of sentences in the cluster , and sfw is the number of sentences that the word appears in . </S>
			<S sid="57" ssid="14">We also stem the question and remove the stop words from it . </S>
			<S sid="58" ssid="15">Then the relevance of a sentence s to the question q is computed by : rel ( s|q ) =Xw ? q log ( tfw , s + 1 ) &#215; log ( tfw , q + 1 ) &#215; ides 2 ) where tfw , s and tfw , q are the number of times appears in s and q , respectively . </S>
			<S sid="59" ssid="16">This model has proven to be successful in query-based sentence retrieval ( Allan et al. , 2003 ) , and is used as our competitive baseline in this study ( e.g . Tables 4 , 5 and 7 . </S>
			<S sid="60" ssid="17">3.3 The mixture model . </S>
			<S sid="61" ssid="18">The baseline system explained above does not make use of any inter-sentence information in a cluster.We hypothesize that a sentence that is similar to the high scoring sentences in the cluster should also a high score . </S>
			<S sid="62" ssid="19">For instance , if a sentence that gets a high score in our baseline model is likely to contain an answer to the question , then a related sentence , which may not be similar to the question itself , is also likely to contain an answer.This idea is captured by the following mixture model , where p ( s|q ) , the score of a sentence s given question q , is determined as the sum of its relevance to the question ( using the same measure as the baseline described above ) and the similarity to the other sentences in the document cluster : p ( s|q ) = d rel ( s|q ) Pz ? C rel ( z|q ) + ( 1-d ) Xv ? C sim ( s , v ) Pz ? C sim ( z , v ) p ( v|q ) ( 3 ) where C is the set of all sentences in the cluster . </S>
			<S sid="63" ssid="20">The value of d , which we will also refer to as the question bias , &#148; is a trade-off between two terms in the Vertices : Sentence IndexSentence Index SalienceSalience SentenceSentence </S>
	</SECTION>
	<SECTION number="4" title="0.1973852892722677 Milan fire brigade officials said that... ">
			<S sid="64" ssid="1">1 0.03614457831325301 At least two people are dead , incl ... </S>
			<S sid="65" ssid="2">0 0.28454242157110576 Officials said the plane was carrying ... </S>
			<S sid="66" ssid="3">2 0.1973852892722677 Italian police said the plane was car.. </S>
			<S sid="67" ssid="4">3 0.28454242157110576 Rescue officials said that at least th ... </S>
			<S sid="68" ssid="5">Graph Figure 2 : LexRank example : sentence similarity graph with a cosine threshold of 0.15 . </S>
			<S sid="69" ssid="6">equation and is determined empirically . </S>
			<S sid="70" ssid="7">For higher values of d , we give more importance to the relevance to the question compared to the similarity to the other sentences in the cluster . </S>
			<S sid="71" ssid="8">The denominator both terms are for normalization , which are described below . </S>
			<S sid="72" ssid="9">We use the cosine measure weighted word IDFs as the similarity between two sentences in a cluster : sim ( x , y ) = Pw ? x , y tfw , xterm y ( ides 2 qPxi ? x ( taxi oxidize 2 &#215;qP yi ? y (  y idyll 2 ( 4 ) Equation 3 can be written in matrix notation follows p = [ dA+ ( 1- d ) B ] Tp ( 5 ) A is the square matrix such that for a given index i , all the elements in the ith column are proportional rel ( i|q ) . </S>
			<S sid="73" ssid="10">B is also a square matrix such that each entry B ( i , j ) is proportional to sim ( i , j ) . </S>
			<S sid="74" ssid="11">Both matrices are normalized so that row sums add up to 1.Note that as a result of this normalization , all rows of the resulting square matrixQ = [ dA+ ( 1-d ) B ] also add up to 1 . </S>
			<S sid="75" ssid="12">Such a matrix is called stochastic defines a Markov chain . </S>
			<S sid="76" ssid="13">If we view each sentence as a state in a Markov chain , thenQ ( i , j ) specifies the transition probability from state i to state jin the corresponding Markov chain . </S>
			<S sid="77" ssid="14">The vector pwe are looking for in Equation 5 is the stationary distribution of the Markov chain . </S>
			<S sid="78" ssid="15">An intuitive interpretation of the stationary distribution can be under- 917 stood by the concept of a random walk on the graph representation of the Markov chain.With probability d , a transition is made from the current node ( sentence ) to the nodes that are similar to the query . </S>
			<S sid="79" ssid="16">With probability ( 1-d ) , a transition is made to the nodes that are lexically similar to the current node . </S>
			<S sid="80" ssid="17">Every transition is weighted according to the similarity distributions . </S>
			<S sid="81" ssid="18">Each element of eigenvector p gives the asymptotic probability of ending at the corresponding state in the long run regardless of the starting state . </S>
			<S sid="82" ssid="19">The stationary distribution of a Markov chain can be computed by a simple iterative algorithm , called power method. 1 A simpler version of Equation 5 , where A is uniform matrix andB is a normalized binary matrix , is known as PageRank ( Brin and Page , 1998 ; Pageet al. , 1998 ) and used to rank the web pages by theGoogle search engine . </S>
			<S sid="83" ssid="20">It was also the model used to rank sentences in ( Erkan and Radev , 2004 ) . </S>
			<S sid="84" ssid="21">3.4 Experiments with topic-sensitive LexRank . </S>
			<S sid="85" ssid="22">We experimented with different values of d on training data . </S>
			<S sid="86" ssid="23">We also considered several threshold for inter-sentence cosine similarities , where we ignored the similarities between the sentences that are below the threshold . </S>
			<S sid="87" ssid="24">In the training phase the experiment , we evaluated all combination with d in the range of [ 0 , 1 ] ( in increments of 0.10 ) and with a similarity threshold ranging from [ 0 , 0.9 ] ( in increments of 0.05 ) . </S>
			<S sid="88" ssid="25">We then found all configurations that outperformed the baseline . </S>
			<S sid="89" ssid="26">These configurations were then applied to development set . </S>
			<S sid="90" ssid="27">Finally , our best sentence retrieval system was applied to our test data set evaluate against the baseline . </S>
			<S sid="91" ssid="28">The remainder of the paper will explain this process and the results in detail . </S>
			<S sid="92" ssid="29">4 Experimental setup . </S>
			<S sid="93" ssid="30">4.1 Corpus . </S>
			<S sid="94" ssid="31">We built a corpus of 20 multi-document clusters of complex news stories , such as plane crashes , political controversies and natural disasters . </S>
			<S sid="95" ssid="32">The data 1The stationary distribution is unique and the power methods guaranteed to converge provided that the Markov chain ergodic Seneta , 1981 ) . </S>
			<S sid="96" ssid="33">A non-ergodic Markov chain can remade ergodic by reserving a small probability for jumping tony other state from the current state ( Page et al. , 1998 ) . </S>
			<S sid="97" ssid="34">clusters and their characteristics are shown in Table 1 . </S>
			<S sid="98" ssid="35">The news articles were collected from various . </S>
			<S sid="99" ssid="36">&#147;Newstracker&#148; clusters were collected automatically by our Web-based news summarization system . </S>
			<S sid="100" ssid="37">The number of clusters randomly assigned the training , development and test data sets were 11 , 3 and 6 , respectively.Next , we assigned each cluster of articles to an annotator , who was asked to read all articles in the cluster . </S>
			<S sid="101" ssid="38">He or she then generated a list of factual questions key to understanding the story . </S>
			<S sid="102" ssid="39">Once collected the questions for each cluster , two dependently annotated nine of the training clusters . </S>
			<S sid="103" ssid="40">For each sentence and question pair in a clusters the judges were asked to indicate whether not the sentence contained a complete answer to the question . </S>
			<S sid="104" ssid="41">Once an acceptable rate of inter-judge agreement was verified on the first nine clusters ( Kappa ( Carletta , 1996 ) of 0.68 ) , the remaining 1 1 clusters were annotated by one judge each.In some cases , the judges did not find any sentences containing the answer for a given question.Such questions were removed from the corpus . </S>
			<S sid="105" ssid="42">The final number of questions annotated for answer the entire corpus was 341 , and the distribution questions per cluster can be found in Table 1 . </S>
			<S sid="106" ssid="43">4.2 Evaluation metrics and methods . </S>
			<S sid="107" ssid="44">To evaluate our sentence retrieval mechanism , reproduced extract files , which contain a list of sentences deemed to be relevant to the question , for the system and from human judgment . </S>
			<S sid="108" ssid="45">To compare different configurations of our system to the baseline system we produced extracts at a fixed length of  2 0sentences . </S>
			<S sid="109" ssid="46">While evaluations of question answering systems are often based on a shorter list of ranked sentences we chose to generate longer lists for several reasons . </S>
			<S sid="110" ssid="47">One is that we are developing a PRsystem , of which the output can then be input to an answer extraction system for further processing . </S>
			<S sid="111" ssid="48">In such a setting , we would most likely want to generate a relatively longer list of candidate sentences . </S>
			<S sid="112" ssid="49">As previously mentioned , in our corpus the question soften have more than one relevant answer , so ideally , our PR system would find many of the relevant sentences , sending them on to the answer component to decide which answer ( s ) should be returned to the user . </S>
			<S sid="113" ssid="50">Each systems extract file lists the document 918 Cluster Sources Articles Questions Data set Sample question Algerian terror AFP , UPI 2 12 train What is the condition under which threat will take its action ? Milan plane MSNBC , CNN , ABC , 9 15 train How many people were in the crash Fox , USAToday building at the time of the crash ? Turkish plane BBC , ABC , 10 12 train To where was the plane headed ? crash FoxNews , YahooMoscow terror UPI , AFP , AP 7 7 train How many people were killed attack the most recent explosion ? Rhode Island MSNBC , CNN , ABC , Lycos , 10 8 train Who was to blame for club fire Fox , BBC , Ananova the fire ? FBI most AFP , UPI 3 14 train How much is the State Department offering for information leading to bin Laden&#146;s arrest ? Russia bombing AP , AFP 2 11 train What was the cause of the blast ? Bali terror CNN , FoxNews , ABC , 10 30 train What were the motivation BBC , Ananova of the attackers ? Washington DC FoxNews , Ha&#146;aretz , BBC , 8 28 train What kinds of equipment or weapon BBC , Washington Times , CBS were used in the killings ? GSPC terror Newstracker 8 29 train What are the charges against the GSPC suspects ? China Novelty 43 25 18 train What was the magnitude of earthquake earthquake in Zhangjiakou ? Gulfair ABC , BBC , CNN , USAToday , 11 29 devoutest How many people FoxNews , Washington Post were on board ? David Beckham AFP 20 28 devoutest How long had Beckham been playing for trade MU before he moved to RM ? Miami airport Newstracker 12 15 devoutest How many concourses evacuation the airport have ? US hurricane DUC d04a 14 14 test In which places had the hurricane landed ? EgyptAir crash Novelty 4 25 29 test How many people were killed ? Kursk submarine Novelty 33 25 30 test When did the Kursk sink ? Hebrew University bombing Newstracker 11 27 test How many people were injured ? Finland mall bombing Newstracker 9 15 test How many people were in the mall at the time of the bombing ? Putin visits Newstracker 12 20 test What issue concerned BritishEngland human rights groups ? </S>
			<S sid="114" ssid="51">Table 1 : Corpus of complex news stories . </S>
			<S sid="115" ssid="52">and sentence numbers of the top 20 sentences . </S>
			<S sid="116" ssid="53">Theobald extracts list the sentences judged containing answers to a given question by the annotators ( and therefore have variable sizes ) in no particular order. 2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank ( MRR ) ( Voorhees and Tice , 2000 ) and Total ReciprocalDocument Rank ( TRDR ) ( Radev et al. , 2005 ) .MRR , used in the TREC Q &amp; A evaluations , is the reciprocal rank of the first correct answer ( or sentence , in our case ) to a given question . </S>
			<S sid="117" ssid="54">This measure gives us an idea of how far down we must look in the ranked list in order to find a correct answer . </S>
			<S sid="118" ssid="55">To contrast , TRDR is the total of the reciprocal ranks of answers found by the system . </S>
			<S sid="119" ssid="56">In the context of answering questions from complex stories , where there is often more than one correct answer to a question , and where answers are typically time-dependent , should focus on maximizing TRDR , which gives us 2For clusters annotated by two judges , all sentences chosen by at least one judge were included . </S>
			<S sid="120" ssid="57">a measure of how many of the relevant sentences were identified by the system . </S>
			<S sid="121" ssid="58">However , we report both the average MRR and TRDR over all questions in a given data set . </S>
	</SECTION>
	<SECTION number="5" title="LexRank versus the baseline system. ">
			<S sid="122" ssid="1">In the training phase , we searched the parameter space for the values of d ( the question bias ) and the similarity threshold in order to optimize the resultingTRDR scores . </S>
			<S sid="123" ssid="2">For our problem , we expected that relatively low similarity threshold pair with a high question bias would achieve the best results . </S>
			<S sid="124" ssid="3">Table  2shows the effect of varying the similarity threshold. 3 The notation LR [ a , d ] is used , where a is the similarity threshold and d is the question bias . </S>
			<S sid="125" ssid="4">The optimal range for the parameter a was between 0.14 and0.20 . </S>
			<S sid="126" ssid="5">This is intuitive because if the threshold is too high such that only the most lexically similar sentences are represented in the graph , the method does find sentences that are related but are more lex3A threshold of -1 means that no threshold was used such that all sentences were included in the graph . </S>
			<S sid="127" ssid="6">919 System Ave. MRR Ave. TRDR LR [ -1.0,0.65 ] 0.5270 0.8117LR [ 0.02,0.65 ] 0.5261 0.7950LR [ 0.16,0.65 ] 0.5131 0.8134LR [ 0.18,0.65 ] 0.5062 0.8020LR [ 0.20,0.65 ] 0.5091 0.7944LR [ -1.0,0.80 ] 0.5288 0.8152LR [ 0.02,0.80 ] 0.5324 0.8043LR [ 0.16,0.80 ] 0.5184 0.8160LR [ 0.18,0.80 ] 0.5199 0.8154LR [ 0.20,0.80 ] 0.5282 0.8152 Table 2 : Training phase : effect of similarity threshold ( a ) on Ave. MRR and TRDR . </S>
			<S sid="128" ssid="7">System Ave. MRR Ave. TRDR LR [ 0.02,0.65 ] 0.5261 0.7950LR [ 0.02,0.70 ] 0.5290 0.7997LR [ 0.02,0.75 ] 0.5299 0.8013LR [ 0.02,0.80 ] 0.5324 0.8043LR [ 0.02,0.85 ] 0.5322 0.8038LR [ 0.02,0.90 ] 0.5323 0.8077LR [ 0.20,0.65 ] 0.5091 0.7944LR [ 0.20,0.70 ] 0.5244 0.8105LR [ 0.20,0.75 ] 0.5285 0.8137LR [ 0.20,0.80 ] 0.5282 0.8152LR [ 0.20,0.85 ] 0.5317 0.8203LR [ 0.20,0.90 ] 0.5368 0.8265 Table 3 : Training phase : effect of question bias ( d ) on Ave. MRR and TRDR . </S>
			<S sid="129" ssid="8">typically diverse ( e.g . paraphrases ) . </S>
			<S sid="130" ssid="9">Table 3 shows the effect of varying the question bias at two different similarity thresholds ( 0.02 and 0.20 ) . </S>
			<S sid="131" ssid="10">It is clear that high question bias is needed . </S>
			<S sid="132" ssid="11">However , a small probability for jumping to a node that is lexically similar to the given sentence ( rather than the question itself is needed . </S>
			<S sid="133" ssid="12">Table 4 shows the configurations of that performed better than the baseline system on the training data , based on mean TRDRscores over the 184 training questions . </S>
			<S sid="134" ssid="13">We applied all four of these configurations to our unseen development data , in order to see if we could differentiate their performances . </S>
			<S sid="135" ssid="14">5.1 Development/testing phase . </S>
			<S sid="136" ssid="15">The scores for the four LexRank systems and the baseline on the development data are shown in System Ave. MRR Ave. TRDR Baseline 0.5518 0.8297 LR [ 0.14,0.95 ] 0.5267 0.8305LR [ 0.18,0.90 ] 0.5376 0.8382LR [ 0.18,0.95 ] 0.5421 0.8382LR [ 0.20,0.95 ] 0.5404 0.8311 Table 4 : Training phase : systems outperforming the baseline in terms of TRDR score . </S>
			<S sid="137" ssid="16">System Ave. MRR Ave. TRDR Baseline 0.5709 1.0002 LR [ 0.14,0.95 ] 0.5882 1.0469LR [ 0.18,0.90 ] 0.5820 1.0288LR [ 0.18,0.95 ] 0.5956 1.0411LR [ 0.20,0.95 ] 0.6068 1.0601 Table 5 : Development testing evaluation . </S>
			<S sid="138" ssid="17">Cluster B-MRR LRMRR B-TRDR LRTRDR Gulfair 0.5446 0.5461 0.9116 0.9797David Beckham trade 0.5074 0.5919 0.7088 0.7991Miami airport 0.7401 0.7517 1.7157  1. 7 0 2 8evacuation Table 6 : Average scores by cluster : baseline versusLR [ 0.20,0.95 ] . </S>
			<S sid="139" ssid="18">Table 5 . </S>
			<S sid="140" ssid="19">This time , all four LexRank systems outperformed the baseline , both in terms of average MRRand TRDR scores . </S>
			<S sid="141" ssid="20">An analysis of the average scores over the 72 questions within each of the three clusters for the best system , LR [ 0.20,0.95 ] , is shown in Table 6 . </S>
			<S sid="142" ssid="21">While LexRank outperforms the baseline system on the first two clusters both in terms of and TRDR , their performances are not substantially different on the third cluster . </S>
			<S sid="143" ssid="22">Therefore , we examined properties of the questions within each cluster in order to see what effect they might have on system hypothesized that the baseline system , which compares the similarity of each sentence to the question using IDF-weighted word overlap , should perform well on questions that provide many content words . </S>
			<S sid="144" ssid="23">To contrast , LexRank might perform better when the question provides fewer content words , since it considers both similarity to the query andinter-sentence similarity . </S>
			<S sid="145" ssid="24">Out of the 72 questions in the set , the baseline system outperformed LexRank on 22 of the questions . </S>
			<S sid="146" ssid="25">In fact , the average number of content words among these  2 2questions was slightly , but not significantly , higher than the average on the remaining questions (  3. 6 3words per question versus 3.46 ) . </S>
			<S sid="147" ssid="26">Given this observation , we experimented with two mixed strategies , in which the number of content words in a question determined whether LexRank or the baseline system was used for sentence retrieval . </S>
			<S sid="148" ssid="27">We tried threshold of 4 and 6 content words , however , this did not improve the performance over the pure strategy system LR [ 0.20,0.95 ] . </S>
			<S sid="149" ssid="28">Therefore , we applied this 920 Ave. MRR Ave. TRDR Baseline 0.5780 0.8673 LR [ 0.20,0.95 ] 0.6189 0.9906p-value na 0.0619 Table 7 : Testing phase : baseline vs. LR [ 0.20,0.95 ] . </S>
			<S sid="150" ssid="29">system versus the baseline to our unseen test set of134 questions . </S>
			<S sid="151" ssid="30">5.2 Testing phase . </S>
			<S sid="152" ssid="31">As shown in Table 7 , LR [ 0.20,0.95 ] outperformed the baseline system on the test data both in terms of average MRR and TRDR scores . </S>
			<S sid="153" ssid="32">The improvement in average TRDR score was statistically significant with a p-value of 0.0619 . </S>
			<S sid="154" ssid="33">Since we are interested in a passage retrieval mechanism that sentences relevant to a given question , providing input to the question answering component of our system , the improvement in average TRDR score is very promising . </S>
			<S sid="155" ssid="34">While we saw in Section 5.1 thatLR [ 0.20,0.95 ] may perform better on some question cluster types than others , we conclude that it beats the competitive baseline when one is looking to optimize mean TRDR scores over a large set of questions . </S>
			<S sid="156" ssid="35">However , in future work , we will continue improve the performance , perhaps by developing mixed strategies using different configurations of . </S>
	</SECTION>
	<SECTION number="6" title="Discussion. ">
			<S sid="157" ssid="1">The idea behind using LexRank for sentence retrieval is that a system that considers only the similarity between candidate sentences and the input and not the similarity between the candidate sentences themselves , is likely to miss some important sentences . </S>
			<S sid="158" ssid="2">When using any metric to compare sentences and a query , there is always likely to bea tie between multiple sentences ( or , similarly , there may be cases where fewer than the number of desired sentences have similarity scores above zero ) .LexRank effectively provides a means to break such ties . </S>
			<S sid="159" ssid="3">An example of such a scenario is illustrated inTables 8 and 9 , which show the top ranked sentence the baseline and LexRank , respectively for the question caused the Kursk to sink ? &#148; from theKursk submarine cluster . </S>
			<S sid="160" ssid="4">It can be seen that all top five sentences chosen by the baseline system have Rank Sentence Score Relevant ? </S>
			<S sid="161" ssid="5">1 The Russian governmental commission on the 4.2282 Naccident of the submarine Kursk sinking in the Barents Sea on August 12 has rejected 1 1 original explanations for the disaster , but still can not conclude what caused the . </S>
			<S sid="162" ssid="6">tragedy indeed , Russian Deputy Premier IlyaKlebanov said here Friday . </S>
			<S sid="163" ssid="7">2 There has been no final word on what caused 4.2282 Nthe submarine to sink while participating in a major naval exercise , but DefenseMinister Igor Sergeyev said the theory . </S>
			<S sid="164" ssid="8">that Kursk may have collided with another is receiving increasingly concrete confirmation. 3 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Y said Thursday that collision with a big object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea . </S>
			<S sid="165" ssid="9">4 Russian Deputy Prime Minister Ilya Klebanov 4.2282 Ysaid Thursday that collision with a big . </S>
			<S sid="166" ssid="10">object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea . </S>
			<S sid="167" ssid="11">5 President Clinton&#146;s national security adviser , 4.2282 NSamuel Berger , has provided his Russian . </S>
			<S sid="168" ssid="12">counterpart with a written summary of whatU.S . naval and intelligence officials believe caused the nuclear-powered submarine Kursk to sink last month in the Barents Sea , officials said Wednesday . </S>
			<S sid="169" ssid="13">Table 8 : Top ranked sentences using baseline system on the question &#147;What caused the Kursk to sink ? &#148; . </S>
			<S sid="170" ssid="14">the same sentence score ( similarity to the query ) , yet the top ranking two sentences are not actually relevant according to the judges . </S>
			<S sid="171" ssid="15">To contrast , LexRankachieved a better ranking of the sentences since it better able to differentiate between them . </S>
			<S sid="172" ssid="16">It should be noted that both for the LexRank and baseline systems , chronological ordering of the documents sentence is preserved , such that in cases where two sentences have the same score , the one published is ranked higher . </S>
	</SECTION>
	<SECTION number="7" title="Conclusion. ">
			<S sid="173" ssid="1">We presented topic-sensitive LexRank and applied it to the problem of sentence retrieval . </S>
			<S sid="174" ssid="2">In a Web-based news summarization setting , users of our system could choose to see the retrieved sentences ( sin Table 9 ) as a question-focused summary . </S>
			<S sid="175" ssid="3">As indicated in Table 9 , each of the top three sentences were judged by our annotators as providing a complete answer to the respective question . </S>
			<S sid="176" ssid="4">While the first two sentences provide the same answer ( a collision caused the Kursk to sink ) , the third sentence a different answer ( an explosion caused the disaster . </S>
			<S sid="177" ssid="5">While the last two sentences do not provide answers according to our judges , they do provide context information about the situation . </S>
			<S sid="178" ssid="6">Alternatively , the user might prefer to see the extracted 921 Rank Sentence Score Relevant ? </S>
			<S sid="179" ssid="7">1 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big . </S>
			<S sid="180" ssid="8">object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea . </S>
			<S sid="181" ssid="9">2 Russian Deputy Prime Minister Ilya Klebanov 0.0133 Ysaid Thursday that collision with a big . </S>
			<S sid="182" ssid="10">object caused the Kursk nuclear submarine to sink to the bottom of the Barents Sea . </S>
			<S sid="183" ssid="11">3 The Russian navy refused to confirm this , 0.0125 Ybut officers have said an explosion in torpedo compartment at the front of the . </S>
			<S sid="184" ssid="12">submarine apparently caused the Kursk to sink. 4 President Clinton&#146;s national security adviser , 0.0124 N Samuel Berger , has provided his Russiancounterpart with a written summary of whatU.S . naval and intelligence officials believe caused the nuclear-powered submarine Kursk to sink last month in the Barents Sea , officials said Wednesday.5 There has been no final word on what caused 0.0123 N the submarine to sink while participating in a major naval exercise , but DefenseMinister Igor Sergeyev said the theory that Kursk may have collided with another is receiving increasingly concrete confirmation . </S>
			<S sid="185" ssid="13">Table 9 : Top ranked sentences using theLR [ 0.20,0.95 ] system on the question &#147;What caused the to sink ? &#148; answers from the retrieved sentences . </S>
			<S sid="186" ssid="14">In this case , the sentences selected by our system would be sent an answer identification component for further processing . </S>
			<S sid="187" ssid="15">As discussed in Section 2 , our goal wast develop a topic-sensitive version of LexRank and to use it to improve a baseline system , which previously been used successfully for query-basedsentence retrieval ( Allan et al. , 2003 ) . </S>
			<S sid="188" ssid="16">In terms of this task , we have shown that over a large set of unaltered questions written by our annotators , LexRankcan , on average , outperform the baseline system , particularly in terms of TRDR scores . </S>
	</SECTION>
	<SECTION number="8" title="Acknowledgments. ">
			<S sid="189" ssid="1">We would like to thank the members of the CLAIRgroup at Michigan and in particular Siwei Shen andYang Ye for their assistance with this project . </S>
	</SECTION>
</PAPER>