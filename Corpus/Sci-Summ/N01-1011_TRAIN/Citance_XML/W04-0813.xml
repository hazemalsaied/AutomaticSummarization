<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Our group participated in the Basque and En­glish lexical sample tasks in Senseval3.</S>
		<S sid ="2" ssid = "2">A language-speci.c feature set was de.ned for Basque.</S>
		<S sid ="3" ssid = "3">Four di.erent learning algorithms were applied, and also a method that combined their outputs.</S>
		<S sid ="4" ssid = "4">Before submission, the performance of the methods was tested for each task on the Senseval3 training data using cross validation.</S>
		<S sid ="5" ssid = "5">Finally, two systems were submitted for each language: the best single algorithm and the best ensemble.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="6" ssid = "6">Our group (BCU, Basque Country University), participated in the Basque and English lexical sample tasks in Senseval3.</S>
			<S sid ="7" ssid = "7">We applied 4 di.er­ent learning algorithms (Decision Lists, Naive Bayes, Vector Space Model, and Support Vector Machines), and also a method that combined their outputs.</S>
			<S sid ="8" ssid = "8">These algorithms were previously tested and tuned on the Senseval2 data for En­glish.</S>
			<S sid ="9" ssid = "9">Before submission, the performance of the methods was tested for each task on the Senseval3 training data using 10 fold cross val­idation.</S>
			<S sid ="10" ssid = "10">Finally, two systems were submitted for each language, the best single algorithm and the best ensemble in cross-validation.</S>
			<S sid ="11" ssid = "11">The main di.erence between the Basque and English systems was the feature set.</S>
			<S sid ="12" ssid = "12">A rich set of features was used for English, includ­ing syntactic dependencies and domain infor­mation, extracted with di.erent tools, and also from external resources like WordNet Domains (Magnini and Cavagli´a, 2000).</S>
			<S sid ="13" ssid = "13">The features for Basque were di.erent, as Basque is an agglu­tinative language, and syntactic information is given by in.ectional su.xes.</S>
			<S sid ="14" ssid = "14">We tried to rep­resent this information in local features, relying on the analysis of a deep morphological analyzer developed in our group (Aduriz et al., 2000).</S>
			<S sid ="15" ssid = "15">In order to improve the performance of the al­gorithms, di.erent smoothing techniques were David Martinez IXA NLP Group Basque Country University Donostia, Spain davidm@si.ehu.es tested on the English Senseval2 lexical sam­ple data (Agirre and Martinez, 2004), and ap­plied to Senseval3.</S>
			<S sid ="16" ssid = "16">These methods helped to obtain better estimations for the features, and to avoid the problem of 0 counts Decision Lists and Naive Bayes.</S>
			<S sid ="17" ssid = "17">This paper is organized as follows.</S>
			<S sid ="18" ssid = "18">The learn­ing algorithms are .rst introduced in Section 2, and Section 3 describes the features applied to each task.</S>
			<S sid ="19" ssid = "19">In Section 4, we present the exper­iments performed on training data before sub­mission; this section also covers the .nal con.g­uration of each algorithm, and the performance obtained on training data.</S>
			<S sid ="20" ssid = "20">Finally, the o.cial results in Senseval3 are presented and discussed in Section 5.</S>
	</SECTION>
	<SECTION title="Learning Algorithms. " number = "2">
			<S sid ="21" ssid = "1">The algorithms presented in this section rely on features extracted from the context of the target word to make their decisions.</S>
			<S sid ="22" ssid = "2">The Decision List (DL) algorithm is de­scribed in (Yarowsky, 1995b).</S>
			<S sid ="23" ssid = "3">In this algorithm the sense with the highest weighted feature is se­lected, as shown below.</S>
			<S sid ="24" ssid = "4">We can avoid undeter­mined values by discarding features that have a 0 probability in the divisor.</S>
			<S sid ="25" ssid = "5">More sophisticated smoothing techniques have also been tried (cf.</S>
			<S sid ="26" ssid = "6">Section 4).</S>
			<S sid ="27" ssid = "7">Pr(sk|fi) arg max w(sk,fi)= log( .) k =k Pr(sj|fi) j The Naive Bayes (NB) algorithm is based on the conditional probability of each sense given the features in the context.</S>
			<S sid ="28" ssid = "8">It also re­quires smoothing.</S>
			<S sid ="29" ssid = "9">m arg max P(sk) .i=1 P(fi|sk) k For the Vector Space Model (V) algo­rithm, we represent each occurrence context as a vector, where each feature will have a 1 or 0 SENSEVAL3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004 Association for Computational Linguistics value to indicate the occurrence/absence of the feature.</S>
			<S sid ="30" ssid = "10">For each sense in training, one cen­troid vector is obtained.</S>
			<S sid ="31" ssid = "11">These centroids are compared with the vectors that represent test­ing examples, by means of the cosine similarity function.</S>
			<S sid ="32" ssid = "12">The closest centroid is used to assign its sense to the testing example.</S>
			<S sid ="33" ssid = "13">No smooth­ing is required to apply this algorithm, but it is possible to use smoothed values.</S>
			<S sid ="34" ssid = "14">Regarding Support Vector Machines (SVM) we utilized SVM-Light (Joachims, 1999), a public distribution of SVM.</S>
			<S sid ="35" ssid = "15">Linear ker­nels were applied, and the soft margin (C) was estimated per each word (cf.</S>
			<S sid ="36" ssid = "16">Section 4).</S>
			<S sid ="37" ssid = "17">3Features 3.1 Features for English.</S>
			<S sid ="38" ssid = "18">We relied onanextensive setoffeatures of di.erent types, obtained by means of di.erent tools and resources.</S>
			<S sid ="39" ssid = "19">The features used can be grouped in four groups: Local collocations: bigrams and trigrams formed with the words around the target.</S>
			<S sid ="40" ssid = "20">These features are constituted with lemmas, word-forms, or PoS tags1 . Other local features are those formed with the previous/posterior lemma/word-form in the context.</S>
			<S sid ="41" ssid = "21">Syntactic dependencies: syntactic depen­dencies were extracted using heuristic patterns, and regular expressions de.ned with the PoS tags around the target2 . The following rela­tions were used: object, subject, noun-modi.er, preposition, and sibling.</S>
			<S sid ="42" ssid = "22">Bag-of-words features:we extract the lemmas of the content words in the whole con­text, and in a ±4-word window around the tar­get.</S>
			<S sid ="43" ssid = "23">We also obtain salient bigrams in the con­text, with the methods and the software de­scribed in (Pedersen, 2001).</S>
			<S sid ="44" ssid = "24">Domain features: The WordNet Domains resource was used to identify the most relevant domains in the context.</S>
			<S sid ="45" ssid = "25">Following the relevance formula presented in (Magnini and Cavagli´a, 2000), we de.ned 2 feature types: (1) the most relevant domain, and (2) a list of domains above a prede.ned threshold3 . Other experiments us­ing domains from SUMO, the EuroWordNet 1The PoS tagging was performed with the fnTBL toolkit(Ngai andFlorian, 2001).</S>
			<S sid ="46" ssid = "26">2This software was kindly provided by David Yarowsky’s group, from Johns Hopkins University.</S>
			<S sid ="47" ssid = "27">3 The software to obtain the relevant domains was kindly provided by Gerard Escudero’s group, from Uni­versitat Politecnica de Catalunya top-ontology, and WordNet’s Semantic Fields were performed, but these features were dis­carded from the .nal set.</S>
			<SUBSECTION>3.2 Features for Basque.</SUBSECTION>
			<S sid ="48" ssid = "28">Basque is an agglutinative language, and syn­tactic information is given by in.ectional suf­.xes.</S>
			<S sid ="49" ssid = "29">The morphological analysis of the text is a necessary previous step in order to select in­formative features.</S>
			<S sid ="50" ssid = "30">The data provided by the task organization includes information about the lemma, declension case, and PoS for the par­ticipating systems.</S>
			<S sid ="51" ssid = "31">Our group used directly the output of the parser (Aduriz et al., 2000), which includes some additional features: number, de­terminer mark, ambiguous analyses and elliptic words.</S>
			<S sid ="52" ssid = "32">For a few examples, the morphological analysis was not available, due to parsing errors.</S>
			<S sid ="53" ssid = "33">In Basque, the determiner, the number and the declension case are appended to the last el­ement of the phrase.</S>
			<S sid ="54" ssid = "34">When de.ning our fea­ture set for Basque, we tried to introduce the same knowledge that is represented by features that work well for English.</S>
			<S sid ="55" ssid = "35">We will describe our feature set with an example: for the phrase ”elizaren arduradunei” (which means ”to the directors of the church”) we get the following analysis from our analyzer: eliza |-ren |arduradun |-ei church |of the |director |to the +pl. The order of the words is the inverse in En­glish.</S>
			<S sid ="56" ssid = "36">We extract the following information for each word: elizaren: Lemma: eliza (church) PoS: noun Declension Case: genitive (of) Number: singular Determiner mark: yes arduradunei: Lemma: arduradun (director) PoS: noun Declension Case: dative (to) Number: plural Determiner mark: yes We will assume that eliza (church) is the target word.</S>
			<S sid ="57" ssid = "37">Words and lemmas are shown in lowercase and the other information in up­percase.</S>
			<S sid ="58" ssid = "38">As local features we de.ned di.erent types of unigrams, bigrams, trigrams and a window of ±4 words.</S>
			<S sid ="59" ssid = "39">The unigrams were con­structed combining word forms, lemmas, case, number, and determiner mark.</S>
			<S sid ="60" ssid = "40">We de.ned 4 kinds of unigrams: Uni wf0 elizaren Uni wf1 eliza SING+DET Uni wf2 eliza GENITIVE Uni wf3 eliza SING+DET GENITIVE As for English, we de.ned bigrams based on word forms, lemmas and parts-of-speech.</S>
			<S sid ="61" ssid = "41">But in order to simulate the bigrams and trigrams used for English, we de.ned di.erent kinds of features.</S>
			<S sid ="62" ssid = "42">For word forms, we distinguished two cases: using the text string (Big wf0), or using the tags from the analysis (Big wf1).</S>
			<S sid ="63" ssid = "43">The word form bigrams for the example are shown below.</S>
			<S sid ="64" ssid = "44">In thecase of the featuretype“Big wf1”, the information is split in three features: Big wf0 elizaren arduradunei Big wf1 eliza GENITIVE Big wf1 GENITIVE arduradun PLUR+DET Big wf1 arduradun PLUR+DET DATIVE Similarly, depending on the use of the de­clension case, we de.ned three kinds of bigrams basedonlemmas: Big lem0 eliza arduradun Big lem1 eliza GENITIVE Big lem1 GENITIVE arduradun Big lem1 arduradun DATIVE Big lem2 eliza GENITIVE Big lem2 arduradun DATIVE The bigrams constructed using Part-of­speech are illustrated below.</S>
			<S sid ="65" ssid = "45">We included the declension case as if it was another PoS: Big pos1 NOUN GENITIVE Big pos1 GENITIVE NOUN Big pos1 NOUN DATIVE Trigrams are built similarly, by combining the information from three consecutive words.</S>
			<S sid ="66" ssid = "46">We also used as local features all the content words in a window of ±4 words around the target.</S>
			<S sid ="67" ssid = "47">Fi­nally, as global features we took all the con­tent lemmas appearing in the context, which was constituted by the target sentence and the two previous and posterior sentences.</S>
			<S sid ="68" ssid = "48">One di.cult case to model in Basque is the el­lipsis.</S>
			<S sid ="69" ssid = "49">For example, the word “elizakoa” means “the one from the church”.</S>
			<S sid ="70" ssid = "50">We were able to extract this information from our analyzer and we represented it in the features, using a special symbol in place of the elliptic word.</S>
			<S sid ="71" ssid = "51">4 Experiments on training data.</S>
			<S sid ="72" ssid = "52">The algorithms that we applied were .rst tested on the Senseval2 lexical sample task for En­glish.</S>
			<S sid ="73" ssid = "53">The best versions were then evaluated by 10 fold cross-validation on the Senseval3 data, both for Basque and English.</S>
			<S sid ="74" ssid = "54">We also used the training data in cross-validation to tune the pa­rameters, such as the smoothed frequencies, or the soft margin for SVM.</S>
			<S sid ="75" ssid = "55">In this section we will describe .rst the parameters of each method (including the smoothing procedure), and then the cross-validation results on the Senseval3 training data.</S>
			<S sid ="76" ssid = "56">4.1 Methods and Parameters.</S>
			<S sid ="77" ssid = "57">DL: On Senseval2 data, we observed that DL improved signi.cantly its performance with a smoothing technique based on (Yarowsky, 1995a).</S>
			<S sid ="78" ssid = "58">For our implementation, the smoothed probabilities were obtained by grouping the ob­servations by raw frequencies and feature types.</S>
			<S sid ="79" ssid = "59">As this method seems sensitive to the feature types and the amount of examples, we tested 3 DL versions: DL smooth (using smoothed probabilities), DL .xed (replacing 0 counts with 0.1), and DL discard (discarding features ap­pearing with only one sense).</S>
			<S sid ="80" ssid = "60">NB: We applied a simple smoothing method presented in (Ng, 1997), where zero counts are replaced by the probability of the given sense divided by the number of examples.</S>
			<S sid ="81" ssid = "61">V: The same smoothing method used for NB was applied for vectors.</S>
			<S sid ="82" ssid = "62">For Basque, two ver­sions were tested: as the Basque parser can re­turn ambiguous analyses, partial weights are as­signed to the features in the context, and we can chose to use these partial weights (p), or assign the full weight to all features (f).</S>
			<S sid ="83" ssid = "63">SVM: No smoothing was applied.</S>
			<S sid ="84" ssid = "64">We esti­mated the soft margin using a greedy process in cross-validation on the training data per each word.</S>
			<S sid ="85" ssid = "65">Combination: Single voting was used, where each system voted for its best ranked sense, and the most voted sense was chosen.</S>
			<S sid ="86" ssid = "66">More sophisticate schemes like ranked voting, were tried on Senseval2 data, but the results did not improve.</S>
			<S sid ="87" ssid = "67">We tested combinations of the 4 algorithms, leaving one out, and the two best.</S>
			<S sid ="88" ssid = "68">The best results were obtained combining 3 methods (leave one out).</S>
			<S sid ="89" ssid = "69">Method Recall vector 73,9 SVM 73,5 DL smooth 69,4 NB 69,4 DL .xed 65,6 DL discard 65,4 MFS 57,1 Table 1: Single systems (English) in cross-validation, sorted by recall.</S>
			<S sid ="90" ssid = "70">Combination Recall SVM-vector-DL smooth-NB SVM-vector-DL .xedNB SVM-vector-DL smooth SVM-vector-DL .xed SVM-vector-NB SVMDL smooth-NB SVMDL .xedNB SVM-vector 73,2 72,7 74,0 73,8 73,6 72,4 71,3 73,1 Table 2: Combined systems (English) in cross-validation, best recall in bold.</S>
			<S sid ="91" ssid = "71">Method Recall SVM 71,1 NB 68,5 vector(f) 66,8 DL smooth 65,9 DL .xed 65,2 vector(p) 65,0 DL discard 60,7 MFS 53,0 Table 3: Single systems (Basque) in cross-validation, sorted by recall.</S>
			<S sid ="92" ssid = "72">Combination Recall SVM-vector-DL smooth-NB SVM-vector-DL .xedNB SVM-vector-DL smooth SVM-vector-DL .xed SVM-vector-NB SVMDL smooth-NB SVMDL .xedNB SVM-vector SVMNB 70,6 71,1 70,6 70,8 71,1 70,2 70,5 69,0 69,8 Table 4: Combined systems (Basque) in cross-validation, best recall in bold.</S>
			<S sid ="93" ssid = "73">Only vector(f) was used for combination.</S>
			<S sid ="94" ssid = "74">4.2 Results on English Training Data.</S>
			<S sid ="95" ssid = "75">The results using cross-validation on the Senseval3 data are shown in Table 1 for single systems, and in Table 2 for combined methods.</S>
			<S sid ="96" ssid = "76">All the algorithms have full-coverage (for En­glish and Basque), therefore the recall and the precision are the same.</S>
			<S sid ="97" ssid = "77">The most frequent sense (MFS) baseline is also provided, and it is easily beaten by all the algorithms.</S>
			<S sid ="98" ssid = "78">We have to note that these .gures are consis­tent with the performance we observed in the Senseval2 data, where the vector method is the best performing single system, and the best combination is SVM-vector-DL smooth.</S>
			<S sid ="99" ssid = "79">There is a small gain when combining 3 systems, which we expected would be higher.</S>
			<S sid ="100" ssid = "80">We submitted the best single system, and the best combination for this task.</S>
			<S sid ="101" ssid = "81">4.3 Results on Basque Training Data.</S>
			<S sid ="102" ssid = "82">The performance on the Senseval3 Basque training data is given in Table 1 for single sys­tems, and in Table 2 for combined methods.</S>
			<S sid ="103" ssid = "83">In this case, the vector method, and DL smooth obtain lower performance in relation to other methods.</S>
			<S sid ="104" ssid = "84">This can be due to the type of fea­tures used, which have not been tested as ex­tensively as for English.</S>
			<S sid ="105" ssid = "85">In fact, it could hap­pen that some features contribute mostly noise.</S>
			<S sid ="106" ssid = "86">Also, the domain tag of the examples, which could provide useful information, was not used.</S>
			<S sid ="107" ssid = "87">There is no improvement when combining dif­ferent systems, and the result of the combina­tion of 4 systems is unusually high in relation to the English experiments.</S>
			<S sid ="108" ssid = "88">We also submit­ted two systems for this task: the best single method in cross-validation (SVM), and the best 3-method combination (SVM-vector-NB).</S>
			<S sid ="109" ssid = "89">5 Results and Conclusions.</S>
			<S sid ="110" ssid = "90">Table 5 shows the performance obtained by our systems and the winning system in the Senseval­3 evaluation.</S>
			<S sid ="111" ssid = "91">We can see that we are very close to the best algorithms in both languages.</S>
			<S sid ="112" ssid = "92">The recall of our systems is 1.2%-1.9% lower than cross-validation for every system and task, which is not surprising when we change the set­ting.</S>
			<S sid ="113" ssid = "93">The combination of methods is useful for English, where we improve the recall in 0.3%, reaching 72.3%.</S>
			<S sid ="114" ssid = "94">The di.erence is statistically signi.cant according to McNemar’s test.</S>
			<S sid ="115" ssid = "95">However, the combination of methods does not improve the results in the the Basque task, where the SVM method alone provides better Table 5: O.cial results for the English and Basque lexical tasks (recall).</S>
			<S sid ="116" ssid = "96">Task Code Method Rec.</S>
			<S sid ="117" ssid = "97">Eng.</S>
			<S sid ="118" ssid = "98">Eng.</S>
			<S sid ="119" ssid = "99">Eng.</S>
			<S sid ="120" ssid = "100">Senseval3 Best BCU comb BCUenglish ? SVM-vector-DL smooth vector 72,9 72,3 72,0 Basq.</S>
			<S sid ="121" ssid = "101">Senseval3 Best ? 70,4 Basq.</S>
			<S sid ="122" ssid = "102">BCUbasque SVM 69,9 Basq.</S>
			<S sid ="123" ssid = "103">BCUBasque comb SVM-vector­ 69,5 NB results (69.9% recall).</S>
			<S sid ="124" ssid = "104">In this case the di.erence is not signi.cant applying McNemar’s test.</S>
			<S sid ="125" ssid = "105">Our disambiguation procedure shows a sim­ilar behavior on the Senseval2 and Senseval3 data for English (both in cross-validation and in the testing part), where the ensemble works best, followed by the vector model.</S>
			<S sid ="126" ssid = "106">This did not apply to the Basque dataset, where some algorithms seem to perform below the expecta­tions.</S>
			<S sid ="127" ssid = "107">For future work, we plan to study better the Basque feature set and include new features, such as domain tags.</S>
			<S sid ="128" ssid = "108">Overall, the ensemble of algorithms provides a more robust system for WSD, and is able to achieve state-of-the-art performance.</S>
			<S sid ="129" ssid = "109">6Acknowledgements We wish to thank both David Yarowsky’s group, from Johns Hopkins University, and Gerard Es­cudero’s group, from Universitat Politecnica de Catalunya, for providing us software for the ac­quisition of features.</S>
			<S sid ="130" ssid = "110">This research has been partially funded by the European Commission (MEANING IST2001-34460).</S>
	</SECTION>
</PAPER>
