A Stochastic Parser Based on a Structural Word Prediction Model
Shinsuke MORI, Masafumi NISHIMURA, Nobuyasu ITOH,
Shiho OGINO, Hideo WATANABE
IBM Research, Tokyo Research Laboratory, IBM Japan, Ltd.
1623-14 Shimotsuruma Yamatoshi, 242-8502, Japan
mori@trl.ibm.co.jp
Abstract
In this paper, we present a stochastic language
model using dependency. This model considers a
sentence as a word sequence and predicts each word
from left to right. The history at each step of pre-
diction is a sequence of partial parse trees covering
the preceding words. First our model predicts the
partial parse trees which have a dependency relation
with the next word among them and then predicts
the next word from only the trees which have a de-
pendency relation with the next word. Our model is
a generative stochastic model, thus this can be used
not only as a parser but also as a language model
of a speech recognizer. In our experiment, we pre-
pared about 1,000 syntactically annotated Japanese
sentences extracted from a �nancial newspaper and
estimated the parameters of our model. We built a
parser based on our model and tested it on approx-
imately 100 sentences of the same newspaper. The
accuracy of the dependency relation was 89.9%, the
highest accuracy level obtained by Japanese stochas-
tic parsers.
1 Introduction
The stochastic language modeling, imported from
the speech recognition area, is one of the successful
methodologies of natural language processing. In
fact, all language models for speech recognition are,
as far as we know, based on an n-gram model and
most practical part-of-speech (POS) taggers are also
based on a word or POS n-gram model or its exten-
sion (Church, 1988; Cutting et al., 1992; Merialdo,
1994; Dermatas and Kokkinakis, 1995). POS tag-
ging is the �rst step of natural language process-
ing, and stochastic taggers have solved this problem
with satisfying accuracy for many applications. The
next step is parsing, or that is to say discovering
the structure of a given sentence. Recently, many
parsers based on the stochastic approach have been
proposed. Although their reported accuracies are
high, they are not accurate enough for many appli-
cations at this stage, and more attempts have to be
made to improve them further.
One of the major applications of a parser is to
parse the spoken text recognized by a speech rec-
ognizer. This attempt is clearly aiming at spoken
language understanding. If we consider how to com-
bine a parser and a speech recognizer, it is better if
the parser is based on a generative stochastic model,
as required for the language model of a speech rec-
ognizer. Here, \generative" means that the sum of
probabilities over all possible sentences is equal to
or less than 1. If the language model is generative,
it allows a seamless combination of the parser and
the speech recognizer. This means that the speech
recognizer has the stochastic parser as its language
model and bene�ts richer information than a nor-
mal n-gram model. Even though such a combina-
tion is not possible in practices , the recognizer out-
puts N -best sentences with their probabilities, and
the parser, taking them as input, parses all of them
and outputs the sentence with its parse tree that
has the highest probability of all possible combina-
tions. As a result, a parser based on a generative
stochastic language model may help a speech rec-
ognizer to select the most syntactically reasonable
sentence among candidates. Therefore, it is better
if the language model of a parser is generative.
In this paper, taking Japanese as the object lan-
guage, we propose a generative stochastic language
model and a parser based on it. This model treats a
sentence as a word sequence and predicts each word
from left to right. The history at each step of predic-
tion is a sequence of partial parse trees covering the
preceding words. To predict a word, our model �rst
predicts which of the partial parse trees at this stage
have dependency relation with the word, and then
predicts the word from the selected partial parse
trees. In Japanese each word depends on a subse-
quent word, that is to say, each dependency relation
is left to right, it is not necessary to predict the di-
rection of each dependency relation. So in order to
extend our model to other languages, the model may
have to predict the direction of each dependency. We
built a parser based on this model, whose parame-
ters are estimated from 1,072 sentences in a �nan-
cial newspaper, and tested it on 119 sentences from
the same newspaper. The accuracy of the depen-
dency relation was 89.9%, the highest obtained by
any Japanese stochastic parsers.
2 Stochastic Language Model based
on Dependency
In this section, we propose a stochastic language
model based on dependency. Unlike most stochas-
tic language models for a parser, our model is the-
oretically based on a hidden Markov model. In our
model a sentence is predicted word by word from left
to right and the state at each step of prediction is
basically a sequence of words whose modi�cand has
not appeared yet. According to a psycholinguistic
report on language structure (Yngve, 1960), there is
an upper limit on the number of the words whose
modi�cands have not appeared yet. This limit is de-
termined by the number of slots in short-term mem-
ory, 7 � 2 (Miller, 1956). With this limitation, we
can design a parser based on a �nite state model.
2.1 Sentence Model
The basic idea of our model is that each word would
be better predicted from the words that have a de-
pendency relation with the word to be predicted
than from the preceding two words (tri-grammodel).
Let us consider the complete structure of the sen-
tence in Figure 1 and a hypothetical structure after
the prediction of the �fth word at the top of Fig-
ure 2. In this hypothetical structure, there are three
trees: one root-only tree (t
b
composed of w
3
) and
two two-node trees (t
a
containing w
1
and w
2
, and t
c
containing w
4
and w
5
). If the last two trees (t
b
and
t
c
) depend on the word w
6
, this word may better
be predicted from these two trees. From this point
of view, our model �rst predicts the trees depending
on the next word and then predicts the next word
from these trees.
Now, let us make the following de�nitions in order
to explain our model formally.
� w = w
1
w
2
� � �w
n
: a sequence of words. Here a
word is de�ned as a pair consisting of a string of
alphabetic characters and a part of speech (e.g.
the/DT).
� t
i
= t
1
t
2
� � � t
k
i
: a sequence of partial parse
trees covering the i-pre�x words (w
1
w
2
� � �w
i
).
� t
+
i
and t
�
i
: subsequences of t
i
having and
not having a dependency relation with the next
word respectively. In Japanese, like many other
languages, no two dependency relations cross
each other; thus t
i
= t
�
i
t
+
i
.
� ht wi : a tree with w as its root and t as the
sequence of all subtrees connected to the root.
After w
i+1
has been predicted from the trees
depending on it (t
+
i
), there are trees remain-
ing (t
�
i
) and a newly produced tree (ht
+
i
w
i+1
i);
thus t
i+1
= t
�
i
� ht
+
i
w
i+1
i.
P (t
5
)
karehe
anothat
aoblue
iending
wasubj
w1 2w 3w 4w 5w 6w
?t 5
�P (t
+
5
jt
5
)
karehe
anothat
aoblue
iending
wasubj
w1 2w 3w 4w 5w 6w
?
t 5- t 5+
�P (w
6
jt
+
5
)
karehe
anothat
aoblue
iending
wasubj apple
ringo
w1 2w 3w 4w 5w 6w
t 5- t 5+< >w6
= P (t
6
); where t
6
= t
�
5
� ht
+
5
w
6
i
Figure 2: Word prediction from a partial parse
� y
max
: upper limit on the number number of
words whose modi�cands have not appeared
yet.
Under these de�nitions, our stochastic language
model is de�ned as follows:
P (w) =
n
Y
i=1
P (w
i
jw
1
w
2
� � �w
i�1
)
�
X
t
n
2T
n
n
Y
i=1
P (w
i
jt
+
i�1
)P (t
+
i�1
jt
i�1
) (1)
where T
n
is all possible binary trees with n nodes.
Here, the �rst factor, (P (w
i
jt
+
i�1
)), is called the word
prediction model and the second, (P (t
+
i�1
jt
i�1
)), the
state prediction model. Let us consider Figure 2
again. At the top is the state just after the predic-
tion of the �fth word. The state prediction model
then predicts the partial parse trees depending on
the next word among all partial parse trees, as shown
in the second �gure. Finally, the word prediction
model predicts the next word from the partial parse
trees depending on it.
As described above, there may be an upper limit
on the number of words whose modi�cands have not
yet appeared. To put it in another way, the length
of the sequence of partial parse trees (t
i
) is limited.
karehe
anothat
aoblue
tabeeat
woobj
iending
tapast
wasubj apple
ringo
w1 2w 3w 4w 5w 6w 7w 9w8w 10w
BT
Figure 1: Dependency structure of a sentence.
Therefore, if the depth of the partial parse tree is also
limited, the number of possible states is limited. Un-
der this constraint, our model can be considered as
a hidden Markov model. In a hidden Markov model,
the �rst factor is called the output probability and
the second, the transition probability.
Since we assume that no two dependency relations
cross each other, the state prediction model only
has to predict the number of the trees depending
on the next word. Thus P (t
+
i�1
jt
i�1
) = P (yjt
i�1
)
where y is the number of trees in the sequence t
+
i�1
.
According to the above assumption, the last y par-
tial parse trees depend on the i-th word. Since the
number of possible parse trees for a word sequence
grows exponentially with the number of the words,
the space of the sequence of partial parse trees is
huge even if the length of the sequence is limited.
This inevitably causes a data-sparseness problem.
To avoid this problem, we limited the number of
levels of nodes used to distinguish trees. In our ex-
periment, only the root and its children are checked
to identify a partial parse tree. Hereafter, we repre-
sent P
LL
to denote this model, in which the lexicon
of the �rst level and that of the second level are con-
sidered. Thus, in our experiment each word and the
number of partial parse trees depending on it are
predicted by a sequence of partial parse trees that
take account of the nodes whose depth is two or less.
It is worth noting that if the dependency structure
of a sentence is linear | that is to say, if each word
depends on the next word, | then our model will
be equivalent to a word tri-gram model.
We introduce an interpolation technique (Jelinek
et al., 1991) into our model like those used in n-gram
models. By loosening tree identi�cation regulations,
we obtain a more general model. For example, if
we check only the POS of the root and the POS
of its children, we will obtain a model similar to a
POS tri-gram model (denoted P
PP
hereafter). If
we check the lexicon of the root, but not that of its
children, the model will be like a word bi-grammodel
(denoted P
NL
hereafter). As a smoothing method,
we can interpolate the model P
LL
, similar to a word
tri-gram model, with a more general model, P
PP
or
P
NL
. In our experiment, as the following formula
indicates, we interpolated seven models of di�erent
generalization levels:
P (w
i
jt
+
i�1
) = �
6
P
LL
(w
i
jt
+
i�1
) + �
5
P
PL
(w
i
jt
+
i�1
)
+�
4
P
PP
(w
i
jt
+
i�1
) + �
3
P
NL
(w
i
jt
+
i�1
)
+�
2
P
NP
(w
i
jt
+
i�1
) + �
1
P
NN
(w
i
jt
+
i�1
)
+�
0
P
w;0�gram
(2)
where X in P
YX
is the check level of the �rst level
of the tree (N: none, P: POS, L: lexicon) and Y is
that of the second level, and P
w;0�gram
is the uniform
distribution over the vocabularyW (P
w;0�gram
(w) =
1=jWj).
The state prediction model (P (yjt
i�1
)) is also in-
terpolated in the same way. In this case, the possi-
ble events are y = 1; 2; : : : ; y
max
, thus; P
y;0�gram
=
1=y
max
.
2.2 Parameter Estimation
Since our model is a hidden Markov model, the pa-
rameters of a model can be estimated from a row
corpus by EM algorithm (Baum, 1972). With this
algorithm, the probability of the row corpus is ex-
pected to be maximized regardless of the structure of
each sentence. So the obtained model is not always
appropriate for a parser.
In order to develop a model appropriate for a
parser, it is better that the parameters are estimated
from a syntactically annotated corpus by a maxi-
mum likelihood estimation (MLE) (Merialdo, 1994)
as follows:
P (wjt
+
)
MLE
=
f(ht
+
w
i
i)
P
w
f(ht
+
w
i
i)
P (t
+
jt)
MLE
=
f(t
+
; t)
f(t)
where f(x) represents the frequency of an event x in
the training corpus.
The interpolation coeÆcients in the formula (2)
are estimated by the deleted interpolation method
(Jelinek et al., 1991).
2.3 Selecting Words to be Lexicalized
Generally speaking, a word-based n-gram model is
better than a POS-based n-gram model in terms of
predictive power; however lexicalization of some in-
frequent words may be harmful because it may cause
a data-sparseness problem. In a practical tagger
(Kupiec, 1989), only the most frequent 100 words are
lexicalized. Also, in a state-of-the-art English parser
(Collins, 1997) only the words that occur more than
4 times in training data are lexicalized.
For this reason, our parser selects the words to be
lexicalized at the time of learning. In the lexical-
ized models described above (P
LL
, P
PL
and P
NL
),
only the selected words are lexicalized. The selec-
tion criterion is parsing accuracy (see section 4) of
a held-out corpus, a small part of the learning cor-
pus excluded from parameter estimation. Thus only
the words that are predicted to improve the parsing
accuracy of the test corpus, or unknown input, are
lexicalized. The algorithm is as follows (see Figure
3):
1. In the initial state all words are in the class of
their POS.
2. All words are sorted in descending order of their
frequency, and the following process is executed
for each word in this order:
(a) The word is lexicalized provisionally and
the accuracy of the held-out corpus is cal-
culated.
(b) If an improvement is observed, the word is
lexicalized de�nitively.
The result of this lexicalization algorithm is used to
identify a partial parse tree. That is to say, only lex-
icalized words are distinguished in lexicalized mod-
els. If no words were selected to be lexicalized, then
P
NL
= P
NP
and P
LL
= P
PL
= P
PP
. It is worth
noting that if we try to join a word with another
word, then this algorithm will be a normal top-down
clustering algorithm.
2.4 Unknown Word Model
To enable our stochastic language model to handle
unknown words, we added an unknown word model
based on a character bi-gram model. If the next
word is not in the vocabulary, the model predicts
its POS and the unknown word model predicts the
string of the word as follows:
P (wjPOS) =
m+1
Y
i=1
P
POS
(x
i
jx
i�1
)
where w = x
1
x
2
� � �x
m
; x
0
= x
m+1
= BT:
BT, a special character corresponding to a word
boundary, is introduced so that the sum of the prob-
ability over all strings is equal to 1.
In the parameter estimation described above, a
learning corpus is divided into k parts. In our ex-
periment, the vocabulary is composed of the words
1w
lexicalize
yes
no
yes
2wPOS2w6
2w 3wPOS2w6
1w
1w
2w 3wPOS2w6
1POSw4 w5 2w 3w
POS2w6
3w
w41POS
w5
w41POS
w5 yes
1w w41POS
w5
Figure 3: A conceptual �gure of the lexicalization
algorithm.
occurring in more than one partial corpus and the
other words are used for parameter estimation of
the unknown word model. The bi-gram probability
of the unknown word model of a POS is estimated
from the words among them and belonging to the
POS as follows:
P
POS
(x
i
jx
i�1
)
MLE
=
f
POS
(x
i
; x
i�1
)
f
POS
(x
i�1
)
:
The character bi-gram model is also interpolated
with a uni-gram model and a zero-gram model.
The interpolation coeÆcients are estimated by the
deleted interpolation method (Jelinek et al., 1991).
3 Syntactic Analysis
Generally, a parser may be considered as a module
that receives a sequence of words annotated with a
POS and outputs its structure. Our parser, which
includes a stochastic unknown word model, however,
is able to accept a character sequence as an input and
execute segmentation, POS tagging, and syntactic
analysis simultaneously
1
. In this section, we explain
our parser, which is based on the language model
described in the preceding section.
3.1 Stochastic Syntactic Analyzer
A syntactic analyzer, based on a stochastic language
model, calculates the parse tree (see Figure 1) with
the highest probability for a given sequence of char-
acters x according to the following formula:
^
T = argmax
w(T )=x
P (T jx)
1
There is no space between words in Japanese
= argmax
w(T )=x
P (T jx)P (x)
= argmax
w(T )=x
P (xjT )P (T ) (
�
�
�
Bayes' formula)
= argmax
w(T )=x
P (T ) (
�
�
�
P (xjT ) = 1);
where w(T ) represents the concatenation of the
word string in the syntactic tree T . P (T ) in the last
line is a stochastic language model. In our parser,
it is the probability of a parse tree T de�ned by the
stochastic dependency model including the unknown
word model described in section 2.
P (T ) =
n
Y
i=1
P (w
i
jt
+
i�1
)P (t
+
i�1
jt
i�1
); (3)
where w
1
w
2
� � �w
n
= w(T ).
3.2 Solution Search Algorithm
As shown in formula (3), our parser is based on a hid-
den Markov model. It follows that Viterbi algorithm
is applicable to search the best solution. Viterbi al-
gorithm is capable of calculating the best solution in
O(n) time, where n is the number of input charac-
ters.
The parser repeats a state transition, reading
characters of the input sentence from left to right. In
order that the structure of the input sentence may be
a tree, the number of trees of the �nal state t
n
must
be 1 and no more. Among the states that satisfy this
condition, the parser selects the state with the high-
est probability. Since our language model uses only
the root and its children of a partial parse tree to dis-
tinguish states, the last state does not have enough
information to construct the parse tree. The parser
can, however, calculate the parse tree from the se-
quence of states, or both the word sequence and the
sequence of y, the number of trees that depend on
the next word. Thus it memorizes these values at
each step of prediction. After the most probable
last state has been selected, the parser constructs
the parse tree by reading these sequences from top
to bottom.
4 Evaluation
We developed a POS-based model and its lexical-
ized version explained in section 2 to evaluate their
predictive power, and implemented parsers based on
them that calculate the most probable dependency
tree from a given character sequence, using the so-
lution search algorithm explained in section 3 to ob-
serve their accuracy. In this section, we present and
discuss the experimental results.
4.1 Conditions on the Experiments
The corpus used in our experiments consists of ar-
ticles extracted from a �nancial newspaper (Nihon
Table 1: Corpus.
#sentences #words #chars
learning 1,072 30,292 46,212
test 119 3,268 4,909
Keizai Shinbun). Each sentence in the articles is
segmented into words and its dependency structure
is annotated by linguists using an editor specially
designed for this task at our site. The corpus was
divided into ten parts; the parameters of the model
were estimated from nine of them and the model
was tested on the rest (see Table 1). A small part
of each leaning corpus is withheld from parameter
estimation and used to select the words to be lex-
icalized. After checking the learning corpus, the
maximum number of partial parse trees is set to 10
(y
max
= 10).
To evaluate the predictive power of our model, we
calculated their cross entropy on the test corpus. In
this process, the annotated tree in the test corpus is
used as the structure of the sentences. Therefore the
probability of each sentence in the test corpus is not
the summation over all its possible derivations. To
compare the POS-based model and the lexicalized
model, we constructed these models using the same
learning corpus and calculated their cross entropy
on the same test corpus. The POS-based model and
the lexicalized model have the same unknown word
model, thus its contribution to the cross entropy is
constant.
We implemented a parser based on the depen-
dency models. Since our models, including a
character-based unknown word model, can return
the best parse tree with its probability for any in-
put, we can build a parser that receives a character
sequence as input. It is not easy to evaluate, how-
ever, because errors may occur in segmentation of
the sequence into words and in estimation of their
POSs. For this reason, in the following description,
we assume a word sequence as the input.
The criterion for a parser is the accuracy of its out-
put dependency relations. This criterion is widely
used to evaluate Japanese dependency parsers. The
accuracy is the ratio of the number of the words an-
notated with the same dependency to the number of
the words as in the corpus:
accuracy
=
#words depending on the correct word
#words
The last word and the second-to-last word of a sen-
tence are excluded, because there is no ambiguity.
The last word has no word to depend on and the
second-to-last word depends always on the last word.
Table 2: Cross entorpy and accuracy of each model.
language model
cross
entropy
accuracy
selectively lexicalized 6.927 89.9%
completely lexicalized 6.651 87.1%
POS-based 7.000 87.5%
linear structure
�
| 78.7%
* Each word depends on the next word.
4.2 Evaluation
Table 2 shows the cross entropy and parsing accu-
racy of the baseline, where all words depend on the
next word, the POS-based dependency model and
two lexicalized dependency models. In the selec-
tively lexicalized model, words to be lexicalized are
selected by the algorithm described in section 2. In
the completely lexicalized model, all words are lexi-
calized. This result attests experimentally that the
parser based on the selectively lexicalized model is
the best parser. As for predictive power, however,
the completely lexicalized model has the lowest cross
entropy. Thus this model is estimated to be the best
language model for speech recognition. Although
there is no direct relation between cross entropy of
the language model and error rate of a speech rec-
ognizer, if we consider a spoken language parser, it
may be better to select the words to be lexicalized
using other criterion.
We calculated the cross entropy and the parsing
accuracy of the model whose parameters are esti-
mated from 1/4, 1/16, and 1/64 of the learning
corpus. The relation between the learning corpus
size and the cross entropy or the parsing accuracy is
shown in Figure 4. The cross entropy has a stronger
tendency to decrease as the corpus size increases.
As for accuracy, there is also a tendency for parsers
to become more accurate as the size of the learning
increases. The size of the corpus we have at this
stage is not at all large. However, its accuracy is at
the top level of Japanese parsers, which use 50,000 -
190,000 sentences. Therefore, we conclude that our
approach is quite promising.
5 Related Works
Historically, structures of natural languages have
been described by a context-free grammar and am-
biguities have been resolved by parsers based on a
context-free grammar (Fujisaki et al., 1989). In re-
cent years, some attempts have been made in the
area of parsing by a �nite state model (O
azer, 1999)
etc. Our parser is also based on a �nite state model.
Unlike these models, we focused on reports on a
limit on language structure caused by the capacity
our memory (Yngve, 1960) (Miller, 1956). Thus our
0
20
40
60
80
100
0 201 401301101 50101
%
cross entropy
#characters in learning corpus
cross entropy
accuracy
parsing accuracy
20
0
16
12
8
4
84.0% 86.2%88.1% 89.9%
Figure 4: Relation between cross entropy and pars-
ing accuracy.
model is psycholinguistically more appropriate.
Recently, in the area of parsers based on a stochas-
tic context-free grammar (SCFG), some researchers
have pointed out the importance of the lexicon
and proposed lexicalized models (Charniak, 1997;
Collins, 1997). In these papers, they reported sig-
ni�cant improvement of parsing accuracy. Taking
these reports into account, we introduced a method
of partial lexicalization and reported signi�cant im-
provement of parsing accuracy. Our lexicalization
method is also applicable to a SCFG-based parser
and improves its parsing accuracy.
The model we present in this paper is a genera-
tive stochastic language model. Chelba and Jelinek
(1998) presented a similar model. In their model,
each word is predicted from two right-most head
words regardless of dependency relation between
these head words and the word. Eisner (1996) also
presented a stochastic structural language model, in
which each word is predicted from its head word
and the nearest one. This model is very similar to
the parser presented by Collins (1996). The great-
est di�erence between our model and these models
is in that our model predicts the next word from the
head words, or partial parse trees, depending on it.
Clearly, it is not always two right-most head words
that have dependency relation with the next word.
It follows that our model is linguistically more ap-
propirate.
There have been some attempts at stochastic
Japanese parser (Haruno et al., 1998) (Fujio and
Matsumoto, 1998) (Mori and Nagao, 1998). These
Japanese parsers are based on a unit called bunsetsu,
a sequence of one or more content words followed by
zero or more function words. The parsers take a
sequence of units and outputs dependency relations
between them. Unlike these parsers, our model de-
scribes dependencies between words; thus our model
can easily be extended to other languages. As for
the accuracy, although a direct comparison is not
easy between our parser (89.9%; 1,072 sentences)
and these parsers (82% - 85%; 50,000 - 190,000 sen-
tences) because of the di�erence of the units and
the corpus, our parser is one of the state-of-the-art
parsers for Japanese language. It should be noted
that our model describes relations among three or
more units (case frame, consecutive dependency re-
lations, etc.); thus our model bene�ts a greater deal
from increase of corpus size.
6 Conclusion
In this paper we have presented a stochastic lan-
guage model based on dependency structure. This
model treats a sentence as a word sequence and pre-
dicts each word from left to right. The history at
each step of prediction is a sequence of partial parse
trees covering the preceding words. To predict a
word, our model �rst selects the partial parse trees
that have a dependency relation with the word, and
then predicts the next word from the selected partial
parse trees. We also presented an algorithm for lexi-
calization. We built parsers based on the POS-based
model and its lexicalized version, whose parameters
are estimated from 1,072 sentences of a �nancial
newspaper. We tested the parsers on 119 sentences
from the same newspaper, which were excluded from
the learning. The accuracy of the dependency rela-
tion of the lexicalized parser was 89.9%, the highest
obtained by any Japanese stochastic parser.
References
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
for probabilistic functions of Markov process. In-
equalities, 3:1{8.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the 14th National Conference on Arti-
�cial Intelligence, pages 598{603.
Ciprian Chelba and Frederic Jelinek. 1998. Exploit-
ing syntactic structure for language modeling. In
Proceedings of the 17th International Conference
on Computational Linguistics, pages 225{231.
Kenneth Ward Church. 1988. A stochastic parts
program and noun phrase parser for unrestricted
text. In Proceedings of the Second Conference on
Applied Natural Language Processing, pages 136{
143.
Michael John Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proceed-
ings of the 34th Annual Meeting of the Association
for Computational Linguistics, pages 184{191.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16{23.
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the Third Conference on
Applied Natural Language Processing, pages 133{
140.
Evangelos Dermatas and George Kokkinakis. 1995.
Automatic stochastic tagging of natural language
texts. Computational Linguistics, 21(2):137{163.
Jason M. Eisner. 1996. Three new probabilistic
models for dependency parsing: An exploration.
In Proceedings of the 16th International Confer-
ence on Computational Linguistics, pages 340{
345.
Masakazu Fujio and Yuji Matsumoto. 1998.
Japanese dependency structure analysis based on
lexicalized statistics. In Proceedings of the Third
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 87{96.
T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and
T. Nishino. 1989. A probabilistic parsing method
for sentence disambiguation. In Proceedings of the
International Parsing Workshop.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1998. Using decision trees to construct a
practical parser. In Proceedings of the 17th Inter-
national Conference on Computational Linguis-
tics, pages 505{511.
Fredelick Jelinek, Robert L. Mercer, and Salim
Roukos. 1991. Principles of lexical language
modeling for speech recognition. In Advances in
Speech Signal Processing, chapter 21, pages 651{
699. Dekker.
Julian Kupiec. 1989. Augmenting a hidden Markov
model for phrase-dependent word tagging. In Pro-
ceedings of the DARPA Speech and Natural Lan-
guage Workshop, pages 92{98.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155{171.
George A. Miller. 1956. The magical number seven,
plus or minus two: Some limits on our capacity
for processing information. The Psychological Re-
view, 63:81{97.
Shinsuke Mori and Makoto Nagao. 1998. A stochas-
tic language model using dependency and its im-
provement by word clustering. In Proceedings of
the 17th International Conference on Computa-
tional Linguistics, pages 898{904.
Kemal O
azer. 1999. Dependency parsing with an
extended �nite state approach. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 254{260.
Victor H. Yngve. 1960. A model and a hypothesis
for language structure. The American Philosoph-
ical Society, 104(5):444{466.
