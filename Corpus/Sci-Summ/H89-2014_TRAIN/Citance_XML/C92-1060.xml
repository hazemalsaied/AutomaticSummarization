<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">A new algorithm is presented for estimating the parameters of a stochastic context-free grammar (SCFG) from ordinary unparsed text.</S>
		<S sid ="2" ssid = "2">Unlike the Inside/OutsideI/O) algorithm which requires a grammar to be spee- fled in Chomsky normal form, the new algorithm canestimate an arbitrary SCFG without ally need for transformation.</S>
		<S sid ="3" ssid = "3">The algorithm has worst-case cubic complexity in the length of a sentence and the number of nonterminais in the grammar.</S>
		<S sid ="4" ssid = "4">Instead of the binary branching tree structure used by the i/O algorithm, thenew algorithm makes use of a trellis structure for computation.</S>
		<S sid ="5" ssid = "5">The trellis is a generalization of that used bythe BaumWelcb algorithm which is used for estimating hidden stochastic regular grammars.</S>
		<S sid ="6" ssid = "6">Tile paper describes tile relationship between the trellis and the more typical parse tree representation.</S>
	</ABSTRACT>
	<SECTION title="INTRODUCTION" number = "1">
			<S sid ="7" ssid = "7">This paper describes an iterative method for estimating the parameters of a hidden stochastic context free grammar (SCFG).</S>
			<S sid ="8" ssid = "8">The &amp;quot;hidden&amp;quot; aspect arises from the fact that ~ome information is not available when the grammar is trained.</S>
			<S sid ="9" ssid = "9">When a parsed corpus is used for training, production probabilities can be estimated by counting the number of times each production is used in the parsed corpus.</S>
			<S sid ="10" ssid = "10">In the case of a hidden SCFG, the characteristic grammar is defined but the parse trees associated with the training corpus are notavailable.</S>
			<S sid ="11" ssid = "11">To proceed in this circumstance, some initim prohabilitie~ are assigned which are iteratively re estimated from their current values, and the training corpus.</S>
			<S sid ="12" ssid = "12">They are adjusted to (locally) maximize the likelihood of generating the training corpus.</S>
			<S sid ="13" ssid = "13">The EM algorithm (Dempster, 1977) embodies the approach justmentioned; the new algorithm can be viewed as its ap plication to arbitrary SCFG&apos;s.</S>
			<S sid ="14" ssid = "14">The use of unparsed training corpora is desirable because changes in thegrammar rules could conceivably require manually re ACRESDECOLING92, NANIES,2328 AO~r 1992parsing the training corpus several times during grammar development.</S>
			<S sid ="15" ssid = "15">Stochastic grammarsenable ambigu ity resolution to performed on the rational basis of niostlikely interpretation.</S>
			<S sid ="16" ssid = "16">They also acconnnodate the development of more robust grammars having high cover age where the attendant ambiguity is generally higher.</S>
			<S sid ="17" ssid = "17">Previous approaches to the problem of estimating hidden SCFG&apos;s include parsing schemes ill which MI derivations of all sentences in the training corpus are enumerated (Fujisaki et al., 1989; Chitrao &amp; Grishman, 1990)).</S>
			<S sid ="18" ssid = "18">An efficient alternative is the Inside/Outside(I/O) algorithm (Baker, 1979) which like the new algorithm, is limited to cubic complexity in both the num ber of nonterminais and length of a ~entence.</S>
			<S sid ="19" ssid = "19">The I/O algorithm requires that tile grammar be in Chonmky normal form (CNF).</S>
			<S sid ="20" ssid = "20">Tile new algorithm hal the samecomplexity, but does not have this restriction, dispens ing with the need to transform to and from GNF.</S>
			<S sid ="21" ssid = "21">TERMINOLOGY The training corpus can be conwmiently segmented into sentences for puposes of training; each sentence comprisinga sequence of words.</S>
			<S sid ="22" ssid = "22">A typical one may consist o f Y + 1 words, indexed from O to Y: The lookup function W(y) returns the index k of the vocabulary entry vk matching tile word w~ at position y ill tile sentence.The algorithm uses a extension of the representation and terminology used for &amp;quot;hidden Markov mod eis&apos;(hidden stochastic regular grammars) for which the Baum-Welch algorithm (Baum, 1972) is applicable (and which is also called the Forward/Backward (F/B)algo~ rithm).</S>
			<S sid ="23" ssid = "23">Grammar rules are represented as networks and illustrated graphically, maintaining a correspondence 387 I&apos;ROC.</S>
			<S sid ="24" ssid = "24">OFCOLING92, NANTES,AUG. 2328.</S>
			<S sid ="25" ssid = "25">1992 Network NP ou; 0.2 Det ADJP Noun 02 Figure 1: Networks for Lexical Ruh~ with the trellis structure on which the computation can be conveniently repre~nted.</S>
			<S sid ="26" ssid = "26">The terminology is closely related to t h a t of Levinson, Rabiner &amp; Sondhi (1983) and also Lari &amp; Young (1990).</S>
			<S sid ="27" ssid = "27">A set o f A f different nonterminals are represented byA; networks.</S>
			<S sid ="28" ssid = "28">A component network for the nontermi nal labeled n has a parameter set (A, B, I, N,F, Top, n).</S>
			<S sid ="29" ssid = "29">To uniquely identify an element of the parameter set requires t h a t it be a function of its nonterminal label e.g. A(n), l(n) etc.).</S>
			<S sid ="30" ssid = "30">However this notation has been topped to make formulae less cumbersome.</S>
			<S sid ="31" ssid = "31">A network labeled N P is shown in Figure 1 which represents the following rules: NP ~ NP ~ N o u n (0.2) Dee N o u n (0.2) N P ~ Dee A D J P N o u n (0.2) N P ==~ A D J P N o u n (0.4) N o u n ==~ &amp;quot; c a t &amp;quot; ( 0 . 0 0 2 ) N o u n ==~ &amp;quot; d o g &amp;quot; ( 0 . 0 0 1 ) D e t ==~ &amp;quot; t h e &amp;quot; ( 0 . 5 ) Dee ~ &amp;quot;a&amp;quot; (0.2) The rule N P =:~ N o u n (0.2) means t h a t if the NPrule is used, the probability is 0.2 t h a t it produces a sin gle N o u n . In Figure 1, states are represented by circles with numbers inside to index them.</S>
			<S sid ="32" ssid = "32">NonierminMstates • re shown with double circles and represent references to other networks, such as ADJP.</S>
			<S sid ="33" ssid = "33">States marked with single circles ate called terminal states and represent part-of-speech categories.</S>
			<S sid ="34" ssid = "34">When a transition is made to a terminal state, a word of the current training sentence is generated.</S>
			<S sid ="35" ssid = "35">The word must have the same category as the state t h a t generated it.</S>
			<S sid ="36" ssid = "36">Rules of the form N o u n =:~ &amp;quot; c a t &amp;quot; ( 0 . 0 0 2 ) and N o u n ==~ &amp;quot; d o g &amp;quot; ( 0 . 0 0 1 ) are collapsed into a state-dependent probability vector b(j), ACT~ DE COLING92, N^~T.S, 2328 Aot~r 1992 Network NP ADJP Noun 0.4~ F 0.4 0.4 Det Figure 2: Equivalent Network l © °°/1:0, °°°&apos;l © og .at 007 F0o0 000,7 i I_the o.s__1 [_tho :: j Noun Figure 3: Reprelentatlon for Terminal Productions which is an element of the output matrix B. Elementsof the vector such as b(j W(y))represent the probabil ity of seeing word wy in terminal state j. A transition to a nonterminal state does not in itself generate any words, however terminal states within the referenced network will do so.</S>
			<S sid ="37" ssid = "37">The parameter N is a matrix whichindicates the label (e.g. n, NP, ADJP) of the network t h a t a nonterminal state refers to.</S>
			<S sid ="38" ssid = "38">The proba bility of making a transition from state i to state j is labeled a(i, j) and collectively these probabilities form the transition matrix A. The initial matrix I contains the production probabilities for rules t h a t are modelledby the network.</S>
			<S sid ="39" ssid = "39">They are indicated in Figure 1 as num bers beneath the state, if they are nonzero, l(i) can beequivalently viewed as the probability t h a t some sub sequence of n is started at state i. The parameter F is the set of final states; any sequence accepted by thenetwork must terminate on a final state.</S>
			<S sid ="40" ssid = "40">In Figure 1 fi nal states are designated with the annotation &amp;quot; F &apos; . The boolean value Top indicates whether the network is the top-level network.</S>
			<S sid ="41" ssid = "41">Only one network m a y be assignedas the top-level network, which models productions in 388 Paoc.</S>
			<S sid ="42" ssid = "42">oF COLING92, NA~rrEs, AuG. 2328, 1992.</S>
			<S sid ="43" ssid = "43">0.2 volving the root symbol of a g r a m m a r . An equivalent network for the same set of rules is shown in Figure 2.</S>
			<S sid ="44" ssid = "44">The lexical rules can be writtencompactly as networks, with fewer states.</S>
			<S sid ="45" ssid = "45">The transi tions from the determiner state each have probability 0.5 (i.e a(1, 2) : a(1,3) = 0.5).</S>
			<S sid ="46" ssid = "46">It should be noted t h a t the algorithm can operate on either network.</S>
			<S sid ="47" ssid = "47">Network NP =a. TRELLIS DIAGRAM &amp;quot;lYellis dia~rsans conveniently relate computationalquantities to the network structure and a training sen tence.</S>
			<S sid ="48" ssid = "48">Each network u has a set of Y + 1 trellises for subsequences of a sentence Wo...wy, starting at each different position and ending at subsequent ones.</S>
			<S sid ="49" ssid = "49">A single trellis spanning positions 0...2 is shown ill Figure 4 for network NP.</S>
			<S sid ="50" ssid = "50">Nonterminal states are associated with a row of start nodes indicating where daughter constituents m a y start, and a row of end nodes t h a t indicate where they end.</S>
			<S sid ="51" ssid = "51">A pair of s t a r t / e n d nodes thus refer to a daughter nonterminal constituent.</S>
			<S sid ="52" ssid = "52">In Figure 4, the A D J P network is referenced via the start state at position O. An adjective is then generated by a terminal state in the trellis for the A D J P network, followed by a transition and another adjective.</S>
			<S sid ="53" ssid = "53">The A D J P network is left at position 1, and a transition ismade to the noun state where the word %at&amp;quot; is gen erated.</S>
			<S sid ="54" ssid = "54">Terminal states are associated with a singlerow of nodes in the trellis (they represent terminal pro ductions t h a t span only a single position).</S>
			<S sid ="55" ssid = "55">The p a t h taken through the trellis is shown with broken a line.</S>
			<S sid ="56" ssid = "56">A path through different trellises has a corresponding unique tree representation, as exemplified in Figure 5.</S>
			<S sid ="57" ssid = "57">In cases where p a r a ~ are ambiguous, several paths exist corresponding to the alternative derivations.</S>
			<S sid ="58" ssid = "58">We shall next consider the c o m p u t a t i o n of the probabilities of the paths.</S>
			<S sid ="59" ssid = "59">Two basic quantities are involved, namelyalpha and beta probabilities.</S>
			<S sid ="60" ssid = "60">Loosely speaking, the al phas represent probabilities of subtrees associated with nonterminals, while the betas refer to the rest of the treestructure external to the subtrees.</S>
			<S sid ="61" ssid = "61">Subsequently, prod ucts of these quantities will be formed, which representthe probabilities of productions being used in generat ing a sentence.</S>
			<S sid ="62" ssid = "62">These are summed over all sentences in the training corpus to obtain the expected number of times each production is used, based on the current production probabilities and the training corpus.</S>
			<S sid ="63" ssid = "63">These are used like frequency counts would be for a parsed corpus, to form ratios t h a t represent new estimates of the production probabilities.</S>
			<S sid ="64" ssid = "64">The procedure is iterated several times until the estimates do not change between iterations (i.e. the overall likelihood of producing the training corpus no longer increases).The algorithm makes use of one set of trellis dia grams to compute alpha probabilities, and another forbeta probabilities.</S>
			<S sid ="65" ssid = "65">These are both split into terminal, nonterminal-start and nontermiual-end probabili ties, corresponding to the three different types of nodes in the trellis diagram.</S>
			<S sid ="66" ssid = "66">The alpha set are labeled a t ,c~,~t, and ante respectively.</S>
			<S sid ="67" ssid = "67">The algorithm was origi nally formulated using solely the trellis representation (Kupiec, 1991) however the definitions that follow will ADJP &apos; and ( ~ Det (~ Noun Q big black cat Figure 4: A Path through a Trelli= Diagram NP ADJP / AdJ big AdJ black Noun cat Figure 5: The EquivMent Tree ACRESDECOLING92, NANTES,2328 Ao~r 1992 389 PROC.</S>
			<S sid ="68" ssid = "68">OFCOLING92, NANTES, AUG. 2328, 1992 also be related to the consituent structures used in the equivalent parse trees.</S>
			<S sid ="69" ssid = "69">In the following equations, three sets will be mentioned: 1.</S>
			<S sid ="70" ssid = "70">Term(n) The set of terminal states in network n..</S>
	</SECTION>
	<SECTION title="Nonterm(n) This is the set of nonterminal states. " number = "2">
			<S sid ="71" ssid = "1">in network n.</S>
	</SECTION>
	<SECTION title="Final(n) The set F of final states in network n.. " number = "3">
			<S sid ="72" ssid = "1">at(z, y, j, n): The probability t h a t network n gener ates the words w,...w~ inclusive and is at the node for terminal state j at position y. ~,(~, v, J, n) = [~a,(x,y-1, i,n)a(i,j)] b(j, W(y)) + [ E a n t , ( x , Y - l,q,n)a(q,j)] b(j, W(Y)) i. q O&lt;y&lt;Y O&lt; x &lt; y j, iETerm(n) q E Nonterm(n) (l) (2) trt(z, x, j,n) = I(j)b(j, W(x)) O&lt; z &lt; Y j E Term(n)at(::, y, j, n) represents a constituent for nontermihal n spanning positions x...y. It is formed by extend ing an incomplete constituent for n, by addition of the terminal w v at state j . The two terms indicate caseswhere the constituent previously ended on either a ter minal or another constituent completed at y - 1 (as in Figure 5, where the complete A D J P constituent is followed by the noun &amp;quot;eat&amp;quot;).</S>
			<S sid ="73" ssid = "2">If j is a final state the extended constituent is complete.antJ (z, y, p, n): The probability t h a t network n gen erates the words wr...wv_l inclusive, and is at the start node of nonterminal state p at pc~ition y. . . .</S>
			<S sid ="74" ssid = "3">(~,y,p,n) = ~ ~ , ( x , y - l,i,n)a(i,p) + E o e , t , ( x , y - 1,q,n)a(q,p) q 0&lt;y&lt; Y p,q ~.</S>
			<S sid ="75" ssid = "4">Nonterm(n) 0 &lt; x &lt; y i e Term(n) (3) . . .</S>
			<S sid ="76" ssid = "5">(~, ~,p, n) = 1(p) 0&lt;x &lt;Y p e Nonterm(n) (4) a n t , (x, y, p, n) represents an incomplete constituent for nonterminal n whose left subtrees span z . . .</S>
			<S sid ="77" ssid = "6">y - 1, ACRESDECOLING92, NANTES,2328 ^o~r 1992 and whose next extension will involve trees dominated by N(p, n), the nonterminal referred to by state p.elate(z, y, p, n): The probability t h a t network n gen erates the words w~:...wy inclusive, and is at the end node of nonterminal state p at position y. a . , .</S>
			<S sid ="78" ssid = "7">( r , y, p, n) = E a,ts(z, v,p, n)t~totat(v, y, N(p, n)) ~&lt;v&lt;_~ 0 &lt;_y &lt; Y p E Nonterm(n) 0&lt; x&lt; y (5) + ~.,.(~,y,p,n) ~,,.,.,(~, y, n) = ~ , ( ~ , ~ , i , n ) i p 0&lt;y&lt;Y 0&lt;v&lt;y i ~_Term(n) &amp;: i E Final(n) p E gontcrm(n) &amp; p e Final(n) (6)crnte(x,y,p, n) represents the probability of a con stituent for n t h a t spans x...y, formed by extending the various constituents ctnts(x,v,p,n) (ending at v - 1) with corresponding completed constituents starting at v, ending at y and dominated by N(p, n).</S>
			<S sid ="79" ssid = "8">The quantity Oqot,a(v, y, n) refers to the probability that network n generates the words w~...w~ inclusive and is in a final state of n at position y. Equivalently it is the probability t h a t nonterminal n dominates allderivations t h a t span positions v...y. The Cttotat probabilities correspond to the &amp;quot;Inner&amp;quot; (bottom-up) probabilities of the I / O algorithm.</S>
			<S sid ="80" ssid = "9">If a network correspond ing to Chomsky normal form is substituted in equation (6), the reeursion for the inner probabilities of the I / O algorithm will be produced after further substitutions using equations (1)-(6).</S>
			<S sid ="81" ssid = "10">In the previous equations (5) and (6) it can be seen t h a t the a,,~, probabilities for a network are defined recursively.</S>
			<S sid ="82" ssid = "11">They will never be self-referential if the g r a m m a r is cycle-free, (i.e. there are no derivations A =:~ A for any nonterminal production A).</S>
			<S sid ="83" ssid = "12">Although only cycle-free g r a m m a r s are of interest here, it is worthmention t h a t if cycles do exist (with associated proba bilities less than unity), the recursions form a geometric series which has a finite sum.The alpha probabilities are all computed first be cause the beta probabilities make use of them.</S>
			<S sid ="84" ssid = "13">The latter are defined recursively in terms of trees t h a t areexternal to a given constituent, and as a result the recursions are less obvious than those for the alpha prob abilities.</S>
			<S sid ="85" ssid = "14">The basic recursion rests on the quantity &apos;6,,~ which involves the following functions .6above and fl, la,: 390 PROC.</S>
			<S sid ="86" ssid = "15">OF COLING92, NANTES, AUG. 2328, 1992.</S>
			<S sid ="87" ssid = "16">i Z; Z: Z ...,.(o, r, ., r, m) (7) mE.IV r:N(r,m)=n O&lt;v&lt;x &amp;quot; r E Nonterm(m) fl.ide(X,y,l,n) = a(l, i)Bt(z, y + 1, i, n)b(i, W ( y + 1)) + i a(l,q) ~ cqotaa(y.1- w, N(q, n)),Snte(x, w, q, n) 1, q II&lt;w&lt;Y i e Term(n) q e Nonterm(n) (8) Given a constituent n spanning x...y, fla6ove(~, Y, n)indicates how the constituents spanning v...y mid labeled m t h a t immediately dominate n relate to the con stituents t h a t are in turn external to m via flute(v, y, r,m).This situation is shown in Figure 6, where for simplic ity ~nte(v,y, r, m) has not been graphically indicated.</S>
			<S sid ="88" ssid = "17">Note t h a t m can dominate x...y as well as left subtrees.fl.ide(X, y, l, n) defines another reeursion for a con stituent labeled n t h a t spans x...y, and is in state I attime y. The recursion relates the addition of right sub trees (spanning y + 1...w) to the remaining external tree, via flnt~(x, w, q, n).</S>
			<S sid ="89" ssid = "18">This is depicted in Figure 7 (again the external trees repret~nted by time(x, w, q, n) are not shown).</S>
			<S sid ="90" ssid = "19">Also omitted from the figure is the first termof Equation 8 which relates the addition of a single ter minal at position y + 1 to an external tree defined by fit(x, y + 1,i, n).</S>
			<S sid ="91" ssid = "20">fir and the various other probabilities for a beta trellis are defined next:fit (x, y, j, n): The probability of generating the pre fix wo...w~-i and suffix w~+l...wY given that network n generated wz...wy and is in terminal state j at position y. The indicator function Ind 0 is used in subsequent equations.</S>
			<S sid ="92" ssid = "21">Its value is unity when its argument is true and zero otherwise.</S>
			<S sid ="93" ssid = "22">O&lt; y&lt;Y O&lt;x&lt;y jETerm(n) (9) 13,(~:,Y,j,n) = flO,ov,(x,Y,n) AcrEs DECOLING92, NANlXS,2328 Ao£rr 1992 O_.v... x y Figure 6: Part of ~obo¢~(x,y, n) A g /: / Y Y Y ,, 2 / x Y I y y+l .... w ...</S>
			<S sid ="94" ssid = "23">Y Figure Y: Part of//,,ne(x.y,l,n ) 0 &lt; x &lt; Y j E Term(n) j c r i . a t ( n ) ,~ ~ &amp;quot;top(n) 0o) 3t(O,Y,j,n) : flt(~c,y,j,n) = ~slde(x,y,j,n) q- l n d ( j E Finul(n))fla,ov.(x, y, n) 1.0 j G Term(n) j c F i . . t ( . ) ~ ~&apos;op(n) (11) Tile first term in Equation 9 describes tile relationship of the tree external to x...y + 1 to tile tree external to x...y, with r ~ p e c t to state j generating the terminal wv4.</S>
			<S sid ="95" ssid = "24">l at time y + 1.</S>
			<S sid ="96" ssid = "25">If the constituent n spamfing x...y is complete, the second term describes the probability of the external tree via coastituents that immediately dominate n. 391 PRoc.</S>
			<S sid ="97" ssid = "26">oF COL1NG92, NANTES.AUG. 2328, 1992.</S>
			<S sid ="98" ssid = "27">m flnte(x, y,p, n): The probability of generating theprefix wo...w,,_~ and suffix w~+x...wy given that net work n generated wr...w~ a n d t h e end node of state p is reached at position y. flnt.(x, y, p, n) = fl.ia.(z, y, p, n) sentence in found from the top-level network nTop.</S>
			<S sid ="99" ssid = "28">P = atot.s(O,Y, n T ~ ) Top(nTop) There are four different kinds of transition: (16) + lnd(p • Finnl(n))#,**~,(~, U,n) 0 &lt;y &lt;Y 0&lt; x &lt; y i • Term(n), p • Nonterm(n) (12) flnt.(x,Y,p,n) = ~at.~.(z,Y,n) 0 &lt; • &lt; Y p ~ Nonterm(n) p ~ Final(n) &amp; ~ Top(n) fl.,.(0, Y,v,n) = Lo p ~.</S>
			<S sid ="100" ssid = "29">Nonterm(n) p • Final(n) &amp; Top(n) 1.</S>
			<S sid ="101" ssid = "30">Terminalnode i to terminal node j..</S>
			<S sid ="102" ssid = "31">2.</S>
			<S sid ="103" ssid = "32">Terminal node i to nonlerminal start node p..</S>
			<S sid ="104" ssid = "33">3.</S>
			<S sid ="105" ssid = "34">Non~erminal end node p to nonierminal start q..</S>
	</SECTION>
	<SECTION title="Nonterminal end node p to terminal node i.. " number = "4">
			<S sid ="106" ssid = "1">The expected total number of times a transition is madefrom state i to state j conditioned on the observed sen tence is E ( ¢ i j ) . The following formulae give E ( ¢ ) for each of the above eases: 1 (la) E(¢,j) = ~ , ( ~ , ~ , ~ , n ) n ( ~ , j ) × (14) b ( j , W ( y + 1))f/t(x,y+ 1 , j , n ) (17)/3,t~(x, y, p, n) has the same form as the previous for mula for fit, but is used with nonterminal states.</S>
			<S sid ="107" ssid = "2">Viafla~,ov~ mad/3slae it relates the tree external to the con stituent n (spanning x...y) to the trees external to v...y and z...w. During the recursion, the trees shown in Figures 6 and 7 are substituted into each other (at thepositions shown with shaded areas).</S>
			<S sid ="108" ssid = "3">Thus the exter nal trees are successively extended to the left and right, until the root of the o u t e r m ~ t tree is reached.</S>
			<S sid ="109" ssid = "4">It can be seen that the values for j3nt~(x,y,p, n) are defined in terms of those in other networks which reference n via/3~bo~e. As a result this computation has a top-down or der.</S>
			<S sid ="110" ssid = "5">In contrast, the cunte(z,y,p , n) probabilities involve other networks that are referred to by network n andso assigned in a bottom-up order.</S>
			<S sid ="111" ssid = "6">If the network topology for Chomsky normal form is substituted in equa tion (12), the reeur~ion for the &amp;quot;Outer&amp;quot; probabilitiesof the I / O algorithm can be derived after further sub stitutions.</S>
			<S sid ="112" ssid = "7">The ~ntt probabilities for final states then correspond to the outer probabilities.</S>
			<S sid ="113" ssid = "8">13,to(X,y,p, n): The probability of generating the prefix wo...w~-i and suffix w~...wy given that network n generated w~...ww_ x and is at the start node of state p at position y. 1 E(¢,,~) = ~ ~ ] ~ ~,(~, y, i, n)a(i,p) × &amp; i s ( x , y + 1,p, n) E(¢p,d = (18) Z ~ c6..(x, y, p, n)a(p, q) × x It /3nt.(x, y + 1,q,n) (19) 1 y~a.,.(z,y,p,n)a(p,i) b(i,W(y+ l))flt(x,y+ l,i,n) × (20) E(¢,,,) = 0= x 0 &lt;_ x &lt; Y x&lt;y&lt;Y Top(n) ~ Top(n) A new estimate 5(i, j) for a typical transition is then: h(i,j) - ~j E(¢ij) E(¢~,A (21) /3.,.(~,~,p,n)= ottot,t(y,v,N(p,n))~n,t(z,v,p,n) ~&lt;v&lt;_Y O&lt; x &lt; Y x _&lt; y 5 Y p • Nonterm(n) (15) Only B matrix elements for terminal states are used, and are re-estimated as follows.</S>
			<S sid ="114" ssid = "9">The expected totalnumber of times the k&apos;th vocabulary entry vk is gener ated in state i conditioned on the observed sentence is E(yl,k).</S>
			<S sid ="115" ssid = "10">A new estimate for b(i, k) can then be found: 1 E(~:,k) = ~ RE-ESTIMATION FORMULAE Once the alpha and beta probabilities are available,it is a straightforward matter to obtain new parame ter estimates (A, B, I).</S>
			<S sid ="116" ssid = "11">The total probability P of a ~ ~,(~,y,i, nl/~,(~,y,i,n) ¢ y:W(y)=k o = • i • Term(n) &amp; Top(n) O&lt; x &lt; Y i • Term(n) &amp; ~ Top(n) ACTESDECOLlNG92.</S>
			<S sid ="117" ssid = "12">NANTES.2328 Aob&apos;r 1992 392 PROC.</S>
			<S sid ="118" ssid = "13">OFCOLING92.</S>
			<S sid ="119" ssid = "14">NAbrfES, AUG. 2328.</S>
			<S sid ="120" ssid = "15">1992 1 x&lt;_u&lt;_Y (22)~(~, k) ~k E(~i,D (23) The initial state matrix I is re-estimated as follows: i(i) = ~ a t ( ~ , ~ , i , n ) ~ t ( z , z , i , n ) 0= z O&lt; x &lt; Y i 6 Term(n) &amp; Top(n) i e Term(n) &amp; ~Top(n) (24) i(p) = -f 1 ~..,.(~, ~,p, n)Z.,.(~, ~,p, n) (25) 0=x p e Nonterm(n) &amp; Top(n) 0 &lt; • &lt; r p e lVonterm(n) ~ ~ Top(n) DISCUSSION The preceding equations have been implemented as a computer program and this section de~cribe~ somepractical issues with regard to a robust implementa tion.</S>
			<S sid ="121" ssid = "16">The first concerns the size of the B matrix.</S>
			<S sid ="122" ssid = "17">Forpedagogical reasons individual words were shown as el ements of this matrix.</S>
			<S sid ="123" ssid = "18">A vocabulary exeeeding 220,000words is actually used by the program so it is not practi cal to try to reliably estimate the B matrix probabilities for each word individually.</S>
			<S sid ="124" ssid = "19">Instead, only common words are represented individually; the rest of the words in the dictionary are partitioned into word equivalence classes (Kupiec, 1989) such that all words that can function as a particular set of part-of-speech categori~ are given the same label.</S>
			<S sid ="125" ssid = "20">Thus &amp;quot;type&amp;quot;, &amp;quot;store&amp;quot; and &amp;quot;dog&amp;quot; would all be labelled as singular-noun-or-nninflected-verb.</S>
			<S sid ="126" ssid = "21">Forthe category set that is used, only 250 different equiva lence elasees are necessary to cover the dictionary.It is important that the initial guesses for param eter values are well-informed.</S>
			<S sid ="127" ssid = "22">All productions for any iven nonterminal were intially assumed to be equally kely, but the B matrix values were conveniently copied from a trained hidden Markov model (HMM) used for text-tagging.</S>
			<S sid ="128" ssid = "23">The HMM was also found very useful for verifying correct program operation.</S>
			<S sid ="129" ssid = "24">The algorithm has worst-case cubic complexity in both the length of a sentence and the number of nonterminal states in the grammar.</S>
			<S sid ="130" ssid = "25">An index can be used to efficiently update terminal states.</S>
			<S sid ="131" ssid = "26">For any word (or equivalence class)the index determines which terminal states require up dating.</S>
			<S sid ="132" ssid = "27">Also when all probabilities in a column of any trellis become zero, no further computation is required for any other columns in the trellis.</S>
			<S sid ="133" ssid = "28">Grammars are currently being developed, and initial experiments havetypically used eight training iterations, on training cor pora comprising 1O,000 sentences or more (having an average of about 22 words per sentence).</S>
			<S sid ="134" ssid = "29">~i To eomphment the training algorithm, a parser hasalso been constructed which i s a corresponding ana logue of the Cocke-Younger-Kasami parser.</S>
			<S sid ="135" ssid = "30">The parser ACRESDECOLING92, NANTES,2328 AO~&apos; 1992 is quite similar to the training algorithm, except that maximum probability paths are propagated instead of sums of probabilities.</S>
			<S sid ="136" ssid = "31">Trained grammars are used by the parser to predict the most likely syntactic structure of new sentences.</S>
			<S sid ="137" ssid = "32">The applications for which the parserwas developed make use of incomplete parses if a sen tence is not covered by the grammar, thus top-down filtering is not used.</S>
	</SECTION>
</PAPER>
