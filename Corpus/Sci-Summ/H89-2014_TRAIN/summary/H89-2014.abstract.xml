
	<ABSTRACT>
	<S sid ="0">Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging</S>
		<S sid ="1" ssid = "1">The paper describes refinements that are currently being investigated in a model for part-of-speech assignment to words in unrestricted text.</S>
		<S sid ="2" ssid = "2">The model has the advantage that a pre-tagged training corpus is not required.</S>
		<S sid ="3" ssid = "3">Words are represented by equivalence classes to reduce the number of parameters required and provide an essentially vocabulary-independent model.</S>
		<S sid ="4" ssid = "4">State chains are used to model selective higher-order conditioning in the model, which obviates the proliferation of parameters attendant in uniformly higher-order models.</S>
		<S sid ="5" ssid = "5">The structure of the state chains is based on both an analysis of errors and linguistic knowledge.</S>
		<S sid ="6" ssid = "6">Examples show how word dependency across phrases can be modeled.</S>
	</ABSTRACT>