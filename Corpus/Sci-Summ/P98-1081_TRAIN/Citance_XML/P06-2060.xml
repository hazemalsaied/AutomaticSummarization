<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">Several NLP tasks are characterized by asymmetric data where one class label NONE, signifying the absence of any structure (named entity, coreference, relation, etc.) dominates all other classes.</S>
		<S sid ="2" ssid = "2">Classifiers built on such data typically have a higher precision and a lower recall and tend to overproduce the NONE class.</S>
		<S sid ="3" ssid = "3">We present a novel scheme for voting among a committee of classifiers that can significantly boost the recall in such situations.</S>
		<S sid ="4" ssid = "4">We demonstrate results showing up to a 16% relative improvement in ACE value for the 2004 ACE relation extraction task for English, Arabic and Chinese.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="5" ssid = "5">Statistical classifiers are widely used for diverse NLP applications such as part of speech tagging (Ratnaparkhi, 1999), chunking (Zhang et al., 2002), semantic parsing (Magerman, 1993), named entity extraction (Borthwick, 1999; Bikel et al., 1997; Florian et al., 2004), coreference resolution (Soon et al., 2001), relation extraction (Kambhatla, 2004), etc. A number of these applications are characterized by a dominance of a NONE class in the training examples.</S>
			<S sid ="6" ssid = "6">For example, for coreference resolution, classifiers might classify whether a given pair of mentions are references to the same entity or not.</S>
			<S sid ="7" ssid = "7">In this case, we typically have a lot more examples of mention pairs that are not coreferential (i.e. the NONE class) than otherwise.</S>
			<S sid ="8" ssid = "8">Similarly, if a classifier is predicting the presence/absence of a semantic relation between two mentions, there are typically far more examples signifying an absence of a relation.</S>
			<S sid ="9" ssid = "9">Classifiers built with asymmetric data dominated by one class (a NONE class donating absence of a relation or coreference or a named entity etc.) can overgenerate the NONE class.</S>
			<S sid ="10" ssid = "10">This often results in a unbalanced classifier where precision is higher than recall.</S>
			<S sid ="11" ssid = "11">In this paper, we present a novel approach for improving the recall of such classifiers by using a new voting scheme from a committee of classifiers.</S>
			<S sid ="12" ssid = "12">There are a plethora of algorithms for combining classifiers (e.g. see (Xu et al., 1992)).</S>
			<S sid ="13" ssid = "13">A widely used approach is a majority voting scheme, where each classifier in the committee gets a vote and the class with the largest number of votes ’wins’ (i.e. the corresponding class is output as the prediction of the committee).</S>
			<S sid ="14" ssid = "14">We are interested in improving overall recall and reduce the overproduction of the class NONE.</S>
			<S sid ="15" ssid = "15">Our scheme predicts the class label C obtaining the second highest number of votes when NONE gets the highest number of votes, provided C gets at least N votes.</S>
			<S sid ="16" ssid = "16">Thus, we predict a label other than NONE when there is some evidence of the presense of the structure we are looking for (relations, coreference, named entities, etc.) even in the absense of a clear majority.</S>
			<S sid ="17" ssid = "17">This paper is organized as follows.</S>
			<S sid ="18" ssid = "18">In section 2, we give an overview of the various schemes for combining classifiers.</S>
			<S sid ="19" ssid = "19">In section 3, we present our vot 460 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 460–466, Sydney, July 2006.</S>
			<S sid ="20" ssid = "20">Qc 2006 Association for Computational Linguistics ing algorithm.</S>
			<S sid ="21" ssid = "21">In section 4, we describe the ACE relation extraction task.</S>
			<S sid ="22" ssid = "22">In section 5, we present empirical results for relation extraction and we discuss our results and conclude in section 6.</S>
	</SECTION>
	<SECTION title="Combining Classifiers. " number = "2">
			<S sid ="23" ssid = "1">Numerous methods for combining classifiers have been proposed and utlized to improve the performance of different NLP tasks such as part of speech tagging (Brill and Wu, 1998), identifying base noun phrases (Tjong Kim Sang et al., 2000), named entity extraction (Florian et al., 2003), etc. Ho et al (1994) investigated different approaches for reranking the outputs of a committee of classifiers and also explored union and intersection methods for reducing the set of predicted categories.</S>
			<S sid ="24" ssid = "2">Florian et al (2002) give a broad overview of methods for combining classifiers and present empirical results for word sense disambiguation.</S>
			<S sid ="25" ssid = "3">Xu et al (1992) and Florian et al (2002) consider three approaches for combining classifiers.</S>
			<S sid ="26" ssid = "4">In the first approach, individual classifiers output posterior probabilities that are merged (e.g. by taking an average) to arrive at a composite posterior probability of each class.</S>
			<S sid ="27" ssid = "5">In the second scheme, each classifier outputs a ranked list of classes instead of a probability distribution and the different ranked lists are merged to arrive at a final ranking.</S>
			<S sid ="28" ssid = "6">Methods using the third approach, often called voting methods, treat each classifier as a black box that outputs only the top ranked class and combines these to arrive at the final decision (class).</S>
			<S sid ="29" ssid = "7">The choice of approach and the specific method of combination may be constrained by the specific classification algorithms in use.</S>
			<S sid ="30" ssid = "8">In this paper, we focus on voting methods, since for small data sets, it is hard to reliably estimate probability distributions or even a complete ordering of classes especially when the number of classes is large.</S>
			<S sid ="31" ssid = "9">A widely used voting method for combining classifiers is a Majority Vote scheme (e.g.</S>
			<S sid ="32" ssid = "10">(Brill and Wu, 1998; Tjong Kim Sang et al., 2000)).</S>
			<S sid ="33" ssid = "11">Each classifier gets to vote for its top ranked class and the class with the highest number of votes ’wins’.</S>
			<S sid ="34" ssid = "12">Henderson et al (1999) use a Majority Vote scheme where different parsers vote on constituents’ mem bership in a hypothesized parse.</S>
			<S sid ="35" ssid = "13">Halteren et al (1998) compare a number of voting methods including a Majority Vote scheme with other combination methods for part of speech tagging.</S>
			<S sid ="36" ssid = "14">In this paper, we induce multiple classifiers by using bagging (Breiman, 1996).</S>
			<S sid ="37" ssid = "15">Following Breiman’s approach, we obtain multiple classifiers by first making bootstrap replicates of the training data and training different classifiers on each of the replicates.</S>
			<S sid ="38" ssid = "16">The bootstrap replicates are induced by repeatedly sampling with replacement training events from the original training data to arrive at replicate data sets of the same size as the training data set.</S>
			<S sid ="39" ssid = "17">Breiman (1996) uses a Majority Vote scheme for combining the output of the classifiers.</S>
			<S sid ="40" ssid = "18">In the next section, we will describe the different voting schemes we explored in our work.</S>
	</SECTION>
	<SECTION title="At-Least-N Voting. " number = "3">
			<S sid ="41" ssid = "1">We are specifically interested in NLP tasks characterized by asymmetric data where, typically, we have far more occurances of a NONE class that siginifies the absense of structure (e.g. a named entity, or a coreference relation or a semantic relation).</S>
			<S sid ="42" ssid = "2">Classifiers trained on such data sets can overgener- ate the NONE class, and thus have a higher precision and lower recall in discovering the underlying structure (i.e. the named entities or coreference links etc.).</S>
			<S sid ="43" ssid = "3">With such tasks, the benefits yielded by a Majority Vote is limited, since, because of the asymmetry in the data, a majority of the classifiers might predict NONE most of the time.</S>
			<S sid ="44" ssid = "4">We propose alternative voting schemes, dubbed At-Least-N Voting, to deal with the overproduction of NONE.</S>
			<S sid ="45" ssid = "5">Given a committee of classifiers (obtained by bagging or some other mechanism), the classifiers first cast their vote.</S>
			<S sid ="46" ssid = "6">If the majority vote is for a class C other than NONE, we simply output C as the prediction.</S>
			<S sid ="47" ssid = "7">If the majority vote is for NONE, we output the class label obtaining the second highest number of votes, provided it has at least N votes.</S>
			<S sid ="48" ssid = "8">Thus, we choose to defer to the minority vote of classifiers which agree on finding some structure even when the majority of classifiers vote for NONE.</S>
			<S sid ="49" ssid = "9">We expect this voting scheme to increase recall at the expense of precision.At-Least-N Voting induces a spectrum of combi nation methods ranging from a Majority Vote (when N is more than half of the total number of classifiers) to a scheme, where the evidence of any structure by even one classifier is believed (At-Least-1 Voting).</S>
			<S sid ="50" ssid = "10">The exact choice of N is an empirical one and depends on the amount of asymmetry in the data and the imbalance between precision and recall in the classifiers.</S>
	</SECTION>
	<SECTION title="The ACE Relation Extraction Task. " number = "4">
			<S sid ="51" ssid = "1">Automatic Content Extraction (ACE) is an annual evaluation conducted by NIST (NIST, 2004) on information extraction, focusing on extraction of entities, events, and relations.</S>
			<S sid ="52" ssid = "2">The Entity Detection and Recognition task entails detection of mentions of entities and grouping together the mentions that are references to the same entity.</S>
			<S sid ="53" ssid = "3">In ACE terminology, mentions are references in text (or audio, chats, ...)</S>
			<S sid ="54" ssid = "4">to real world entities.</S>
			<S sid ="55" ssid = "5">Similarly relation mentions are references in text to semantic relations between entity mentions and relations group together all relation mentions that identify the same semantic relation between the same entities.</S>
			<S sid ="56" ssid = "6">In the frament of text: John’s son, Jim went for a walk.</S>
			<S sid ="57" ssid = "7">Jim liked his father.</S>
			<S sid ="58" ssid = "8">all the underlined words are mentions referring to two entities, John, and Jim.</S>
			<S sid ="59" ssid = "9">Morover, John and Jim have a family relation evidenced as two relation mentions ”John’s son” between the entity mentions ”John” and ”son” and ”his father” between the entity mentions ”his” and ”father”.</S>
			<S sid ="60" ssid = "10">In the relation extraction task, systems must predict the presence of a predetermined set of binary relations among mentions of entities, label the relation, and identify the two arguments.</S>
			<S sid ="61" ssid = "11">In the 2004 ACE evaluation, systems were evaluated on their efficacy in correctly identifying relations among both system output entities and with ’true’ entities (i.e. as annotated by human annotators as opposed to system output).</S>
			<S sid ="62" ssid = "12">In this paper, we present results for extracting relations between ’true’ entities.</S>
			<S sid ="63" ssid = "13">Table 1 shows the set of relation types, subtypes, and their frequency counts in the training data for the 2004 ACE evaluation.</S>
			<S sid ="64" ssid = "14">For training classifiers, the.</S>
			<S sid ="65" ssid = "15">great paucity of positive training events (where relations exist) compared to the negative events (where Table 1: The set of types and subtypes of relations used in the 2004 ACE evaluation.</S>
			<S sid ="66" ssid = "16">relations do not exist) suggest that schemes for improving recall might benefit this task.</S>
	</SECTION>
	<SECTION title="Experimental Results. " number = "5">
			<S sid ="67" ssid = "1">In this section, we present results of experiments comparing three different methods of combining classifiers for ACE relation extraction: • At-Least-N for different values of N, • Majority Voting, and • a simple algorithm, called summing, where we add the posterior scores for each class from all the classifiers and select the class with the maximum summed score.</S>
			<S sid ="68" ssid = "2">Since the official ACE evaluation set is not publicly available, to facilitate comparison with our results and for internal testing of our algorithms, for each language (English, Arabic, and Chinese), we Table 2: The Division of LDC annotated data into training and development test sets.</S>
			<S sid ="69" ssid = "3">divided the ACE 2004 training data provided by LDC in a roughly 75%:25% ratio into a training set and a test set.</S>
			<S sid ="70" ssid = "4">Table 2 summarizes the number of documents and the number of relation mentions in each data set.</S>
			<S sid ="71" ssid = "5">The test sets were deliberately chosen to be the most recent 25% of documents in chronological order, since entities and relations in news tend to repeat and random shuffles can greatly reduce the out-of-vocabulary problem.</S>
			<S sid ="72" ssid = "6">5.1 Maximum Entropy Classifiers.</S>
			<S sid ="73" ssid = "7">We used bagging (Breiman, 1996) to create replicate training sets of the same size as the original training set by repeatedly sampling with replacement from the training set.</S>
			<S sid ="74" ssid = "8">We created 25 replicate training sets (bags) for each language (Arabic, Chinese, English) and trained separate maximum entropy classifiers on each bag.</S>
			<S sid ="75" ssid = "9">We then applied At-Least-N (N = 1,2,5), Majority Vote, and Summing algorithms with the trained classifiers and measured the resulting performance on our development set.</S>
			<S sid ="76" ssid = "10">For each bag, we built maximum entropy models to predict the presence of relation mentions and the type and subtype of relations, when their presence is predicted.</S>
			<S sid ="77" ssid = "11">Our models operate on every pair of mentions in a document that are not references to the same entity, to extract relation mentions.</S>
			<S sid ="78" ssid = "12">Since there are 23 unique type-subtype pairs in Table 1, our classifiers have 47 classes: two classes for each pair corresponding to the two argument orderings (e.g. ”John’s son” vs. ”his father”) and a NONE class signifying no relation.</S>
			<S sid ="79" ssid = "13">Similar to our earlier work (Kambhatla, 2004), we used a combination of lexical, syntactic, and semantic features including all the words in between the two mentions, the entity types and subtypes of the two mentions, the number of words in between the two mentions, features derived from the small est parse fragment connecting the two mentions, etc. These features were held constant throughout these experiments.</S>
			<S sid ="80" ssid = "14">5.2 Results.</S>
			<S sid ="81" ssid = "15">We report the F-measure, precision and recall for extracting relation mentions for all three languages.</S>
			<S sid ="82" ssid = "16">We also report ACE value1, the official metric used by NIST that assigns 0% value to a system that produces no output and a 100% value to a system that extracts all relations without generating any false alarms.</S>
			<S sid ="83" ssid = "17">Note that the ACE value counts each relation only once even if it is expressed in text many times as different relation mentions.</S>
			<S sid ="84" ssid = "18">The reader is referred to the NIST web site (NIST, 2004) for more details on the ACE value computation.</S>
			<S sid ="85" ssid = "19">Figures 1(a), 1(b), and 1(c) show the F measure, precision, and recall respectively for the English test set obtained by different classifier combination techniques as we vary the number of bags.</S>
			<S sid ="86" ssid = "20">Figures 2(a), 2(b), and 2(c) show similar curves for Chinese, and Figures 3(a), 3(b), and 3(c) show similar curves for Arabic.</S>
			<S sid ="87" ssid = "21">All these figures show the performance of a single classifier as a straight line.</S>
			<S sid ="88" ssid = "22">From the plots, it is clear that our hope of increasing recall by combining classifiers is realized for all three languages.</S>
			<S sid ="89" ssid = "23">As expected, the recall rises fastest for At-Least-N when N is small, i.e when small minority opinion or even a single dissenting opinion is being trusted.</S>
			<S sid ="90" ssid = "24">Of course, the rise in recall is at the expense of a loss of precision.</S>
			<S sid ="91" ssid = "25">Overall, At-Least-N for intermediate ranges of N (N=5 for English and Chinese and N=2 for Arabic) performs best where the moderate loss in precision is more than offset by a rise in recall.</S>
			<S sid ="92" ssid = "26">Both the Majority Vote method and the Summing method succeed in avoiding a sharp loss of precision.</S>
			<S sid ="93" ssid = "27">However, they fail to increase the recall significantly either.</S>
			<S sid ="94" ssid = "28">Table 3 summarizes the best results (F measure) for each classifier combination method for all three languages compared with the result for a single classifier.</S>
			<S sid ="95" ssid = "29">At their best operating points, all three combination methods handily outperform the single classifier.</S>
			<S sid ="96" ssid = "30">At-Least-N seems to have a slight edge over the other two methods, but the difference is small.</S>
			<S sid ="97" ssid = "31">1Here we use the ACE value metric used for the ACE 2004 evaluation 50 49 48 47 46 45 44 43 0 5 10 15 20 25 Number of Bags (a) F-measure 68 66 64 62 60 58 56 54 52 50 48 46 0 5 10 15 20 25 Number of Bags (b) Precision 46 44 42 40 38 36 34 0 5 10 15 20 25 Number of Bags (c) Recall Figure 1: Comparing F-measure, precision, and recall of different voting schemes for English relation extraction.</S>
			<S sid ="98" ssid = "32">67 66 65 64 63 62 61 0 5 10 15 20 25 Number of Bags (a) F-measure 76 74 72 70 68 66 64 62 60 58 56 0 5 10 15 20 25 Number of Bags (b) Precision 70 68 66 64 62 60 58 56 54 52 0 5 10 15 20 25 Number of Bags (c) Recall Figure 2: Comparing F-measure, precision, and recall of different voting schemes for Chinese relation extraction.</S>
			<S sid ="99" ssid = "33">31 44 30 30 29 28 27 At-Least-1.</S>
			<S sid ="100" ssid = "34">At-Least-2 At-Least-5 26 Majority Vote.</S>
			<S sid ="101" ssid = "35">Summing 25 Single.</S>
			<S sid ="102" ssid = "36">0 5 10 15 20 25 Number of Bags (a) F-measure 42 40 38 36 34 32 30 28 0 5 10 15 20 25 Number of Bags (b) Precision 28 26 24 22 20 18 0 5 10 15 20 25 Number of Bags (c) Recall Figure 3: Comparing F-measure, precision, and recall of different voting schemes for Arabic relation extraction.</S>
			<S sid ="103" ssid = "37">English Arabic Chinese Single 46.87 27.47 63.75 At-Least-N 49.52 30.41 66.79 Majority Vote 49.24 29.02 66.21 Summing 48.66 29.02 66.40 Table 3: Comparing the best F-measure obtained by At-Least-N Voting with Majority Voting, Summing and the single best classifier.</S>
			<S sid ="104" ssid = "38">Table 4: Comparing the ACE Value obtained by At- Least-N Voting with the single best classifier for the operating points used in Table 3.</S>
			<S sid ="105" ssid = "39">Table 4 shows the ACE value obtained by our best performing classifier combination method (At- Least-N at the operating points in Table 3) compared with a single classifier.</S>
			<S sid ="106" ssid = "40">Note that while the improvement for Chinese is slight, for Arabic performance improves by over 16% relative and for English, the improvement is over 7% relative over the single classifier2.</S>
			<S sid ="107" ssid = "41">Since the ACE value collapses relation mentions referring to the same relation, finding new relations (i.e. recall) is more important.</S>
			<S sid ="108" ssid = "42">This might explain the relatively larger difference in ACE value between the single classifier performance and At- Least-N.</S>
			<S sid ="109" ssid = "43">The rules of the ACE evaluation prohibit us from presenting a detailed comparison of our relation extraction system with the other participants.</S>
			<S sid ="110" ssid = "44">However, our relation extraction system (using the At- Least-N classifier combination scheme as described here) performed very competitively in 2004 ACE evaluation both in the system output relation extraction task (RDR) and the relation extraction task where the ’true’ mentions and entities are given.</S>
			<S sid ="111" ssid = "45">Due to time limitations, we did not try At-Least-N with N &gt; 5.</S>
			<S sid ="112" ssid = "46">From the plots, there is a potential for getting greater gains by experimenting with a larger 2Note that ACE value metric used in the ACE 2004 evaluation weights entitites differently based on their type.</S>
			<S sid ="113" ssid = "47">Thus, relations with PERSON-NAME arguments end up contributing a lot more the overall score than relations with FACILITY- PRONOUN arguments.</S>
			<S sid ="114" ssid = "48">number of bags and with a larger N.</S>
	</SECTION>
	<SECTION title="Discussion. " number = "6">
			<S sid ="115" ssid = "1">Several NLP problems exhibit a dominance of a NONE class that typically signifies a lack of structure like a named entity, coreference, etc. Especially when coupled with small training sets, this results in classifiers with unbalanced precision and recall.</S>
			<S sid ="116" ssid = "2">We have argued that a classifier voting scheme that is focused on improving recall can help increase overall performance in such situations.</S>
			<S sid ="117" ssid = "3">We have presented a class of voting methods, dubbed At-Least-N that defer to the opinion of a minority of classifiers (consisting of N members) even when the majority predicts NONE.</S>
			<S sid ="118" ssid = "4">This can boost recall at the expense of precision.</S>
			<S sid ="119" ssid = "5">However, by varying N and the number of classifiers, we can pick an operating point that improves the overall F-measure.</S>
			<S sid ="120" ssid = "6">We have presented results for ACE relation extraction for three languages comparing At-Least N with Majority Vote and Summing methods for combining classifiers.</S>
			<S sid ="121" ssid = "7">All three classifier combination methods significantly outperform a single classifier.</S>
			<S sid ="122" ssid = "8">Also, At-Least-N consistently gave us the best per formance across different languages.</S>
			<S sid ="123" ssid = "9">We used bagging to induce multiple classifiers for our task.</S>
			<S sid ="124" ssid = "10">Because of the random bootstrap sampling, different replicate training sets might tilt towards one class or another.</S>
			<S sid ="125" ssid = "11">Thus, if we have many classifiers trained on the replicate training sets, some of them are likely to be better at predicting certain classes than others.</S>
			<S sid ="126" ssid = "12">In future, we plan to experiment with other methods for collecting a committee of classifiers.</S>
	</SECTION>
</PAPER>
