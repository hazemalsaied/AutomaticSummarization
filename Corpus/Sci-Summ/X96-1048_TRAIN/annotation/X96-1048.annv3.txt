Citance Number: 1 | Reference Article:  X96-1048.xml | Citing Article:  A97-1028.xml | Citation Marker Offset:  ['3'] | Citation Marker:  Sundheim, 1995b | Citation Offset:  ['3'] | Citation Text:  <S sid ="3" ssid = "3">Named Entity evaluation began as a part of recent Message Understanding Conferences (MUC), whose objective was to standardize the evaluation of IE tasks (Sundheim, 1995b).</S> | Reference Offset:  ['357'] | Reference Text:  <S sid ="357" ssid = "158">In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.</S> | Discourse Facet:  Implication_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 2 | Reference Article:  X96-1048.xml | Citing Article:  E12-2021.xml | Citation Marker Offset:  ['31'] | Citation Marker:  Sundheim, 1996 | Citation Offset:  ['31'] | Citation Text:  <S sid ="31" ssid = "20">nary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2).</S> | Reference Offset:  ['15'] | Reference Text:  <S sid ="15" ssid = "15">• Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 3 | Reference Article:  X96-1048.xml | Citing Article:  J00-4003.xml | Citation Marker Offset:  ['360'] | Citation Marker:  Sundheim 1995 | Citation Offset:  ['358','359','360'] | Citation Text:  <S sid ="358" ssid = "329">This so-called named entity recognition task has received considerable attention recently (Mani and MacMillan 1996; McDonald 1996; Paik et al. 1996; Bike!</S><S sid ="359" ssid = "330">et al. 1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 1999) and was one of the tasks evaluated in the Sixth and Seventh Message Under­ standing Conferences.</S><S sid ="360" ssid = "331">In MUC6, 15 different systems participated in the competition (Sundheim 1995).</S>This so-called named entity recognition task has received considerable attention recently ... and was one of the tasks evaluated in the Sixth and Seventh Message Understanding Conferences. In MUC-6, 15 different systems participated in the competition (Sundheim 1995) | Reference Offset:  ['4'] | Reference Text:  <S sid ="4" ssid = "4">The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 4 | Reference Article:  X96-1048.xml | Citing Article:  J00-4003.xml | Citation Marker Offset:  ['20'] | Citation Marker:  Sundheim 1995 | Citation Offset:  ['12','20'] | Citation Text:  <S sid ="12" ssid = "12">As a result, quantitative evaluation is now commonplace in areas of language engineering such as parsing, and quantitative evaluation techniques are being proposed for semantic</S><S sid ="20" ssid = "20"> interpretation as well, for example, at the Sixth and Seventh Message Understa nd­ ing Conferences (MUC6 and MUC7) (Sundheim 1995; Chinchor 1997), which also included evaluations of systems on the so-called coreference task, a subtask of which is the resolution of definite descriptions.</S> | Reference Offset:  ['4', '113'] | Reference Text:  <S sid ="4" ssid = "4">The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.</S><S sid ="113" ssid = "113">In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 5 | Reference Article:  X96-1048.xml | Citing Article:  J00-4003.xml | Citation Marker Offset:  ['72'] | Citation Marker:  Sundheim 1995 | Citation Offset:  ['71','72'] | Citation Text:  <S sid ="71" ssid = "42">The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions: in our second experiment, the agreement on 5 Previous attempts to annotate anaphoric relations had resulted in very low agreement levels; for.</S><S sid ="72" ssid = "43">example, in the corefercnce annotation experiments for MUC6 (Sundheim 1995), relations other than identity were dropped due to difficulties in annotating them.</S> | Reference Offset:  ['129', '132', '133'] | Reference Text:  <S sid ="129" ssid = "129">In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.</S><S sid ="132" ssid = "132">There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..</S><S sid ="133" ssid = "133">Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.</S> | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 6 | Reference Article:  X96-1048.xml | Citing Article:  W97-1307.xml | Citation Marker Offset:  ['29'] | Citation Marker:  Sund heim, 1995 | Citation Offset:  ['29','30'] | Citation Text:  <S sid ="29" ssid = "29">Algorit hmThis algorithm was first implemented for the MUC 6 FASTUS system (Appelt et al., 1995) , and prod.</S><S sid ="30" ssid = "30">uced one of the top scores (a recall of 59% and precision of 72%) in the MUC6 Coreference Task, which evaluated systems&apos; ability to recog nize coreference among noun phrases (Sund heim, 1995).</S> | Reference Offset:  ['13'] | Reference Text:  <S sid ="13" ssid = "13">• Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 7 | Reference Article:  X96-1048.xml | Citing Article:  P06-1059.xml | Citation Marker Offset:  ['10'] | Citation Marker:  Sundheim, 1995 | Citation Offset:  ['10'] | Citation Text:  <S sid ="10" ssid = "10">For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995).</S> | Reference Offset:  ['174'] | Reference Text:  <S sid ="174" ssid = "174">5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..</S> | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 8 | Reference Article:  X96-1048.xml | Citing Article:  C04-1126.xml | Citation Marker Offset:  ['49'] | Citation Marker:  Sundheim, 1995 | Citation Offset:  ['49','50','51'] | Citation Text:  <S sid ="49" ssid = "18">The MUC organisers provided strict guidelines about what constituted a succession event and how the templates should be .lled which the an­notators sometimes found di.cult to interpret (Sundheim, 1995).</S><S sid ="50" ssid = "19">Interannotator agreement was measured on 30 texts which were examined by two annotators.</S><S sid ="51" ssid = "20">It was found to be 83% when one annotator’s templates were assumed to be correct and compared with the other.</S> | Reference Offset:  ['27', '28', '31', '227'] | Reference Text:  <S sid ="27" ssid = "27">CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.</S><S sid ="28" ssid = "28">The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.</S><S sid ="31" ssid = "31">The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.</S><S sid ="227" ssid = "28">Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 9 | Reference Article:  X96-1048.xml | Citing Article:  C04-1126.xml | Citation Marker Offset:  ['34'] | Citation Marker:  Sundheim, 1995 | Citation Offset:  ['34'] | Citation Text:  <S sid ="34" ssid = "3">The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995).</S> | Reference Offset:  ['212','213'] | Reference Text:  <S sid ="212" ssid = "13">In this article, the management succession scenario will be used as the basis for discussion.</S><S sid ="213" ssid = "14">The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.</S> | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 10 | Reference Article:  X96-1048.xml | Citing Article:  W99-0612.xml | Citation Marker Offset:  ['142'] | Citation Marker:  Sundheim 1995 | Citation Offset:  ['141','142'] | Citation Text:  <S sid ="141" ssid = "141">It is not clear what resources are required to adapt systems to new languages.&quot;</S><S sid ="142" ssid = "142">It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995).</S> | Reference Offset:  ['54', '57'] | Reference Text:  <S sid ="54" ssid = "54">When the outputs are scored in &quot;key-to-response&quot; mode, as though one annotator&apos;s output represented the &quot;key&quot; and the other the &quot;response,&quot; the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.</S><S sid ="57" ssid = "57">Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&amp;R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA &quot;fast&quot; configuration 95.66, SRA &quot;fastest&quot; configuration 92.61, SRA &quot;nonames&quot; configuration 94.92, SRI 94.0, Sterling Software 92.74..</S> | Discourse Facet:  Results_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 11 | Reference Article:  X96-1048.xml | Citing Article:  E99-1001.xml | Citation Marker Offset:  ['17'] | Citation Marker: 1995 | Citation Offset:  ['17'] | Citation Text:  <S sid ="17" ssid = "17">In an article on the Named Entity recognition competition (part of MUC6)Sundheim (1995) remarks that &quot;common organization names, first names of people and location names can be handled by recourse to list lookup, although there are drawbacks&quot; (Sundheim 1995: 16).</S> | Reference Offset:  ['72'] | Reference Text:  <S sid ="72" ssid = "72">Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..</S> | Discourse Facet:  Method_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


Citance Number: 12 | Reference Article:  X96-1048.xml | Citing Article:  M98-1003.xml | Citation Marker Offset:  ['3'] | Citation Marker:  Sundheim1995 | Citation Offset:  ['3'] | Citation Text:  <S sid ="3" ssid = "3">TheorganizersofMUC6didnotattempttocomparethedifcultyoftheMUC6tasktothepreviousMUCtaskssayingthat.theproblemofcomingupwithareasonable,objectivewayofmeasuringrelativetaskdifcultyhasnotbeenadequatelyaddressed&quot;[Sundheim1995].</S> | Reference Offset:  ['245', '246'] | Reference Text:  <S sid ="245" ssid = "46">No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.</S><S sid ="246" ssid = "47">The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.</S> | Discourse Facet:  Implication_Citation | Annotator:  Muthu Kumar Chandrasekaran, NUS |


