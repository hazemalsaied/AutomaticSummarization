<COMMUNITY>
<S sid ="4" ssid = "4">The Named Entity and Coreference tasks entailed Standard Generalized Markup Language (SGML) annotation of texts and were being conducted for the first time.</S>
<S sid ="13" ssid = "13">• Coreference (CO) --Insert SGML tags into the text to link strings that represent coreferring noun phrases.</S>
<S sid ="15" ssid = "15">• Scenario Template (ST) --Drawing evidence from anywhere in the text, extract prespecified event information, and relate the event information to the particular organization and person entities involved in the event.</S>
<S sid ="27" ssid = "27">CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium.</S><S sid ="28" ssid = "28">The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994.</S><S sid ="31" ssid = "31">The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes, whose retrieval engine is based on a context-vector model, producing a ranked list of hits according to degree of match with a keyword search query.</S>
<S sid ="54" ssid = "54">When the outputs are scored in &quot;key-to-response&quot; mode, as though one annotator&apos;s output represented the &quot;key&quot; and the other the &quot;response,&quot; the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill (ERR) score of 6%.</S><S sid ="57" ssid = "57">Summary NE scores on primary metrics for the top 16 (out of 20) systems tested, in order of decreasing F-Measure (P&amp;R) 1 1 Key to F-measure scores: BBN baseline configuration 93.65, BBN experimental configuration 92.88, Knight-Ridder 85.73, Lockheed-Martin 90.84, UManitoba 93.33, UMass 84.95, MITRE 91.2, NMSU CRL baseline configuration 85.82, NYU 88.19, USheffield 89.06, SRA baseline configuration 96.42, SRA &quot;fast&quot; configuration 95.66, SRA &quot;fastest&quot; configuration 92.61, SRA &quot;nonames&quot; configuration 94.92, SRI 94.0, Sterling Software 92.74..</S>
<S sid ="72" ssid = "72">Common organization names, first names of people, and location names can be handled by recourse to list lookup, although there are drawbacks: some names may be on more than one list, the lists will not be complete and may not match the name as it is realized in the text (e.g., may not cover the needed abbreviated form of an organization name, may not cover the complete person name), etc..</S>
<S sid ="113" ssid = "113">In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus.</S>
<S sid ="129" ssid = "129">In the middle of the effort of preparing the test data for the formal evaluation, an interannotator variability test was conducted.</S><S sid ="132" ssid = "132">There was a large number of factors that contributed to the 20% disagreement, including overlooking coreferential NPs, using different interpretations of vague portions of the guidelines, and making different subjective decisions when the text of an article was ambiguous, sloppy, etc..</S><S sid ="133" ssid = "133">Most human errors pertained to definite descriptions and bare nominals, not to names and pronouns.</S>
<S sid ="174" ssid = "174">5 The highest score for the PERSON object, 95% recall and 95% precision, is close to the highest score on the NE subcategorization for person, which was 98% recall and 99% precision..</S> 
<S sid ="212" ssid = "13">In this article, the management succession scenario will be used as the basis for discussion.</S><S sid ="213" ssid = "14">The management succession template consists of four object types, which are linked together via one-way pointers to form a hierarchical structure.</S>
<S sid ="227" ssid = "28">Of the 100 texts in the test set, 54 were relevant to the management succession scenario, including six that were only marginally relevant.</S>
<S sid ="245" ssid = "46">No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks.</S><S sid ="246" ssid = "47">The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation, and even without that additional factor, the problem of coming up with a reasonable, objective way of measuring relative task difficulty has not been adequately addressed.</S>
<S sid ="357" ssid = "158">In addition, there are plans to put evaluations on line, with public access, starting with the NE evaluation; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure.</S>
</COMMUNITY>