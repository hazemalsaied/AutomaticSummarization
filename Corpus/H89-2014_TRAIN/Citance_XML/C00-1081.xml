<PAPER>
	<ABSTRACT>
		<S sid ="1" ssid = "1">In this paper, we present a stochastic language model using dependency.</S>
		<S sid ="2" ssid = "2">This model considers a sentence as a word sequence and predicts each wordfrom left to right.</S>
		<S sid ="3" ssid = "3">The history at each step of prediction is a sequence of partial parse trees covering the preceding words.</S>
		<S sid ="4" ssid = "4">First our model predicts the partial parse trees which have a dependency relation with the next word among them and then predictsthe next word from only the trees which have a dependency relation with the next word.</S>
		<S sid ="5" ssid = "5">Our model is a generative stochastic model, thus this can be used not only as a parser but also as a language modelof a speech recognizer.</S>
		<S sid ="6" ssid = "6">In our experiment, we prepared about 1,000 syntactically annotated Japanese sentences extracted from a �nancial newspaper and estimated the parameters of our model.</S>
		<S sid ="7" ssid = "7">We built aparser based on our model and tested it on approximately 100 sentences of the same newspaper.</S>
		<S sid ="8" ssid = "8">The accuracy of the dependency relation was 89.9%, thehighest accuracy level obtained by Japanese stochastic parsers.</S>
	</ABSTRACT>
	<SECTION title="Introduction" number = "1">
			<S sid ="9" ssid = "9">The stochastic language modeling, imported from the speech recognition area, is one of the successful methodologies of natural language processing.</S>
			<S sid ="10" ssid = "10">In fact, all language models for speech recognition are, as far as we know, based on an n-gram model and most practical part-of-speech (POS) taggers are alsobased on a word or POS n-gram model or its exten sion (Church, 1988; Cutting et al., 1992; Merialdo,1994; Dermatas and Kokkinakis, 1995).</S>
			<S sid ="11" ssid = "11">POS tagging is the �rst step of natural language process ing, and stochastic taggers have solved this problem with satisfying accuracy for many applications.</S>
			<S sid ="12" ssid = "12">The next step is parsing, or that is to say discovering the structure of a given sentence.</S>
			<S sid ="13" ssid = "13">Recently, many parsers based on the stochastic approach have been proposed.</S>
			<S sid ="14" ssid = "14">Although their reported accuracies arehigh, they are not accurate enough for many appli cations at this stage, and more attempts have to be made to improve them further.</S>
			<S sid ="15" ssid = "15">One of the major applications of a parser is toparse the spoken text recognized by a speech rec ognizer.</S>
			<S sid ="16" ssid = "16">This attempt is clearly aiming at spokenlanguage understanding.</S>
			<S sid ="17" ssid = "17">If we consider how to com bine a parser and a speech recognizer, it is better if the parser is based on a generative stochastic model,as required for the language model of a speech rec ognizer.</S>
			<S sid ="18" ssid = "18">Here, \generative&quot; means that the sum of probabilities over all possible sentences is equal to or less than 1.</S>
			<S sid ="19" ssid = "19">If the language model is generative, it allows a seamless combination of the parser and the speech recognizer.</S>
			<S sid ="20" ssid = "20">This means that the speech recognizer has the stochastic parser as its languagemodel and bene�ts richer information than a normal n-gram model.</S>
			<S sid ="21" ssid = "21">Even though such a combination is not possible in practices , the recognizer out puts N -best sentences with their probabilities, and the parser, taking them as input, parses all of them and outputs the sentence with its parse tree thathas the highest probability of all possible combina tions.</S>
			<S sid ="22" ssid = "22">As a result, a parser based on a generativestochastic language model may help a speech rec ognizer to select the most syntactically reasonable sentence among candidates.</S>
			<S sid ="23" ssid = "23">Therefore, it is better if the language model of a parser is generative.In this paper, taking Japanese as the object lan guage, we propose a generative stochastic language model and a parser based on it.</S>
			<S sid ="24" ssid = "24">This model treats a sentence as a word sequence and predicts each wordfrom left to right.</S>
			<S sid ="25" ssid = "25">The history at each step of predic tion is a sequence of partial parse trees covering the preceding words.</S>
			<S sid ="26" ssid = "26">To predict a word, our model �rst predicts which of the partial parse trees at this stage have dependency relation with the word, and then predicts the word from the selected partial parsetrees.</S>
			<S sid ="27" ssid = "27">In Japanese each word depends on a subse quent word, that is to say, each dependency relationis left to right, it is not necessary to predict the di rection of each dependency relation.</S>
			<S sid ="28" ssid = "28">So in order to extend our model to other languages, the model may have to predict the direction of each dependency.</S>
			<S sid ="29" ssid = "29">Webuilt a parser based on this model, whose parameters are estimated from 1,072 sentences in a �nan cial newspaper, and tested it on 119 sentences fromthe same newspaper.</S>
			<S sid ="30" ssid = "30">The accuracy of the depen dency relation was 89.9%, the highest obtained by any Japanese stochastic parsers.</S>
	</SECTION>
	<SECTION title="Stochastic Language Model based. " number = "2">
			<S sid ="31" ssid = "1">on Dependency In this section, we propose a stochastic languagemodel based on dependency.</S>
			<S sid ="32" ssid = "2">Unlike most stochastic language models for a parser, our model is the oretically based on a hidden Markov model.</S>
			<S sid ="33" ssid = "3">In our model a sentence is predicted word by word from left to right and the state at each step of prediction is basically a sequence of words whose modi�cand has not appeared yet.</S>
			<S sid ="34" ssid = "4">According to a psycholinguistic report on language structure (Yngve, 1960), there is an upper limit on the number of the words whosemodi�cands have not appeared yet.</S>
			<S sid ="35" ssid = "5">This limit is determined by the number of slots in short-term mem ory, 7 � 2 (Miller, 1956).</S>
			<S sid ="36" ssid = "6">With this limitation, we can design a parser based on a �nite state model.</S>
			<S sid ="37" ssid = "7">2.1 Sentence Model.</S>
			<S sid ="38" ssid = "8">The basic idea of our model is that each word wouldbe better predicted from the words that have a de pendency relation with the word to be predicted than from the preceding two words (trigrammodel).Let us consider the complete structure of the sen tence in Figure 1 and a hypothetical structure afterthe prediction of the �fth word at the top of Fig ure 2.</S>
			<S sid ="39" ssid = "9">In this hypothetical structure, there are three trees: one root-only tree (t b composed of w 3 ) and two two-node trees (t a containing w 1 and w 2 , and t c containing w 4 and w 5 ).</S>
			<S sid ="40" ssid = "10">If the last two trees (t b and t c ) depend on the word w 6 , this word may better be predicted from these two trees.</S>
			<S sid ="41" ssid = "11">From this point of view, our model �rst predicts the trees depending on the next word and then predicts the next word from these trees.</S>
			<S sid ="42" ssid = "12">Now, let us make the following de�nitions in order to explain our model formally.</S>
			<S sid ="43" ssid = "13">� w = w 1 w 2 � � �w n : a sequence of words.</S>
			<S sid ="44" ssid = "14">Here a word is de�ned as a pair consisting of a string of alphabetic characters and a part of speech (e.g. the/DT).</S>
			<S sid ="45" ssid = "15">� t i = t 1 t 2 � � � t k i : a sequence of partial parse trees covering the i-pre�x words (w 1 w 2 � � �w i ).</S>
			<S sid ="46" ssid = "16">� t + i and t � i : subsequences of t i having and not having a dependency relation with the next word respectively.</S>
			<S sid ="47" ssid = "17">In Japanese, like many other languages, no two dependency relations cross each other; thus t i = t � i t + i . � ht wi : a tree with w as its root and t as the sequence of all subtrees connected to the root.</S>
			<S sid ="48" ssid = "18">After w i+1 has been predicted from the trees depending on it (t + i), there are trees remain ing (t � i ) and a newly produced tree (ht + i w i+1 i); thus t i+1 = t � i � ht + i w i+1 i. P (t 5 ) karehe anothat aoblue iending wasubj w1 2w 3w 4w 5w 6w ?t 5 �P (t + 5 jt 5 ) karehe anothat aoblue iending wasubj w1 2w 3w 4w 5w 6w ? t 5- t 5+ �P (w 6 jt + 5 ) karehe anothat aoblue iending wasubj apple ringo w1 2w 3w 4w 5w 6w t 5- t 5+&lt; &gt;w6 = P (t 6 ); where t 6 = t � 5 � ht + 5 w 6 i Figure 2: Word prediction from a partial parse � y max : upper limit on the number number of words whose modi�cands have not appeared yet.</S>
			<S sid ="49" ssid = "19">Under these de�nitions, our stochastic language model is de�ned as follows: P (w) = n Y i=1 P (w i jw 1 w 2 � � �w i�1 ) � X t n 2T n n Y i=1 P (w i jt + i�1 )P (t + i�1 jt i�1 ) (1) where T n is all possible binary trees with n nodes.</S>
			<S sid ="50" ssid = "20">Here, the �rst factor, (P (w i jt + i�1 )), is called the word prediction model and the second, (P (t + i�1 jt i�1 )), the state prediction model.</S>
			<S sid ="51" ssid = "21">Let us consider Figure 2again.</S>
			<S sid ="52" ssid = "22">At the top is the state just after the predic tion of the �fth word.</S>
			<S sid ="53" ssid = "23">The state prediction model then predicts the partial parse trees depending on the next word among all partial parse trees, as shown in the second �gure.</S>
			<S sid ="54" ssid = "24">Finally, the word prediction model predicts the next word from the partial parse trees depending on it.</S>
			<S sid ="55" ssid = "25">As described above, there may be an upper limit on the number of words whose modi�cands have not yet appeared.</S>
			<S sid ="56" ssid = "26">To put it in another way, the length of the sequence of partial parse trees (t i ) is limited.</S>
			<S sid ="57" ssid = "27">karehe anothat aoblue tabeeat woobj iending tapast wasubj apple ringo w1 2w 3w 4w 5w 6w 7w 9w8w 10w BT Figure 1: Dependency structure of a sentence.</S>
			<S sid ="58" ssid = "28">Therefore, if the depth of the partial parse tree is alsolimited, the number of possible states is limited.</S>
			<S sid ="59" ssid = "29">Un der this constraint, our model can be considered as a hidden Markov model.</S>
			<S sid ="60" ssid = "30">In a hidden Markov model, the �rst factor is called the output probability and the second, the transition probability.</S>
			<S sid ="61" ssid = "31">Since we assume that no two dependency relations cross each other, the state prediction model only has to predict the number of the trees depending on the next word.</S>
			<S sid ="62" ssid = "32">Thus P (t + i�1 jt i�1 ) = P (yjt i�1 ) where y is the number of trees in the sequence t + i�1 .According to the above assumption, the last y par tial parse trees depend on the i-th word.</S>
			<S sid ="63" ssid = "33">Since the number of possible parse trees for a word sequence grows exponentially with the number of the words, the space of the sequence of partial parse trees is huge even if the length of the sequence is limited.</S>
			<S sid ="64" ssid = "34">This inevitably causes a data-sparseness problem.</S>
			<S sid ="65" ssid = "35">To avoid this problem, we limited the number oflevels of nodes used to distinguish trees.</S>
			<S sid ="66" ssid = "36">In our ex periment, only the root and its children are checkedto identify a partial parse tree.</S>
			<S sid ="67" ssid = "37">Hereafter, we repre sent P LL to denote this model, in which the lexiconof the �rst level and that of the second level are con sidered.</S>
			<S sid ="68" ssid = "38">Thus, in our experiment each word and the number of partial parse trees depending on it are predicted by a sequence of partial parse trees that take account of the nodes whose depth is two or less.</S>
			<S sid ="69" ssid = "39">It is worth noting that if the dependency structure of a sentence is linear | that is to say, if each word depends on the next word, | then our model will be equivalent to a word tri-gram model.</S>
			<S sid ="70" ssid = "40">We introduce an interpolation technique (Jelinek et al., 1991) into our model like those used in n-gram models.</S>
			<S sid ="71" ssid = "41">By loosening tree identi�cation regulations, we obtain a more general model.</S>
			<S sid ="72" ssid = "42">For example, if we check only the POS of the root and the POS of its children, we will obtain a model similar to a POS tri-gram model (denoted P PP hereafter).</S>
			<S sid ="73" ssid = "43">If we check the lexicon of the root, but not that of its children, the model will be like a word bigrammodel (denoted P NL hereafter).</S>
			<S sid ="74" ssid = "44">As a smoothing method, we can interpolate the model P LL , similar to a word tri-gram model, with a more general model, P PP or P NL . In our experiment, as the following formula.</S>
			<S sid ="75" ssid = "45">indicates, we interpolated seven models of di�erent generalization levels: P (w i jt + i�1 ) = � 6 P LL (w i jt + i�1 ) + � 5 P PL (w i jt + i�1 ) +� 4 P PP (w i jt + i�1 ) + � 3 P NL (w i jt + i�1 ) +� 2 P NP (w i jt + i�1 ) + � 1 P NN (w i jt + i�1 ) +� 0 P w;0�gram (2) where X in P YX is the check level of the �rst level of the tree (N: none, P: POS, L: lexicon) and Y is that of the second level, and P w;0�gram is the uniform distribution over the vocabularyW (P w;0�gram (w) = 1=jWj).</S>
			<S sid ="76" ssid = "46">The state prediction model (P (yjt i�1)) is also interpolated in the same way.</S>
			<S sid ="77" ssid = "47">In this case, the possi ble events are y = 1; 2; : : : ; y max , thus; P y;0�gram = 1=y max . 2.2 Parameter Estimation.</S>
			<S sid ="78" ssid = "48">Since our model is a hidden Markov model, the pa rameters of a model can be estimated from a row corpus by EM algorithm (Baum, 1972).</S>
			<S sid ="79" ssid = "49">With thisalgorithm, the probability of the row corpus is ex pected to be maximized regardless of the structure of each sentence.</S>
			<S sid ="80" ssid = "50">So the obtained model is not always appropriate for a parser.</S>
			<S sid ="81" ssid = "51">In order to develop a model appropriate for a parser, it is better that the parameters are estimatedfrom a syntactically annotated corpus by a maxi mum likelihood estimation (MLE) (Merialdo, 1994) as follows: P (wjt + ) MLE = f(ht + w i i) P w f(ht + w i i) P (t + jt) MLE = f(t + ; t) f(t) where f(x) represents the frequency of an event x in the training corpus.</S>
			<S sid ="82" ssid = "52">The interpolation coeÆcients in the formula (2) are estimated by the deleted interpolation method (Jelinek et al., 1991).</S>
			<S sid ="83" ssid = "53">2.3 Selecting Words to be Lexicalized.</S>
			<S sid ="84" ssid = "54">Generally speaking, a word-based n-gram model is better than a POS-based n-gram model in terms ofpredictive power; however lexicalization of some in frequent words may be harmful because it may cause a data-sparseness problem.</S>
			<S sid ="85" ssid = "55">In a practical tagger (Kupiec, 1989), only the most frequent 100 words are lexicalized.</S>
			<S sid ="86" ssid = "56">Also, in a state-of-the-art English parser (Collins, 1997) only the words that occur more than 4 times in training data are lexicalized.</S>
			<S sid ="87" ssid = "57">For this reason, our parser selects the words to belexicalized at the time of learning.</S>
			<S sid ="88" ssid = "58">In the lexical ized models described above (P LL , P PL and P NL ),only the selected words are lexicalized.</S>
			<S sid ="89" ssid = "59">The selec tion criterion is parsing accuracy (see section 4) ofa held-out corpus, a small part of the learning cor pus excluded from parameter estimation.</S>
			<S sid ="90" ssid = "60">Thus only the words that are predicted to improve the parsing accuracy of the test corpus, or unknown input, are lexicalized.</S>
			<S sid ="91" ssid = "61">The algorithm is as follows (see Figure 3): 1.</S>
			<S sid ="92" ssid = "62">In the initial state all words are in the class of.</S>
			<S sid ="93" ssid = "63">their POS.</S>
			<S sid ="94" ssid = "64">2.</S>
			<S sid ="95" ssid = "65">All words are sorted in descending order of their.</S>
			<S sid ="96" ssid = "66">frequency, and the following process is executed for each word in this order: (a) The word is lexicalized provisionally andthe accuracy of the held-out corpus is cal culated.</S>
			<S sid ="97" ssid = "67">(b) If an improvement is observed, the word is lexicalized de�nitively.</S>
			<S sid ="98" ssid = "68">The result of this lexicalization algorithm is used toidentify a partial parse tree.</S>
			<S sid ="99" ssid = "69">That is to say, only lexicalized words are distinguished in lexicalized mod els.</S>
			<S sid ="100" ssid = "70">If no words were selected to be lexicalized, then P NL = P NP and P LL = P PL = P PP . It is worth.</S>
			<S sid ="101" ssid = "71">noting that if we try to join a word with another word, then this algorithm will be a normal top-down clustering algorithm.</S>
			<S sid ="102" ssid = "72">2.4 Unknown Word Model.</S>
			<S sid ="103" ssid = "73">To enable our stochastic language model to handle unknown words, we added an unknown word model based on a character bi-gram model.</S>
			<S sid ="104" ssid = "74">If the next word is not in the vocabulary, the model predicts its POS and the unknown word model predicts the string of the word as follows: P (wjPOS) = m+1 Y i=1 P POS (x i jx i�1 ) where w = x 1 x 2 � � �x m ; x 0 = x m+1 = BT: BT, a special character corresponding to a wordboundary, is introduced so that the sum of the prob ability over all strings is equal to 1.</S>
			<S sid ="105" ssid = "75">In the parameter estimation described above, alearning corpus is divided into k parts.</S>
			<S sid ="106" ssid = "76">In our ex periment, the vocabulary is composed of the words 1w lexicalize yes no yes 2wPOS2w6 2w 3wPOS2w6 1w 1w 2w 3wPOS2w6 1POSw4 w5 2w 3w POS2w6 3w w41POS w5 w41POS w5 yes 1w w41POS w5 Figure 3: A conceptual �gure of the lexicalization algorithm.</S>
			<S sid ="107" ssid = "77">occurring in more than one partial corpus and the other words are used for parameter estimation of the unknown word model.</S>
			<S sid ="108" ssid = "78">The bi-gram probability of the unknown word model of a POS is estimated from the words among them and belonging to the POS as follows: P POS (x i jx i�1 ) MLE = f POS (x i ; x i�1 ) f POS (x i�1 ) : The character bi-gram model is also interpolated with a uni-gram model and a zero-gram model.</S>
			<S sid ="109" ssid = "79">The interpolation coeÆcients are estimated by the deleted interpolation method (Jelinek et al., 1991).</S>
	</SECTION>
	<SECTION title="Syntactic Analysis. " number = "3">
			<S sid ="110" ssid = "1">Generally, a parser may be considered as a module that receives a sequence of words annotated with a POS and outputs its structure.</S>
			<S sid ="111" ssid = "2">Our parser, which includes a stochastic unknown word model, however, is able to accept a character sequence as an input and execute segmentation, POS tagging, and syntactic analysis simultaneously 1 . In this section, we explain.</S>
			<S sid ="112" ssid = "3">our parser, which is based on the language model described in the preceding section.</S>
			<S sid ="113" ssid = "4">3.1 Stochastic Syntactic Analyzer.</S>
			<S sid ="114" ssid = "5">A syntactic analyzer, based on a stochastic language model, calculates the parse tree (see Figure 1) withthe highest probability for a given sequence of char acters x according to the following formula: ^ T = argmax w(T )=x P (T jx) 1 There is no space between words in Japanese = argmax w(T )=x P (T jx)P (x) = argmax w(T )=x P (xjT )P (T ) ( � � � Bayes&apos; formula) = argmax w(T )=x P (T ) ( � � � P (xjT ) = 1); where w(T ) represents the concatenation of the word string in the syntactic tree T . P (T ) in the last line is a stochastic language model.</S>
			<S sid ="115" ssid = "6">In our parser, it is the probability of a parse tree T de�ned by the stochastic dependency model including the unknown word model described in section 2.</S>
			<S sid ="116" ssid = "7">P (T ) = n Y i=1 P (w i jt + i�1 )P (t + i�1 jt i�1 ); (3) where w 1 w 2 � � �w n = w(T ).</S>
			<S sid ="117" ssid = "8">3.2 Solution Search Algorithm.</S>
			<S sid ="118" ssid = "9">As shown in formula (3), our parser is based on a hid den Markov model.</S>
			<S sid ="119" ssid = "10">It follows that Viterbi algorithmis applicable to search the best solution.</S>
			<S sid ="120" ssid = "11">Viterbi al gorithm is capable of calculating the best solution inO(n) time, where n is the number of input charac ters.</S>
			<S sid ="121" ssid = "12">The parser repeats a state transition, reading characters of the input sentence from left to right.</S>
			<S sid ="122" ssid = "13">In order that the structure of the input sentence may be a tree, the number of trees of the �nal state t n must be 1 and no more.</S>
			<S sid ="123" ssid = "14">Among the states that satisfy thiscondition, the parser selects the state with the high est probability.</S>
			<S sid ="124" ssid = "15">Since our language model uses onlythe root and its children of a partial parse tree to dis tinguish states, the last state does not have enough information to construct the parse tree.</S>
			<S sid ="125" ssid = "16">The parsercan, however, calculate the parse tree from the se quence of states, or both the word sequence and the sequence of y, the number of trees that depend on the next word.</S>
			<S sid ="126" ssid = "17">Thus it memorizes these values at each step of prediction.</S>
			<S sid ="127" ssid = "18">After the most probable last state has been selected, the parser constructs the parse tree by reading these sequences from top to bottom.</S>
	</SECTION>
	<SECTION title="Evaluation. " number = "4">
			<S sid ="128" ssid = "1">We developed a POS-based model and its lexical ized version explained in section 2 to evaluate their predictive power, and implemented parsers based on them that calculate the most probable dependencytree from a given character sequence, using the solution search algorithm explained in section 3 to ob serve their accuracy.</S>
			<S sid ="129" ssid = "2">In this section, we present and discuss the experimental results.</S>
			<S sid ="130" ssid = "3">4.1 Conditions on the Experiments.</S>
			<S sid ="131" ssid = "4">The corpus used in our experiments consists of ar ticles extracted from a �nancial newspaper (Nihon Table 1: Corpus.</S>
			<S sid ="132" ssid = "5">#sentences #words #chars learning 1,072 30,292 46,212 test 119 3,268 4,909 Keizai Shinbun).</S>
			<S sid ="133" ssid = "6">Each sentence in the articles is segmented into words and its dependency structure is annotated by linguists using an editor specially designed for this task at our site.</S>
			<S sid ="134" ssid = "7">The corpus was divided into ten parts; the parameters of the model were estimated from nine of them and the model was tested on the rest (see Table 1).</S>
			<S sid ="135" ssid = "8">A small part of each leaning corpus is withheld from parameterestimation and used to select the words to be lex icalized.</S>
			<S sid ="136" ssid = "9">After checking the learning corpus, the maximum number of partial parse trees is set to 10 (y max = 10).</S>
			<S sid ="137" ssid = "10">To evaluate the predictive power of our model, we calculated their cross entropy on the test corpus.</S>
			<S sid ="138" ssid = "11">In this process, the annotated tree in the test corpus is used as the structure of the sentences.</S>
			<S sid ="139" ssid = "12">Therefore the probability of each sentence in the test corpus is not the summation over all its possible derivations.</S>
			<S sid ="140" ssid = "13">To compare the POS-based model and the lexicalized model, we constructed these models using the same learning corpus and calculated their cross entropy on the same test corpus.</S>
			<S sid ="141" ssid = "14">The POS-based model and the lexicalized model have the same unknown word model, thus its contribution to the cross entropy is constant.We implemented a parser based on the depen dency models.</S>
			<S sid ="142" ssid = "15">Since our models, including a character-based unknown word model, can returnthe best parse tree with its probability for any in put, we can build a parser that receives a charactersequence as input.</S>
			<S sid ="143" ssid = "16">It is not easy to evaluate, how ever, because errors may occur in segmentation of the sequence into words and in estimation of their POSs.</S>
			<S sid ="144" ssid = "17">For this reason, in the following description, we assume a word sequence as the input.The criterion for a parser is the accuracy of its out put dependency relations.</S>
			<S sid ="145" ssid = "18">This criterion is widely used to evaluate Japanese dependency parsers.</S>
			<S sid ="146" ssid = "19">Theaccuracy is the ratio of the number of the words an notated with the same dependency to the number of the words as in the corpus: accuracy = #words depending on the correct word #wordsThe last word and the second-to-last word of a sen tence are excluded, because there is no ambiguity.</S>
			<S sid ="147" ssid = "20">The last word has no word to depend on and the second-to-last word depends always on the last word.</S>
			<S sid ="148" ssid = "21">Table 2: Cross entorpy and accuracy of each model.</S>
			<S sid ="149" ssid = "22">language model cross entropy accuracy selectively lexicalized 6.927 89.9% completely lexicalized 6.651 87.1% POS-based 7.000 87.5% linear structure � | 78.7% * Each word depends on the next word.</S>
			<S sid ="150" ssid = "23">4.2 Evaluation.</S>
			<S sid ="151" ssid = "24">Table 2 shows the cross entropy and parsing accu racy of the baseline, where all words depend on the next word, the POS-based dependency model andtwo lexicalized dependency models.</S>
			<S sid ="152" ssid = "25">In the selec tively lexicalized model, words to be lexicalized are selected by the algorithm described in section 2.</S>
			<S sid ="153" ssid = "26">Inthe completely lexicalized model, all words are lexi calized.</S>
			<S sid ="154" ssid = "27">This result attests experimentally that the parser based on the selectively lexicalized model is the best parser.</S>
			<S sid ="155" ssid = "28">As for predictive power, however, the completely lexicalized model has the lowest cross entropy.</S>
			<S sid ="156" ssid = "29">Thus this model is estimated to be the best language model for speech recognition.</S>
			<S sid ="157" ssid = "30">Although there is no direct relation between cross entropy ofthe language model and error rate of a speech rec ognizer, if we consider a spoken language parser, it may be better to select the words to be lexicalized using other criterion.</S>
			<S sid ="158" ssid = "31">We calculated the cross entropy and the parsingaccuracy of the model whose parameters are esti mated from 1/4, 1/16, and 1/64 of the learning corpus.</S>
			<S sid ="159" ssid = "32">The relation between the learning corpus size and the cross entropy or the parsing accuracy is shown in Figure 4.</S>
			<S sid ="160" ssid = "33">The cross entropy has a stronger tendency to decrease as the corpus size increases.</S>
			<S sid ="161" ssid = "34">As for accuracy, there is also a tendency for parsers to become more accurate as the size of the learning increases.</S>
			<S sid ="162" ssid = "35">The size of the corpus we have at this stage is not at all large.</S>
			<S sid ="163" ssid = "36">However, its accuracy is atthe top level of Japanese parsers, which use 50,000 190,000 sentences.</S>
			<S sid ="164" ssid = "37">Therefore, we conclude that our approach is quite promising.</S>
	</SECTION>
	<SECTION title="Related Works. " number = "5">
			<S sid ="165" ssid = "1">Historically, structures of natural languages havebeen described by a context-free grammar and am biguities have been resolved by parsers based on acontext-free grammar (Fujisaki et al., 1989).</S>
			<S sid ="166" ssid = "2">In re cent years, some attempts have been made in the area of parsing by a �nite state model (O azer, 1999) etc. Our parser is also based on a �nite state model.</S>
			<S sid ="167" ssid = "3">Unlike these models, we focused on reports on a limit on language structure caused by the capacity our memory (Yngve, 1960) (Miller, 1956).</S>
			<S sid ="168" ssid = "4">Thus our 0 20 40 60 80 100 0 201 401301101 50101 % cross entropy #characters in learning corpus cross entropy accuracy parsing accuracy 20 0 16 12 8 4 84.0% 86.2%88.1% 89.9%Figure 4: Relation between cross entropy and pars ing accuracy.</S>
			<S sid ="169" ssid = "5">model is psycholinguistically more appropriate.Recently, in the area of parsers based on a stochas tic context-free grammar (SCFG), some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Charniak, 1997;Collins, 1997).</S>
			<S sid ="170" ssid = "6">In these papers, they reported sig ni�cant improvement of parsing accuracy.</S>
			<S sid ="171" ssid = "7">Taking these reports into account, we introduced a methodof partial lexicalization and reported signi�cant im provement of parsing accuracy.</S>
			<S sid ="172" ssid = "8">Our lexicalization method is also applicable to a SCFG-based parser and improves its parsing accuracy.The model we present in this paper is a genera tive stochastic language model.</S>
			<S sid ="173" ssid = "9">Chelba and Jelinek (1998) presented a similar model.</S>
			<S sid ="174" ssid = "10">In their model, each word is predicted from two rightmost head words regardless of dependency relation between these head words and the word.</S>
			<S sid ="175" ssid = "11">Eisner (1996) also presented a stochastic structural language model, in which each word is predicted from its head word and the nearest one.</S>
			<S sid ="176" ssid = "12">This model is very similar tothe parser presented by Collins (1996).</S>
			<S sid ="177" ssid = "13">The great est di�erence between our model and these models is in that our model predicts the next word from the head words, or partial parse trees, depending on it.</S>
			<S sid ="178" ssid = "14">Clearly, it is not always two rightmost head words that have dependency relation with the next word.It follows that our model is linguistically more ap propirate.</S>
			<S sid ="179" ssid = "15">There have been some attempts at stochastic Japanese parser (Haruno et al., 1998) (Fujio and Matsumoto, 1998) (Mori and Nagao, 1998).</S>
			<S sid ="180" ssid = "16">These Japanese parsers are based on a unit called bunsetsu, a sequence of one or more content words followed by zero or more function words.</S>
			<S sid ="181" ssid = "17">The parsers take a sequence of units and outputs dependency relationsbetween them.</S>
			<S sid ="182" ssid = "18">Unlike these parsers, our model de scribes dependencies between words; thus our model can easily be extended to other languages.</S>
			<S sid ="183" ssid = "19">As for the accuracy, although a direct comparison is not easy between our parser (89.9%; 1,072 sentences)and these parsers (82% - 85%; 50,000190,000 sen tences) because of the di�erence of the units and the corpus, our parser is one of the state-of-the-art parsers for Japanese language.</S>
			<S sid ="184" ssid = "20">It should be noted that our model describes relations among three ormore units (case frame, consecutive dependency re lations, etc.); thus our model bene�ts a greater deal from increase of corpus size.</S>
	</SECTION>
	<SECTION title="Conclusion. " number = "6">
			<S sid ="185" ssid = "1">In this paper we have presented a stochastic lan guage model based on dependency structure.</S>
			<S sid ="186" ssid = "2">Thismodel treats a sentence as a word sequence and pre dicts each word from left to right.</S>
			<S sid ="187" ssid = "3">The history at each step of prediction is a sequence of partial parse trees covering the preceding words.</S>
			<S sid ="188" ssid = "4">To predict a word, our model �rst selects the partial parse trees that have a dependency relation with the word, and then predicts the next word from the selected partialparse trees.</S>
			<S sid ="189" ssid = "5">We also presented an algorithm for lexi calization.</S>
			<S sid ="190" ssid = "6">We built parsers based on the POS-based model and its lexicalized version, whose parameters are estimated from 1,072 sentences of a �nancial newspaper.</S>
			<S sid ="191" ssid = "7">We tested the parsers on 119 sentences from the same newspaper, which were excluded fromthe learning.</S>
			<S sid ="192" ssid = "8">The accuracy of the dependency rela tion of the lexicalized parser was 89.9%, the highest obtained by any Japanese stochastic parser.</S>
	</SECTION>
</PAPER>
