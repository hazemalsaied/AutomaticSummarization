#Try to read the readme file before reading this file
##Total Features Number using contrast: 152

### mainpulating the section : 0

### mainpulating the section : 1

1 - **represent**: ,Contrast  ,Feature 

2 - **environment**: ,Contrast 

3 - **advantage**: ,Contrast  ,Feature 

4 - **description**: ,Contrast 

5 - **natural**: ,Contrast  ,Feature 

6 - **term**: ,Contrast  ,Feature 

7 - **language**: ,Contrast  ,Feature  ,Abstract 

8 - **part**: ,Contrast  ,Feature 

9 - **representing**: ,Contrast 

10 - **based**: ,Contrast  ,Feature 

11 - **strategy**: ,Contrast  ,Feature  ,Abstract 

12 - **copying**: ,Contrast  ,Feature 

13 - **proposed**: ,Contrast  ,Feature 

14 - **input**: ,Contrast  ,Feature 

15 - **disadvantage**: ,Contrast 

16 - **called**: ,Contrast  ,Feature 

17 - **grammatical**: ,Contrast 

18 - **structure**: ,Contrast  ,Feature  ,Abstract 

19 - **time**: ,Contrast  ,Abstract 

### mainpulating the section : 2

1 - **partial**: ,Contrast 

2 - **represented**: ,Contrast  ,Feature 

3 - **feature**: ,Contrast  ,Feature  ,Abstract 

4 - **typed**: ,Contrast 

5 - **rooted**: ,Contrast 

6 - **ordering**: ,Contrast 

7 - **relationship**: ,Contrast 

8 - **feature-value**: ,Contrast  ,Feature 

9 - **exists**: ,Contrast 

10 - **equal**: ,Contrast  ,Feature 

11 - **empty**: ,Contrast 

12 - **lower**: ,Contrast 

13 - **greatest**: ,Contrast  ,Feature 

14 - **describing**: ,Contrast 

15 - **bound**: ,Contrast 

16 - **directed**: ,Contrast  ,Feature 

17 - **describe**: ,Contrast 

18 - **symbol**: ,Contrast  ,Feature 

### mainpulating the section : 3

1 - **nodel**: ,Contrast  ,Feature 

2 - **node1**: ,Contrast  ,Feature 

3 - **node2**: ,Contrast  ,Feature 

4 - **current**: ,Contrast  ,Feature 

5 - **shared**: ,Contrast 

6 - **obtained**: ,Contrast 

7 - **meet**: ,Contrast  ,Feature 

8 - **give**: ,Contrast 

9 - **tile**: ,Contrast  ,Feature 

10 - **represents**: ,Contrast 

11 - **created**: ,Contrast  ,Feature 

12 - **pair**: ,Contrast  ,Feature 

13 - **proposes**: ,Contrast 

14 - **common**: ,Contrast 

15 - **arc**: ,Contrast  ,Feature 

16 - **label**: ,Contrast  ,Feature 

17 - **treat**: ,Contrast  ,Feature 

18 - **copied**: ,Contrast  ,Feature 

19 - **procedure**: ,Contrast  ,Feature 

20 - **input**: ,Contrast  ,Feature 

21 - **required**: ,Contrast 

22 - **original**: ,Contrast 

### mainpulating the section : 4

1 - **existing**: ,Contrast 

2 - **consisting**: ,Contrast  ,Feature 

3 - **larger**: ,Contrast 

4 - **newly**: ,Contrast  ,Feature 

5 - **size**: ,Contrast 

6 - **copy**: ,Contrast  ,Feature  ,Abstract 

7 - **slot**: ,Contrast  ,Feature 

8 - **disjunctive**: ,Contrast 

9 - **depends**: ,Contrast 

10 - **arc**: ,Contrast  ,Feature 

11 - **definite**: ,Contrast  ,Feature 

12 - **newnode**: ,Contrast  ,Feature 

13 - **copied**: ,Contrast  ,Feature 

14 - **applies**: ,Contrast 

15 - **procedure**: ,Contrast  ,Feature 

16 - **dereference**: ,Contrast 

17 - **sentence**: ,Contrast 

18 - **unique**: ,Contrast  ,Feature 

19 - **dependency**: ,Contrast  ,Feature 

20 - **introduced**: ,Contrast 

21 - **constant**: ,Contrast 

22 - **node**: ,Contrast  ,Feature 

23 - **structure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 5

1 - **semantic**: ,Contrast  ,Feature 

2 - **computation**: ,Contrast  ,Abstract 

3 - **treating**: ,Contrast  ,Feature 

4 - **learned**: ,Contrast 

5 - **sake**: ,Contrast 

6 - **occur**: ,Contrast  ,Feature 

7 - **information**: ,Contrast  ,Feature 

8 - **feature**: ,Contrast  ,Feature  ,Abstract 

9 - **representation**: ,Contrast  ,Feature 

10 - **tendency**: ,Contrast  ,Feature 

11 - **related**: ,Contrast 

12 - **obtains**: ,Contrast 

13 - **small**: ,Contrast  ,Feature 

14 - **fail**: ,Contrast  ,Feature 

15 - **pair**: ,Contrast  ,Feature 

16 - **achieved**: ,Contrast 

17 - **depends**: ,Contrast 

18 - **efficiency**: ,Contrast  ,Feature  ,Abstract 

19 - **applied**: ,Contrast  ,Feature 

20 - **analysis**: ,Contrast  ,Feature  ,Abstract 

21 - **sentence**: ,Contrast 

22 - **unification**: ,Contrast  ,Feature  ,Abstract 

23 - **unnecessary**: ,Contrast 

24 - **agreement**: ,Contrast  ,Feature 

25 - **failure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 6

1 - **graph**: ,Contrast  ,Feature  ,Abstract 

2 - **swapping**: ,Contrast  ,Feature  ,Abstract 

3 - **total**: ,Contrast  ,Feature 

4 - **sharing**: ,Contrast  ,Feature 

5 - **incremental**: ,Contrast  ,Feature 

6 - **collection**: ,Contrast  ,Feature  ,Abstract 

7 - **natural**: ,Contrast  ,Feature 

8 - **generation**: ,Contrast  ,Feature  ,Abstract 

9 - **language**: ,Contrast  ,Feature  ,Abstract 

10 - **strategic**: ,Contrast  ,Feature 

11 - **copy**: ,Contrast  ,Feature  ,Abstract 

12 - **wastage**: ,Contrast  ,Feature 

13 - **efficient**: ,Contrast  ,Feature  ,Abstract 

14 - **access**: ,Contrast  ,Feature  ,Abstract 

15 - **based**: ,Contrast  ,Feature 

16 - **achieves**: ,Contrast  ,Feature 

17 - **lazy**: ,Contrast  ,Feature  ,Abstract 

18 - **tending**: ,Contrast  ,Feature 

19 - **efficiency**: ,Contrast  ,Feature  ,Abstract 

20 - **strategy**: ,Contrast  ,Feature  ,Abstract 

21 - **treat**: ,Contrast  ,Feature 

22 - **make**: ,Contrast  ,Feature 

23 - **avoids**: ,Contrast  ,Feature 

24 - **application**: ,Contrast  ,Feature 

25 - **identical**: ,Contrast  ,Feature 

26 - **unification**: ,Contrast  ,Feature  ,Abstract 

27 - **reduces**: ,Contrast  ,Feature 

28 - **unification-based**: ,Contrast  ,Feature 

29 - **fi'om**: ,Contrast  ,Feature 

30 - **processing**: ,Contrast  ,Feature 

31 - **introduces**: ,Contrast  ,Feature 

32 - **page**: ,Contrast  ,Feature  ,Abstract 

33 - **garbage**: ,Contrast  ,Feature  ,Abstract 

34 - **log**: ,Contrast  ,Feature 

35 - **overhead**: ,Contrast  ,Feature 

36 - **combined**: ,Contrast  ,Feature 

37 - **gain**: ,Contrast  ,Feature 

### mainpulating the section : 7

##Total Features Number : 108

### mainpulating the section : 0

### mainpulating the section : 1

1 - **represent** ,Feature 

2 - **advantage** ,Feature 

3 - **natural** ,Feature 

4 - **term** ,Feature 

5 - **language** ,Feature  ,Abstract 

6 - **part** ,Feature 

7 - **based** ,Feature 

8 - **strategy** ,Feature  ,Abstract 

9 - **copying** ,Feature 

10 - **proposed** ,Feature 

11 - **input** ,Feature 

12 - **called** ,Feature 

13 - **structure** ,Feature  ,Abstract 

### mainpulating the section : 2

1 - **represented** ,Feature 

2 - **feature** ,Feature  ,Abstract 

3 - **feature-value** ,Feature 

4 - **equal** ,Feature 

5 - **greatest** ,Feature 

6 - **directed** ,Feature 

7 - **symbol** ,Feature 

### mainpulating the section : 3

1 - **nodel** ,Feature 

2 - **node1** ,Feature 

3 - **node2** ,Feature 

4 - **current** ,Feature 

5 - **meet** ,Feature 

6 - **tile** ,Feature 

7 - **created** ,Feature 

8 - **pair** ,Feature 

9 - **arc** ,Feature 

10 - **label** ,Feature 

11 - **treat** ,Feature 

12 - **copied** ,Feature 

13 - **procedure** ,Feature 

14 - **input**: ,Contrast  ,Feature 

### mainpulating the section : 4

1 - **consisting** ,Feature 

2 - **newly** ,Feature 

3 - **copy** ,Feature  ,Abstract 

4 - **slot** ,Feature 

5 - **arc**: ,Contrast  ,Feature 

6 - **definite** ,Feature 

7 - **newnode** ,Feature 

8 - **copied**: ,Contrast  ,Feature 

9 - **procedure**: ,Contrast  ,Feature 

10 - **unique** ,Feature 

11 - **dependency** ,Feature 

12 - **node** ,Feature 

13 - **structure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 5

1 - **semantic** ,Feature 

2 - **treating** ,Feature 

3 - **occur** ,Feature 

4 - **information** ,Feature 

5 - **feature**: ,Contrast  ,Feature  ,Abstract 

6 - **representation** ,Feature 

7 - **tendency** ,Feature 

8 - **small** ,Feature 

9 - **fail** ,Feature 

10 - **pair**: ,Contrast  ,Feature 

11 - **efficiency** ,Feature  ,Abstract 

12 - **applied** ,Feature 

13 - **analysis** ,Feature  ,Abstract 

14 - **unification** ,Feature  ,Abstract 

15 - **agreement** ,Feature 

16 - **failure** ,Feature  ,Abstract 

### mainpulating the section : 6

1 - **graph** ,Feature  ,Abstract 

2 - **swapping** ,Feature  ,Abstract 

3 - **total** ,Feature 

4 - **sharing** ,Feature 

5 - **incremental** ,Feature 

6 - **collection** ,Feature  ,Abstract 

7 - **natural**: ,Contrast  ,Feature 

8 - **generation** ,Feature  ,Abstract 

9 - **language**: ,Contrast  ,Feature  ,Abstract 

10 - **strategic** ,Feature 

11 - **copy**: ,Contrast  ,Feature  ,Abstract 

12 - **wastage** ,Feature 

13 - **efficient** ,Feature  ,Abstract 

14 - **access** ,Feature  ,Abstract 

15 - **based**: ,Contrast  ,Feature 

16 - **achieves** ,Feature 

17 - **lazy** ,Feature  ,Abstract 

18 - **tending** ,Feature 

19 - **efficiency**: ,Contrast  ,Feature  ,Abstract 

20 - **strategy**: ,Contrast  ,Feature  ,Abstract 

21 - **treat**: ,Contrast  ,Feature 

22 - **make** ,Feature 

23 - **avoids** ,Feature 

24 - **application** ,Feature 

25 - **identical** ,Feature 

26 - **unification**: ,Contrast  ,Feature  ,Abstract 

27 - **reduces** ,Feature 

28 - **unification-based** ,Feature 

29 - **fi'om** ,Feature 

30 - **processing** ,Feature 

31 - **introduces** ,Feature 

32 - **page** ,Feature  ,Abstract 

33 - **garbage** ,Feature  ,Abstract 

34 - **log** ,Feature 

35 - **overhead** ,Feature 

36 - **combined** ,Feature 

37 - **gain** ,Feature 

### mainpulating the section : 7

#The Summary
**ze take 5 percent of the important and long sentences for making the summary**

That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.

Once complex IeSs are extended as above, an atomic FS can be seen as an extended complex FS whose type symbol has only Top as its greater type symbol and only Bottom as its lesser type symbol and which has an empty set of feature value pairs.

The ComplementArcs procedure takes two lists of arcs as NODE TYPESYMBOL: <symbol> [ ARCS: <a list of ARC structures > FORWARD: "<aNODEstructure orNIL> / COPY: < a NODEstructure or Nil, > GENERATION: <an integer> ARC LABEL: <symbol> VALUE: <:a NODEstructure> Figure 4: Data Structures for Wroblewski's method Input graph GI Input graph 62 Â¢ .......'77 ........ i : Sobg,'aphs not required to be copied L ...........................................

5, the subgraphs of the result DG surrounded by the dashed rectangle can be shared with subgraphs of input structures G1 and G2, Section 4 proposes a method t.hat avoids this problem, Wroblewski's method first treats arcs with labels that exist in both input nodes and then treats arcs with unique labels.

In TFS unification based on Wrobtewski's method, a DG is represented by tile NODE and ARC structures corresponding to a TFS and a feature-value pair respectively, as shown in Fig.

The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.

(2) In the worst eases, in which there are unique label arcs but all result structures are newly created, the method CopyNode PROCEDURE CopyNode(node, arc, ancestor) node = Dereference(node).

By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.

The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.

The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.


/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/spatial/distance.py:287: RuntimeWarning: invalid value encountered in double_scalars
  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))
/System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python/scipy/spatial/distance.py:287: RuntimeWarning: invalid value encountered in true_divide
  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))

#analysing the annotation 1

##The Citing Sentences : 
While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.

###The Corpus Reference Sentences : 
1.0


**21** : itowever, the problem with his method is that a unitication result graph consists only of newly created structures.

1.0


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

0.851747961982


**23** : Copying sharable parts is called redundant copying.


###Our Reference Sentences : 
0.0185977564291

**79** : A unification example is shown in Fig.


0.0200902635506

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.0264736722128

**199** : In such cases, the SING unification method obtains efl]ciency gains.


0.0393002798359

**207** : The SING unification method introduces the concept of feature unification strategy.


0.071379548053

**208** : 'the method treats features tending to fail in unification first.



#analysing the annotation 2

##The Citing Sentences : 
The extension is classified into class (1) above.Based on this paper's formalization, unification algorithms have been developed using graph unification techniques[23, 16].

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.0678491444859

**51** : This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).


0.0734701127403

**207** : The SING unification method introduces the concept of feature unification strategy.


0.0838291586746

**79** : A unification example is shown in Fig.


0.085222462505

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.0911815816357

**199** : In such cases, the SING unification method obtains efl]ciency gains.



#analysing the annotation 3

##The Citing Sentences : 
Other versions based on more efficient graph unification methods such as Wroblewski's and Kogure's method [23, 16] have also been developed.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.0845197086677

**51** : This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).


0.0900401516569

**207** : The SING unification method introduces the concept of feature unification strategy.


0.113308576755

**79** : A unification example is shown in Fig.


0.114657048569

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.120424422497

**199** : In such cases, the SING unification method obtains efl]ciency gains.



#analysing the annotation 4

##The Citing Sentences : 
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].

###The Corpus Reference Sentences : 
0.242718344332


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.

1.0


**206** : This reduces repeated calculation of substructures.


###Our Reference Sentences : 
0.0370145553162

**52** : These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).


0.0630777274991

**41** : The method is called the lazy i2!cremental copy IFaph unification reel, hod (the LING unifieation method for short).


0.0723752480822

**207** : The SING unification method introduces the concept of feature unification strategy.


0.0818348456643

**79** : A unification example is shown in Fig.


0.0832311824268

**17** : These methods take two DGs as their inputs and give a unification result DG.



##The Citing Sentences : 
Thus treatments such as strategic unification [6] have been developed.

###The Corpus Reference Sentences : 
0.195094968811


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.

1.0


**206** : This reduces repeated calculation of substructures.


###Our Reference Sentences : 
0.0240939462149

**79** : A unification example is shown in Fig.


0.0255780947836

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.0319257541639

**199** : In such cases, the SING unification method obtains efl]ciency gains.


0.0370145553162

**52** : These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).


0.0420522254303

**207** : The SING unification method introduces the concept of feature unification strategy.



#analysing the annotation 5

##The Citing Sentences : 
This observation is the basis for a reordering method proposed by Kogure [1990].

###The Corpus Reference Sentences : 
1.0


**3** : The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify ;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.


###Our Reference Sentences : 
0.0583063478114

**15** : Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.


0.202373259989

**5** : Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.


0.69469318639

**139** : In Section 5, a method which uses this generalized strategy is proposed.


0.843564299064

**20** : Ile proposed an incremental copy graph unification method to avoid over copying and early copying.


0.885127022502

**50** : That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.



#analysing the annotation 6

##The Citing Sentences : 
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).

###The Corpus Reference Sentences : 
1.0


**186** : in this method, theretbre, the failure tendency information is acquired by a learning process.

0.86987815227


**187** : That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.

0.867804599312


**188** : in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.


###Our Reference Sentences : 
0.867804599312

**188** : in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.


0.86987815227

**187** : That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.


0.900944172397

**138** : This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.


0.901215009742

**7** : These formalisms were applied in the field of natural language processing and, based on these formalisms, ~:~ystems such as machine translation systems were developed [l<ol;u, e et a l 8gJ.


0.904953619687

**191** : By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.



#analysing the annotation 7

##The Citing Sentences : 
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.

###The Corpus Reference Sentences : 
0.763770456652


**39** : This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.

0.535471293429


**40** : This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.

1.0


**78** : Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.


###Our Reference Sentences : 
0.0863653152437

**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.


0.127574745018

**38** : Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing.


0.187212654705

**27** : I"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly.


0.20791485771

**36** : However, Pereira's method can create skeleton-enviromnent structures that are deeply embedded, for example, in reeursively constructing large phrase structure fl'om their parts.


0.227637051666

**177** : Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different.



#analysing the annotation 8

##The Citing Sentences : 
At least two schemes have been proposed recently ])a.~ed Ul)OU this observation (namely [Kogure.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.0583063478114

**15** : Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.


0.202373259989

**5** : Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.


0.69469318639

**139** : In Section 5, a method which uses this generalized strategy is proposed.


0.843564299064

**20** : Ile proposed an incremental copy graph unification method to avoid over copying and early copying.


0.885127022502

**50** : That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.



##The Citing Sentences : 
1990] and [Emele, 1991]); however, both schemes are I)ased upon the increlllent'al Col)yiug sehellle all(l ~-LS ([e- scribed in [Tomal)eehi, 1991] incremental copying schemes inherently suffcr fi'om Early Copying as defined in that article.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.0583063478114

**15** : Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.


0.202373259989

**5** : Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.


0.438791320636

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.6352310802

**202** : The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.


0.636736713979

**18** : Previous research identified DG copying as a significant overhead.



#analysing the annotation 9
**Not valid annotation**

#analysing the annotation 10

##The Citing Sentences : 
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).

###The Corpus Reference Sentences : 
0.275477915861


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.


###Our Reference Sentences : 
0.0893151646625

**51** : This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).


0.0948066905448

**207** : The SING unification method introduces the concept of feature unification strategy.


0.121554145377

**79** : A unification example is shown in Fig.


0.12289007741

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.125984249398

**14** : Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.



#analysing the annotation 11

##The Citing Sentences : 
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).

###The Corpus Reference Sentences : 
1.0


**11** : For example, a spoken Present.

1.0


**14** : Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.


###Our Reference Sentences : 
0.0223458409515

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0422292702505

**18** : Previous research identified DG copying as a significant overhead.


0.0903200705834

**24** : A better method would nfinimize the copying of sharable varts.


0.179663306214

**29** : Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.


0.304200191555

**23** : Copying sharable parts is called redundant copying.



#analysing the annotation 12

##The Citing Sentences : 
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).

###The Corpus Reference Sentences : 
0.215907511544


**203** : The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.

0.89281533427


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

0.782210360585


**23** : Copying sharable parts is called redundant copying.

0.760653149559


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.0665658861999

**207** : The SING unification method introduces the concept of feature unification strategy.


0.0711523674708

**79** : A unification example is shown in Fig.


0.0725649500438

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.078606523784

**199** : In such cases, the SING unification method obtains efl]ciency gains.


0.114323398444

**52** : These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).



#analysing the annotation 13

##The Citing Sentences : 
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug

###The Corpus Reference Sentences : 
0.775888802495


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

1.0


**23** : Copying sharable parts is called redundant copying.

1.0


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.0184744716576

**202** : The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.


0.0747540756131

**51** : This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).


0.0989957387286

**52** : These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).


0.151639313175

**207** : The SING unification method introduces the concept of feature unification strategy.


0.162343940883

**20** : Ile proposed an incremental copy graph unification method to avoid over copying and early copying.



#analysing the annotation 14

##The Citing Sentences : 
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].

###The Corpus Reference Sentences : 
0.0604994260593


**23** : Copying sharable parts is called redundant copying.

0.289950275727


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.114607311152

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.202692778111

**23** : Copying sharable parts is called redundant copying.


0.252413051466

**18** : Previous research identified DG copying as a significant overhead.


0.289950275727

**24** : A better method would nfinimize the copying of sharable varts.


0.317739105725

**29** : Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.



#analysing the annotation 15

##The Citing Sentences : 
Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.

###The Corpus Reference Sentences : 
0.894429769458


**23** : Copying sharable parts is called redundant copying.

0.962945667857


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.341615837898


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.117480257221

**109** : For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.


0.121469044368

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


0.16024296921

**163** : IF Current?(node) THEN Return(node).


0.165855330865

**148** : The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.


0.176122601841

**150** : It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1" and returns Nil_.



##The Citing Sentences : 
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.

###The Corpus Reference Sentences : 
0.145811244848


**23** : Copying sharable parts is called redundant copying.

0.627973464365


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.882286969643


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0317712745054

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0685104736788

**18** : Previous research identified DG copying as a significant overhead.


0.115281663726

**24** : A better method would nfinimize the copying of sharable varts.


0.117480257221

**109** : For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.


0.121469044368

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.



##The Citing Sentences : 
Copying of nodes will be delayed until a destructive change is about to take place.

###The Corpus Reference Sentences : 
nan


**23** : Copying sharable parts is called redundant copying.

nan


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

nan


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0317712745054

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0685104736788

**18** : Previous research identified DG copying as a significant overhead.


0.115281663726

**24** : A better method would nfinimize the copying of sharable varts.


0.117480257221

**109** : For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.


0.121469044368

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.



##The Citing Sentences : 
Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.

###The Corpus Reference Sentences : 
0.808777461858


**23** : Copying sharable parts is called redundant copying.

0.928485387571


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.978129368424


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0317712745054

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0685104736788

**18** : Previous research identified DG copying as a significant overhead.


0.09765030875

**175** : ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.


0.115281663726

**24** : A better method would nfinimize the copying of sharable varts.


0.117480257221

**109** : For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.



#analysing the annotation 17
**Not valid annotation**
