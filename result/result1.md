#Try to read the readme file before reading this file
##Total Features Number using contrast: 154

### mainpulating the section : 0

### mainpulating the section : 1

1 - **advantage**: ,Contrast  ,Feature 

2 - **term**: ,Contrast  ,Feature 

3 - **method**: ,Contrast  ,Feature  ,Abstract 

4 - **part**: ,Contrast  ,Feature 

5 - **representing**: ,Contrast 

6 - **relatively**: ,Contrast  ,Feature 

7 - **based**: ,Contrast  ,Feature 

8 - **i.e.**: ,Contrast 

9 - **strategy**: ,Contrast  ,Feature  ,Abstract 

10 - **copying**: ,Contrast  ,Feature 

11 - **proposed**: ,Contrast  ,Feature 

12 - **called**: ,Contrast  ,Feature 

13 - **structure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 2

1 - **partial**: ,Contrast 

2 - **every**: ,Contrast  ,Feature 

3 - **represented**: ,Contrast  ,Feature 

4 - **allows**: ,Contrast 

5 - **contains**: ,Contrast 

6 - **type**: ,Contrast  ,Feature 

7 - **following**: ,Contrast 

8 - **example**: ,Contrast  ,Feature 

9 - **typed**: ,Contrast 

10 - **specifying**: ,Contrast 

11 - **used**: ,Contrast  ,Feature 

12 - **shown**: ,Contrast  ,Feature 

13 - **rooted**: ,Contrast 

14 - **ordering**: ,Contrast 

15 - **relationship**: ,Contrast 

16 - **namely**: ,Contrast 

17 - **feature-value**: ,Contrast  ,Feature 

18 - **set**: ,Contrast  ,Feature 

19 - **exists**: ,Contrast 

20 - **equal**: ,Contrast  ,Feature 

21 - **empty**: ,Contrast 

22 - **using**: ,Contrast  ,Feature 

23 - **describing**: ,Contrast 

24 - **described**: ,Contrast 

25 - **directed**: ,Contrast  ,Feature 

26 - **describe**: ,Contrast 

27 - **symbol**: ,Contrast  ,Feature 

### mainpulating the section : 3

1 - **nodel**: ,Contrast  ,Feature 

2 - **node1**: ,Contrast  ,Feature 

3 - **node2**: ,Contrast  ,Feature 

4 - **current**: ,Contrast  ,Feature 

5 - **two**: ,Contrast  ,Feature 

6 - **meet**: ,Contrast  ,Feature 

7 - **give**: ,Contrast 

8 - **process**: ,Contrast  ,Feature 

9 - **created**: ,Contrast  ,Feature 

10 - **one**: ,Contrast  ,Feature 

11 - **take**: ,Contrast  ,Feature 

12 - **pair**: ,Contrast  ,Feature 

13 - **arc**: ,Contrast  ,Feature 

14 - **result**: ,Contrast  ,Feature 

15 - **label**: ,Contrast  ,Feature 

16 - **value**: ,Contrast  ,Feature 

17 - **procedure**: ,Contrast  ,Feature 

18 - **input**: ,Contrast  ,Feature 

19 - **treat**: ,Contrast  ,Feature 

### mainpulating the section : 4

1 - **new**: ,Contrast  ,Feature 

2 - **consisting**: ,Contrast  ,Feature 

3 - **larger**: ,Contrast 

4 - **e.g.**: ,Contrast  ,Feature 

5 - **actually**: ,Contrast 

6 - **newly**: ,Contrast  ,Feature 

7 - **size**: ,Contrast 

8 - **need**: ,Contrast  ,Feature 

9 - **disjunctive**: ,Contrast 

10 - **arc**: ,Contrast  ,Feature 

11 - **definite**: ,Contrast  ,Feature 

12 - **whole**: ,Contrast  ,Feature 

13 - **newnode**: ,Contrast  ,Feature 

14 - **copied**: ,Contrast  ,Feature 

15 - **dependency**: ,Contrast  ,Feature 

16 - **introduced**: ,Contrast 

17 - **constant**: ,Contrast 

18 - **node**: ,Contrast  ,Feature 

### mainpulating the section : 5

1 - **semantic**: ,Contrast  ,Feature 

2 - **computation**: ,Contrast  ,Abstract 

3 - **whose**: ,Contrast  ,Feature 

4 - **case**: ,Contrast  ,Feature 

5 - **treating**: ,Contrast  ,Feature 

6 - **learned**: ,Contrast 

7 - **example**: ,Contrast  ,Feature 

8 - **occur**: ,Contrast  ,Feature 

9 - **information**: ,Contrast  ,Feature 

10 - **feature**: ,Contrast  ,Feature  ,Abstract 

11 - **representation**: ,Contrast  ,Feature 

12 - **order**: ,Contrast  ,Feature  ,Abstract 

13 - **tendency**: ,Contrast  ,Feature 

14 - **related**: ,Contrast 

15 - **obtains**: ,Contrast 

16 - **small**: ,Contrast  ,Feature 

17 - **number**: ,Contrast  ,Feature 

18 - **depends**: ,Contrast 

19 - **applied**: ,Contrast  ,Feature 

20 - **analysis**: ,Contrast  ,Feature  ,Abstract 

21 - **fail**: ,Contrast  ,Feature 

22 - **sentence**: ,Contrast 

23 - **possible**: ,Contrast 

24 - **unification**: ,Contrast  ,Feature  ,Abstract 

25 - **unnecessary**: ,Contrast 

26 - **often**: ,Contrast 

27 - **agreement**: ,Contrast  ,Feature 

28 - **failure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 6

1 - **graph**: ,Contrast  ,Feature  ,Abstract 

2 - **swapping**: ,Contrast  ,Feature  ,Abstract 

3 - **rate**: ,Contrast  ,Feature 

4 - **total**: ,Contrast  ,Feature 

5 - **sharing**: ,Contrast  ,Feature 

6 - **process**: ,Contrast  ,Feature 

7 - **incremental**: ,Contrast  ,Feature 

8 - **collection**: ,Contrast  ,Feature  ,Abstract 

9 - **natural**: ,Contrast  ,Feature 

10 - **strategic**: ,Contrast  ,Feature 

11 - **generation**: ,Contrast  ,Feature  ,Abstract 

12 - **method**: ,Contrast  ,Feature  ,Abstract 

13 - **language**: ,Contrast  ,Feature  ,Abstract 

14 - **copy**: ,Contrast  ,Feature  ,Abstract 

15 - **wastage**: ,Contrast  ,Feature 

16 - **efficient**: ,Contrast  ,Feature  ,Abstract 

17 - **also**: ,Contrast  ,Feature 

18 - **access**: ,Contrast  ,Feature  ,Abstract 

19 - **achieves**: ,Contrast  ,Feature 

20 - **lazy**: ,Contrast  ,Feature  ,Abstract 

21 - **tending**: ,Contrast  ,Feature 

22 - **efficiency**: ,Contrast  ,Feature  ,Abstract 

23 - **strategy**: ,Contrast  ,Feature  ,Abstract 

24 - **combined**: ,Contrast  ,Feature 

25 - **thus**: ,Contrast  ,Feature 

26 - **make**: ,Contrast  ,Feature 

27 - **avoids**: ,Contrast  ,Feature 

28 - **without**: ,Contrast  ,Feature 

29 - **application**: ,Contrast  ,Feature 

30 - **identical**: ,Contrast  ,Feature 

31 - **unification**: ,Contrast  ,Feature  ,Abstract 

32 - **reduces**: ,Contrast  ,Feature 

33 - **unification-based**: ,Contrast  ,Feature 

34 - **fi'om**: ,Contrast  ,Feature 

35 - **overall**: ,Contrast  ,Feature 

36 - **introduces**: ,Contrast  ,Feature 

37 - **page**: ,Contrast  ,Feature  ,Abstract 

38 - **garbage**: ,Contrast  ,Feature  ,Abstract 

39 - **log**: ,Contrast  ,Feature 

40 - **overhead**: ,Contrast  ,Feature 

41 - **treat**: ,Contrast  ,Feature 

### mainpulating the section : 7

##Total Features Number : 150

### mainpulating the section : 0

### mainpulating the section : 1

1 - **represent** ,Feature 

2 - **advantage** ,Feature 

3 - **natural** ,Feature 

4 - **term** ,Feature 

5 - **used** ,Feature 

6 - **method** ,Feature  ,Abstract 

7 - **language** ,Feature  ,Abstract 

8 - **system** ,Feature 

9 - **part** ,Feature 

10 - **relatively** ,Feature 

11 - **based** ,Feature 

12 - **strategy** ,Feature  ,Abstract 

13 - **copying** ,Feature 

14 - **proposed** ,Feature 

15 - **input** ,Feature 

16 - **problem** ,Feature 

17 - **called** ,Feature 

18 - **structure** ,Feature  ,Abstract 

### mainpulating the section : 2

1 - **every** ,Feature 

2 - **represented** ,Feature 

3 - **type** ,Feature 

4 - **example** ,Feature 

5 - **feature** ,Feature  ,Abstract 

6 - **used** ,Feature 

7 - **shown** ,Feature 

8 - **feature-value** ,Feature 

9 - **set** ,Feature 

10 - **equal** ,Feature 

11 - **greatest** ,Feature 

12 - **using** ,Feature 

13 - **directed** ,Feature 

14 - **symbol** ,Feature 

### mainpulating the section : 3

1 - **nodel** ,Feature 

2 - **node1** ,Feature 

3 - **node2** ,Feature 

4 - **current** ,Feature 

5 - **two** ,Feature 

6 - **meet** ,Feature 

7 - **process** ,Feature 

8 - **tile** ,Feature 

9 - **created** ,Feature 

10 - **first** ,Feature 

11 - **one** ,Feature 

12 - **take** ,Feature 

13 - **pair** ,Feature 

14 - **arc** ,Feature 

15 - **result** ,Feature 

16 - **label** ,Feature 

17 - **copied** ,Feature 

18 - **value** ,Feature 

19 - **procedure** ,Feature 

20 - **input** ,Feature 

21 - **treat** ,Feature 

### mainpulating the section : 4

1 - **new** ,Feature 

2 - **consisting** ,Feature 

3 - **two**: ,Contrast  ,Feature 

4 - **e.g.** ,Feature 

5 - **data** ,Feature  ,Abstract 

6 - **newly** ,Feature 

7 - **number** ,Feature 

8 - **copy** ,Feature  ,Abstract 

9 - **need** ,Feature 

10 - **slot** ,Feature 

11 - **arc**: ,Contrast  ,Feature 

12 - **result**: ,Contrast  ,Feature 

13 - **definite** ,Feature 

14 - **whole** ,Feature 

15 - **newnode** ,Feature 

16 - **copied** ,Feature 

17 - **procedure**: ,Contrast  ,Feature 

18 - **unique** ,Feature 

19 - **dependency** ,Feature 

20 - **node** ,Feature 

21 - **structure**: ,Contrast  ,Feature  ,Abstract 

### mainpulating the section : 5

1 - **semantic** ,Feature 

2 - **whose** ,Feature 

3 - **case** ,Feature 

4 - **treating** ,Feature 

5 - **example**: ,Contrast  ,Feature 

6 - **process**: ,Contrast  ,Feature 

7 - **occur** ,Feature 

8 - **information** ,Feature 

9 - **feature** ,Feature  ,Abstract 

10 - **representation** ,Feature 

11 - **order** ,Feature  ,Abstract 

12 - **tendency** ,Feature 

13 - **small** ,Feature 

14 - **number** ,Feature 

15 - **pair**: ,Contrast  ,Feature 

16 - **efficiency** ,Feature  ,Abstract 

17 - **applied** ,Feature 

18 - **analysis** ,Feature  ,Abstract 

19 - **fail** ,Feature 

20 - **unification** ,Feature  ,Abstract 

21 - **agreement** ,Feature 

22 - **failure** ,Feature  ,Abstract 

### mainpulating the section : 6

1 - **graph** ,Feature  ,Abstract 

2 - **swapping** ,Feature  ,Abstract 

3 - **rate** ,Feature 

4 - **total** ,Feature 

5 - **two**: ,Contrast  ,Feature 

6 - **sharing** ,Feature 

7 - **process**: ,Contrast  ,Feature 

8 - **incremental** ,Feature 

9 - **collection** ,Feature  ,Abstract 

10 - **data** ,Feature  ,Abstract 

11 - **natural** ,Feature 

12 - **strategic** ,Feature 

13 - **generation** ,Feature  ,Abstract 

14 - **method**: ,Contrast  ,Feature  ,Abstract 

15 - **language** ,Feature  ,Abstract 

16 - **copy** ,Feature  ,Abstract 

17 - **wastage** ,Feature 

18 - **efficient** ,Feature  ,Abstract 

19 - **also** ,Feature 

20 - **access** ,Feature  ,Abstract 

21 - **based**: ,Contrast  ,Feature 

22 - **achieves** ,Feature 

23 - **lazy** ,Feature  ,Abstract 

24 - **tending** ,Feature 

25 - **efficiency** ,Feature  ,Abstract 

26 - **strategy**: ,Contrast  ,Feature  ,Abstract 

27 - **combined** ,Feature 

28 - **thus** ,Feature 

29 - **make** ,Feature 

30 - **avoids** ,Feature 

31 - **without** ,Feature 

32 - **application** ,Feature 

33 - **identical** ,Feature 

34 - **unification**: ,Contrast  ,Feature  ,Abstract 

35 - **reduces** ,Feature 

36 - **unification-based** ,Feature 

37 - **fi'om** ,Feature 

38 - **processing** ,Feature 

39 - **overall** ,Feature 

40 - **introduces** ,Feature 

41 - **page** ,Feature  ,Abstract 

42 - **garbage** ,Feature  ,Abstract 

43 - **log** ,Feature 

44 - **overhead** ,Feature 

45 - **treat**: ,Contrast  ,Feature 

46 - **gain** ,Feature 

### mainpulating the section : 7

#The Summary
**ze take 5 percent of the important and long sentences for making the summary**

This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.

That is, an FS unification method is proposed that introduces a strategy called the e_arly failure Â£inding strategy (the EFF strategy) to make FS unification efficient, in this method, FS unification orders are not specified explicitly by rule wril.ers, but are controlled by learned information on tendencies of FS constraint application failures.

These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).

The type symbol lattice contains the greatest type symbol Top, which subsumes every type symbol, and the least type symbol Bottom, which is subsumed by every I.ype symbol.

The procedure applies itself ,'ecursively to each such arc pair values and adds to the output node every arc with the same label as its label and the unification result of their values unless the tmification result is Bottom.

The node specified by the feature path <a> fi'om input graph G1 (Gl/<a>) has an arc with the label c and the corresponding node of input graph G2 does not.

The output node has been created only when neither input node is current; or otherwise the output node is an existing current node.

The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.

(2) In the worst eases, in which there are unique label arcs but all result structures are newly created, the method CopyNode PROCEDURE CopyNode(node, arc, ancestor) node = Dereference(node).

(3) Unevenness of FS unification failure tendency: in extreme cases, if every feature has the same unification failure tendency, this method has no advantage.

The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.

The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.

Thus, the efficiency gain fi'om this method is high when the overall FS unification failure rate of the application process is high.

The combined method Inakes each FS unification efficient and also reduces garbage collection and page swapping occurrences by avoiding memory wastage, thus increasing the total efficiency of li'S unification-based natural language processing systems such aa analysis and generation systems based on IlI'SG.

#analysing the annotation 1

##The Citing Sentences : 
While an improvement over simple destructive unification, Tomabechi's approach still suffers from what Kogure (Kogure, 1990) calls redundant copying.

###The Corpus Reference Sentences : 
1.0


**21** : itowever, the problem with his method is that a unitication result graph consists only of newly created structures.

1.0


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

0.855199934141


**23** : Copying sharable parts is called redundant copying.


###Our Reference Sentences : 
0.0632387973041

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.0822250726662

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.0969302678112

**135** : This order is related to the unification failure tendency.


0.150480240411

**207** : The SING unification method introduces the concept of feature unification strategy.


0.154821678959

**199** : In such cases, the SING unification method obtains efl]ciency gains.



#analysing the annotation 2

##The Citing Sentences : 
The extension is classified into class (1) above.Based on this paper's formalization, unification algorithms have been developed using graph unification techniques[23, 16].

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.133502857924

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.151065021358

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.164667217446

**135** : This order is related to the unification failure tendency.


0.16604985862

**103** : Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.


0.184802162839

**207** : The SING unification method introduces the concept of feature unification strategy.



#analysing the annotation 3

##The Citing Sentences : 
Other versions based on more efficient graph unification methods such as Wroblewski's and Kogure's method [23, 16] have also been developed.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.0719820182931

**207** : The SING unification method introduces the concept of feature unification strategy.


0.0942341361611

**51** : This method is called the strategic ij!~crementaI copy graph unification method (the SING unification method).


0.100878391292

**54** : Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.


0.107103291038

**52** : These two methods can be combined into a single method called the strategic lazy ijAcremeatal copy g~raph unification method (the SLING unification method).


0.110057058142

**208** : 'the method treats features tending to fail in unification first.



#analysing the annotation 4

##The Citing Sentences : 
This is inefficient with many copy operations due to unfications of unnecessary features that do not contribute to successful unification [6].

###The Corpus Reference Sentences : 
0.247858115675


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.

1.0


**206** : This reduces repeated calculation of substructures.


###Our Reference Sentences : 
0.127933092254

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.145608143694

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.159297773475

**135** : This order is related to the unification failure tendency.


0.181941450309

**207** : The SING unification method introduces the concept of feature unification strategy.


0.212527449695

**194** : The efficiency of the SING unification method depends on the following factors: (1) The overall FS unification failure rate of the process: in extreme cases, if Go unification failure occurs, the method has no advantages except the overhead of feature unification order sorting.



##The Citing Sentences : 
Thus treatments such as strategic unification [6] have been developed.

###The Corpus Reference Sentences : 
0.19604813917


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.

1.0


**206** : This reduces repeated calculation of substructures.


###Our Reference Sentences : 
0.0678622905311

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.0867548569359

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.10138747289

**135** : This order is related to the unification failure tendency.


0.127933092254

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.145608143694

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.



#analysing the annotation 5

##The Citing Sentences : 
This observation is the basis for a reordering method proposed by Kogure [1990].

###The Corpus Reference Sentences : 
0.629846149941


**3** : The other, called ti~e strategic incremental copy graph unification method, uses an early failure finding strategy which first tries to unify ;ubstructures tending to fail in unification; this method is; based on stochastic data on tim likelihood of failure and ,'educes unnecessary computation.


###Our Reference Sentences : 
0.00435074333693

**56** : Section 3 and 4 introduce the LING method and the SING method, respectively.


0.00913450793453

**30** : I)eveloping a method which avoids memory wastage is very important.


0.0222077383095

**55** : The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method.


0.0417541754942

**24** : A better method would nfinimize the copying of sharable varts.


0.0638043430268

**54** : Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.



#analysing the annotation 6

##The Citing Sentences : 
Thus for any automatic counting scheme some constant shuffling and reshuffling of the conjunct order needs to be applied until the order stabilizes (see also [Kogure 1990]).

###The Corpus Reference Sentences : 
1.0


**186** : in this method, theretbre, the failure tendency information is acquired by a learning process.

0.946201018708


**187** : That is, the SING unification method applied in an analysis system uses the failure tendency information acquired by a learning analysis process.

0.93781563644


**188** : in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.


###Our Reference Sentences : 
0.818679831778

**188** : in the learning process, when FS unification is applied, feature treatment orders are randomized for the sake of random extraction.


0.849693791439

**135** : This order is related to the unification failure tendency.


0.858770050433

**138** : This order strategy can be generalized to the EFF and applied to the ordering of arcs with common labels.


0.867607165241

**191** : By using learned failure tendency information, feature value unification is applied in an order that first treats features with the greatest tendency to fail.


0.89463587073

**201** : Moreover, it is possible for each type symbol to select whether to apply feature unification order sorting or not.



#analysing the annotation 7

##The Citing Sentences : 
The lazy copying approach ([Kogure, 1990], and [Emele, 1991] for lazy copying in TFS with historical backtracking) copies only overlapping parts of the structure.

###The Corpus Reference Sentences : 
0.788202874194


**39** : This paper proposes an FS unification method that allows structure sharing with constant m'der node access time.

0.630549146509


**40** : This method achieves structure sharing by introducing lazy copying to Wroblewski's incremental copy graph unification method.

1.0


**78** : Then, the unification of tl anti t2 is defined as their greatest lower bound or the meet.


###Our Reference Sentences : 
0.193185097074

**27** : I"or example, in unifying an FS representing constraints on phrase structures and an FS representing a daughter phrase structure, such eases occur very h'equent, ly.


0.263452413562

**177** : Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different.


0.304241337801

**10** : Tiffs dependency is especially crucial for lexicon-driven approaches such as tlPSO[Pollard and Sag 861 and JPSG[Gunji 871 because rich lexieal information and phrase structure information is described in terms of FSs.


0.339287511059

**38** : Avoiding this problem in his method requires a special operation of merging a skeleton-environment structure into a skeleton structure, but this prevents structure sharing.


0.352018105363

**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.



#analysing the annotation 8

##The Citing Sentences : 
At least two schemes have been proposed recently ])a.~ed Ul)OU this observation (namely [Kogure.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.335484521045

**176** : However, these two cases are very rare.


0.571265631308

**15** : Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.


0.662783642076

**5** : Various kinds of grammatical formalisms without t,ranstormation were proposed from the late 1970s I;hrough the 1980s l(]azder eL al 85, l(aplan and Bresnan 82, Kay 1~5, Pollm'd and Sag 871.


0.768524475001

**177** : Usually, the number of features in two input structures is relatively small and the sizes of the two input structures are often very different.


0.789309123182

**100** : The procedure assumes the existence of two procedures, namely, SharedArcs and ComplementArcs.



##The Citing Sentences : 
1990] and [Emele, 1991]); however, both schemes are I)ased upon the increlllent'al Col)yiug sehellle all(l ~-LS ([e- scribed in [Tomal)eehi, 1991] incremental copying schemes inherently suffcr fi'om Early Copying as defined in that article.

###The Corpus Reference Sentences : 
nan


**0** : Strategic Lazy Incremental Copy Graph Unification


###Our Reference Sentences : 
0.335484521045

**176** : However, these two cases are very rare.


0.450332171702

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.571265631308

**15** : Several FS unificatioa methods were proposed in IKarttunen 86, l'ereira 85, Wroblewski 871.


0.640339648586

**18** : Previous research identified DG copying as a significant overhead.


0.65641914338

**23** : Copying sharable parts is called redundant copying.



#analysing the annotation 9
**Not valid annotation**

#analysing the annotation 10

##The Citing Sentences : 
2In the large-scale HPSG-based spoken Japanese analysis system developed at ATR, sometimes 98 percent of the elapsed time is devoted to graph unification ([Kogure, 1990]).

###The Corpus Reference Sentences : 
0.290367582027


**205** : Furthermore, structure sharing increases the portion of token identical substructures of FSs which makes it efficient to keep unification results of substructures of FSs and reuse them.


###Our Reference Sentences : 
0.143195180176

**14** : Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.


0.177220466943

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.193896561909

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.20279928499

**103** : Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.


0.203853233373

**42** : In a natural language proeessing system that uses deelarative constraint rules in terms of FSs, FS unification provides constraint-checking and structure- building mechanisms.



#analysing the annotation 11

##The Citing Sentences : 
That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]).

###The Corpus Reference Sentences : 
1.0


**11** : For example, a spoken Present.

1.0


**14** : Japanese analysis system based on llPSG[Kogure 891 uses 90% - 98% of the elapsed time in FS unification.


###Our Reference Sentences : 
0.194167861625

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.36440250983

**18** : Previous research identified DG copying as a significant overhead.


0.457701273726

**23** : Copying sharable parts is called redundant copying.


0.621587903228

**29** : Memory is wasted by such redundant copying and this causes frequent garbage collection and page swapping which decrease the total system efficiency.


0.791612408614

**20** : Ile proposed an incremental copy graph unification method to avoid over copying and early copying.



#analysing the annotation 12

##The Citing Sentences : 
A more eNcient unification algorithm would avoid this redundant copying (copying structures that can be shared by the input and resultant graphs) (Kogure, 1990).

###The Corpus Reference Sentences : 
0.415786530912


**203** : The LING unification method achieves structure sharing without the O(log d) data access overhead of Pereira's method.

0.913816302896


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

0.786141680146


**23** : Copying sharable parts is called redundant copying.

0.919434146491


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.108515074347

**17** : These methods take two DGs as their inputs and give a unification result DG.


0.12658369016

**8** : In such unification-based formalisms, feature ~trueture (FS) unification is the most fundamental and ..~ignifieant operation.


0.140578142281

**135** : This order is related to the unification failure tendency.


0.172160898453

**207** : The SING unification method introduces the concept of feature unification strategy.


0.195671500348

**199** : In such cases, the SING unification method obtains efl]ciency gains.



#analysing the annotation 13

##The Citing Sentences : 
Kogure (1990) proposed a lazy incremental copy graph (LING) unification that uses dependency-directed eol)yiug

###The Corpus Reference Sentences : 
0.787830373682


**22** : This is unnecessary because there are often input snbgraphs that can be used as part of the result graph without any modification, or as sharable parts between one of the input graphs and the result graph.

1.0


**23** : Copying sharable parts is called redundant copying.

1.0


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.143744347944

**202** : The strategic lazy incremental copy graph (SLING) unification method combines two incremental copy graph unification methods: the lazy incremental copy graph (LING) unification method and the strategic incremental copy graph (SING) unification method.


0.164621141511

**103** : Output graph G3 Figure 5: Incremental copy graph unification In this figure, type symbols are omitted.


0.250423452513

**194** : The efficiency of the SING unification method depends on the following factors: (1) The overall FS unification failure rate of the process: in extreme cases, if Go unification failure occurs, the method has no advantages except the overhead of feature unification order sorting.


0.252588352792

**207** : The SING unification method introduces the concept of feature unification strategy.


0.253078562956

**17** : These methods take two DGs as their inputs and give a unification result DG.



#analysing the annotation 14

##The Citing Sentences : 
A better method would avoid (eliminate) such redundant copying as it is called by [Kogure 90].

###The Corpus Reference Sentences : 
0.712853258467


**23** : Copying sharable parts is called redundant copying.

0.0239172217489


**24** : A better method would nfinimize the copying of sharable varts.


###Our Reference Sentences : 
0.0239172217489

**24** : A better method would nfinimize the copying of sharable varts.


0.02576633488

**56** : Section 3 and 4 introduce the LING method and the SING method, respectively.


0.0501447295676

**30** : I)eveloping a method which avoids memory wastage is very important.


0.062676881381

**55** : The section also introduces the key idea of the EFF strategy wlfich comes from observations of his method.


0.0774081195828

**54** : Section 3 explains a TFS unification method based on Wroblewski's method and then explains the problem with his method.



#analysing the annotation 15

##The Citing Sentences : 
Similarly, in Kogure's approach, not all redundant copying is avoided in cases where there exists a feature path (a sequence of nodes connected by arcs) to a node that needs to be copied.

###The Corpus Reference Sentences : 
0.894978257599


**23** : Copying sharable parts is called redundant copying.

0.976055415894


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.435042147988


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.132666928984

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


0.15421575254

**163** : IF Current?(node) THEN Return(node).


0.164438751999

**148** : The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.


0.177668656399

**150** : It then adds the arc copies and arcs of node/' that are not copied to the new node, and returns the new node; (3) otherwise, CopyNode adds the pair consisting of the ancestor node node2 and the are arcl into the COPY- DEPENDENCY slot of node 1" and returns Nil_.


0.178401303897

**109** : For example, consider the case when feature a is first treated at the root nodes of G1 and G2 in Fig.



##The Citing Sentences : 
As it has been noticed by [Godden 90] and [Kogure 90], the key idea of avoiding "redundant copying" is to do copying lazily.

###The Corpus Reference Sentences : 
0.145185191504


**23** : Copying sharable parts is called redundant copying.

0.752889472693


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.890612255971


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0438932898548

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0678492240318

**18** : Previous research identified DG copying as a significant overhead.


0.132666928984

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


0.15421575254

**163** : IF Current?(node) THEN Return(node).


0.164438751999

**148** : The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.



##The Citing Sentences : 
Copying of nodes will be delayed until a destructive change is about to take place.

###The Corpus Reference Sentences : 
1.0


**23** : Copying sharable parts is called redundant copying.

1.0


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.980119685417


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0438932898548

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0678492240318

**18** : Previous research identified DG copying as a significant overhead.


0.132666928984

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


0.15421575254

**163** : IF Current?(node) THEN Return(node).


0.164438751999

**148** : The revised CopyNode procedure takes as its inputs the node to be copied node I and the arc arc I with node I as its value and node2 as its immediate ancestor node (i.e., the arc's initial node), and does the following (set Fig.



##The Citing Sentences : 
Kogure uses a revised copynode procedure which maintains copy dependency information in order to avoid immediate copying.

###The Corpus Reference Sentences : 
0.816265510477


**23** : Copying sharable parts is called redundant copying.

0.955392326533


**141** : 5 disables structure sharing, ttowever, this whole copying is not necessary if a lazy evaluation method is used.

0.9805111084


**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


###Our Reference Sentences : 
0.0438932898548

**19** : Wroblewski claims that copying is wrong when an algorithm copies too much (over copying) or copies too soon (early copying).


0.0678492240318

**18** : Previous research identified DG copying as a significant overhead.


0.127203341088

**175** : ENDPROCEDURE Figure 7: The revised CopyNode procedure has the disadvantage of treating copy dependency information.


0.132666928984

**142** : With such a method, it is possible to delay copying a node until either its own contents need to change (e.g., node G3/Ka c !7>) or until it is found to have an arc (sequence) to a node t, hat needs to be copied (e.g., node X G3/<a c> in Fig.


0.15421575254

**163** : IF Current?(node) THEN Return(node).



#analysing the annotation 17
**Not valid annotation**
