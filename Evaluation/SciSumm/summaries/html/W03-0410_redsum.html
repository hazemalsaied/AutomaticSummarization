<html>
<head><title>W03-0410_redsum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We gratefully acknowledge the financial support of NSERC of Canada and Bell University Labs . </a>
<a name="1">[1]</a> <a href="#1" id=1>The results for these feature sets in clustering are given in the second sub column Ling ) under each of the , , and measures in Table 2 . </a>
<a name="2">[2]</a> <a href="#2" id=2>We use this baseline in calculating reductions in error rate of . The remaining columns of the table give the , , and measures as described in Section 4.2 , for each of the feature sets we explored in clustering , which we discuss in turn below . </a>
<a name="3">[3]</a> <a href="#3" id=3>We chose hierarchical clustering because it may be possible to find coherent clusters of verbs even when there are not exactly good clusters , where is the number of classes . </a>
<a name="4">[4]</a> <a href="#4" id=4>7 5 0 Table 1 : Verb classes ( see Section 3.1 ) , their Levin class numbers , and the number of experimental verbs in each ( see Section 3.2 ) . </a>
<a name="5">[5]</a> <a href="#5" id=5>In performing hierarchical clustering , both a vector distance measure and a cluster distance ( linkage measure are specified . </a>
<a name="6">[6]</a> <a href="#6" id=6>Another striking result is the difference in values , which are very much higher than those for Ling ( which are in turn much higher than for Full ) . </a>
<a name="7">[7]</a> <a href="#7" id=7>In our clustering experiments , we find that smaller subsets of features generally perform better than the full set of features . </a>
<a name="8">[8]</a> <a href="#8" id=8>This capture what is reflected ins the deprogram </a></body>
</html>
