We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery. In performing hierarchical clustering, both a vector distance measure and a cluster distance (ÃḃÂÂlinkageÃḃÂÂ) measure are specified. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Each set of verbs was judged (by the authorsÃḃÂÂ intuition alone) to be ÃḃÂÂrepresentativeÃḃÂÂ of the class. Learning the argument structure properties of verbsÃḃÂÂthe semantic roles they assign and their mapping to syntactic positionsÃḃÂÂis both particularly important and difficult. Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of ÃḃÂÂbetterÃḃÂÂ and ÃḃÂÂworseÃḃÂÂ clusterings. We found that the mean and values are the same as that of the Seed set reported above, but mean is a little lower. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. We are indebted to Allan Jepson for helpful discussions and suggestions. is the number of classes (a
