<html>
<head><title>C00-2123_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>A search restriction especially useful for the translation direction from German to English is presented.</a>
<a name="1">[1]</a> <a href="#1" id=1>The goal of machine translation is the translation of a text given in some source language into a target language.</a>
<a name="2">[2]</a> <a href="#2" id=2>The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</a>
<a name="3">[3]</a> <a href="#3" id=3>The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</a>
<a name="4">[4]</a> <a href="#4" id=4>The sentence length probability p(JjI) is omitted without any loss in performance.</a>
<a name="5">[5]</a> <a href="#5" id=5>The advantage is that we can recombine search hypotheses by dynamic programming.</a>
<a name="6">[6]</a> <a href="#6" id=6>The resulting algorithm is depicted in Table 1.</a>
<a name="7">[7]</a> <a href="#7" id=7>The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</a>
<a name="8">[8]</a> <a href="#8" id=8>Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</a>
<a name="9">[9]</a> <a href="#9" id=9>Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</a>
<a name="10">[10]</a> <a href="#10" id=10>Here, we process only full-form words within the translation procedure.</a>
<a name="11">[11]</a> <a href="#11" id=11>The perplexity for the trigram language model used is 265.</a>
<a name="12">[12]</a> <a href="#12" id=12>This measure has the advantage of being completely automatic.</a>
<a name="13">[13]</a> <a href="#13" id=13>We apply a beam search concept as in speech recognition.</a>
<a name="14">[14]</a> <a href="#14" id=14>However there is no global pruning.</a>
<a name="15">[15]</a> <a href="#15" id=15>However there is no global pruning.</a>
<a name="16">[16]</a> <a href="#16" id=16>Table 4 shows translation results for the three approaches.</a>
<a name="17">[17]</a> <a href="#17" id=17>In the last example, the less restrictive IbmS word reordering leads to a better translation, although the QmS translation is still acceptable.</a>
<a name="18">[18]</a> <a href="#18" id=18>In this paper, we have presented a new, eÃÂcient DP-based search procedure for statistical machine translation.</a>
<a name="19">[19]</a> <a href="#19" id=19>3) A tight coupling with the speech recognizer output.</a></body>
</html>
