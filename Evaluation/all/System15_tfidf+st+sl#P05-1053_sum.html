<html>
<head><title>P05-1053_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="1">[1]</a> <a href="#1" id=1>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="2">[2]</a> <a href="#2" id=2>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="3">[3]</a> <a href="#3" id=3>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="4">[4]</a> <a href="#4" id=4>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="5">[5]</a> <a href="#5" id=5>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="6">[6]</a> <a href="#6" id=6>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="7">[7]</a> <a href="#7" id=7>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="8">[8]</a> <a href="#8" id=8>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="9">[9]</a> <a href="#9" id=9>We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="10">[10]</a> <a href="#10" id=10>We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="11">[11]</a> <a href="#11" id=11>We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="12">[12]</a> <a href="#12" id=12>Entities can be of five types persons, organizations, locations, facilities and geopolitical entities (GPE geographically defined regions that indicate a political boundary, e.g. countries, states, cities, etc.).</a>
<a name="13">[13]</a> <a href="#13" id=13>For example, we want to determine whether a person is at a location, based on the evidence in the context.</a>
<a name="14">[14]</a> <a href="#14" id=14>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="15">[15]</a> <a href="#15" id=15>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="16">[16]</a> <a href="#16" id=16>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="17">[17]</a> <a href="#17" id=17>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="18">[18]</a> <a href="#18" id=18>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="19">[19]</a> <a href="#19" id=19>explicit relations occur in text with explicit evidence suggesting the relationships.</a>
<a name="20">[20]</a> <a href="#20" id=20>explicit relations occur in text with explicit evidence suggesting the relationships.</a>
<a name="21">[21]</a> <a href="#21" id=21>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="22">[22]</a> <a href="#22" id=22>Section 3 and Section 4 describe our approach and various features employed respectively.</a>
<a name="23">[23]</a> <a href="#23" id=23>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="24">[24]</a> <a href="#24" id=24>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="25">[25]</a> <a href="#25" id=25>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="26">[26]</a> <a href="#26" id=26>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="27">[27]</a> <a href="#27" id=27>Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.</a>
<a name="28">[28]</a> <a href="#28" id=28>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="29">[29]</a> <a href="#29" id=29>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="30">[30]</a> <a href="#30" id=30>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="31">[31]</a> <a href="#31" id=31>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="32">[32]</a> <a href="#32" id=32>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="33">[33]</a> <a href="#33" id=33>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="34">[34]</a> <a href="#34" id=34>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="35">[35]</a> <a href="#35" id=35>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="36">[36]</a> <a href="#36" id=36>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="37">[37]</a> <a href="#37" id=37>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="38">[38]</a> <a href="#38" id=38>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="39">[39]</a> <a href="#39" id=39>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="40">[40]</a> <a href="#40" id=40>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="41">[41]</a> <a href="#41" id=41>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="42">[42]</a> <a href="#42" id=42>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="43">[43]</a> <a href="#43" id=43>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="44">[44]</a> <a href="#44" id=44>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="45">[45]</a> <a href="#45" id=45>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="46">[46]</a> <a href="#46" id=46>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="47">[47]</a> <a href="#47" id=47>Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.</a>
<a name="48">[48]</a> <a href="#48" id=48>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="49">[49]</a> <a href="#49" id=49>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="50">[50]</a> <a href="#50" id=50>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="51">[51]</a> <a href="#51" id=51>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="52">[52]</a> <a href="#52" id=52>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="53">[53]</a> <a href="#53" id=53>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="54">[54]</a> <a href="#54" id=54>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="55">[55]</a> <a href="#55" id=55>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="56">[56]</a> <a href="#56" id=56>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="57">[57]</a> <a href="#57" id=57>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="58">[58]</a> <a href="#58" id=58>Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE.</a>
<a name="59">[59]</a> <a href="#59" id=59>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.</a>
<a name="60">[60]</a> <a href="#60" id=60>Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.</a>
<a name="61">[61]</a> <a href="#61" id=61>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="62">[62]</a> <a href="#62" id=62>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="63">[63]</a> <a href="#63" id=63>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="64">[64]</a> <a href="#64" id=64>Therefore, we must extend SVMs to multi-class (e.g. K) such as the ACE RDC task.</a>
<a name="65">[65]</a> <a href="#65" id=65>For efficiency, we apply the one vs. others strategy, which builds K classifiers so as to separate one class from all others, instead of the pairwise strategy, which builds K*(K-1)/2 classifiers considering all pairs of classes.</a>
<a name="66">[66]</a> <a href="#66" id=66>Moreover, we only apply the simple linear kernel, although other kernels can peform better.</a>
<a name="67">[67]</a> <a href="#67" id=67>For each pair of mentions3, we compute various lexical, syntactic and semantic features.</a>
<a name="68">[68]</a> <a href="#68" id=68>For each pair of mentions3, we compute various lexical, syntactic and semantic features.</a>
<a name="69">[69]</a> <a href="#69" id=69>Normally, the above overlap features are too general to be effective alone.</a>
<a name="70">[70]</a> <a href="#70" id=70>In this paper, we separate the features of base phrase chunking from those of full parsing.</a>
<a name="71">[71]</a> <a href="#71" id=71>In this paper, we separate the features of base phrase chunking from those of full parsing.</a>
<a name="72">[72]</a> <a href="#72" id=72>In this paper, we separate the features of base phrase chunking from those of full parsing.</a>
<a name="73">[73]</a> <a href="#73" id=73>Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.</a>
<a name="74">[74]</a> <a href="#74" id=74>Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.</a>
<a name="75">[75]</a> <a href="#75" id=75>We also extend the list by collecting the trigger words from the head words of the mentions in the training data according to their indicating relationships.</a>
<a name="76">[76]</a> <a href="#76" id=76>This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.</a>
<a name="77">[77]</a> <a href="#77" id=77>In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.</a>
<a name="78">[78]</a> <a href="#78" id=78>However, The remaining words that do not have above four classes are manually classified.</a>
<a name="79">[79]</a> <a href="#79" id=79>Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1 Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.</a>
<a name="80">[80]</a> <a href="#80" id=80>In this way, we model relation extraction as a multi-class classification problem with 43 classes, two for each relation subtype (except the above 6 symmetric subtypes) and a Ãá¸ÃÂÃÂNONEÃá¸ÃÂÃÂ class for the case where the two mentions are not related.</a>
<a name="81">[81]</a> <a href="#81" id=81>Table 2 also measures the contributions of different features by gradually increasing the feature set.</a>
<a name="82">[82]</a> <a href="#82" id=82>Table 2 also measures the contributions of different features by gradually increasing the feature set.</a>
<a name="83">[83]</a> <a href="#83" id=83>However, full parsing is always prone to long distance errors although the CollinsÃá¸ÃÂÃÂ parser used in our system represents the state-of-the-art in full parsing.</a>
<a name="84">[84]</a> <a href="#84" id=84>However, full parsing is always prone to long distance errors although the CollinsÃá¸ÃÂÃÂ parser used in our system represents the state-of-the-art in full parsing.</a>
<a name="85">[85]</a> <a href="#85" id=85>Ãá¸ÃÂÃ¡áºÂ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype Ãá¸ÃÂÃÂROLE.Citizen-OfÃá¸ÃÂÃÂ from Ãá¸ÃÂÃÂROLE.</a>
<a name="86">[86]</a> <a href="#86" id=86>It also indicates the number of testing instances, the number of correctly classified instances and the number of wrongly classified instances for each type or subtype.</a>
<a name="87">[87]</a> <a href="#87" id=87>This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.</a>
<a name="88">[88]</a> <a href="#88" id=88>This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.</a>
<a name="89">[89]</a> <a href="#89" id=89>This suggests that relation detection is critical for relation extraction.</a></body>
</html>
