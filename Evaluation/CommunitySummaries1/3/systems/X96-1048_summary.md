The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations . 
The other two tasks , Template Element and Scenario Template , were information extraction tasks that followed on from the MUC evaluations conducted in previous years . 
Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems . 
NAMED ENTITY The Named Entity ( NE ) task requires insertion of SGML tags into the text stream . 
Even the simplest of the tasks , Named Entity , occasionally requires in-depth processing , e.g. , to determine whether `` 60 pounds '' is an expression of weight or of monetary value . 
About half the systems focused only on individual co reference which has direct relevance to the other MUC6 evaluation tasks . 
The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference ( MUC6 ) in November . 
No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks . 
The amount of agreement between the two annotators was found to be 80 % recall and 82 % precision . 
The author is turning over government leadership of the MUC work to Elaine Marsh at the Naval Research Laboratory in Washington , D.C. Ms. Marsh has many years of experience in computational linguistics to offer , along with extensive familiarity with the MUC evaluations , and will undoubtedly lead the work exceptionally well . 
Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [ 1 ] . 
The identification of a name as that of an organization ( hence , instantiation of an ORGANIZATION object ) or as a person ( PERSON object ) is a named entity identification task . 
Â• Scenario Template ( ST ) -- Drawing evidence from anywhere in the text , extract respecified event information , and relate the event information to the particular organization and person entities involved in the event . 
Human performance was measured in terms of annotator variability on only 30 texts in the test set and showed agreement to be approximately 83 % , when one annotator 's templates were treated as the `` key '' and the other annotator 's templates were treated as the `` response '' . 
This capability has other useful applications as well , e.g. , it enables text highlighting in a browser . 
CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium . 
The system-generated outputs are from three different systems , since no one system did better than all other systems on all three events . 
It was also unexpected that one of the systems would match human performance on the task . 
Common organization names , first names of people , and location names can be handled by recourse to list lookup , although there are drawbacks : some names may be on more than one list , the lists will not be complete and may not match the name as it is realized in the text ( e.g. , may not cover the needed abbreviated form of an organization name , may not cover the complete person name ) , etc.. 
OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION 