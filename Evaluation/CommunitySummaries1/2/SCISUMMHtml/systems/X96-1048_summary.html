<html>
<head><title>X96-1048_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The introduction of two new tasks into the MUC evaluations and the restructuring of information extraction into two separate tasks have infused new life into the evaluations . </a>
<a name="1">[1]</a> <a href="#1" id=1>The other two tasks , Template Element and Scenario Template , were information extraction tasks that followed on from the MUC evaluations conducted in previous years . </a>
<a name="2">[2]</a> <a href="#2" id=2>Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems . </a>
<a name="3">[3]</a> <a href="#3" id=3>NAMED ENTITY The Named Entity ( NE ) task requires insertion of SGML tags into the text stream . </a>
<a name="4">[4]</a> <a href="#4" id=4>Identification of certain common types of names , which constitutes a large portion of the Named Entity task and a critical portion of the Template Element task , has proven to be largely a solved problem . </a>
<a name="5">[5]</a> <a href="#5" id=5>The system-generated outputs are from three different systems , since no one system did better than all other systems on all three events . </a>
<a name="6">[6]</a> <a href="#6" id=6>OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION</a>
<a name="7">[7]</a> <a href="#7" id=7>The evolution and design of the MUC6 evaluation are discussed in the paper by Grishman and Sundheim in this volume . </a>
<a name="8">[8]</a> <a href="#8" id=8>The amount of agreement between the two annotators was found to be 80 % recall and 82 % precision . </a>
<a name="9">[9]</a> <a href="#9" id=9>An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task . </a>
<a name="10">[10]</a> <a href="#10" id=10>For MUC6 , text filtering scores were as high as 98 % recall ( with precision in the 80th percentile ) or 96 % precision ( with recall in the 80th percentile ) . </a>
<a name="11">[11]</a> <a href="#11" id=11>The identification of a name as that of an organization ( hence , instantiation of an ORGANIZATION object ) or as a person ( PERSON object ) is a named entity identification task . </a>
<a name="12">[12]</a> <a href="#12" id=12>Â• Scenario Template ( ST ) -- Drawing evidence from anywhere in the text , extract respecified event information , and relate the event information to the particular organization and person entities involved in the event . </a>
<a name="13">[13]</a> <a href="#13" id=13>Human performance was measured in terms of annotator variability on only 30 texts in the test set and showed agreement to be approximately 83 % , when one annotator 's templates were treated as the `` key '' and the other annotator 's templates were treated as the `` response '' . </a>
<a name="14">[14]</a> <a href="#14" id=14>Systems are measured for their performance on distinguishing relevant from relevant texts via the text filtering metric , which uses the classic information retrieval definitions of recall and precision . </a>
<a name="15">[15]</a> <a href="#15" id=15>CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium . </a>
<a name="16">[16]</a> <a href="#16" id=16>The two SGML-based tasks required innovations to tie system-internal data structures to the original text so that the annotations could be inserted by the system without altering the original text in any other way . </a>
<a name="17">[17]</a> <a href="#17" id=17>It was also unexpected that one of the systems would match human performance on the task . </a>
<a name="18">[18]</a> <a href="#18" id=18>Common organization names , first names of people , and location names can be handled by recourse to list lookup , although there are drawbacks : some names may be on more than one list , the lists will not be complete and may not match the name as it is realized in the text ( e.g. , may not cover the needed abbreviated form of an organization name , may not cover the complete person name ) , etc.. </a>
<a name="19">[19]</a> <a href="#19" id=19>The latest in a series of natural language processing system EVALUATION was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference ( MUC6 ) in November . </a></body>
</html>
