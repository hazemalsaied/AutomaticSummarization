The Named Entity and Coreference tasks entailed Standard Generalized Markup Language ( SGML ) annotation of texts and were being conducted for the first time . 
â¢ Coreference ( CO ) -- Insert SGML tags into the text to link strings that represent referring noun phrases . 
â¢ Scenario Template ( ST ) -- Drawing evidence from anywhere in the text , extract respecified event information , and relate the event information to the particular organization and person entities involved in the event . 
CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium . 
The articles used in the evaluation were drawn from a corpus of approximately 58,000 articles spanning the period of January 1993 through June 1994 . 
The training set and test set each consisted of 100 articles and were drawn from the corpus using a text retrieval system called Managing Gigabytes , whose retrieval engine is based on a context-vector model , producing a ranked list of hits according to degree of match with a keyword search query . 
When the outputs are scored in `` key-to-response '' mode , as though one annotator's output represented the `` key '' and the other the `` response , '' the humans achieved an overall F-measure of 96.68 and a corresponding error per response fill ( ERR ) score of 6 % . 
Summary NE scores on primary metrics for the top 16 ( out of 20 ) systems tested , in order of decreasing F-Measure ( P & R ) 1 1 Key to F-measure scores : BBN baseline configuration 93.65 , BBN experimental configuration 92.88 , Knight-Ridder 85.73 , Lockheed-Martin 90.84 , UManitoba 93.33 , UMass 84.95 , MITRE 91.2 , NMSU CRL baseline configuration 85.82 , NYU 88.19 , USheffield 89.06 , SRA baseline configuration 96.42 , SRA `` fast '' configuration 95.66 , SRA `` fastest '' configuration 92.61 , SRA `` nonages '' configuration 94.92 , SRI 94.0 , Sterling Software 92.74.. 
Common organization names , first names of people , and location names can be handled by recourse to list lookup , although there are drawbacks : some names may be on more than one list , the lists will not be complete and may not match the name as it is realized in the text ( erg , may not cover the needed abbreviated form of an organization name , may not cover the complete person name ) , etc.. 
In the middle of the spectrum are definite descriptions and pronouns whose choice of referent is constrained by such factors as structural relations and discourse focus . 
In the middle of the effort of preparing the test data for the formal evaluation , an annotator variability test was conducted . 
There was a large number of factors that contributed to the 20 % disagreement , including overlooking referential NPs , using different interpretations of vague portions of the guidelines , and making different subjective decisions when the text of an article was ambiguous , sloppy , etc.. 
Most human errors pertained to definite descriptions and bare nominals , not to names and pronouns . 
5 The highest score for the PERSON object , 95 % recall and 95 % precision , is close to the highest score on the NE categorization for person , which was 98 % recall and 99 % precision.. 
In this article , the management succession scenario will be used as the basis for discussion . 
The management succession template consists of four object types , which are linked together via one-way pointers to form a hierarchical structure . 
Of the 100 texts in the test set , 54 were relevant to the management succession scenario , including six that were only marginally relevant . 
No analysis has been done of the relative difficulty of the MUC6 ST task compared to previous extraction evaluation tasks . 
The one-month limitation on development in preparation for MUC6 would be difficult to factor into the computation , and even without that additional factor , the problem of coming up with a reasonable , objective way of measuring relative task difficulty has not been adequately addressed . 
In addition , there are plans to put evaluations on line , with public access , starting with the NE evaluation ; this is intended to make the NE task familiar to new sites and to give them a convenient and low-pressure way to try their hand at following a standardized test procedure . 
