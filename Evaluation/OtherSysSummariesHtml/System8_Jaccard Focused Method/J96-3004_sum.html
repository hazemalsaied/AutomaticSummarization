<html>
<head><title>J96-3004_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>com Â§Cambridge, UK Email nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( ,  & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1  & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.</a>
<a name="1">[1]</a> <a href="#1" id=1>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="2">[2]</a> <a href="#2" id=2>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="3">[3]</a> <a href="#3" id=3>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="4">[4]</a> <a href="#4" id=4>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="5">[5]</a> <a href="#5" id=5>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="6">[6]</a> <a href="#6" id=6>All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..</a>
<a name="7">[7]</a> <a href="#7" id=7>2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..</a>
<a name="8">[8]</a> <a href="#8" id=8>Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.</a>
<a name="9">[9]</a> <a href="#9" id=9>It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.</a>
<a name="10">[10]</a> <a href="#10" id=10>Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.</a>
<a name="11">[11]</a> <a href="#11" id=11>The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.</a>
<a name="12">[12]</a> <a href="#12" id=12>For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.</a>
<a name="13">[13]</a> <a href="#13" id=13>Previous Work.</a>
<a name="14">[14]</a> <a href="#14" id=14>There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).</a>
<a name="15">[15]</a> <a href="#15" id=15>There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).</a>
<a name="16">[16]</a> <a href="#16" id=16>There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).</a>
<a name="17">[17]</a> <a href="#17" id=17>Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.</a>
<a name="18">[18]</a> <a href="#18" id=18>In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.</a>
<a name="19">[19]</a> <a href="#19" id=19>In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.</a>
<a name="20">[20]</a> <a href="#20" id=20>Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.</a>
<a name="21">[21]</a> <a href="#21" id=21>(See Sproat and Shih 1995.)</a>
<a name="22">[22]</a> <a href="#22" id=22>(See Sproat and Shih 1995.)</a>
<a name="23">[23]</a> <a href="#23" id=23>(See Sproat and Shih 1995.)</a>
<a name="24">[24]</a> <a href="#24" id=24>(See Sproat and Shih 1995.)</a>
<a name="25">[25]</a> <a href="#25" id=25>(See Sproat and Shih 1995.)</a>
<a name="26">[26]</a> <a href="#26" id=26>The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.</a>
<a name="27">[27]</a> <a href="#27" id=27>The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.</a>
<a name="28">[28]</a> <a href="#28" id=28>(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).</a>
<a name="29">[29]</a> <a href="#29" id=29>(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).</a>
<a name="30">[30]</a> <a href="#30" id=30>The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.</a>
<a name="31">[31]</a> <a href="#31" id=31>The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.</a>
<a name="32">[32]</a> <a href="#32" id=32>The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.</a>
<a name="33">[33]</a> <a href="#33" id=33>Others depend upon various lexical heurisÂ­ tics for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.</a>
<a name="34">[34]</a> <a href="#34" id=34>Others depend upon various lexical heurisÂ­ tics for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.</a>
<a name="35">[35]</a> <a href="#35" id=35>Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).</a>
<a name="36">[36]</a> <a href="#36" id=36>Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).</a>
<a name="37">[37]</a> <a href="#37" id=37>Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).</a>
<a name="38">[38]</a> <a href="#38" id=38>Lexical-knowledge-based approaches that include statistical information generally presume that one starts with all possible segmentations of a sentence, and picks the best segmentation from the set of possible segmentations using a probabilistic or costÂ­ based scoring mechanism.</a>
<a name="39">[39]</a> <a href="#39" id=39>Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.</a>
<a name="40">[40]</a> <a href="#40" id=40>Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.</a>
<a name="41">[41]</a> <a href="#41" id=41>The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).</a>
<a name="42">[42]</a> <a href="#42" id=42>The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).</a>
<a name="43">[43]</a> <a href="#43" id=43>More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.</a>
<a name="44">[44]</a> <a href="#44" id=44>Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.</a>
<a name="45">[45]</a> <a href="#45" id=45>Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).</a>
<a name="46">[46]</a> <a href="#46" id=46>Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).</a>
<a name="47">[47]</a> <a href="#47" id=47>Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.</a>
<a name="48">[48]</a> <a href="#48" id=48>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="49">[49]</a> <a href="#49" id=49>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="50">[50]</a> <a href="#50" id=50>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="51">[51]</a> <a href="#51" id=51>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="52">[52]</a> <a href="#52" id=52>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="53">[53]</a> <a href="#53" id=53>However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate.</a>
<a name="54">[54]</a> <a href="#54" id=54>Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.</a>
<a name="55">[55]</a> <a href="#55" id=55>For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.</a>
<a name="56">[56]</a> <a href="#56" id=56>The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.</a>
<a name="57">[57]</a> <a href="#57" id=57>The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.</a>
<a name="58">[58]</a> <a href="#58" id=58>Chinese word segmentation can be viewed as a stochastic transduction problem.</a>
<a name="59">[59]</a> <a href="#59" id=59>Chinese word segmentation can be viewed as a stochastic transduction problem.</a>
<a name="60">[60]</a> <a href="#60" id=60>Word frequencies are estimated by a re-estimation procedure that involves applyÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.</a>
<a name="61">[61]</a> <a href="#61" id=61>This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.</a>
<a name="62">[62]</a> <a href="#62" id=62>This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.</a>
<a name="63">[63]</a> <a href="#63" id=63>This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.</a>
<a name="64">[64]</a> <a href="#64" id=64>In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.</a>
<a name="65">[65]</a> <a href="#65" id=65>One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD.</a>
<a name="66">[66]</a> <a href="#66" id=66>Full Chinese personal names are in one respect simple they are always of the form family+given.</a>
<a name="67">[67]</a> <a href="#67" id=67>Given names are most commonly two hanzi long, occasionally one hanzi long there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following 1.</a>
<a name="68">[68]</a> <a href="#68" id=68>The first probability is estimated from a name count in a text database, and the rest of the probabilities are estimated from a large list of personal names.n Note that in Chang et al.'s model the p(rule 9) is estimated as the product of the probability of finding G 1 in the first position of a two-hanzi given name and the probability of finding G2 in the second position of a two-hanzi given name, and we use essentially the same estimate here, with some modifications as described later on.</a>
<a name="69">[69]</a> <a href="#69" id=69>This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.</a>
<a name="70">[70]</a> <a href="#70" id=70>There are two weaknesses in Chang et al.'s model, which we improve upon.</a>
<a name="71">[71]</a> <a href="#71" id=71>We of course also fail to identify, by the methods just described, given names used without their associated family name.</a>
<a name="72">[72]</a> <a href="#72" id=72>This is in general very difficult, given the extremely free manner in which Chinese given names are formed, and given that in these cases we lack even a family name to give the model confidence that it is identifying a name.</a>
<a name="73">[73]</a> <a href="#73" id=73>Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.</a>
<a name="74">[74]</a> <a href="#74" id=74>Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.</a>
<a name="75">[75]</a> <a href="#75" id=75>Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.</a>
<a name="76">[76]</a> <a href="#76" id=76>As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.</a>
<a name="77">[77]</a> <a href="#77" id=77>As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.</a>
<a name="78">[78]</a> <a href="#78" id=78>A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.</a>
<a name="79">[79]</a> <a href="#79" id=79>16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.</a>
<a name="80">[80]</a> <a href="#80" id=80>However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).</a>
<a name="81">[81]</a> <a href="#81" id=81>However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).</a>
<a name="82">[82]</a> <a href="#82" id=82>For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.</a>
<a name="83">[83]</a> <a href="#83" id=83>For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.</a>
<a name="84">[84]</a> <a href="#84" id=84>For a given "word" in the automatic segmentation, if at least k of the huÂ­ man judges agree that this is a word, then that word is considered to be correct.</a>
<a name="85">[85]</a> <a href="#85" id=85>The performance was 80.99% recall and 61.83% precision.</a>
<a name="86">[86]</a> <a href="#86" id=86>Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus seemingly, our system finds as many names as their system, but with four times as many false hits.</a>
<a name="87">[87]</a> <a href="#87" id=87>Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus seemingly, our system finds as many names as their system, but with four times as many false hits.</a>
<a name="88">[88]</a> <a href="#88" id=88>However, we have reason to doubt Chang et al.'s performance claims.</a>
<a name="89">[89]</a> <a href="#89" id=89>Examples are given in Table 4.</a>
<a name="90">[90]</a> <a href="#90" id=90>In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.</a>
<a name="91">[91]</a> <a href="#91" id=91>This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.</a>
<a name="92">[92]</a> <a href="#92" id=92>This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.</a>
<a name="93">[93]</a> <a href="#93" id=93>(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)</a>
<a name="94">[94]</a> <a href="#94" id=94>This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.</a>
<a name="95">[95]</a> <a href="#95" id=95>Second, comparisons of different methods are not meaningful unless one can evalÂ­ uate them on the same corpus.</a>
<a name="96">[96]</a> <a href="#96" id=96>The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).</a>
<a name="97">[97]</a> <a href="#97" id=97>The method reported in this paper makes use solely of unigram probabilities, and is therefore a zeroeth-order model the cost of a particular segmentation is estimated as the sum of the costs of the individual words in the segmentation.</a>
<a name="98">[98]</a> <a href="#98" id=98>Despite these limitations, a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages.</a>
<a name="99">[99]</a> <a href="#99" id=99>The use of weighted transducers in particular has the attractive property that the model, as it stands, can be straightforwardly interfaced to other modules of a larger speech or natural language system presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind.</a>
<a name="100">[100]</a> <a href="#100" id=100>As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.</a>
<a name="101">[101]</a> <a href="#101" id=101>As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.</a>
<a name="102">[102]</a> <a href="#102" id=102>As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.</a>
<a name="103">[103]</a> <a href="#103" id=103>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="104">[104]</a> <a href="#104" id=104>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="105">[105]</a> <a href="#105" id=105>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="106">[106]</a> <a href="#106" id=106>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="107">[107]</a> <a href="#107" id=107>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="108">[108]</a> <a href="#108" id=108>Since the transducers are built from human-readable descriptions using a lexical toolkit (Sproat 1995), the system is easily maintained and extended.</a></body>
</html>
