<html>
<head><title>P98-2143_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Robust pronoun resolution with limited knowledge</a>
<a name="1">[1]</a> <a href="#1" id=1>Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</a>
<a name="2">[2]</a> <a href="#2" id=2>One of the disadvantages of developing a knowledgeÃÂ­ based system, however, is that it is a very labourÃÂ­ intensive and time-consuming task.</a>
<a name="3">[3]</a> <a href="#3" id=3>This paper presÃÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</a>
<a name="4">[4]</a> <a href="#4" id=4>Co reference resolution is a field in which major progress has been made in the last decade . </a>
<a name="5">[5]</a> <a href="#5" id=5>Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.</a>
<a name="6">[6]</a> <a href="#6" id=6>Some of the limitations of the traditional rule based approaches ( Mitkov , 1998 ) could be overcome by machine learning techniques , which allow automating the acquisition of knowledge from annotated corpora . </a>
<a name="7">[7]</a> <a href="#7" id=7>( Mitkov , 1998 ; Poesio et al. , 2002 ; Markert and Nissim , 2005 ) ) , machine learning methods were embraced ( cf . </a>
<a name="8">[8]</a> <a href="#8" id=8>For the most part , anaphora resolution has focused on traditional linguistic methods ( Carbonell & Brown 1988 ; Carter 1987 ; Hobbs 1978 ; Ingria & Stallard 1989 ; Lappin & McCord 1990 ; Lappin & Leass 1994 ; Mitkov 1994 ; Rich & LuperFoy 1988 ; Sidner 1979 ; Webber 1979 ) . </a>
<a name="9">[9]</a> <a href="#9" id=9>However , to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense . </a>
<a name="10">[10]</a> <a href="#10" id=10>While various alternatives have been proposed , making use of e.g . neural networks , a situation seÂ­ antics framework , or the principles of reasoning with uncertainty ( e.g . Connoly et al . 1994 ; Mitkov 1995 ; Tin & Akman 1995 ) , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems , and to enhance further the automatic proÂ­ cussing of growing language resources . </a>
<a name="11">[11]</a> <a href="#11" id=11>Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain Andros linguistic knowledge ( Baldwin 1997 ; Dagan & ital 1990 ; Kennedy & Boguraev 1996 ; Mitkov 1998 ; Nasukawa 1994 ; Williams et al . 1996 ) . </a>
<a name="12">[12]</a> <a href="#12" id=12>Our work is a continuation of these latest trends in the search for inexpensive , fast and reliable procedures for anaphÂ­ ora resolution . </a>
<a name="13">[13]</a> <a href="#13" id=13>It is also an example of how anaphora in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing . </a>
<a name="14">[14]</a> <a href="#14" id=14>Finally , our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English , but also for other languages ( in our case Polish and Arabic ) . </a>
<a name="15">[15]</a> <a href="#15" id=15>With a view to avoiding complex syntactic , semanÂ­ tic and discourse analysis ( which is vital for realÂ­ world applications ) , we developed a robust , knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analysis the input in order to identify antecedents of anaphora . </a>
<a name="16">[16]</a> <a href="#16" id=16>`` Non-prepositional '' noun phrases A `` pure '' , `` non-prepositional '' noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase ( 0 , -1 ) . </a>
<a name="17">[17]</a> <a href="#17" id=17>It makes use of only a part-of-speech tagger , plus simple noun phrase rules ( sentence constituents are identified at the level of noun phrase at most ) and operates on the basis of antecedent-tracking preferences ( referred to hereafter as `` antecedent indicators '' ) . </a>
<a name="18">[18]</a> <a href="#18" id=18>The approach works as follows : it takes as an input the output of a text processed by a part-of-speech tagger , identifies the noun phrases which precede the anaphora within a distance of 2 sentences , checks them for gender and number agreement with the anaphora and then applies the genre-specific antecedent indicators to the reÂ­ raining candidates ( see next section ) . </a>
<a name="19">[19]</a> <a href="#19" id=19>The noun phrase with the highest aggregate score is proposed as antecedent ; in the rare event of a tie , priority is given to the candidate with the higher score for imÂ­ mediate reference . </a>
<a name="20">[20]</a> <a href="#20" id=20>If immediate reference has not been identified , then priority is given to the Candi date with the best collocation pattern score . </a>
<a name="21">[21]</a> <a href="#21" id=21>If this does not help , the candidate with the higher score for indicating verbs is preferred . </a>
<a name="22">[22]</a> <a href="#22" id=22>If still no choice is possible , the most recent from the remaining candiÂ­ dates is selected as the antecedent . </a>
<a name="23">[23]</a> <a href="#23" id=23>2.1 Antecedent indicators . </a>
<a name="24">[24]</a> <a href="#24" id=24>Antecedent indicators ( preferences ) play a decisive role in tracking down the antecedent from a set of possible candidates . </a>
<a name="25">[25]</a> <a href="#25" id=25>Candidates are assigned a score ( -1 , 0 , 1 or 2 ) for each indicator ; the candidate with the highest aggregate score is proposed as the anteÂ­ cement . </a>
<a name="26">[26]</a> <a href="#26" id=26>The antecedent indicators have been identiÂ­ fie empirically and are related to salience ( definiteness , givenness , indicating verbs , lexical reiteration , section heading preference , `` nonÂ­ prepositional '' noun phrases ) , to structural matches ( collocation , immediate reference ) , to referential distance or to preference of terms . </a>
<a name="27">[27]</a> <a href="#27" id=27>Whilst some of the indicators are more genre-specific ( term preferÂ­ enc and others are less genre-specific ( `` immediate reference '' ) , the majority appear to be genreÂ­ independent . </a>
<a name="28">[28]</a> <a href="#28" id=28>In the following we shall outline some the indicators used and shall illustrate them by exÂ­ ample . </a>
<a name="29">[29]</a> <a href="#29" id=29>Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphora than indefinite ones ( definite noun phrases score 0 and indefinite ones are penalty by -1 ) . </a>
<a name="30">[30]</a> <a href="#30" id=30>We regard a noun phrase as definite if the head noun is modified by a definite article , or by demonstrative or possesÂ­ give pronouns . </a>
<a name="31">[31]</a> <a href="#31" id=31>This rule is ignored if there are no definite articles , possessive or demonstrative proÂ­ nouns in the paragraph ( this exception is taken into account because some English user 's guides tend to omit articles ) . </a>
<a name="32">[32]</a> <a href="#32" id=32>Givenness Noun phrases in previous sentences representing the `` given information '' ( theme ) 1 are deemed good candidates for antecedents and score I ( candidates not representing the theme score 0 ) . </a>
<a name="33">[33]</a> <a href="#33" id=33>In a coherent text ( Firbas 1992 ) , the given or known information , or theme , usually appears first , and thus forms a coÂ­ referential link with the preceding text . </a>
<a name="34">[34]</a> <a href="#34" id=34>For anaphora in simple sentences , noun phrases in the previous senÂ­ sentence are the best candidate for antecedent , followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back { 1 , 0 , -1 ) . </a>
<a name="35">[35]</a> <a href="#35" id=35>1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence . </a>
<a name="36">[36]</a> <a href="#36" id=36>Empirical evidence sugÂ­ tests that because of the salience of the noun phrases which follow them , the verbs listed above are particularly good indicators . </a>
<a name="37">[37]</a> <a href="#37" id=37>Lexically reiterated items include reÂ­ pated synonymous noun phrases which may often be preceded by definite articles or demonstratives . </a>
<a name="38">[38]</a> <a href="#38" id=38>Also , a sequence of noun phrases with the same head counts as lexical reiteration ( e.g . `` toner bottle '' , `` bottle of toner '' , `` the bottle '' ) . </a>
<a name="39">[39]</a> <a href="#39" id=39>The new information , or heme provides some information about the theme . </a>
<a name="40">[40]</a> <a href="#40" id=40>Indicating verbs If a verb is a member of the Verb_set = { discuss , present , illustrate , identify , summarize examine , describe , define , show , check , develop , review , reÂ­ port , outline , consider , investigate , explore , assess , analysis synthesis study , survey , deal , cover } , we consider the first NP following it as the preferred anÂ­ antecedent scores 1 and 0 ) . </a>
<a name="41">[41]</a> <a href="#41" id=41>Lexical reiteration Lexically reiterated items are likely candidates for antecedent ( a NP scores 2 if is repeated within the same paragraph twice or more , 1 if repeated once and 0 if not ) . </a>
<a name="42">[42]</a> <a href="#42" id=42>Section heading preference If a noun phrase occurs in the heading of the section , part of which is the current sentence , then we conÂ­ sider it as the preferred candidate ( 1 , 0 ) . </a>
<a name="43">[43]</a> <a href="#43" id=43>Example : Insert the cassette into the VCR making sure iti is suitable for the length of recording . </a>
<a name="44">[44]</a> <a href="#44" id=44>Here `` the VCR '' is penalty -1 ) for being part of the prepositional phrase `` into the VCR '' . </a>
<a name="45">[45]</a> <a href="#45" id=45>This preference can be explained in terms of saliÂ­ enc from the point of view of the centering theory . </a>
<a name="46">[46]</a> <a href="#46" id=46>The latter proposes the ranking `` subject , direct obÂ­ sect indirect object '' ( Brennan et al . 1987 ) and noun phrases which are parts of prepositional phrases are usually indirect objects . </a>
<a name="47">[47]</a> <a href="#47" id=47>Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun ( 2,0 ) . </a>
<a name="48">[48]</a> <a href="#48" id=48>The collocation preference here is restricted to the patterns `` noun phrase ( pronoun ) , verb '' and `` verb , noun phrase ( pronoun ) '' . </a>
<a name="49">[49]</a> <a href="#49" id=49>Owing to lack of syntactic information , this preference is somewhat weaker than the collocation preference described in ( Dagan & ital 1990 ) . </a>
<a name="50">[50]</a> <a href="#50" id=50>Example : Press the key down and turn the volume up ... </a>
<a name="51">[51]</a> <a href="#51" id=51>Press iti again . </a>
<a name="52">[52]</a> <a href="#52" id=52>Immediate reference In technical manuals the `` immediate reference '' clue can often be useful in identifying the antecedent . </a>
<a name="53">[53]</a> <a href="#53" id=53>The heuristics used is that in constructions of the form `` ... ( You ) V 1 NP ... con ( you ) V 2 it ( con ( you ) V3 it ) '' , where con e { and/or/before/after ... } , the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun `` it '' immeÂ­ mediately following V2 and is therefore given preference ( scores 2 and 0 ) . </a>
<a name="54">[54]</a> <a href="#54" id=54>Input is checked against agreement and for a number of antecedent indicators . </a>
<a name="55">[55]</a> <a href="#55" id=55>It is also quite freÂ­ Quent with imperative constructions . </a>
<a name="56">[56]</a> <a href="#56" id=56>Example : To print the paper , you can stand the printer up or lay iti flat . </a>
<a name="57">[57]</a> <a href="#57" id=57>To turn on the printer , press the Power button and hold iti down for a moment . </a>
<a name="58">[58]</a> <a href="#58" id=58>Unwrap the paperiness form iti and align  then load iti into the drawer . </a>
<a name="59">[59]</a> <a href="#59" id=59>Referential distance In complex sentences , noun phrases in the previous clause 2 are the best candidate for the antecedent of an anaphora in the subsequent clause , followed by noun phrases in the previous sentence , then by nouns situated 2 sentences further back and finally nouns 3 sentences further back ( 2 , 1 , 0 , -1 ) . </a>
<a name="60">[60]</a> <a href="#60" id=60>Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms ( score 1 if the NP is a term and 0 if not ) . </a>
<a name="61">[61]</a> <a href="#61" id=61> 2 1dentification of clauses in complex sentences is do e heuristically . </a>
<a name="62">[62]</a> <a href="#62" id=62>As already mentioned , each of the antecedent inÂ­ indicators assigns a score with a value { -1 , 0 , 1 , 2 } . </a>
<a name="63">[63]</a> <a href="#63" id=63>These scores have been determined experimentally on an empirical basis and are constantly being upÂ­ dated . </a>
<a name="64">[64]</a> <a href="#64" id=64>Top symptoms like `` lexical reiteration '' asÂ­ sign score `` 2 '' whereas `` non-prepositional '' noun phrases are given a negative score of `` -1 '' . </a>
<a name="65">[65]</a> <a href="#65" id=65>We should point out that the antecedent indicators are preferences and not absolute factors . </a>
<a name="66">[66]</a> <a href="#66" id=66>There might be cases where one or more of the antecedent indicators do not `` point '' to the correct antecedent . </a>
<a name="67">[67]</a> <a href="#67" id=67>For inÂ­ stance , in the sentence `` Insert the cassette into the VCRi making sure iti is turned on '' , the indicator `` non-prepositional noun phrases '' would penalty the correct antecedent . </a>
<a name="68">[68]</a> <a href="#68" id=68>When all preferences ( antecedent indicators ) are taken into account , however , the right antecedent is still very likely to be tracked down - in the above example , the `` non-prepositional noun phrases '' heuristics ( penalty ) would be overturned by the `` collocation preference '' heuristics . </a>
<a name="69">[69]</a> <a href="#69" id=69>2.2 Informal description of the algorithm . </a>
<a name="70">[70]</a> <a href="#70" id=70>Evaluation shows a success rate of 89.7 % for the genre of techÂ­ finical manuals and at least in this genre , the approach appears to be more successful than other similar methods . </a>
<a name="71">[71]</a> <a href="#71" id=71>The algorithm for pronoun resolution can be deÂ­ scribed informally as follows : 1 . </a>
<a name="72">[72]</a> <a href="#72" id=72>Examine the current sentence and the two preÂ­ . </a>
<a name="73">[73]</a> <a href="#73" id=73>ceding sentences ( if available ) . </a>
<a name="74">[74]</a> <a href="#74" id=74>Mitkov ( 1998 ) obtains a success rate of 89.7 % for pronominal references , working with English technical manuals . </a>
<a name="75">[75]</a> <a href="#75" id=75>We have also adapted and evaluated the approach for Polish ( 93.3 % success rate ) and for Arabic ( 95.2 % success rate ) . </a>
<a name="76">[76]</a> <a href="#76" id=76>Look for noun phrases 3 only to the left of the anaphor 4 2 . </a>
<a name="77">[77]</a> <a href="#77" id=77>languages An attractive feature of any NLP approach would be its language `` universality '' . </a>
<a name="78">[78]</a> <a href="#78" id=78>Select from the noun phrases identified only . </a>
<a name="79">[79]</a> <a href="#79" id=79>those which agree in gender and numberS with the pronominal anaphora and group them as a set of potential candidates </a>
<a name="80">[80]</a> <a href="#80" id=80>ital candidate and assign scores ; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences , a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat anaphora non-anaphoric `` it '' occurring in constructions such as `` It is important '' , `` It is necessary '' is eliminated by a `` referential filter '' 5Note that this restriction may not always apply in lanÂ­ gages other than English ( e.g . German ) ; on the other hand , there are certain collective nouns in English which do not agree in number with their antecedents ( e.g . `` government '' , `` team '' , `` parliament '' etc . can be referred to by `` they '' ; equally some plural nouns ( e.g . `` data '' ) can be referred to by `` it '' ) and are exempted from the agreeÂ­ met test . </a>
<a name="81">[81]</a> <a href="#81" id=81>While we acknowledge that most of the monolingual NLP approaches are not automatically transferable ( with the same degree of efficiency ) to other languages , it would be highly desirable if this could be done with minimal adaptaÂ­ son . </a>
<a name="82">[82]</a> <a href="#82" id=82>For this purpose we have drawn up a compreÂ­ tensive list of all such cases ; to our knowledge , no other computational treatment of pronominal anaphora resoluÂ­ son has addressed the problem of `` agreement excepÂ­ sons . </a>
<a name="83">[83]</a> <a href="#83" id=83>antecedent . </a>
<a name="84">[84]</a> <a href="#84" id=84>If two candidates have an equal score , the candidate with the higher score for immediate reference is proposed as antecedent . </a>
<a name="85">[85]</a> <a href="#85" id=85>If immediate reference does not hold , propose the candidate with higher score for collocation pattern . </a>
<a name="86">[86]</a> <a href="#86" id=86>The robust approach adapted for Polish demonstrated a high success rate of 93.3 % in resolvÂ­ ing anaphora with critical success rate of 86.2 % ) . </a>
<a name="87">[87]</a> <a href="#87" id=87>If collocation pattern suggests a tie or does not hold , select the candidate with higher score for indicating verbs . </a>
<a name="88">[88]</a> <a href="#88" id=88>If this indicator does not hold again , go for the most recent candidate . </a>
<a name="89">[89]</a> <a href="#89" id=89>Similarly to the evaluation for English , we comÂ­ pared the approach for Polish with ( i ) a Baseline Model which discounts candidates on the basis of agreement in number and gender and , if there were still competing candidates , selects as the antecedent the most recent subject matching the anaphora in gender and number ( ii ) a Baseline Model which checks agreement in number and gender and , if there were still more than one candidate left , picks up as the antecedent the most recent noun phrase that agrees with the anaphora . </a>
<a name="90">[90]</a> <a href="#90" id=90>Our preference-based approach showed clear suÂ­ priority over both baseline models . </a>
<a name="91">[91]</a> <a href="#91" id=91>3 . </a>
<a name="92">[92]</a> <a href="#92" id=92>We have recently adapted the approach for AraÂ­ bic as well ( Mitkov & Belguith 1998 ) . </a>
<a name="93">[93]</a> <a href="#93" id=93>Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge . </a>
<a name="94">[94]</a> <a href="#94" id=94>We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish . </a>
<a name="95">[95]</a> <a href="#95" id=95>Evaluation . </a>
<a name="96">[96]</a> <a href="#96" id=96>For practical reasons , the approach presented does not incorporate syntactic and semantic information ( other than a list of domain terms ) and it is not realÂ­ is tic to expect its performance to be as good as an approach which makes use of syntactic and semantic knowledge in terms of constraints and preferences . </a>
<a name="97">[97]</a> <a href="#97" id=97>The lack of syntactic information , for instance , means giving up c-cornmand constraints and subject preference ( or on other occasions object preference , see Mitkov I995 ) which could be used in center tracking . </a>
<a name="98">[98]</a> <a href="#98" id=98>Syntactic parallelism , useful in discrimiÂ­ eating between identical pronouns on the basis of their syntactic function , also has to be forgone . </a>
<a name="99">[99]</a> <a href="#99" id=99>Lack of semantic knowledge rules out the use of verb seÂ­ antics and semantic parallelism . </a>
<a name="100">[100]</a> <a href="#100" id=100>Our evaluation , however , suggests that much less is lost than might be feared . </a>
<a name="101">[101]</a> <a href="#101" id=101>In fact , our evaluation shows that the reÂ­ cults are comparable to syntax-based methods ( Lappin & Leass I994 ) . </a>
<a name="102">[102]</a> <a href="#102" id=102>We believe that the good success rate is due to the fact that a number of anteÂ­ cement indicators are taken into account and no facÂ­ tor is given absolute preference . </a>
<a name="103">[103]</a> <a href="#103" id=103>In particular , this strategy can often override incorrect decisions linked with strong centering preference ( Mitkov & Belguith I998 ) or syntactic and semantic parallelism preferÂ­ hences see below ) . </a>
<a name="104">[104]</a> <a href="#104" id=104>3.1 Evaluation A . Our first evaluation exercise ( Mitkov & Stys 1997 ) was based on a random sample text from a technical manual in English ( Minolta 1994 ) . </a>
<a name="105">[105]</a> <a href="#105" id=105>Evaluation reports a success rate of 89.7 % which is better than the sucÂ­ less rates of the approaches selected for comparison and tested on the same data . </a>
<a name="106">[106]</a> <a href="#106" id=106>There were 71 pronouns in the 140 page technical manual ; 7 of the pronouns were non-anaphoric and 16 anaphoric . </a>
<a name="107">[107]</a> <a href="#107" id=107>The resolution of anaphora was carried out with a sucÂ­ less rate of 95.8 % . </a>
<a name="108">[108]</a> <a href="#108" id=108>The approach being robust ( an attempt is made to resolve each anaphora and a proÂ­ posed antecedent is returned ) , this figure represents both `` precision '' and `` recall '' if we use the MUC terminology . </a>
<a name="109">[109]</a> <a href="#109" id=109>To avoid any terminological confusion , we shall therefore use the more neutral term `` success rate '' while discussing the evaluation . </a>
<a name="110">[110]</a> <a href="#110" id=110>In order to evaluate the effectiveness of the apÂ­ roach and to explore if I how far it is superior over the baseline models for anaphora resolution , we also tested the sample text on ( i ) a Baseline Model which checks agreement in number and gender and , where more than one candidate remains , picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphora ii ) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphora </a>
<a name="111">[111]</a> <a href="#111" id=111>The success rate of the `` Baseline Subject '' was 29.2 % , whereas the success rate of `` Baseline Most Recent NP '' was 62.5 % . </a>
<a name="112">[112]</a> <a href="#112" id=112>Given that our knowledgeÂ­ poor approach is basically an enhancement of a baseline model through a set of antecedent indicaÂ­ tors , we see a dramatic improvement in performance ( 95.8 % ) when these preferences are called upon . </a>
<a name="113">[113]</a> <a href="#113" id=113>Typically , our preference-based model proved superior to both baseline models when the anteceÂ­ dent was neither the most recent subject nor the most recent noun phrase matching the anaphora in gender and number . </a>
<a name="114">[114]</a> <a href="#114" id=114>Example : Identify the drawer by the lit paper port LED and add paper to itj . </a>
<a name="115">[115]</a> <a href="#115" id=115>The aggregate score for `` the drawer '' is 7 ( definiteness 1 + givenness 0 + term preference 1 + indicating verbs I + lexical reiteration 0 + section heading 0 + collocation 0 + referential distance 2 + non-prepositional noun phrase 0 + immediate referÂ­ enc 2 = 7 ) , whereas aggregate score for the most recent matching noun phrase ( `` the lit paper port LED '' ) is 4 ( definiteness 1 + givenness 0 + term preference I + indicating verbs 0 + lexical reiteraÂ­ son 0 + section heading 0 + collocation 0 + referenÂ­ ital distance 2 + non-prepositional noun phrase 0 + immediate reference 0 = 4 ) . </a>
<a name="116">[116]</a> <a href="#116" id=116>From this example we can also see that our knowledge-poor approach successfully tackles cases in which the anaphora and theÂ· antecedent have not only different syntactic functions but also different semantic roles . </a>
<a name="117">[117]</a> <a href="#117" id=117>Usually knowledge-based apÂ­ roaches have difficulties in such a situation because they use preferences such as `` syntactic parallelism '' or `` semantic parallelism '' . </a>
<a name="118">[118]</a> <a href="#118" id=118>Our robust approach does not use these because it has no information about the syntactic structure of the sentence or about the synÂ­ tactic functionalism role of each individual word . </a>
<a name="119">[119]</a> <a href="#119" id=119>As far as the typical failure cases are concerned , we anticipate the knowledge-poor approach to have difficulties with sentences which have a more comÂ­ lex syntactic structure . </a>
<a name="120">[120]</a> <a href="#120" id=120>This should not be surprising , given that the approach does not rely on any syntactic knowledge and in particular , it does not produce any parse tree . </a>
<a name="121">[121]</a> <a href="#121" id=121>Indeed , the approach fails on the sentence : The paper through key can be used to feed [ a blank sheet of paper ] j through the copier out into the copy tray without making a copy on itj . </a>
<a name="122">[122]</a> <a href="#122" id=122>where `` blank sheet of paper '' scores only 2 as opÂ­ posed to the `` the paper through key '' which scores 6 . </a>
<a name="123">[123]</a> <a href="#123" id=123>3.2 Evaluation B . We carried out a second evaluation of the approach on a different set of sample texts from the genre of technical manuals ( 47-page Portable Style-Writer User 's Guide ( Stylewriter 1994 ) . </a>
<a name="124">[124]</a> <a href="#124" id=124>Out of 223 proÂ­ nouns in the text , 167 were non-anaphoric ( deictic and non-anaphoric `` it '' ) . </a>
<a name="125">[125]</a> <a href="#125" id=125>The evaluation carried out was manual to ensure that no added error was genÂ­ rated e.g . due to possible wrong sentence detection or POS tagging ) . </a>
<a name="126">[126]</a> <a href="#126" id=126>Another reason for doing it by hand is to ensure a fair comparison with Breck Baldwin 's method , which not being available to us , had to be hand-simulated ( see 3.3 ) . </a>
<a name="127">[127]</a> <a href="#127" id=127>The evaluation indicated 83.6 % success rate . </a>
<a name="128">[128]</a> <a href="#128" id=128>The `` Baseline subject '' model tested on the same data scored 33.9 % recall and 67.9 % precision , whereas `` Baseline most recent '' scored 66.7 % . </a>
<a name="129">[129]</a> <a href="#129" id=129>Note that `` Baseline subject '' can be assessed both in terms of recall and precision because this `` version '' is not robust : in the event of no subject being available , it is not able to propose an antecedent ( the manual guide used as evaluation text contained many imÂ­ imperative sentences ) . </a>
<a name="130">[130]</a> <a href="#130" id=130>In the second experiment we evaluated the apÂ­ roach from the point of view also of its `` critical success rate '' . </a></body>
</html>
