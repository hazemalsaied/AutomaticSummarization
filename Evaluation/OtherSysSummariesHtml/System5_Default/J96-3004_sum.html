<html>
<head><title>J96-3004_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..</a>
<a name="1">[1]</a> <a href="#1" id=1>2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..</a>
<a name="2">[2]</a> <a href="#2" id=2>2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..</a>
<a name="3">[3]</a> <a href="#3" id=3>For example, suppose one is building a ITS system for Mandarin Chinese.</a>
<a name="4">[4]</a> <a href="#4" id=4>Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information.</a>
<a name="5">[5]</a> <a href="#5" id=5>Morphologically derived words such as, xue2shengl+men0.</a>
<a name="6">[6]</a> <a href="#6" id=6>There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).</a>
<a name="7">[7]</a> <a href="#7" id=7>Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.</a>
<a name="8">[8]</a> <a href="#8" id=8>Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.</a>
<a name="9">[9]</a> <a href="#9" id=9>Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).</a>
<a name="10">[10]</a> <a href="#10" id=10>Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).</a>
<a name="11">[11]</a> <a href="#11" id=11>Chinese word segmentation can be viewed as a stochastic transduction problem.</a>
<a name="12">[12]</a> <a href="#12" id=12>Chinese word segmentation can be viewed as a stochastic transduction problem.</a>
<a name="13">[13]</a> <a href="#13" id=13>An input ABCD can be represented as an FSA as shown in Figure 2(b).</a>
<a name="14">[14]</a> <a href="#14" id=14>An input ABCD can be represented as an FSA as shown in Figure 2(b).</a>
<a name="15">[15]</a> <a href="#15" id=15>This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.</a>
<a name="16">[16]</a> <a href="#16" id=16>This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.</a>
<a name="17">[17]</a> <a href="#17" id=17>This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.</a>
<a name="18">[18]</a> <a href="#18" id=18>This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.</a>
<a name="19">[19]</a> <a href="#19" id=19>This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.</a>
<a name="20">[20]</a> <a href="#20" id=20>(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.</a>
<a name="21">[21]</a> <a href="#21" id=21>(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.</a>
<a name="22">[22]</a> <a href="#22" id=22>(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.</a>
<a name="23">[23]</a> <a href="#23" id=23>4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B XÂ¥ ..SPl 'How do you say octopus in Japanese?' previously shown in Figure 1.</a>
<a name="24">[24]</a> <a href="#24" id=24>irL as the product of the probability estimate for iÂ¥JJ1l., and the probability estimate just derived for unseen plurals in ir, p(iÂ¥1J1l.ir,) p(iÂ¥1J1l.)p(unseen(f,)).</a>
<a name="25">[25]</a> <a href="#25" id=25>There is a (costless) transition between the NC node and f,.</a>
<a name="26">[26]</a> <a href="#26" id=26>There are two weaknesses in Chang et al.'s model, which we improve upon.</a>
<a name="27">[27]</a> <a href="#27" id=27>There are two weaknesses in Chang et al.'s model, which we improve upon.</a>
<a name="28">[28]</a> <a href="#28" id=28>42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.</a>
<a name="29">[29]</a> <a href="#29" id=29>42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.</a>
<a name="30">[30]</a> <a href="#30" id=30>As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.</a>
<a name="31">[31]</a> <a href="#31" id=31>As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.</a>
<a name="32">[32]</a> <a href="#32" id=32>Virginia) and -sia are normally transliterated as fbSi!</a>
<a name="33">[33]</a> <a href="#33" id=33>An anti-greedy algorithm, AG instead of the longest match, take the.</a>
<a name="34">[34]</a> <a href="#34" id=34>The method being described-henceforth ST..</a>
<a name="35">[35]</a> <a href="#35" id=35>The method being described-henceforth ST..</a>
<a name="36">[36]</a> <a href="#36" id=36>The method being described-henceforth ST..</a>
<a name="37">[37]</a> <a href="#37" id=37>The method being described-henceforth ST..</a>
<a name="38">[38]</a> <a href="#38" id=38>The method being described-henceforth ST..</a>
<a name="39">[39]</a> <a href="#39" id=39>The method being described-henceforth ST..</a>
<a name="40">[40]</a> <a href="#40" id=40>The method being described-henceforth ST..</a>
<a name="41">[41]</a> <a href="#41" id=41>The method being described-henceforth ST..</a>
<a name="42">[42]</a> <a href="#42" id=42>The method being described-henceforth ST..</a>
<a name="43">[43]</a> <a href="#43" id=43>from the subset of the United Informatics corpus not used in the training of the models.</a>
<a name="44">[44]</a> <a href="#44" id=44>The result of this is shown in Figure 7.</a>
<a name="45">[45]</a> <a href="#45" id=45>The result of this is shown in Figure 7.</a>
<a name="46">[46]</a> <a href="#46" id=46>However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).</a>
<a name="47">[47]</a> <a href="#47" id=47>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="48">[48]</a> <a href="#48" id=48>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="49">[49]</a> <a href="#49" id=49>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="50">[50]</a> <a href="#50" id=50>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="51">[51]</a> <a href="#51" id=51>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="52">[52]</a> <a href="#52" id=52>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="53">[53]</a> <a href="#53" id=53>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="54">[54]</a> <a href="#54" id=54>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="55">[55]</a> <a href="#55" id=55>Under this scheme, n human judges are asked independently to segment a text.</a>
<a name="56">[56]</a> <a href="#56" id=56>On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.</a>
<a name="57">[57]</a> <a href="#57" id=57>On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.</a>
<a name="58">[58]</a> <a href="#58" id=58>On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.</a>
<a name="59">[59]</a> <a href="#59" id=59>In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.</a>
<a name="60">[60]</a> <a href="#60" id=60>Examples are given in Table 4.</a>
<a name="61">[61]</a> <a href="#61" id=61>paper, and is missing 6 examples from the A set.</a>
<a name="62">[62]</a> <a href="#62" id=62>In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.</a>
<a name="63">[63]</a> <a href="#63" id=63>However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.</a>
<a name="64">[64]</a> <a href="#64" id=64>However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.</a>
<a name="65">[65]</a> <a href="#65" id=65>Two sets of examples from Gan are given in (1) and (2) ( Gan's Appendix B, exx.</a>
<a name="66">[66]</a> <a href="#66" id=66>(a) 1 Â§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1Â§ . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' $ chel jinglguo4 car pass by 2.</a>
<a name="67">[67]</a> <a href="#67" id=67>An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).</a>
<a name="68">[68]</a> <a href="#68" id=68>An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).</a>
<a name="69">[69]</a> <a href="#69" id=69>While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.</a>
<a name="70">[70]</a> <a href="#70" id=70>While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.</a>
<a name="71">[71]</a> <a href="#71" id=71>While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.</a></body>
</html>
