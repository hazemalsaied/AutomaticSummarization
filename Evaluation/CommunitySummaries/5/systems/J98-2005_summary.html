<html>
<head><title>J98-2005_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Estimation of Probabilistic Context-Free Grammars</a>
<a name="1">[1]</a> <a href="#1" id=1>The assignment of probabilities to the productions of a context-free grammar may generate an improper distribution the probability of all finite parse trees is less than one.</a>
<a name="2">[2]</a> <a href="#2" id=2>We show here that estimated production probabilities always yield proper distributions.</a>
<a name="3">[3]</a> <a href="#3" id=3>Context-free grammars ( CFG 's ) are useful because of their relatively broad coverage and because of the availability of efficient parsing algorithms . </a>
<a name="4">[4]</a> <a href="#4" id=4>( 8~fl ea ~ ( B -- ~/3 ) = ~=lf B -- ~/3 ; cai ) ( 3 ) c~ s.t . H < B-~ ) e~ ~i=lf B -- -+o4cai ) The maximum-likelihood estimator is the natural , `` relative frequency , '' estimator . </a>
<a name="5">[5]</a> <a href="#5" id=5>Given a set of finite parse trees wl , w2 ... .. w , , the maximum-likelihood estimator for p ( see Section 2 ) is , sensibly enough , the `` relative frequency '' estimator y'~nlf A ~ AA ; wi ) ~i=1 f ( A ~ AA ; wi ) + f ( A ~ a ; wi ) ] where f ( . ; w ) is the number of occurrences of the production `` . '' in the tree w. The sentence a m , although ambiguous ( there are multiple parses when m > 2 ) , always involves m - 1 of the A ~ AA productions and m of the A ~ a productions . </a>
<a name="6">[6]</a> <a href="#6" id=6>Furthermore , CFG 's are readily fit with a probability distribution ( to make probabilistic CFG 's -- or PCFG 's ) , rendering them suitable for ambiguous languages through the maximum a posteriori rule of choosing the most probable parse . </a>
<a name="7">[7]</a> <a href="#7" id=7>impose proper probability distributions on D ( Chi and Geman 1998 ) . </a>
<a name="8">[8]</a> <a href="#8" id=8>Dumpster Laird , and Rubin [ 1977 ] put the idea into a much more general setting and coined the Chi and Geman Probabilistic Context-Free Grammars term EM for Expectation-Maximization . </a>
<a name="9">[9]</a> <a href="#9" id=9>More generally , let G -- ( V , T , R , S ) denote a context-free grammar with finite variable set V , start symbol S E V , finite terminal set T , and finite production ( or rule ) set R . </a>
<a name="10">[10]</a> <a href="#10" id=10>For each nonterminal symbol , a ( normalized ) probability is placed on the set of all productions from that symbol . </a>
<a name="11">[11]</a> <a href="#11" id=11>Unfortunately , this simple procedure runs into an unexpected complication : the language generated by the grammar may have probability less than one . </a>
<a name="12">[12]</a> <a href="#12" id=12>The reason is that the derivation tree may have probability greater than zero of never terminating -- some mass can be lost to infinity . </a>
<a name="13">[13]</a> <a href="#13" id=13>This phenomenon is well known and well understood , and there are tests for `` tightness '' ( by which we mean total probability mass equal to one ) involving a matrix derived from the expected growth in numbers of symbols generated by the probabilistic rules ( see for example Booth and Thompson [ 1973 ] , Grenander [ 1976 ] , and Harris [ 1963 ] ) . </a>
<a name="14">[14]</a> <a href="#14" id=14>What if the production probabilities are estimated from data ? </a>
<a name="15">[15]</a> <a href="#15" id=15>For example , there is a simple maximum-likelihood prescription for estimating the production probabilities from a corpus of trees ( see Section 2 ) , resulting in a PCFG . </a>
<a name="16">[16]</a> <a href="#16" id=16>Suppose , for example , that we have a parsed corpus that we treat as a collection of ( independent ) samples from a grammar . </a>
<a name="17">[17]</a> <a href="#17" id=17>It is reasonable to hope that if the trees in the sample are finite , then an estimate of production probabilities based upon the sample will produce a system that assigns probability zero to the set of infinite trees . </a>
<a name="18">[18]</a> <a href="#18" id=18>It is not hard to show that Sh is nondecreasing and converges to min ( 1 , I ) , meaning that a proper probability is obtained if and only if p < ~ . </a>
<a name="19">[19]</a> <a href="#19" id=19>Is it tight ? </a>
<a name="20">[20]</a> <a href="#20" id=20>In the simple example here , the estimator converges in one step and is the same ~ as if we had observed the entire parse tree for each wi . </a></body>
</html>
