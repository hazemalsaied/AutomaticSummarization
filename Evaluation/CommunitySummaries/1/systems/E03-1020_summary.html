<html>
<head><title>E03-1020_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph . </a>
<a name="1">[1]</a> <a href="#1" id=1>Based on the intuition that nouns which co-occur in a list are often semantically related , we extract contexts of the form Noun , Noun , ... Andros Noun , e.g . `` gnomic DNA from rat , mouse and dog '' . </a>
<a name="2">[2]</a> <a href="#2" id=2>Flow within dense regions in the graph is concentrated by both expansion and inflation . </a>
<a name="3">[3]</a> <a href="#3" id=3>Discovering Corpus-Specific Word Senses</a>
<a name="4">[4]</a> <a href="#4" id=4>The word sense clustering algorithm as outlined below can be applied to any kind of similarity measure based on any set of features . </a>
<a name="5">[5]</a> <a href="#5" id=5>sz44 CD miltrA , literate h , ) Cik Figure 1 : Local graph of the word mouse </a>
<a name="6">[6]</a> <a href="#6" id=6>We conducted a pilot experiment to examine the performance of our algorithm on a set of words with varying degree of ambiguity . </a>
<a name="7">[7]</a> <a href="#7" id=7>Finally , section 6 sketches applications of the algorithm and discusses future work . </a>
<a name="8">[8]</a> <a href="#8" id=8>The algorithm is based on a graph model representing words and relationships between them.</a>
<a name="9">[9]</a> <a href="#9" id=9>Then senses of target word were iteratively learned by clustering the local graph of similar words around target word . </a>
<a name="10">[10]</a> <a href="#10" id=10>Doro and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w ( without w ) and clustering it with MCL . </a>
<a name="11">[11]</a> <a href="#11" id=11>The model from which we discover distinct word senses is built automatically from the British National corpus , which is tagged for parts of speech . </a>
<a name="12">[12]</a> <a href="#12" id=12>Usually , one sense of an ambiguous word w is much more frequent than its other senses present in the corpus . </a>
<a name="13">[13]</a> <a href="#13" id=13>As they rely on the detection of high-density areas in a network of cooccurrences , ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) are the closest methods to ours . </a>
<a name="14">[14]</a> <a href="#14" id=14>In our case , we chose a more general approach by working at the level of a simiÂ­larity graph when the similarity of two words is given by their relation of cooccurrence , our situaÂ­tion is comparable to the one of ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) </a>
<a name="15">[15]</a> <a href="#15" id=15>Preliminary observations show that the different neighbors in Table 1 can be used to indicate with great accuracy which of the senses is being used . </a>
<a name="16">[16]</a> <a href="#16" id=16>Similar to the approach as presented in ( Dorow and Widdows , 2003 ) we construct a word graph . </a>
<a name="17">[17]</a> <a href="#17" id=17>The same corpus evidence which supports a clustering of an ambiguous word into distinct senses can be used to decide which sense is referred to in a given context ( Schiitze , 1998 ) . </a>
<a name="18">[18]</a> <a href="#18" id=18>This is achieved , in a manner similar to Pantel and Lin 's ( 2002 ) sense clustering approach , by removing c 's features from the set of features used for finding similar words . </a>
<a name="19">[19]</a> <a href="#19" id=19>The algorithm is based on a graph model representing words and relationships between them . </a>
<a name="20">[20]</a> <a href="#20" id=20>Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word . </a></body>
</html>
