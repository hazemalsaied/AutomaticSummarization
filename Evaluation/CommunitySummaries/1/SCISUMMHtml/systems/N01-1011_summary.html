<html>
<head><title>N01-1011_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In these experiments , there were no decision trees that used all of the digram features identify ed by the ? altering step , and for many words the decision tree learner went on to eliminate most of the candidate features . </a>
<a name="1">[1]</a> <a href="#1" id=1>A decision stump is a one node decision tree ( Holte , 1993 ) that is created by stopping the decision tree learner after the single most informative feature is added to the tree . </a>
<a name="2">[2]</a> <a href="#2" id=2>There is no search of the feature space performed to build a representative model as is the case with decision trees . </a>
<a name="3">[3]</a> <a href="#3" id=3>The last line of Table 1 shows the win-tie-loss score of the decision thermopower divergence method relative to every other method . </a>
<a name="4">[4]</a> <a href="#4" id=4>There is a further assumption that each feature is conditionally independent of all other features , given the sense of the ambiguous word . </a>
<a name="5">[5]</a> <a href="#5" id=5>Each sense { tagged occurrence of an ambiguous word is converted into a feature vector , where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process . </a>
<a name="6">[6]</a> <a href="#6" id=6>Note that the parts of speech are encoded as n for noun , a for adjective , v for verb , and p for words where the part of speech was not provided . </a>
<a name="7">[7]</a> <a href="#7" id=7>Decision trees are among the most widely used machine learning algorithms . </a>
<a name="8">[8]</a> <a href="#8" id=8>For example , there were two tasks associated with bet , one for its use as a noun and the other as a verb . </a>
<a name="9">[9]</a> <a href="#9" id=9>A preliminary version of this paper appears in ( Pedersen , 2001 ) . </a></body>
</html>
