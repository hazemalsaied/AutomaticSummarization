The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference ( MUC6 ) in November . 
Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [ 1 ] . 
One of the innovations of MUC6 was to formalize the general structure of event templates , and all three scenarios defined in the course of MUC6 conformed to that general structure . 
Even the simplest of the tasks , Named Entity , occasionally requires in-depth processing , e.g. , to determine whether `` 60 pounds '' is an expression of weight or of monetary value . 
NAMED ENTITY The Named Entity ( NE ) task requires insertion of SGML tags into the text stream . 
Both possibilities present increased opportunities for systems to under generate or over generate . 
These two slots caused problems for the annotators as well as for the systems . 
CO Results Overall In all , seven sites participated in the MUC6 co reference evaluation . 
An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task . 
OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION
COREFERENCE The task as defined for MUC6 was restricted to noun phrases ( NPs ) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe . 
The University of Durham reported that they had intended to use gazetteer and company name lists , but did n't , because they found that the lists did not have much effect on their system 's performance . 
It also facilitates information extraction , since some of the information in the extraction templates is in the form of literal text strings , which some systems have in the past had difficulty reproducing in their output . 
In addition to categorization errors , the walk through text provides other interesting examples of system errors at the object level and the slot level , plus a number of examples of system successes . 
The approximate 5050 split between relevant and relevant texts was Template Level ( Doc_Nr ) JCCESSION_EVE/~ ( Post , Vacancy_Reason ) In_and_Out r IN_AND_OUT `` Succession Org ( New_Status , On_the_Job , Rel Other_Org ) j IO Template Element Level PERSON ORGANIZATION  1ame Per_Alias , ( Org_Name , Org_Alias , Org_Descriptor , Per_Title ) ~Q0rg_Type , Org_Locale , Org_Country ) Figure 7 . 
Three scenarios were defined in the course of MUC6 : ( 1 ) a scenario concerning the event of organizations placing orders to buy aircraft with aircraft manufacturers ( the `` aircraft order '' scenario ) ; ( 2 ) a scenario concerning the event of contract negotiations between labor unions and companies ( the `` labor negotiations '' scenario ) ; ( 3 ) a scenario concerning changes in corporate managers occupying executive posts ( the `` management succession '' scenario ) . 
The ORG_COUNTRY slot is a special case in a way , since it is required to be filled when the ORG_LOCALE slot is filled . 
It was also unexpected that one of the systems would match human performance on the task . 
For generation of a PERSON object , the text must provide the name of the person ( full name or part of a name ) . 
Participants were invited to enter their systems in as many as four different task-oriented EVALUATION . 