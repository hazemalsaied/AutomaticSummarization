Finally , note that the smallest decision trees are functionally equivalent to our benchmark methods . 
Learning continues until all the training examples are accounted for by the decision tree . 
during the association between two words , while the decision tree seeks bi grams that partition instances of the ambiguous word into into distinct senses . 
A win shows it was more accurate than the method in the column , a loss means it was less accurate , and a tie means it was equally accurate . 
Word sense disambiguation is the process of selecting the most appropriate meaning for a word , based on the context in which it occurs . 
Decision trees have been used in supervised learning approaches to word sense disambiguation , and have fared well in a number of comparative studies ( e.g. , ( Mooney , 1996 ) , ( Pedersen and Bruce , 1997 ) ) . 
Note that the parts of speech are encoded as n for noun , a for adjective , v for verb , and p for words where the part of speech was not provided . 
The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2 . 
The counts in n +1 and n 1+ indicate how often words big and cat occur as the ? rst and second words of any digram in the corpus . 
A preliminary version of this paper appears in ( Pedersen , 2001 ) . 