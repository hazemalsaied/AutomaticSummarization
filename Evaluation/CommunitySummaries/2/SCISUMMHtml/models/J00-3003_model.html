<html>
<head><title>J00-3003_model</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>While there is hardly consensus on exactly how discourse structure should be described , some agreement exists that a useful first level of analysis involves the identification of dialog acts ( DAs ) . </a>
<a name="1">[1]</a> <a href="#1" id=1>Thus , DAs can be thought of as a tag set that classifies utterances according to a combination of pragmatic , semantic , and syntactic criteria . </a>
<a name="2">[2]</a> <a href="#2" id=2>Interaction dominance ( Linell 1990 ) might be measured more accurately using DA distributions than with simpler techniques , and could serve as an indicator of the type or genre of discourse at hand . </a>
<a name="3">[3]</a> <a href="#3" id=3>In all these cases , DA labels would enrich the available input for higher-level processing of the spoken words . </a>
<a name="4">[4]</a> <a href="#4" id=4>Tag STATEMENT BACKCHANNEL/ACKNOWLEDGE OPINION ABANDONED/UNINTERPRETABLE AGREEMENT/ACCEPT APPRECIATION YEs-No-QUESTION NONVERBAL YES ANSWERS CONVENTIONAL-CLOSING WH-QUESTION NO ANSWERS RESPONSE ACKNOWLEDGMENT HEDGE DECLARATIVE YES-No-QuESTION OTHER BACKCHANNEL-QUESTION QUOTATION SUMMARIZE/REFORMULATE AFFIRMATIVE NON-YES ANSWERS ACTION-DIRECTIVE COLLABORATIVE COMPLETION REPEAT-PHRASE OPEN-QUESTION RHETORICAL-QUESTIONS HOLD BEFORE ANSWER/AGREEMENT REJECT NEGATIVE NON-NO ANSWERS SIGNAL-NON-UNDERSTANDING OTHER ANSWERS CONVENTIONAL-OPENING OR-CLAUSE DISPREFERRED ANSWERS 3RD-PARTY-TALK OFFERS , OPTIONS ~ COMMITS SELF-TALK D OWNPLAYER MAYBE/AcCEPT-PART TAG-QUESTION DECLARATIVE WH-QUESTION APOLOGY THANKING Example % Me , I 'm in the legal department . </a>
<a name="5">[5]</a> <a href="#5" id=5>< .1 % Hey thanks a lot < .1 % The goal of this article is twofold : On the one hand , we aim to present a comprehensive framework for modeling and automatic classification of DAs , founded on well-known statistical methods . </a>
<a name="6">[6]</a> <a href="#6" id=6>For example , our model draws on the use of DA n-grams and the hidden Markov models of conversation present in earlier work , such as Nagata and Morimoto ( 1993 , 1994 ) and Woszczyna and Waibel ( 1994 ) ( see Section 7 ) . </a>
<a name="7">[7]</a> <a href="#7" id=7>However , our framework generalizes earlier models , giving us a clean probabilistic approach for performing DA classification from unreliable words and non lexical evidence . </a>
<a name="8">[8]</a> <a href="#8" id=8>For the speech recognition task , our framework provides a mathematically principled way to condition the speech recognizer on conversation context through dialog structure , as well as on non lexical information correlated with DA identity . </a>
<a name="9">[9]</a> <a href="#9" id=9>Second , we present results obtained with this approach on a large , widely available corpus of spontaneous conversational speech . </a>
<a name="10">[10]</a> <a href="#10" id=10>The domain we chose to model is the Switchboard corpus of human-human conversational telephone speech ( Godfrey , Holliman , and McDaniel 1992 ) distributed by the Linguistic Data Consortium . </a>
<a name="11">[11]</a> <a href="#11" id=11>The computation of likelihoods P ( EIU ) depends on the types of evidence used . </a>
<a name="12">[12]</a> <a href="#12" id=12>Prosodic features-Evidence is given by the acoustic features F capturing various aspects of pitch , duration , energy , etc. , of the speech signal ; the associated likelihoods are P ( F I U ) . </a>
<a name="13">[13]</a> <a href="#13" id=13>The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model ( HMM ) ( Rabiner and Juang 1986 ) . </a>
<a name="14">[14]</a> <a href="#14" id=14>The HMM states correspond to DAs , observations correspond to utterances , transition probabilities are given by the discourse grammar ( see Section 4 ) , and observation probabilities are given by the local likelihoods P ( Eil Ui ) . </a>
<a name="15">[15]</a> <a href="#15" id=15>We conducted preliminary experiments to assess how neural networks compare to decision trees for the type of data studied here . </a>
<a name="16">[16]</a> <a href="#16" id=16>Table 9 Combined utterance classification accuracies ( chance = 35 % ) . </a>
<a name="17">[17]</a> <a href="#17" id=17>Discourse Grammar Accuracy ( % ) Prosody Recognizer Combined None 38.9 42.8 56.5 Unigram 48.3 61.8 62.4 Bigram 49.7 64.3 65.0 Table 10 Accuracy ( in % ) for individual and combined models for two subtasks , using uniform priors ( chance = 50 % ) . </a>
<a name="18">[18]</a> <a href="#18" id=18>A back channel is a short utterance that plays discourse-structuring roles , e.g. , indicating that the speaker should go on talking . </a>
<a name="19">[19]</a> <a href="#19" id=19>These are usually referred to in the conversation analysis literature as `` continuers '' and have been studied extensively ( Jefferson 1984 ; Schegloff 1982 ; Yngve 1970 ) . </a></body>
</html>
