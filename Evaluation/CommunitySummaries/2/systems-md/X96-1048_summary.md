Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one . 
Nearly half the sites chose to participate in all four tasks , and all but one site participated in at least one SGML task and one extraction task . 
Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [ 1 ] . 
( BBN system ) Table 5 . 
Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems . 
About half the systems focused only on individual co reference which has direct relevance to the other MUC6 evaluation tasks . 
This period comprised the `` evaluation epoch . '' 
COREFERENCE The task as defined for MUC6 was restricted to noun phrases ( NPs ) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe . 
An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task . 
Finally , a change in administration of the MUC evaluations is occurring that will bring fresh ideas . 
All but two of the systems posted F-measure scores in the 7080 % range , and four of the systems were able to achieve recall in the 7080 % range while maintaining precision in the 8090 % range , as shown in the figure 4 . 
As indicated in table 2 , all systems performed better on identifying person names than on identifying organization or location names , and all but a few systems performed better on location names than on organization names . 
One of the innovations of MUC6 was to formalize the general structure of event templates , and all three scenarios defined in the course of MUC6 conformed to that general structure . 
This capability has other useful applications as well , e.g. , it enables text highlighting in a browser . 
CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium . 
Management Succession Template Structure intentional and is comparable to the richness of the MUC3 `` TST2 '' test set and the MUC4 `` TST4 '' test set . 
All the participating sites also submitted systems for evaluation on the TE and NE tasks . 
Statistically , large differences of up to 15 points may not be reflected as a difference in the ranking of the systems . 
For generation of an ORGANIZATION object , the text must provide either the name ( full or part ) or a descriptor of the organization . 
OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION 