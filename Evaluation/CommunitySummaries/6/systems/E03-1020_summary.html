<html>
<head><title>E03-1020_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Doro and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w ( without w ) and clustering it with MCL . </a>
<a name="1">[1]</a> <a href="#1" id=1>Following the method in ( Widdows and Dorow , 2002 ) , we build a graph in which each node represents a noun and two nodes have an edge between them if they co-occur in lists more than a given number of times 1 . </a>
<a name="2">[2]</a> <a href="#2" id=2>The idea underlying the MCL-algorithm is that random walks within the graph will tend to stay in the same cluster rather than jump between clusters . </a>
<a name="3">[3]</a> <a href="#3" id=3>Discovering Corpus-Specific Word Senses</a>
<a name="4">[4]</a> <a href="#4" id=4>there are other related efforts on word sense discrimination ( Dorow and Widdows , 2003 ; Fukumoto and Suzuki , 1999 ; Pedersen and Bruce , 1997 ) . </a>
<a name="5">[5]</a> <a href="#5" id=5>Then senses of target word were iteratively learned by clustering the local graph of similar words around target word . </a>
<a name="6">[6]</a> <a href="#6" id=6>Their algorithm required a threshold as input , which controlled the number of senses . </a>
<a name="7">[7]</a> <a href="#7" id=7>The algorithm in ( Dorow and Widdows , 2003 ) represented target noun word , its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times . </a>
<a name="8">[8]</a> <a href="#8" id=8>As they rely on the detection of high-density areas in a network of cooccurrences , ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) are the closest methods to ours . </a>
<a name="9">[9]</a> <a href="#9" id=9>From a global viewpoint , these two differences lead ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) to build finer senses than ours . </a>
<a name="10">[10]</a> <a href="#10" id=10>Instead we link each word to its top n neighbors where n can be determined by the user ( cf . </a>
<a name="11">[11]</a> <a href="#11" id=11>Output the list of class-labels which best represent the different senses of w in the corpus . </a>
<a name="12">[12]</a> <a href="#12" id=12>Discrimination against previously extracted sense clusters enables us to discover new senses . </a>
<a name="13">[13]</a> <a href="#13" id=13>Similar to the approach as presented in ( Dorow and Widdows , 2003 ) we construct a word graph . </a>
<a name="14">[14]</a> <a href="#14" id=14>In our case , we chose a more general approach by working at the level of a simiÂ­larity graph when the similarity of two words is given by their relation of cooccurrence , our situaÂ­tion is comparable to the one of ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) </a>
<a name="15">[15]</a> <a href="#15" id=15>The family of such algorithms is described in ( Widdows , 2003 ) . </a>
<a name="16">[16]</a> <a href="#16" id=16>However , whereas there are many edges within an area of meaning , there is only a small number of ( weak ) links between different areas of meaning . </a>
<a name="17">[17]</a> <a href="#17" id=17>Section 5 describes the experiment and presents a sample of the results . </a>
<a name="18">[18]</a> <a href="#18" id=18>This is achieved , in a manner similar to Pantel and Lin 's ( 2002 ) sense clustering approach , by removing c 's features from the set of features used for finding similar words . </a>
<a name="19">[19]</a> <a href="#19" id=19>Sense clusters are iteratively computed by clustering the local graph of similar words around an ambiguous word . </a>
<a name="20">[20]</a> <a href="#20" id=20>The same corpus evidence which supports a clustering of an ambiguous Word into distinct senses can be used to decide which sense is referred to in a given context ( Schiitze , 1998 ) . </a></body>
</html>
