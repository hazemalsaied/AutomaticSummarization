<html>
<head><title>N01-1011_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We believe that the approach in this paper is the ? rst time that decision trees based strictly on digram features have been employed . </a>
<a name="1">[1]</a> <a href="#1" id=1>The most accurate method is the decision tree based on a feature set determined by the power divergence statistic . </a>
<a name="2">[2]</a> <a href="#2" id=2>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</a>
<a name="3">[3]</a> <a href="#3" id=3>Learning continues until all the training examples are accounted for by the decision tree . </a>
<a name="4">[4]</a> <a href="#4" id=4>Each feature selected during the search process is represented by a node in the learned decision tree . </a>
<a name="5">[5]</a> <a href="#5" id=5>Decision trees are among the most widely used machine learning algorithms . </a>
<a name="6">[6]</a> <a href="#6" id=6>Test instances are disambiguated by ? ding a path through the learned decision tree from the root to a leaf node that corresponds with the observed features . </a>
<a name="7">[7]</a> <a href="#7" id=7>Then the decision tree learning algorithm is described , as are some benchmark learning algorithms that are included for purposes of comparison . </a>
<a name="8">[8]</a> <a href="#8" id=8>This paper describes an approach where a decision tree is learned from some number of sentences where each instance of an ambiguous word has been manually annotated with a sense { tag that denotes the most appropriate sense for that context . </a>
<a name="9">[9]</a> <a href="#9" id=9>In light of this , ( Pedersen , 1996 ) presents Fisher 's exact test as an alternative since it does not rely on the distributional assumptions that underly both Pearson 's test and the likelihood ratio . </a>
<a name="10">[10]</a> <a href="#10" id=10>Decision trees have been used in supervised learning approaches to word sense disambiguation , and have fared well in a number of comparative studies ( e.g. , ( Mooney , 1996 ) , ( Pedersen and Bruce , 1997 ) ) . </a>
<a name="11">[11]</a> <a href="#11" id=11>We have presented an ensemble approach to word sense disambiguation ( Pedersen , 2000 ) where multiple Naive Bayesian class ers , each based on co { occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line . </a>
<a name="12">[12]</a> <a href="#12" id=12>Word sense disambiguation is the process of selecting the most appropriate meaning for a word , based on the context in which it occurs . </a>
<a name="13">[13]</a> <a href="#13" id=13>We have developed an approach to word sense disambiguation that represents text entirely in terms of the occurrence of bi grams which we de ? ne to be two cat : cat totals big n 11 = 10 n 12 = 20 n 1+ = 30 : big n 21 = 40 n 22 = 930 n 2+ = 970 totals n +1 =50 n +2 =950 n ++ =1000 Figure 1 : Representation of Bigram Counts consecutive words that occur in a text . </a>
<a name="14">[14]</a> <a href="#14" id=14>These typically include the part { of { speech of surrounding words , the presence of certain key words within some window of context , and various syntactic properties of the sentence and the ambiguous word . </a>
<a name="15">[15]</a> <a href="#15" id=15>In general the total context available for each ambiguous word is less than 100 surrounding words . </a>
<a name="16">[16]</a> <a href="#16" id=16>The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2 . </a>
<a name="17">[17]</a> <a href="#17" id=17>The following process is repeated for each task . </a>
<a name="18">[18]</a> <a href="#18" id=18>For example , there were two tasks associated with bet , one for its use as a noun and the other as a verb . </a>
<a name="19">[19]</a> <a href="#19" id=19>A preliminary version of this paper appears in ( Pedersen , 2001 ) . </a>
<a name="20">[20]</a> <a href="#20" id=20>The evaluation at SENSEVAL was based on precision and recall , so we converted those scores to accuracy by taking their product . </a>
<a name="21">[21]</a> <a href="#21" id=21>The last line of Table 1 shows the win-tie-loss scores of the Decision thermopower divergence methods relative to every other methods . </a></body>
</html>
