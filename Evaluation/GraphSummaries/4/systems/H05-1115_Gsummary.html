<html>
<head><title>H05-1115_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>However , a small probability for jumping to a node that is lexically similar to the given sentence ( rather than the question itself is needed . </a>
<a name="1">[1]</a> <a href="#1" id=1>Recently , graph-based methods have proved useful for number of NLP and IR tasks such as documentre-ranking in ad hoc IR ( Kurland and Lee , 2005 ) and analyzing sentiments in text ( Pang and Lee,2004 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>Since we are interested in a passage retrieval mechanism that sentences relevant to a given question , providing input to the question answering component of our system , the improvement in average TRDR score is very promising . </a>
<a name="3">[3]</a> <a href="#3" id=3>Table 4 shows the configurations of that performed better than the baseline system on the training data , based on mean TRDRscores over the 184 training questions . </a>
<a name="4">[4]</a> <a href="#4" id=4>The value of d , which we will also refer to as the question bias , Â” is a trade-off between two terms in the Vertices : Sentence IndexSentence Index SalienceSalience SentenceSentence </a>
<a name="5">[5]</a> <a href="#5" id=5>With probability ( 1-d ) , a transition is made to the nodes that are lexically similar to the current node . </a>
<a name="6">[6]</a> <a href="#6" id=6>In ( Erkan and Radev , 2004 ) , we introduce method and successfully applied it generic multi-document summarization . </a>
<a name="7">[7]</a> <a href="#7" id=7>To apply LexRank , a similarity graph is produce the sentences in an input document set . </a>
<a name="8">[8]</a> <a href="#8" id=8>Once the similarity graph constructed the sentences are then ranked according to their eigenvector centrality . </a>
<a name="9">[9]</a> <a href="#9" id=9>As discussed in Section 2 , our goal wast develop a topic-sensitive version of LexRank and to use it to improve a baseline system , which previously been used successfully for query-basedsentence retrieval ( Allan et al. , 2003 ) . </a>
<a name="10">[10]</a> <a href="#10" id=10>Below , we describe a topic-sensitive version of LexRank , which is more appropriate for the question-focusedsentence retrieval problem . </a>
<a name="11">[11]</a> <a href="#11" id=11>Our goal is to build a question-focused sentence retrieval mechanism using a topic-sensitive version of the method . </a>
<a name="12">[12]</a> <a href="#12" id=12>The stationary distribution of a Markov chain can be computed by a simple iterative algorithm , called power method. 1 A simpler version of Equation 5 , where A is uniform matrix andB is a normalized binary matrix , is known as PageRank ( Brin and Page , 1998 ; Pageet al. , 1998 ) and used to rank the web pages by theGoogle search engine . </a>
<a name="13">[13]</a> <a href="#13" id=13>As previously mentioned , the original LexRank method performed welling the context of generic summarization . </a>
<a name="14">[14]</a> <a href="#14" id=14>Presently , we introduce topic-sensitive LexRank in creating sentence retrieval system . </a>
<a name="15">[15]</a> <a href="#15" id=15>3.4 Experiments with topic-sensitive LexRank . </a>
<a name="16">[16]</a> <a href="#16" id=16>3.1 The LexRank method . </a>
<a name="17">[17]</a> <a href="#17" id=17>In topic-sensitive LexRank , we first stem all of the sentences in a set of articles and compute word IDFsby the following formula : ides log ( N + 1 0.5 + sfw ) ( 1 ) whereN is the total number of sentences in the cluster , and sfw is the number of sentences that the word appears in . </a>
<a name="18">[18]</a> <a href="#18" id=18>In the training phase the experiment , we evaluated all combination with d in the range of [ 0 , 1 ] ( in increments of 0.10 ) and with a similarity threshold ranging from [ 0 , 0.9 ] ( in increments of 0.05 ) . </a>
<a name="19">[19]</a> <a href="#19" id=19>We presented topic-sensitive LexRank and applied it to the problem of Sentence Retrieval . </a></body>
</html>
