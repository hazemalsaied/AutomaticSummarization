<html>
<head><title>H89-2014_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The implementation of the hidden Markov model is based on that of Rabiner , Levinson and Sondhi ( 1983 ) . </a>
<a name="1">[1]</a> <a href="#1" id=1>The basic network can not model the dependency of the number of the verb on its subject , which precedes it by a prepositional phrase . </a>
<a name="2">[2]</a> <a href="#2" id=2>The `` augmented network '' uniquely models all second-order dependencies of the type determiner -noun - X , and determiner -adjective -X ( X ranges over { cl ... cn } ) . </a>
<a name="3">[3]</a> <a href="#3" id=3>This has the disadvantage that they can not share training data . </a>
<a name="4">[4]</a> <a href="#4" id=4>Only context has been supplied to aid the training procedure , and the latter is responsible for deciding which alternative is more likely , based on the training data . </a>
<a name="5">[5]</a> <a href="#5" id=5>The training corpus was a collection of electronic mail messages concerning the design of the Common-Lisp programming language -a somewhat less than ideal representation of English . </a>
<a name="6">[6]</a> <a href="#6" id=6>This work was sponsored in part by the Defense Advanced Research Projects Agency ( DOD ) , under the Information Science and Technology Office , contract # N0014086-C-8996 . </a>
<a name="7">[7]</a> <a href="#7" id=7>95 To complete the description of the augmented model it is necessary to mention tying of the model states ( Jelinek and Mercer , 1980 ) . </a>
<a name="8">[8]</a> <a href="#8" id=8>To model such dependency across the phrase , the networks shown in Figure 2 can be used . </a>
<a name="9">[9]</a> <a href="#9" id=9>It can be trained reliably on moderate amounts of training text , and through the use of selectively augmented networks it can model high-order dependencies without requiring an excessive number of parameters . </a>
<a name="10">[10]</a> <a href="#10" id=10>In the 21 category model reported in Kupiec ( 1989 ) only 129 equivalence classes were required to cover a 30,000 word dictionary . </a>
<a name="11">[11]</a> <a href="#11" id=11>Many Lisp-specific words were not in the vocabulary , and thus tagged as unknown , however the lisp category was nevertheless created for frequently occurring Lisp symbols in an attempt to reduce bias in the estimation . </a>
<a name="12">[12]</a> <a href="#12" id=12>In a ranked list of words in the corpus the most frequent 100 words account for approximately 50 % of the total tokens in the corpus , and thus data is available to estimate them reliably . </a>
<a name="13">[13]</a> <a href="#13" id=13>The most frequent 100 words of the corpus were assigned individually in the model , thereby enabling them to have different distributions over their categories . </a>
<a name="14">[14]</a> <a href="#14" id=14>The basic model tagged these sentences correctly , except for- `` range '' and `` rises '' which were tagged as noun and plural-noun respectively 1 . </a>
<a name="15">[15]</a> <a href="#15" id=15>SINGULAR ALL STATES IN BASIC NETWORK NOT SHOWN Figure 2 : Augmented Networks for Example of Subject/Verb Agreement For example , consider the word `` up '' in the following sentences : `` He ran up a big bill '' . </a>
<a name="16">[16]</a> <a href="#16" id=16>In fact , the number of equivalence classes is essentially independent of the size of the dictionary , enabling new words to be added without any modification to the model . </a>
<a name="17">[17]</a> <a href="#17" id=17>A 30,000 word dictionary was used , supplemented by inflectional analysis for words not found directly in the dictionary . </a>
<a name="18">[18]</a> <a href="#18" id=18>The performance of a tagging program depends on the choice and number of categories used , and the correct tag assignment for words is not always obvious . </a>
<a name="19">[19]</a> <a href="#19" id=19>A stochastic methods for assigning part-of-speech categories to unrestricted English text has been described . </a></body>
</html>
