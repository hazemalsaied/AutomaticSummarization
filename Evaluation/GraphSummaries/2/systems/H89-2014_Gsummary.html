<html>
<head><title>H89-2014_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The work described here also makes use of a hidden Markov model . </a>
<a name="1">[1]</a> <a href="#1" id=1>In this regard , word equivalence classes were used ( Kupiec , 1989 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>For an n category model this requires n 3 transition probabilities . </a>
<a name="3">[3]</a> <a href="#3" id=3>In the 21 category model reported in Kupiec ( 1989 ) only 129 equivalence classes were required to cover a 30,000 word dictionary . </a>
<a name="4">[4]</a> <a href="#4" id=4>To model such dependency across the phrase , the networks shown in Figure 2 can be used . </a>
<a name="5">[5]</a> <a href="#5" id=5>The implementation of the hidden Markov model is based on that of Rabiner , Levinson and Sondhi ( 1983 ) . </a>
<a name="6">[6]</a> <a href="#6" id=6>The most frequent 100 words of the corpus were assigned individually in the model , thereby enabling them to have different distributions over their categories . </a>
<a name="7">[7]</a> <a href="#7" id=7>In this situation , the Baum-Welch algorithm ( Baum , 1972 ) can be used to estimate the model parameters . </a>
<a name="8">[8]</a> <a href="#8" id=8>Several workers have addressed the problem of tagging text . </a>
<a name="9">[9]</a> <a href="#9" id=9>However they are both members of the equivalence class noun-or-verb , and so are considered to behave identically . </a>
<a name="10">[10]</a> <a href="#10" id=10>Methods have ranged from locally-operating rules ( Greene and Rubin , 1971 ) , to statistical methods ( Church , 1989 ; DeRose , 1988 ; Garside , Leech and Sampson , 1987 ; Jelinek , 1985 ) and back-propagation ( Benello , Mackie and Anderson , 1989 ; Nakamura and Shikano , 1989 ) . </a></body>
</html>
