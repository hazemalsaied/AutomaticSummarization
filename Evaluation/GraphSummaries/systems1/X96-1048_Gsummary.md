Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one . 
The other two tasks , Template Element and Scenario Template , were information extraction tasks that followed on from the MUC evaluations conducted in previous years . 
The latest in a series of natural language processing system evaluations was concluded in October 1995 and was the topic of the Sixth Message Understanding Conference ( MUC6 ) in November . 
Some work on expanding the scope of the NE task has been carried out in the context of a foreign- language NE evaluation conducted in the spring of 1996 . 
The changes occurred only in performance on identifying organizations . 
Whereas the Text Filter row in the score report shows the system 's ability to do text filtering ( document detection ) , the All Objects row and the individual Slot rows show the system 's ability to do information extraction . 
As indicated in table 2 , all systems performed better on identifying person names than on identifying organization or location names , and all but a few systems performed better on location names than on organization names . 
The amount of agreement between the two annotators was found to be 80 % recall and 82 % precision . 
At the top level is the TEMPLATE object , of which there is one instantiated for every document . 
It was also unexpected that one of the systems would match human performance on the task . 
common organization names , first names of people , and location names can be handled by recourse to list lookup , although there are drawbacks : some names may be on more than one list , the lists will not be complete and may not match the names as it is realized in the texts ( e.g . , may not cover the needed abbreviated form of an organization names , may not cover the complete person names ) , etc.. 