Exploring Various Knowledge in Relation Extraction
Exploring Various Knowledge in Relation Extraction
Normally , the above overlap features are too general to be effective alone . 
Moreover , we only apply the simple linear kernel , although other kernels can perform better . 
For example , we want to determine whether a person is at a location , based on the evidence in the context . 
Normally , the above overlap features are too general to be effective alone . 
It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . 
For example , we want to determine whether a person is at a location , based on the evidence in the context . 
Normally , the above overlap features are too general to be effective alone . 
The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . 
This category of features concerns about the information inherent only in the full parse tree . 
Instead of exploring the full parse tree information directly as previous related work , we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance . 
This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.
This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking . 
Helenka et al ( 2003 ) proposed extracting relations by computing kernel functions between parse trees . 
Moreover , we only apply the simple linear kernel , although other kernels can perform better . 
â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . 
â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . 
Moreover , we only apply the simple linear kernel , although other kernels can perform better . 
Moreover , we only apply the simple linear kernel , although other kernels can perform better . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . 
This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . 
This may be due to the fact that most of relations in the ACE corpus are quite local . 
The ACE corpus is gathered from various newspapers , newswire and broadcasts . 
Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . 
Exploring Various Knowledge in Relation Extraction
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . 
Table 5 separates the performance of relation detection from overall performance on the testing set . 
We use SVM as our learning algorithm with the full feature set from Zhou et al . 
We use SVM as our learning algorithm with the full feature set from Zhou et al . 
In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . 
It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . 
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Exploring Various Knowledge in Relation Extraction
This category of features concerns about the information inherent only in the full parse tree . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
explicit relations occur in text with explicit evidence suggesting the relationships . 
Normally , the above overlap features are too general to be effective alone . 
Exploring Various Knowledge in Relation Extraction
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
Normally , the above overlap features are too general to be effective alone . 
Normally , the above overlap features are too general to be effective alone . 
Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . 
Exploring Various Knowledge in Relation Extraction
Normally , the above overlap features are too general to be effective alone . 
Exploring Various Knowledge in Relation Extraction
Moreover , we only apply the simple linear kernel , although other kernels can perform better . 
Basically , SVMs are binary classifiers . 
Basically , SVMs are binary classifiers . 
In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . 
In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . 
Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . 
The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
It shows that 73 % ( 627/864 of errors results from relation detection and 27 % ( 237/864 of errors results from relation characterization , among which 17.8 % ( 154/864 of errors are from misclassification across relation types and 9.6 % ( 83/864 # of relations of errors are from misclassification of relation sub- types inside the same relation types . 
In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . 
In this way , we model relation extraction as a multi-class classification problem with 43 classes , two for each relation subtype ( except the above 6 symmetric subtypes ) and a âNONEâ class for the case where the two mentions are not related . 
In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . 
Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . 
Normally , the above overlap features are too general to be effective alone . 
Normally , the above overlap features are too general to be effective alone . 
In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . 
Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . 
Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . 
For example , we want to determine whether a person is at a location , based on the evidence in the context . 
Based on the structural risk minimization of the statistical learning theory , SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set . 
For example , we want to determine whether a person is at a location , based on the evidence in the context . 
â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . 
It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
Exploring Various Knowledge in Relation Extraction
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
â¢ The usefulness of mention level features is quite limited . 
Normally , the above overlap features are too general to be effective alone . 
Normally , the above overlap features are too general to be effective alone . 
Exploring Various Knowledge in Relation Extraction
Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6 : Distribution of errors 6 Discussion and Conclusion . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . 
Normally , the above overlap features are too general to be effective alone . 
We also demonstrate how semantic information such as WordNet ( Miller 1990 ) and Name List can be used in the feature-based framework . 
In this paper , we separate the features of base phrase chunking from those of full parsing . 
â¢ CPHBNULL when no phrase in between â¢ CPHBFL : the only phrase head when only one phrase in between â¢ CPHBF : first phrase head in between when at least two phrases in between â¢ CPHBL : last phrase head in between when at least two phrase heads in between â¢ CPHBO : other phrase heads in between except first and last phrase heads when at least three phrases in between â¢ CPHBM1F : first phrase head before M1 â¢ CPHBM1L : second phrase head before M1 â¢ CPHAM2F : first phrase head after M2 â¢ CPHAM2F : second phrase head after M2 â¢ CPP : path of phrase labels connecting the two mentions in the chunking â¢ CPPH : path of phrase labels connecting the two mentions in the chunking augmented with head words , if at most two phrases in between 4.6 Dependency Tree . 
Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . 
This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . 
The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . 
â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . 
Exploring Various Knowledge in Relation Extraction
Normally , the above overlap features are too general to be effective alone . 
Normally , the above overlap features are too general to be effective alone . 
While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . 
The RDC task detects and classifies implicit and explicit Relation 1 between Entity identified by the EDT task . 