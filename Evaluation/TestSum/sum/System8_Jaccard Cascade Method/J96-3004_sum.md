com Â§Cambridge, UK Email nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( ,  & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1  & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.
com Â§Cambridge, UK Email nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( ,  & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1  & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.
com Â§Cambridge, UK Email nc201@eng.cam.ac.uk Â© 1996 Association for Computational Linguistics (a) B ) ( ,  & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' (b) P l a u s i b l e S e g m e n t a t i o n I B X I I 1  & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' (c) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [Â§] lxI 1&I ri4 wen2 zhangl yu2zen3 me0 shuol 'Japan' 'essay' 'fish' 'how' 'say' A Chinese sentence in (a) illustrating the lack of word boundaries.
Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
For example, suppose one is building a ITS system for Mandarin Chinese.
For that application, at a minimum, one would want to know the phonological word boundaries.
TIS systems in general need to do more than simply compute the.
It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.
It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.
Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.
The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.
There are thus some very good reasons why segmentation into words is an important task.
For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.
(See Sproat and Shih 1995.)
(See Sproat and Shih 1995.)
(See Sproat and Shih 1995.)
(See Sproat and Shih 1995.)
(See Sproat and Shih 1995.)
(See Sproat and Shih 1995.)
The second concerns the methods used (if any) to exÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.
The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
The most popular approach to dealing with segÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.
Others depend upon various lexical heurisÂ­ tics for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.
Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).
Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).
The simplest approach involves scoring the various analyses by costs based on word frequency, and picking the lowest cost path; variants of this approach have been described in Chang, Chen, and Chen (1991) and Chang and Chen (1993).
More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.
Note that Chang, Chen, and Chen (1991), in addition to word-frequency information, include a constraint-satisfication model, so their method is really a hybrid approach.
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Some of these approaches (e.g., Lin, Chiang, and Su [1993]) attempt to identify unknown words, but do not acÂ­ tually tag the words as belonging to one or another class of expression.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
However, it is almost universally the case that no clear definition of what constitutes a "correct" segmentation is given, so these performance measures are hard to evaluate.
The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.
The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.
Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.
Chinese word segmentation can be viewed as a stochastic transduction problem.
Chinese word segmentation can be viewed as a stochastic transduction problem.
Chinese word segmentation can be viewed as a stochastic transduction problem.
Chinese word segmentation can be viewed as a stochastic transduction problem.
More formally, we start by representing the dictionary D as a Weighted Finite State TransÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.
7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong.
The segmentation chosen is the best path through the WFST, shown in (d).
The segmentation chosen is the best path through the WFST, shown in (d).
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.
In any event, to date, we have not compared different methods for deriving the set of initial frequency estimates.
Both of these analyses are shown in Figure 4; fortunately, the correct analysis is also the one with the lowest cost, so it is this analysis that is chosen.
4.3 Morphological Analysis.
One class comprises words derived by productive morphologiÂ­ cal processes, such as plural noun formation using the suffix ir, menD.
Full Chinese personal names are in one respect simple they are always of the form family+given.
Given names are most commonly two hanzi long, occasionally one hanzi long there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following 1.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
First, the model assumes independence between the first and second hanzi of a double given name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
(See also Wu and Fung [1994].)
A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.
A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.
16 As one reviewer points out, one problem with the unigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words.
However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).
The performance was 80.99% recall and 61.83% precision.
Interestingly, Chang et al. report 80.67% recall and 91.87% precision on an 11,000 word corpus seemingly, our system finds as many names as their system, but with four times as many false hits.
Examples are given in Table 4.
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.
This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.
This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives, and models for personal names and foreign names in transliteration.
(For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)
This is not to say that a set of standards by which a particular segmentation would count as correct and another incorrect could not be devised; indeed, such standards have been proposed and include the published PRCNSC (1994) and ROCLING (1993), as well as the unpublished Linguistic Data Consortium standards (ca.
The major problem for our segÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).
For example, as Gan (1994) has noted, one can construct examples where the segmenÂ­ tation is locally ambiguous but can be determined on the basis of sentential or even discourse context.
Two sets of examples from Gan are given in (1) and (2) ( Gan's Appendix B, exx.
The model described here thus demonstrates great potential for use in widespread applications.