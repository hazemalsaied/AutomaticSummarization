Exploring Various Knowledge in Relation Extraction
This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.
The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.
The RDC task detects and classifies implicit and explicit relations1 between entities identified by the EDT task.
For example, we want to determine whether a person is at a location, based on the evidence in the context.
Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.
Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.
Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.
It achieves 52.8 F- measure on the 24 ACE relation subtypes.
It achieves 52.8 F- measure on the 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.
The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output.
The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community, and there are good implementations of the algorithm available.
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).
In this paper, we use the binary-class SVMLight2 deleveloped by Joachims (1998).
The semantic relation is determined between two mentions.
For each pair of mentions3, we compute various lexical, syntactic and semantic features.
This has an effect of choosing the base phrase contained in the extent annotation.
This has an effect of choosing the base phrase contained in the extent annotation.
This has an effect of choosing the base phrase contained in the extent annotation.
This has an effect of choosing the base phrase contained in the extent annotation.
This has an effect of choosing the base phrase contained in the extent annotation.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
This feature considers the entity level of both the mentions, which can be NAME, NOMIAL and PRONOUN â¢ ML12 combination of mention levels 4.4 Overlap.
Normally, the above overlap features are too general to be effective alone.
In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.
The testing set is held out only for final evaluation.
âGrandParentâ, âSpouseâ and âSiblingâ are automatically set with the same classes without change.
Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1 Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.
Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1 Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.
It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.
â¢ Chunking features are very useful.
â¢ To our surprise, incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively.
However, full parsing is always prone to long distance errors although the Collinsâ parser used in our system represents the state-of-the-art in full parsing.
â¢ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype âROLE.Citizen-Ofâ from âROLE.
It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.
It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.
It shows that our system achieves the performance of 84.8%/66.7%/74.7 in precision/recall/F- measure on relation detection.
It also shows that feature-based methods dramatically outperform kernel methods.
It also shows that feature-based methods dramatically outperform kernel methods.
It also shows that feature-based methods dramatically outperform kernel methods.
It also shows that feature-based methods dramatically outperform kernel methods.
It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.
This suggests that relation detection is critical for relation extraction.
In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.
Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure, yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted.
The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.