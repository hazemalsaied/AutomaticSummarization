An initial step of any text analysis task is the tokenization of the input into words.
 All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
 All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
 3 Throughout this paper we shall give Chinese examples in traditional orthography, followed.
 Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
 For example, suppose one is building a ITS system for Mandarin Chinese.
 In various dialects of Mandarin certain phonetic rules apply at the word.
 It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.
 It has been shown for English (Wang and Hirschberg 1992; Hirschberg 1993; Sproat 1994, inter alia) that grammatical part of speech provides useful information for these tasks.
 There are thus some very good reasons why segmentation into words is an important task.
 Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.'
 Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.'
 Personal names such as 00, 3R; zhoulenl-lai2 'Zhou Enlai.'
 The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
 The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
 This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.
 There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
 There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
 Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
 Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
 Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
 (See Sproat and Shih 1995.)
 (See Sproat and Shih 1995.)
 (See Sproat and Shih 1995.)
 (See Sproat and Shih 1995.)
 However, the characterization given in the main body of the text is correct sufficiently often to be useful.
 Papers that use this method or minor variants thereof include Liang (1986), Li et al.
 (1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
 (1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
 (1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
 Others depend upon various lexical heuris tics for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.
 Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).
 Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).
 Approaches differ in the algorithms used for scoring and selecting the best path, as well as in the amount of contextual information used in the scoring process.
 More complex approaches such as the relaxation technique have been applied to this problem Fan and Tsai (1988}.
 Several papers report the use of part-of-speech information to rank segmentations (Lin, Chiang, and Su 1993; Peng and Chang 1993; Chang and Chen 1993); typically, the probability of a segmentation is multiplied by the probability of the tagging(s) for that segmentation to yield an estimate of the total probability for the analysis.
 Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
 Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
 Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
 Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.
 For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.
 Chinese word segmentation can be viewed as a stochastic transduction problem.
 More formally, we start by representing the dictionary D as a Weighted Finite State Trans ducer (WFST) (Pereira, Riley, and Sproat 1994).
 As we have seen, the lexicon of basic words and stems is represented as a WFST; most arcs in this WFST represent mappings between hanzi and pronunciations, and are costless.
 The method just described segments dictionary words, but as noted in Section 1, there are several classes of words that should be handled that are not found in a standard dictionary.
 One class comprises words derived by productive morphologi cal processes, such as plural noun formation using the suffix ir, menD.
 !!\ yu2 e_nc [!!zen3 l!f moO t_adv il!shuot ,_vb i i i 1  10.03 13...
 !!\ yu2 e_nc [!!zen3 l!f moO t_adv il!shuot ,_vb i i i 1  10.03 13...
 Figure 5 shows how this model is implemented as part of the dictionary WFST.
 Figure 5 shows how this model is implemented as part of the dictionary WFST.
 The transition from f, to a final state transduces c to the grammatical tag PL with cost cost(unseen(f,)) cost(iJJ1l.ir,) == cost(iJJ1l.)
 10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.
 0 Figure 5 An example of affixation the plural affix.
 This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.
 4.5 Transliterations of Foreign Words.
 4.5 Transliterations of Foreign Words.
 4.5 Transliterations of Foreign Words.
 Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
 As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).
 As with personal names, we also derive an estimate from text of the probability of finding a transliterated name of any kind (PTN).
 In this section we present a partial evaluation of the current system, in three parts.
 Various segmentation approaches were then compared with human performance 1.
 A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 Two measures that can be used to compare judgments are 1.
 For each pair of judges, consider one judge as the standard,.
 For each pair of judges, consider one judge as the standard,.
 This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.
 In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 Wu and Fung introduce an evaluation method they call nk-blind.
 In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.
 Fortunately, we were able to obtain a copy of the full set of sentences from Chang et al. on which Wang, Li, and Chang tested their system, along with the output of their system.18 In what follows we will discuss all cases from this set where our performance on names differs from that of Wang, Li, and Chang.
 This is a rather important source of errors in name identifi cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.
 Note that Wang, Li, and Chang's.
 set was based on an earlier version of the Chang et a!.
 (For some recent corpus-based work on Chinese abbreviations, see Huang, Ahrens, and Chen [1993].)
 An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).
 Consider first the examples in (2).
 Consider first the examples in (2).