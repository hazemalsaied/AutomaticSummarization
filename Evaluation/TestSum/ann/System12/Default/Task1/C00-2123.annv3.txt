Citance Number: 1 |  Reference Article:  C00-2123.xml  |  Citing Article:  C02-1050.xml  |  Citation Marker Offset:  ['41']  |  Citation Marker:  2000  |  Citation Offset:  ['41']  |  Citation Text: <S sid ="39" ssid = "19">Under this constraint, many researchers had contributed algorithms and associated pruning strategies, such as Berger et al.</S><S sid ="40" ssid = "20">(1996), Och et al.</S><S sid ="41" ssid = "21">(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.</S>  | Reference Offset: ['199','130','86',] | Reference Text: <S sid ="199" ssid = "8">Table 6: Example Translations for the Verbmobil task.</S><S sid ="130" ssid = "93">The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.</S><S sid ="86" ssid = "49">No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !</S> | Discourse Facet: Method_Citation | Annotator: xukey, WHU |
Citance Number: 2 |  Reference Article:  C00-2123.xml  |  Citing Article:  C02-1050.xml  |  Citation Marker Offset:  ['8']  |  Citation Marker:  Tillman and Ney, 2000  |  Citation Offset:  ['8']  |  Citation Text:  <S sid ="8" ssid = "8">There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</S>  | Reference Offset: ['12','10','86',] | Reference Text: <S sid ="12" ssid = "12">A simple extension will be used to handle this problem.</S><S sid ="10" ssid = "10">These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.</S><S sid ="86" ssid = "49">No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !</S> | Discourse Facet: Result_Citation | Annotator: xukey, WHU |
Citance Number: 3 |  Reference Article:  C00-2123.xml  |  Citing Article:  C02-1050.xml  |  Citation Marker Offset:  ['43']  |  Citation Marker:  2000  |  Citation Offset:  ['43']  |  Citation Text:  <S sid ="43" ssid = "1">The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.</S>  | Reference Offset: ['107','90','109',] | Reference Text: <S sid ="107" ssid = "70">(S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account.</S><S sid ="90" ssid = "53">(f1; ;mg n fl1; l2; l3g ;m) German to English the monotonicity constraint is violated mainly with respect to the German verbgroup.</S><S sid ="109" ssid = "72">There are 13 types of extensions needed to describe the verbgroup reordering.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 4 |  Reference Article:  C00-2123.xml  |  Citing Article:  C02-1050.xml  |  Citation Marker Offset:  ['80']  |  Citation Marker:  2000  |  Citation Offset:  ['80']  |  Citation Text:  <S sid ="80" ssid = "38">The computational complexity for the left-to-right and right-to-left is the same, O( | Reference Offset: ['42','176','91',] | Reference Text: <S sid ="42" ssid = "5">The resulting algorithm has a complexity of O(n!).</S><S sid ="176" ssid = "36">The computing time is low, since no reordering is carried out.</S><S sid ="91" ssid = "54">In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.</S> | Discourse Facet: Result_Citation | Annotator: xukey, WHU |
Citance Number: 5 |  Reference Article:  C00-2123.xml  |  Citing Article:  C04-1091.xml  |  Citation Marker Offset:  ['22']  |  Citation Marker:  Tillman and Ney, 2000  |  Citation Offset:  ['22']  |  Citation Text:  <S sid ="22" ssid = "22">Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) â‰ˆ O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).</S>  | Reference Offset: ['130','103','132',] | Reference Text: <S sid ="130" ssid = "93">The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.</S><S sid ="103" ssid = "66">2.</S><S sid ="132" ssid = "95">Otherwise for the predecessor search hypothesis, we would have chosen a position that would not have been among the first n uncovered positions.</S> | Discourse Facet: Result_Citation | Annotator: xukey, WHU |
Citance Number: 6 |  Reference Article:  C00-2123.xml  |  Citing Article:  E06-1004.xml  |  Citation Marker Offset:  ['22']  |  Citation Marker:  Tillman, 2000  |  Citation Offset:  ['22']  |  Citation Text:  <S sid ="21" ssid = "21">Ã¢â‚¬Â¢ Conditional Probability Given the model parameters and a sentence pair (f , e), compute P (f  | Reference Offset: ['187','25','29',] | Reference Text: <S sid ="187" ssid = "47">Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.</S><S sid ="25" ssid = "9">To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</S><S sid ="29" ssid = "13">The details are given in (Och and Ney, 2000).</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 7 |  Reference Article:  C00-2123.xml  |  Citing Article:  H01-1062.xml  |  Citation Marker Offset:  ['113']  |  Citation Marker:  20  |  Citation Offset:  ['113']  |  Citation Text:  <S sid ="113" ssid = "16">To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: Ã¢â‚¬Â¢ single-word based approach [20];</S>  | Reference Offset: ['117','111','126',] | Reference Text: <S sid ="117" ssid = "80">f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ ô€€€L; ; Jg.</S><S sid ="111" ssid = "74">For each extension a new position is added to the coverage set.</S><S sid ="126" ssid = "89">Here, we process only full-form words within the translation procedure.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 8 |  Reference Article:  C00-2123.xml  |  Citing Article:  J03-1005.xml  |  Citation Marker Offset:  ['117']  |  Citation Marker:  2000  |  Citation Offset:  ['115','117']  |  Citation Text:  <S sid ="115" ssid = "73">This article will present a DP-based beam search decoder for the IBM4 translation model.</S><S sid ="117" ssid = "75">A preliminary version of the work presented here was published in Tillmann and Ney (2000).</S>  | Reference Offset: ['120','41','124',] | Reference Text: <S sid ="120" ssid = "83">The proof is given in (Tillmann, 2000).</S><S sid ="41" ssid = "4">A straightforward way to find the shortest tour is by trying all possible permutations of the n cities.</S><S sid ="124" ssid = "87">Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 9 |  Reference Article:  C00-2123.xml  |  Citing Article:  J04-2003.xml  |  Citation Marker Offset:  ['35']  |  Citation Marker:  Tillmann and Ney 2000  |  Citation Offset:  ['35']  |  Citation Text:  <S sid ="35" ssid = "35">Many existing systems for statistical machine translation 1 1 (GarcÂ´Ä±a-Varea and Casacuberta 2001; Germann et al. 2001; NieÃŸen et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.</S>  | Reference Offset: ['20','32','42',] | Reference Text: <S sid ="20" ssid = "4">For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.</S><S sid ="32" ssid = "16">The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.</S><S sid ="42" ssid = "5">The resulting algorithm has a complexity of O(n!).</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 10 |  Reference Article:  C00-2123.xml  |  Citing Article:  J04-2003.xml  |  Citation Marker Offset:  ['235']  |  Citation Marker:  Tillmann and Ney 2000  |  Citation Offset:  ['235']  |  Citation Text:  <S sid ="235" ssid = "35">Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).</S>  | Reference Offset: ['141','128','75',] | Reference Text: <S sid ="141" ssid = "1">4.1 The Task and the Corpus.</S><S sid ="128" ssid = "91">During the search process, a partial hypothesis is extended by choosing a source sentence position, which has not been aligned with a target sentence position yet.</S><S sid ="75" ssid = "38">am 11.</S> | Discourse Facet: Method_Citation | Annotator: xukey, WHU |
Citance Number: 11 |  Reference Article:  C00-2123.xml  |  Citing Article:  J04-4002.xml  |  Citation Marker Offset:  ['282']  |  Citation Marker:  Tillmann and Ney 2000  |  Citation Offset:  ['282']  |  Citation Text:  <S sid ="282" ssid = "48">We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).</S>  | Reference Offset: ['157','104','107',] | Reference Text: <S sid ="157" ssid = "17">On average, 6 reference translations per automatic translation are available.</S><S sid ="104" ssid = "67">The 13 positions of the source sentence are processed in the order shown.</S><S sid ="107" ssid = "70">(S; C; j); Not only the coverage set C and the positions j; j0, but also the verbgroup states S; S0 are taken into account.</S> | Discourse Facet: Method_Citation | Annotator: xukey, WHU |
Citance Number: 12 |  Reference Article:  C00-2123.xml  |  Citing Article:  N03-1010.xml  |  Citation Marker Offset:  ['16']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['16']  |  Citation Text:  <S sid ="16" ssid = "16">Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).</S>  | Reference Offset: ['199','87','134',] | Reference Text: <S sid ="199" ssid = "8">Table 6: Example Translations for the Verbmobil task.</S><S sid ="87" ssid = "50">(f1; ;mg ; l) 2 (f1; ;mg n fl; l1g ; l0) !</S><S sid ="134" ssid = "97">In general, m; l; l0 6= fl1; l2; l3g and in line umber 3 and 4, l0 must be chosen not to violate the above reordering restriction.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 13 |  Reference Article:  C00-2123.xml  |  Citing Article:  P01-1027.xml  |  Citation Marker Offset:  ['127']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['127']  |  Citation Text:  <S sid ="127" ssid = "34">We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.</S>  | Reference Offset: ['111','96','33',] | Reference Text: <S sid ="111" ssid = "74">For each extension a new position is added to the coverage set.</S><S sid ="96" ssid = "59">The translation of one position in the source sentence may be postponed for up to L = 3 source positions, and the translation of up to two source positions may be anticipated for at most R = 10 source positions.</S><S sid ="33" ssid = "17">We use a solution to this problem similar to the one presented in (Och et al., 1999), where target words are joined during training.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 14 |  Reference Article:  C00-2123.xml  |  Citing Article:  P03-1039.xml  |  Citation Marker Offset:  ['113']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['113']  |  Citation Text:  <S sid ="113" ssid = "22">The decoding algorithm employed for this chunk + weight Ãƒâ€” j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J | Reference Offset: ['153','37','155',] | Reference Text: <S sid ="153" ssid = "13">Table 3: Training and test conditions for the Verbmobil task (*number of words without punctuation marks).</S><S sid ="37" ssid = "21">In the following, we assume that this word joining has been carried out.</S><S sid ="155" ssid = "15">Search CPU time mWER SSER Method [sec] [%] [%] MonS 0:9 42:0 30:5 QmS 10:6 34:4 23:8 IbmS 28:6 38:2 26:2 4.2 Performance Measures.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 15 |  Reference Article:  C00-2123.xml  |  Citing Article:  P03-1039.xml  |  Citation Marker Offset:  ['120']  |  Citation Marker:  Tillman and Ney, 2000  |  Citation Offset:  ['120']  |  Citation Text:  <S sid ="120" ssid = "29">The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).</S>  | Reference Offset: ['173','17','21',] | Reference Text: <S sid ="173" ssid = "33">Here, the pruning threshold t0 = 10:0 is used.</S><S sid ="17" ssid = "1">In this section, we brie y review our translation approach.</S><S sid ="21" ssid = "5">The alignment model uses two kinds of parameters: alignment probabilities p(aj jajô€€€1; I; J), where the probability of alignment aj for position j depends on the previous alignment position ajô€€€1 (Ney et al., 2000) and lexicon probabilities p(fj jeaj ).</S> | Discourse Facet: Result_Citation | Annotator: xukey, WHU |
Citance Number: 16 |  Reference Article:  C00-2123.xml  |  Citing Article:  W01-0505.xml  |  Citation Marker Offset:  ['13']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['13']  |  Citation Text:  <S sid ="13" ssid = "13">They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).</S>  | Reference Offset: ['203','127','77',] | Reference Text: <S sid ="203" ssid = "12">We can do that . Input: Das ist zu knapp , weil ich ab dem dritten in Kaiserslautern bin . Genaugenommen nur am dritten . Wie ware es denn am ahm Samstag , dem zehnten Februar ? MonS: That is too tight , because I from the third in Kaiserslautern . In fact only on the third . How about ahm Saturday , the tenth of February ? QmS: That is too tight , because I am from the third in Kaiserslautern . In fact only on the third . Ahm how about Saturday , February the tenth ? IbmS: That is too tight , from the third because I will be in Kaiserslautern . In fact only on the third . Ahm how about Saturday , February the tenth ? Input: Wenn Sie dann noch den siebzehnten konnten , ware das toll , ja . MonS: If you then also the seventeenth could , would be the great , yes . QmS: If you could then also the seventeenth , that would be great , yes . IbmS: Then if you could even take seventeenth , that would be great , yes . Input: Ja , das kommt mir sehr gelegen . Machen wir es dann am besten so . MonS: Yes , that suits me perfectly . Do we should best like that . QmS: Yes , that suits me fine . We do it like that then best . IbmS: Yes , that suits me fine . We should best do it like that .</S><S sid ="127" ssid = "90">the number of permutations carried out for the word reordering is given.</S><S sid ="77" ssid = "40">Mai.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 17 |  Reference Article:  C00-2123.xml  |  Citing Article:  W01-1404.xml  |  Citation Marker Offset:  ['5']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['5']  |  Citation Text:  <S sid ="5" ssid = "5">Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).</S>  | Reference Offset: ['173','137','17',] | Reference Text: <S sid ="173" ssid = "33">Here, the pruning threshold t0 = 10:0 is used.</S><S sid ="137" ssid = "100">In this case, we have no finite-state restrictions for the search space.</S><S sid ="17" ssid = "1">In this section, we brie y review our translation approach.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 18 |  Reference Article:  C00-2123.xml  |  Citing Article:  W01-1407.xml  |  Citation Marker Offset:  ['110']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['110']  |  Citation Text:  <S sid ="110" ssid = "26">We used a translation system called Ã¢â‚¬Å“single- word based approachÃ¢â‚¬ described in (Tillmann and Ney, 2000) and compared to other approaches in (Ney et al., 2000).</S>  | Reference Offset: ['113','118','124',] | Reference Text: <S sid ="113" ssid = "76">Here, $ is the sentence boundary symbol, which is thought to be at position 0 in the target sentence.</S><S sid ="118" ssid = "81">The final score is obtained from: max e;e0 j2fJô€€€L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.</S><S sid ="124" ssid = "87">Source sentence words are aligned with hypothesized target sentence words, where the choice of a new source word, which has not been aligned with a target word yet, is restricted1.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 19 |  Reference Article:  C00-2123.xml  |  Citing Article:  W01-1408.xml  |  Citation Marker Offset:  ['47']  |  Citation Marker:  Tillmann, 2001; Tillmann and Ney, 2000  |  Citation Offset:  ['47']  |  Citation Text:  <S sid ="47" ssid = "23">Search algorithms We evaluate the following two search algorithms: Ã¢â‚¬Â¢ beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.</S>  | Reference Offset: ['186','79','99',] | Reference Text: <S sid ="186" ssid = "46">Table 5: Effect of the beam threshold on the number of search errors (147 sentences).</S><S sid ="79" ssid = "42">Figure 2: Order in which source positions are visited for the example given in Fig.1.</S><S sid ="99" ssid = "62">Final (F): The rest of the sentence is processed monotonically taking account of the already covered positions.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
Citance Number: 20 |  Reference Article:  C00-2123.xml  |  Citing Article:  W02-1020.xml  |  Citation Marker Offset:  ['62']  |  Citation Marker:  Tillmann and Ney, 2000  |  Citation Offset:  ['61','62']  |  Citation Text:  <S sid ="61" ssid = "19">It is faster because the search problem for noisy- channel models is NP-complete (Knight, 1999), and even the fastest dynamic-programming heuristics used in statistical MT (Niessen et al., 1998; Till- mann and Ney, 2000), are polynomial in J â€”for in p(v1, w2</S><S sid ="62" ssid = "20">, wmâˆ’1, um | Reference Offset: ['170','28','24',] | Reference Text: <S sid ="170" ssid = "30">We show translation results for three approaches: the monotone search (MonS), where no word reordering is allowed (Tillmann, 1997), the quasimonotone search (QmS) as presented in this paper and the IBM style (IbmS) search as described in Section 3.2.</S><S sid ="28" ssid = "12">The inverted alignment probability p(bijbiô€€€1; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.</S><S sid ="24" ssid = "8">2.1 Inverted Alignments.</S> | Discourse Facet: Aim_Citation | Annotator: xukey, WHU |
