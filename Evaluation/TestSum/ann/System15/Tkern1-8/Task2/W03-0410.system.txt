We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. Ward linkage essentially minimizes the distances of all cluster points to the centroid, and thus is less sensitive to outliers than some other methods. The feature set was previously shown to work well in a supervised learning setting, using known English verb classes. We find that the unsupervised method we tried cannot be consistently applied to our data. In performing hierarchical clustering, both a vector distance measure and a cluster distance (ÃḃÂÂlinkageÃḃÂÂ) measure are specified. Silhouette values vary from +1 to -1, with +1 indicating that the point is near the centroid of its own cluster, and -1 indicating that the point is very close to another cluster (and therefore likely in the wrong cluster). Essentially, the measure numerically captures what
