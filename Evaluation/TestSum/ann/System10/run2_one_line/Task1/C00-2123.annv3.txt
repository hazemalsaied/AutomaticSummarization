Citance Number: 1 | Reference Article: C00-2123.xml | Citing Article: C02-1050.xml | Citation Marker Offset: ['41'] | Citation Marker: 2000 | Citation Offset: ['41'] | Citation Text: <S sid="41" ssid="21">(2001), Wang and Waibel (1997), Tillmann and Ney (2000) GarciaVarea and Casacuberta (2001) and Germann et al.</S> | Reference Offset: ['29' ] | Reference Text: <S sid="29" ssid="13">The details are given in (Och and Ney, 2000).</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 10 | Reference Article: C00-2123.xml | Citing Article: J04-2003.xml | Citation Marker Offset: ['235'] | Citation Marker: Tillmann and Ney 2000 | Citation Offset: ['235'] | Citation Text: <S sid="235" ssid="35">Some recent publications deal with the automatic detection of multiword phrases (Och and Weber 1998; Tillmann and Ney 2000).</S> | Reference Offset: ['29' ] | Reference Text: <S sid="29" ssid="13">The details are given in (Och and Ney, 2000).</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 11 | Reference Article: C00-2123.xml | Citing Article: J04-4002.xml | Citation Marker Offset: ['282'] | Citation Marker: Tillmann and Ney 2000 | Citation Offset: ['282'] | Citation Text: <S sid="282" ssid="48">We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).</S> | Reference Offset: ['32' , '55' , '91' ] | Reference Text: <S sid="32" ssid="16">The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="91" ssid="54">In German, the verbgroup usually consists of a left and a right verbal brace, whereas in English the words of the verbgroup usually form a sequence of consecutive words.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 12 | Reference Article: C00-2123.xml | Citing Article: N03-1010.xml | Citation Marker Offset: ['16'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['16'] | Citation Text: <S sid="16" ssid="16">Och et al. report word error rates of 68.68% for optimal search (based on a variant of the A* algorithm), and 69.65% for the most restricted version of a decoder that combines dynamic programming with a beam search (Tillmann and Ney, 2000).</S> | Reference Offset: ['6' , '55' , '105' , '120' , '140' ] | Reference Text: <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="105" ssid="68">A position is presented by the word at that position.</S> <S sid="120" ssid="83">The proof is given in (Tillmann, 2000).</S> <S sid="140" ssid="103">The proof is given in (Tillmann, 2000).</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 13 | Reference Article: C00-2123.xml | Citing Article: P01-1027.xml | Citation Marker Offset: ['127'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['127'] | Citation Text: <S sid="127" ssid="34">We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score.</S> | Reference Offset: ['3' , '9' , '35' , '48' , '51' , '84' , '85' ] | Reference Text: <S sid="3" ssid="3">A search restriction especially useful for the translation direction from German to English is presented.</S> <S sid="9" ssid="9">The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</S> <S sid="35" ssid="19">An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.</S> <S sid="48" ssid="11">This algorithm can be applied to statistical machine translation.</S> <S sid="51" ssid="14">The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 14 | Reference Article: C00-2123.xml | Citing Article: P03-1039.xml | Citation Marker Offset: ['113'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['113'] | Citation Text: <S sid="113" ssid="22">The decoding algorithm employed for this chunk + weight ? j f req(EA j , J j ) based statistical translation is based on the beam search algorithm for word alignment statistical in which Ptm(J|E) and Plm (E) are translationmodel and language model probability, respec translation presented in (Tillmann and Ney, 2000), tively1 , f req(EA j , J j ) is the frequency for the which generates outputs in left-to-right order by consuming input in an arbitrary order.</S> | Reference Offset: ['6' , '10' , '11' , '36' , '56' , '55' , '82' , '84' , '85' , '110' , '120' , '125' , '140' ] | Reference Text: <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="10" ssid="10">These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.</S> <S sid="11" ssid="11">The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.</S> <S sid="36" ssid="20">E.g. when 'Zahnarzttermin' is aligned to dentist's, the extended lexicon model might learn that 'Zahnarzttermin' actuallyhas to be aligned to both dentist's and ap pointment.</S> <S sid="56" ssid="19">Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="82" ssid="45">The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="110" ssid="73">The details are given in (Tillmann, 2000).</S> <S sid="120" ssid="83">The proof is given in (Tillmann, 2000).</S> <S sid="125" ssid="88">A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.</S> <S sid="140" ssid="103">The proof is given in (Tillmann, 2000).</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 15 | Reference Article: C00-2123.xml | Citing Article: P03-1039.xml | Citation Marker Offset: ['120'] | Citation Marker: Tillman and Ney, 2000 | Citation Offset: ['120'] | Citation Text: <S sid="120" ssid="29">The generation of possible output chunks is estimated through an inverted lexicon model and sequences of inserted strings (Tillmann and Ney, 2000).</S> | Reference Offset: ['32' , '20' , '28' ] | Reference Text: <S sid="32" ssid="16">The baseline alignment model does not permit that a source word is aligned to two or more target words, e.g. for the translation direction from German toEnglish, the German compound noun 'Zahnarztter min' causes problems, because it must be translated by the two target words dentist's appointment.</S> <S sid="20" ssid="4">For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.</S> <S sid="28" ssid="12">The inverted alignment probability p(bijbi  1; I; J) and the lexicon probability p(fbi jei) are obtained by relative frequency estimates from the Viterbi alignment path after the final training iteration.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 16 | Reference Article: C00-2123.xml | Citing Article: W01-0505.xml | Citation Marker Offset: ['13'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['13'] | Citation Text: <S sid="13" ssid="13">They were usually incorporated in the EM algorithm (Brown et al., 1993; Kupiec, 1993; Tillmann and Ney, 2000; Och et al., 2000).</S> | Reference Offset: ['29' ] | Reference Text: <S sid="29" ssid="13">The details are given in (Och and Ney, 2000).</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 17 | Reference Article: C00-2123.xml | Citing Article: W01-1404.xml | Citation Marker Offset: ['5'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['5'] | Citation Text: <S sid="5" ssid="5">Some of these studies have concentrated on finite-state or extended finite-state machinery, such as (Vilar and others, 1999), others have chosen models closer to context-free grammars and context-free transduction, such as (Alshawi et al., 2000; Watanabe et al., 2000; Yamamoto and Matsumoto, 2000), and yet other studies cannot be comfortably assigned to either of these two frameworks, such as (Brown and others, 1990) and (Tillmann and Ney, 2000).</S> | Reference Offset: ['20' ] | Reference Text: <S sid="20" ssid="4">For the translation model Pr(fJ 1 jeI 1), we go on the assumption that each source word is aligned to exactly one target word.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 19 | Reference Article: C00-2123.xml | Citing Article: W01-1408.xml | Citation Marker Offset: ['47'] | Citation Marker: Tillmann, 2001; Tillmann and Ney, 2000 | Citation Offset: ['47'] | Citation Text: <S sid="47" ssid="23">Search algorithms We evaluate the following two search algorithms: ? beam search algorithm (BS): (Tillmann, 2001; Tillmann and Ney, 2000) In this algorithm the search space is explored in a breadth-first manner.</S> | Reference Offset: ['6' , '84' ] | Reference Text: <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 2 | Reference Article: C00-2123.xml | Citing Article: C02-1050.xml | Citation Marker Offset: ['8'] | Citation Marker: Tillman and Ney, 2000 | Citation Offset: ['8'] | Citation Text: <S sid="8" ssid="8">There exists stack decoding algorithm (Berger et al., 1996), A* search algorithm (Och et al., 2001; Wang and Waibel, 1997) and dynamic-programming algorithms (Tillmann and Ney, 2000; GarciaVarea and Casacuberta, 2001), and all translate a given input string word-by-word and render the translation in left-to-right, with pruning technologies assuming almost linearly aligned translation source and target texts.</S> | Reference Offset: ['6' , '55' , '79' , '82' , '84' , '85' , '97' , '130' ] | Reference Text: <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="79" ssid="42">Figure 2: Order in which source positions are visited for the example given in Fig.1.</S> <S sid="82" ssid="45">The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="97" ssid="60">To formalize the approach, we introduce four verbgroup states S: Initial (I): A contiguous, initial block of source positions is covered.</S> <S sid="130" ssid="93">The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 20 | Reference Article: C00-2123.xml | Citing Article: W02-1020.xml | Citation Marker Offset: ['62'] | Citation Marker: Tillmann and Ney, 2000 | Citation Offset: ['62'] | Citation Text: <S sid="62" ssid="20">, wm?1, um|h, s) = stance O(mJ 4V 3) in (Tillmann and Ney, 2000).</S> | Reference Offset: ['6' , '55' , '106' , '187' , '6' , '55' , '106' , '187' ] | Reference Text: <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="106" ssid="69">Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !</S> <S sid="187" ssid="47">Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.</S> <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="106" ssid="69">Using these states, we define partial hypothesis extensions, which are of the following type: (S0;C n fjg; j0) !</S> <S sid="187" ssid="47">Search t0 CPU time #search mWER Method [sec] error [%] QmS 0.0 0.07 108 42:6 1.0 0.13 85 37:8 2.5 0.35 44 36:6 5.0 1.92 4 34:6 10.0 10.6 0 34:5 IbmS 0.0 0.14 108 43:4 1.0 0.3 84 39:5 2.5 0.8 45 39:1 5.0 4.99 7 38:3 10.0 28.52 0 38:2 Table 6 shows example translations obtained by the three different approaches.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 3 | Reference Article: C00-2123.xml | Citing Article: C02-1050.xml | Citation Marker Offset: ['43'] | Citation Marker: 2000 | Citation Offset: ['43'] | Citation Text: <S sid="43" ssid="1">The decoding methods presented in this paper explore the partial candidate translation hypotheses greedily, as presented in Tillmann and Ney (2000) and Och et al.</S> | Reference Offset: ['84' , '85' , '157' ] | Reference Text: <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="157" ssid="17">On average, 6 reference translations per automatic translation are available.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 5 | Reference Article: C00-2123.xml | Citing Article: C04-1091.xml | Citation Marker Offset: ['22'] | Citation Marker: Tillman and Ney, 2000 | Citation Offset: ['22'] | Citation Text: <S sid="22" ssid="22">Tillman and Ney showed how to improve the complexity of the Held-Karp algorithm for restricted word reordering and gave a O (l3m4) ? O (m7) algo rithm for French-English translation (Tillman and Ney, 2000).</S> | Reference Offset: ['3' , '48' , '55' , '82' , '84' , '86' , '85' , '119' , '117' , '125' , '130' ] | Reference Text: <S sid="3" ssid="3">A search restriction especially useful for the translation direction from German to English is presented.</S> <S sid="48" ssid="11">This algorithm can be applied to statistical machine translation.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="82" ssid="45">The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="86" ssid="49">No: Predecessor coverage set Successor coverage set 1 (f1; ;mg n flg ; l0) !</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="119" ssid="82">The complexity of the quasimonotone search is O(E3 J (R2+LR)).</S> <S sid="117" ssid="80">f1; ; Jg denotes a coverage set including all positions from the starting position 1 to position J and j 2 fJ   L; ; Jg.</S> <S sid="125" ssid="88">A procedural definition to restrict1In the approach described in (Berger et al., 1996), a mor phological analysis is carried out and word morphemes rather than full-form words are used during the search.</S> <S sid="130" ssid="93">The restriction can be expressed in terms of the number of uncovered source sentence positions to the left of the rightmost position m in the coverage set.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 7 | Reference Article: C00-2123.xml | Citing Article: H01-1062.xml | Citation Marker Offset: ['113'] | Citation Marker: 20 | Citation Offset: ['113'] | Citation Text: <S sid="113" ssid="16">To summarize these experimental tests, we briefly report experimental offline results for the following translation approaches: ? single-word based approach [20]; ? alignment template approach [15]; ? cascaded transducer approach [23]:unlike the other two-approaches, this approach re quires a semiautomatic training procedure, in which the structure of the finite state transducers is designed manually.</S> | Reference Offset: ['35' , '56' , '84' , '85' ] | Reference Text: <S sid="35" ssid="19">An extended lexicon model is defined, and its likelihood is compared to a baseline lexicon model, which takes only single-word dependencies into account.</S> <S sid="56" ssid="19">Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 8 | Reference Article: C00-2123.xml | Citing Article: J03-1005.xml | Citation Marker Offset: ['117'] | Citation Marker: 2000 | Citation Offset: ['117'] | Citation Text: <S sid="117" ssid="75">A preliminary version of the work presented here was published in Tillmann and Ney (2000).</S> | Reference Offset: ['3' , '6' , '84' , '85' , '3' , '6' , '84' , '85' ] | Reference Text: <S sid="3" ssid="3">A search restriction especially useful for the translation direction from German to English is presented.</S> <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="3" ssid="3">A search restriction especially useful for the translation direction from German to English is presented.</S> <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
Citance Number: 9 | Reference Article: C00-2123.xml | Citing Article: J04-2003.xml | Citation Marker Offset: ['35'] | Citation Marker: Tillmann and Ney 2000 | Citation Offset: ['35'] | Citation Text: <S sid="35" ssid="35">Many existing systems for statistical machine translation 1 1 (Garc??a-Varea and Casacuberta 2001; Germann et al. 2001; Nie?en et al. 1998; Och, Tillmann, and Ney 1999) implement models presented by Brown, Della Pietra, Della Pietra, and Mercer (1993): The correspondence between the words in the source and the target strings is described by alignments that assign target word positions to each source word position.</S> | Reference Offset: ['3' , '6' , '10' , '11' , '48' , '51' , '56' , '55' , '79' , '82' , '84' , '85' , '105' , '118' ] | Reference Text: <S sid="3" ssid="3">A search restriction especially useful for the translation direction from German to English is presented.</S> <S sid="6" ssid="6">We are given a source string fJ 1 = f1:::fj :::fJ of length J, which is to be translated into a target string eI 1 = e1:::ei:::eI of length I. Among all possible target strings, we will choose the string with the highest probability: ^eI 1 = arg max eI 1 fPr(eI 1jfJ 1 )g = arg max eI 1 fPr(eI 1) Pr(fJ 1 jeI 1)g : (1) The argmax operation denotes the search problem, i.e. the generation of the output sentence in the target language.</S> <S sid="10" ssid="10">These alignment models are similar to the concept of hidden Markov models (HMM) in speech recognition.</S> <S sid="11" ssid="11">The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words, e.g. when translating German compound nouns.</S> <S sid="48" ssid="11">This algorithm can be applied to statistical machine translation.</S> <S sid="51" ssid="14">The cities of the traveling salesman problem correspond to source Table 1: DP algorithm for statistical machine translation.</S> <S sid="56" ssid="19">Each distance in the traveling salesman problem now corresponds to the negative logarithm of the product of the translation, alignment and language model probabilities.</S> <S sid="55" ssid="18">e0; e are the last two target words, C is a coverage set for the already covered source positions and j is the last position visited.</S> <S sid="79" ssid="42">Figure 2: Order in which source positions are visited for the example given in Fig.1.</S> <S sid="82" ssid="45">The complexity of the algorithm is O(E3 J2 2J), where E is the size of the target language vocabulary.</S> <S sid="84" ssid="47">Restrictions: Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence.</S> <S sid="85" ssid="48">On the other hand, only very restricted reorderings are necessary, e.g. for the translation direction from Table 2: Coverage set hypothesis extensions for the IBM reordering.</S> <S sid="105" ssid="68">A position is presented by the word at that position.</S> <S sid="118" ssid="81">The final score is obtained from: max e;e0 j2fJ  L;;Jg p($je; e0) Qe0 (e; I; f1; ; Jg; j); where p($je; e0) denotes the trigram language model, which predicts the sentence boundary $ at the end of the target sentence.</S>  | Discourse Facet: Method_Citation | Annotator: Swastika Bhattacharya |
