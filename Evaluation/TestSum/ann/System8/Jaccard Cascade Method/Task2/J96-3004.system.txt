A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.
For languages like English one can assume, to a first approximation, that word boundaries are given by whitespace or punctuation.
In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.
In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.
The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and--since the primary intended application of this model is to text-to-speech synthesis--provides pronunciations for these words.
We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of a pool of human segmenters, and the system is shown to perform quite well.
Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).
An initial step of any textÂ­ analysis task is the tokenization of the input into words.
For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.
Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated
