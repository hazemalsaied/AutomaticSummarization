Citance Number: 1 | Reference Article:  C04-1089.xml | Citing Article:  C10-1070.xml | Citation Marker Offset:  ['79'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['78','79'] | Citation Text:  <S sid ="78" ssid = "38">To our knowledge, this association measure has not been used yet in translation spotting.</S><S sid ="79" ssid = "39">It is computed as: (O11 + 1 )(O22 + 1 ) scribed in (Shao and Ng, 2004).</S> | Reference Offset:  ['55', '61', '57', '97', '62'] | Reference Text:  <S sid ="55" ssid = "1">our task is to compute P(C (c)|C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c)|C (e)) , estimated as: most relevant to the query is returned from a document collection.</S><S sid ="61" ssid = "7">We use backoff and linear interpolation for probability estimation: P(tc|Tc (C (e))) = α ⋅ Pml (tc|Tc (C (e))) + (1 −α ) ⋅ Pml (tc ) that the ith pinyin syllable maps to in the particular alignment a. Since most Chinese characters have only one pronunciation and hence one pinyin form, we assume that Chinese character-to-pinyin mapping is one-to-one to simplify the problem.</S><S sid ="57" ssid = "3">q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.</S><S sid ="97" ssid = "25">For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c)|C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e|c) (as described in from a half-month period.</S><S sid ="62" ssid = "8">We use the Pml (tc|Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 2 | Reference Article:  C04-1089.xml | Citing Article:  C10-2164.xml | Citation Marker Offset:  ['15'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['15'] | Citation Text:  <S sid ="15" ssid = "1">At present, the methods for OOV term translation have changed from the basic pattern based on bilingual dictionary, transliteration or parallel corpus to the intermediate pattern based on comparable corpus (Lee et al., 2006; Shao and Ng, 2004; Virga and Khudanpur, 2003), and 1435 Coling 2010: Poster Volume, pages 1435â€“1443, Beijing, August 2010 then become a new pattern based on Web mining (Fang et al., 2006; Sproat et al., 2006).</S> | Reference Offset:  ['2', '3', '4', '6', '7'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="6" ssid = "6">New words such as person names, organization names, technical terms, etc. appear frequently.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 3 | Reference Article:  C04-1089.xml | Citing Article:  D10-1042.xml | Citation Marker Offset:  ['54'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['53','54'] | Citation Text:  <S sid ="53" ssid = "8">Using parallel corpora (Kupiec, 1993; Feng et al., 2004), e.g., bilingual Wikipedia entries on the same person, renders high accuracy but suffers from high scarcity.</S><S sid ="54" ssid = "9">To alleviate such scarcity, (Fung and Yee, 1998; Shao and Ng, 2004) explore a more vast resource of comparable corpora, which share no parallel document- or sentence-alignments as in parallel corpora but describe similar contents in two languages, e.g., news articles on the same event.</S> | Reference Offset:  ['8', '66', '55', '59', '97'] | Reference Text:  <S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S><S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S><S sid ="55" ssid = "1">our task is to compute P(C (c)|C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c)|C (e)) , estimated as: most relevant to the query is returned from a document collection.</S><S sid ="59" ssid = "5">If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.</S><S sid ="97" ssid = "25">For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c)|C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e|c) (as described in from a half-month period.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 4 | Reference Article:  C04-1089.xml | Citing Article:  D12-1003.xml | Citation Marker Offset:  ['38'] | Citation Marker:  2004 | Citation Offset:  ['38'] | Citation Text:  <S sid ="38" ssid = "7">Various correlation measures have been used: log-likelihood ratio (Rapp, 1999; Chiao and Zweigenbaum, 2002), tfidf (Fung and Yee, 1998), pointwise mutual information (PMI) (Andrade et al., 2010), context heterogeneity (Fung, 1995), etc. Shao and Ng (2004) represented contexts using language models.</S> | Reference Offset:  ['2', '4', '7', '8', '9'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S><S sid ="9" ssid = "9">But parallel corpora are scarce resources, especially for uncommon lan guage pairs.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 5 | Reference Article:  C04-1089.xml | Citing Article:  D12-1003.xml | Citation Marker Offset:  ['68'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['68'] | Citation Text:  <S sid ="68" ssid = "37">Various clues have been considered when computing the similarities: concept class information obtained from a multilingual thesaurus (DeÂ´jean et al., 2002), co-occurrence models generated from aligned documents (Prochasson and Fung, 2011), and transliteration information (Shao and Ng, 2004).</S> | Reference Offset:  ['11', '16', '50', '59', '66'] | Reference Text:  <S sid ="11" ssid = "11">For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.</S><S sid ="16" ssid = "16">On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.</S><S sid ="50" ssid = "27">In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose, the first fraction n!</S><S sid ="59" ssid = "5">If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.</S><S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 6 | Reference Article:  C04-1089.xml | Citing Article:  N09-1048.xml | Citation Marker Offset:  ['15'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['15'] | Citation Text:  <S sid ="15" ssid = "15">The other is multilingual parallel and comparable corpora (e.g., Wikipedia1), wherein features such as co- occurrence frequency and context are popularly employed (Cheng et al., 2004; Shao and Ng, 2004; Cao et al., 2007; Lin et al., 2008).</S> | Reference Offset:  ['16', '50', '59', '66', '68'] | Reference Text:  <S sid ="16" ssid = "16">On the other hand, the work of (Cao and Li, 2002; Fung and Yee, 1998; Koehn and Knight, 2002; Rapp, 1995; Rapp, 1999) used the context of w to locate its translation in a second language.</S><S sid ="50" ssid = "27">In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose, the first fraction n!</S><S sid ="59" ssid = "5">If an English word is ambiguous and has K translated Chinese words listed in the bilingual dictionary, then each of the K trans over all the alignments that this pinyin form of c can map to an English word e. For each possible alignment, we calculate the probability by taking lated Chinese words is counted as occurring 1/K times in Tc (C (e)) for the purpose of probability the product of each mapping.</S><S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S><S sid ="68" ssid = "1">must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e|c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 7 | Reference Article:  C04-1089.xml | Citing Article:  N09-1048.xml | Citation Marker Offset:  ['50'] | Citation Marker:  2004 | Citation Offset:  ['50'] | Citation Text:  <S sid ="50" ssid = "10">Shao and Ng (2004) presented a method to mine new translations from Chinese and English news documents of the same period from different news agencies, combining both transliteration and context information.</S> | Reference Offset:  ['3', '4', '8', '11', '12'] | Reference Text:  <S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S><S sid ="11" ssid = "11">For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora.</S><S sid ="12" ssid = "12">Being more readily available, comparable corpora are thus more suitable than parallel corpora for the task of acquiring new word translations, although relatively less research has been done in the past on comparable corpora.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 8 | Reference Article:  C04-1089.xml | Citing Article:  P06-1011.xml | Citation Marker Offset:  ['147'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['147'] | Citation Text:  <S sid ="147" ssid = "1">Much of the work involving comparable corpora has focused on extracting word translations (Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Koehn and Knight, 2000; Gaussier et al., 2004; Shao and Ng, 2004; Shinyama and Sekine, 2004).</S> | Reference Offset:  ['3', '4', '7', '8', '9'] | Reference Text:  <S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S><S sid ="9" ssid = "9">But parallel corpora are scarce resources, especially for uncommon lan guage pairs.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 9 | Reference Article:  C04-1089.xml | Citing Article:  P13-1059.xml | Citation Marker Offset:  ['208'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['208'] | Citation Text:  <S sid ="208" ssid = "17">Some recent research used comparable corpora to mine name translation pairs (Feng et al., 2004; Kutsumi et al., 2004; Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Lu and Zhao, 2006; Hassan et al., 2007).</S> | Reference Offset:  ['2', '3', '4', '7', '8'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 10 | Reference Article:  C04-1089.xml | Citing Article:  P13-1062.xml | Citation Marker Offset:  ['164'] | Citation Marker:  2004 | Citation Offset:  ['163','164'] | Citation Text:  <S sid ="163" ssid = "3">Evaluation E C We processed news articles for an entire year in tion of maximum bipartite matching (West, 1999) on a bipartite graph GB = (VB = (Si , Sj ), EB ) 2008 by Xinhua news who publishes news in E C both English and Chinese, which were also used with edge weights that are defined by TS . The maximum bipartite matching finds a subset of by Kim et al.</S><S sid ="164" ssid = "4">(2011) and Shao and Ng (2004).</S> | Reference Offset:  ['57', '68', '62', '97', '66'] | Reference Text:  <S sid ="57" ssid = "3">q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.</S><S sid ="68" ssid = "1">must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e|c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.</S><S sid ="62" ssid = "8">We use the Pml (tc|Tc (C (e))) = dT (C (e )) (tc ) ∑dT (C ( e )) (t ) expect ation maxi mizati on (EM) algorit hm to genera te mappi ng proba bilitie s from pinyin syl c t∈Tc (C ( e )) lables to English letter sequences.</S><S sid ="97" ssid = "25">For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c)|C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e|c) (as described in from a half-month period.</S><S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S> | Discourse Facet:  ['Results_Citation', 'Method_Citation'] | Annotator:  CIST |


Citance Number: 11 | Reference Article:  C04-1089.xml | Citing Article:  P13-1107.xml | Citation Marker Offset:  ['212'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['212'] | Citation Text:  <S sid ="212" ssid = "7">Other similar research lines are the TACKBP Entity Linking (EL) (Ji et al., 2010; Ji et al., 2011), which links a named entity in news and web documents to an appropriate knowledge base (KB) entry, the task of mining name translation pairs from comparable corpora (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007) and the link prediction problem (Adamic and Adar, 2001; LibenNowell and Kleinberg, 2003; Sun et al., 2011b; Hasan et al., 2006; Wang et al., 2007; Sun et al., 2011a).</S> | Reference Offset:  ['2', '3', '4', '7', '8'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 12 | Reference Article:  C04-1089.xml | Citing Article:  P13-2036.xml | Citation Marker Offset:  ['7'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['7'] | Citation Text:  <S sid ="7" ssid = "7">Recently, holistic approaches combining such similarities have been studied (Shao and Ng, 2004; You et al., 2010; Kim et al., 2011).</S> | Reference Offset:  ['66', '68', '168', '50', '169'] | Reference Text:  <S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S><S sid ="68" ssid = "1">must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e|c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.</S><S sid ="168" ssid = "7">The work that is most similar to ours is the recent research of (Huang et al., 2004).</S><S sid ="50" ssid = "27">In particular, our experiment was conducted on comparable corpora that are not very closely related and as such, most of the Chinese words have no translations of times term t occurs in the query Q , n = ∑t ct is the total number of terms in query Q . For ranking purpose, the first fraction n!</S><S sid ="169" ssid = "8">They attempted to improve named entity translation by combining phonetic and semantic information.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 13 | Reference Article:  C04-1089.xml | Citing Article:  P13-2036.xml | Citation Marker Offset:  ['8'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['8'] | Citation Text:  <S sid ="8" ssid = "8">(Shao and Ng, 2004) rank translation candidates using PH and CX independently and return results with the highest average rank.</S> | Reference Offset:  ['47', '55', '96', '97', '159'] | Reference Text:  <S sid ="47" ssid = "24">∏ t c t ! t The candidate ei that is ranked the highest according to the average rank is taken to be the cor where t is a term in the corpus, ct is the number rect translation and is output.</S><S sid ="55" ssid = "1">our task is to compute P(C (c)|C (e)) for each In a typical information retrieval (IR) problem, a query is given and a ranked list of documents English word e and find the e that gives the highest P(C (c)|C (e)) , estimated as: most relevant to the query is returned from a document collection.</S><S sid ="96" ssid = "24">for each pair of Chinese source word and English translation candidate word.</S><S sid ="97" ssid = "25">For each Chinese source word, we ranked all its English translation candidate words according to the estimated P(C (c)|C (e)) . For each Chinese source word c and an English translation candidate word e , we also calcu We then divided the Chinese corpus from Jul to Dec 1995 into 12 periods, each containing text lated the probability P(e|c) (as described in from a half-month period.</S><S sid ="159" ssid = "87">On the other hand, using our method of combining both sources of information and setting M = ∞, 19 Chinese words (i.e., the first 22 Chinese words in Table 3 except 巴佐亚,坩埚,普利法) have their correct English translations at rank one position.</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 14 | Reference Article:  C04-1089.xml | Citing Article:  W11-1215.xml | Citation Marker Offset:  ['38'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['38'] | Citation Text:  <S sid ="38" ssid = "16">Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Tao and Zhai, 2005; Hassan et al., 2007; Udupa et al., 2009; Ji, 2009).</S> | Reference Offset:  ['2', '3', '4', '7', '8'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 15 | Reference Article:  C04-1089.xml | Citing Article:  W11-2206.xml | Citation Marker Offset:  ['185'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['185'] | Citation Text:  <S sid ="185" ssid = "3">Some recent research used comparable corpora to re-score name transliterations (Sproat et al., 2006; Klementiev and Roth, 2006) or mine new word translations (Udupa et al., 2009; Ji, 2009; Fung and Yee, 1998; Rapp, 1999; Shao and Ng, 2004; Hassan et al., 2007).</S> | Reference Offset:  ['2', '3', '4', '7', '8'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S> | Discourse Facet:  ['Method_Citation', 'Aim_Citation'] | Annotator:  CIST |


Citance Number: 16 | Reference Article:  C04-1089.xml | Citing Article:  W13-2501.xml | Citation Marker Offset:  ['23'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['23'] | Citation Text:  <S sid ="23" ssid = "1">The traditional approch to translation extraction from comparable corpora and most of its extensions (Fung, 1998; Rapp, 1999; Shao and Ng, 2004; Otero, 2007; Yu and Tsujii, 2009; Marsi and Krahmer, 2010) presuppose the availability of a bilingual lexicon for translating source vectors into the target language.</S> | Reference Offset:  ['2', '3', '4', '7', '8'] | Reference Text:  <S sid ="2" ssid = "2">As such, the bilingual lexicon of a machine translation system has to be constantly updated with these new word translations.</S><S sid ="3" ssid = "3">Comparable corpora such as news documents of the same period from different news agencies are readily available.</S><S sid ="4" ssid = "4">In this paper, we present a new approach to mining new word translations from comparable corpora, by using context information to complement transliteration information.</S><S sid ="7" ssid = "7">In order for a machine translation system to translate these new words correctly, its bilingual lexicon needs to be constantly updated with new word translations.</S><S sid ="8" ssid = "8">Much research has been done on using parallel corpora to learn bilingual lexicons (Melamed, 1997; Moore, 2003).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |


Citance Number: 17 | Reference Article:  C04-1089.xml | Citing Article:  W13-2512.xml | Citation Marker Offset:  ['66'] | Citation Marker:  Shao and Ng, 2004 | Citation Offset:  ['66'] | Citation Text:  <S sid ="66" ssid = "18">Hybrid methods exploit that a term or a named entity can be translated in various ways across languages (Shao and Ng, 2004; Feng et al., 2004; Lu and Zhao, 2006).</S> | Reference Offset:  ['57', '66', '168', '68', '142'] | Reference Text:  <S sid ="57" ssid = "3">q(tc ) is the number (i.e., the surrounding words) of a Chinese word c . Each C (e) , the context of an English word of occurrenc es of tc in C (c) . Tc (C (e)) is the bag of Chinese words obtained by translating the First, each Chinese character in a Chinese English words in C(e) , as determined by a bi word c is converted to pinyin form.</S><S sid ="66" ssid = "12">mates, dT (C ( e)) (tc ) is the number of occurre nces That is, if an English letter sequenc e e1 precede s of the term tc in Tc (C(e)) , andPml (tc ) is esti another English letter sequence e2 in an English mated similarly by counting the occurrences of word, then the pinyin syllable mapped to e1 tc in the Chinese translation of the whole English corpus.</S><S sid ="168" ssid = "7">The work that is most similar to ours is the recent research of (Huang et al., 2004).</S><S sid ="68" ssid = "1">must precede the pinyin syllable mapped to e2 . Our method differs from (Knight and Graehl, 1998) and (AlOnaizan and Knight, 2002b) in that our method does not generate candidates but For the transliteration model, we use a modified only estimatesP(e|c) for candidates e appearmodel of (Knight and Graehl, 1998) and (Al ing in the English corpus.</S><S sid ="142" ssid = "70">The translations of 6 of the 43 words are words in the dictionary (denoted as “comm.” in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as “insuff”).</S> | Discourse Facet:  Method_Citation | Annotator:  CIST |