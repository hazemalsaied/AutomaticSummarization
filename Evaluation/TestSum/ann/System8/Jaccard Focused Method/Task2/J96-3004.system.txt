For language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.
We evaluate the system's performance by comparing its segmentation 'Tudgments" with the judgments of pool of human segmenters, and the system is shown to perform quite well.
The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.
In various Asian languages, including Chinese, on the other hand, whitespace is never used to delimit words, so one must resort to lexical information to "reconstruct" the word-boundary information.
For languages like English one can assume, to first approximation, that word boundaries are given by whitespace or punctuation.
Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces.
orthographic words are thus only starting point for further analysis and can only be regarded as useful hint at the desired division of the sentence into words.
Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.
In this paper we present stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.
Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).
Examples will usually be accompanied by
