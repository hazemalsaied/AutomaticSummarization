<html>
<head><title>P05-1053_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="1">[1]</a> <a href="#1" id=1>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="2">[2]</a> <a href="#2" id=2>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM.</a>
<a name="3">[3]</a> <a href="#3" id=3>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="4">[4]</a> <a href="#4" id=4>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="5">[5]</a> <a href="#5" id=5>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="6">[6]</a> <a href="#6" id=6>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="7">[7]</a> <a href="#7" id=7>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement.</a>
<a name="8">[8]</a> <a href="#8" id=8>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="9">[9]</a> <a href="#9" id=9>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="10">[10]</a> <a href="#10" id=10>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="11">[11]</a> <a href="#11" id=11>We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="12">[12]</a> <a href="#12" id=12>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="13">[13]</a> <a href="#13" id=13>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="14">[14]</a> <a href="#14" id=14>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="15">[15]</a> <a href="#15" id=15>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="16">[16]</a> <a href="#16" id=16>Our study illustrates that the base phrase chunking information contributes to most of the performance inprovement from syntactic aspect while additional full parsing information does not contribute much, largely due to the fact that most of relations defined in ACE corpus are within a very short distance.</a>
<a name="17">[17]</a> <a href="#17" id=17>We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.</a>
<a name="18">[18]</a> <a href="#18" id=18>427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.</a>
<a name="19">[19]</a> <a href="#19" id=19>427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.</a>
<a name="20">[20]</a> <a href="#20" id=20>427 Proceedings of the 43rd Annual Meeting of the ACL, pages 427â434, Ann Arbor, June 2005.</a>
<a name="21">[21]</a> <a href="#21" id=21>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="22">[22]</a> <a href="#22" id=22>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="23">[23]</a> <a href="#23" id=23>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="24">[24]</a> <a href="#24" id=24>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="25">[25]</a> <a href="#25" id=25>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="26">[26]</a> <a href="#26" id=26>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="27">[27]</a> <a href="#27" id=27>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="28">[28]</a> <a href="#28" id=28>Section 3 and Section 4 describe our approach and various features employed respectively.</a>
<a name="29">[29]</a> <a href="#29" id=29>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="30">[30]</a> <a href="#30" id=30>The relation extraction task was formulated at the 7th Message Understanding Conference (MUC7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities.</a>
<a name="31">[31]</a> <a href="#31" id=31>Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.</a>
<a name="32">[32]</a> <a href="#32" id=32>Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees.</a>
<a name="33">[33]</a> <a href="#33" id=33>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="34">[34]</a> <a href="#34" id=34>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="35">[35]</a> <a href="#35" id=35>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="36">[36]</a> <a href="#36" id=36>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="37">[37]</a> <a href="#37" id=37>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="38">[38]</a> <a href="#38" id=38>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="39">[39]</a> <a href="#39" id=39>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="40">[40]</a> <a href="#40" id=40>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="41">[41]</a> <a href="#41" id=41>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="42">[42]</a> <a href="#42" id=42>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="43">[43]</a> <a href="#43" id=43>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="44">[44]</a> <a href="#44" id=44>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="45">[45]</a> <a href="#45" id=45>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="46">[46]</a> <a href="#46" id=46>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="47">[47]</a> <a href="#47" id=47>Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived from word, entity type, mention level, overlap, dependency tree and parse tree.</a>
<a name="48">[48]</a> <a href="#48" id=48>Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.</a>
<a name="49">[49]</a> <a href="#49" id=49>Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.</a>
<a name="50">[50]</a> <a href="#50" id=50>Zhang (2004) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines.</a>
<a name="51">[51]</a> <a href="#51" id=51>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="52">[52]</a> <a href="#52" id=52>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="53">[53]</a> <a href="#53" id=53>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="54">[54]</a> <a href="#54" id=54>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="55">[55]</a> <a href="#55" id=55>Tree kernel-based approaches proposed by Zelenko et al (2003) and Culotta et al (2004) are able to explore the implicit feature space without much feature engineering.</a>
<a name="56">[56]</a> <a href="#56" id=56>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al (2000) which integrates various tasks such as part-of-speech tagging, named entity recognition, template element extraction and relation extraction, in a single model.</a>
<a name="57">[57]</a> <a href="#57" id=57>Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.</a>
<a name="58">[58]</a> <a href="#58" id=58>Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.</a>
<a name="59">[59]</a> <a href="#59" id=59>Evaluation on the ACE corpus shows that our system outperforms Kambhatla (2004) by about 3 F-measure on extracting 24 ACE relation subtypes.</a>
<a name="60">[60]</a> <a href="#60" id=60>It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.</a>
<a name="61">[61]</a> <a href="#61" id=61>It also shows that our system outperforms tree kernel-based systems (Culotta et al 2004) by over 20 F-measure on extracting 5 ACE relation types.</a>
<a name="62">[62]</a> <a href="#62" id=62>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="63">[63]</a> <a href="#63" id=63>Moreover, we only apply the simple linear kernel, although other kernels can peform better.</a>
<a name="64">[64]</a> <a href="#64" id=64>For details about SVMLight, please see http//svmlight.joachims.org/</a>
<a name="65">[65]</a> <a href="#65" id=65>In addition, we distinguish the argument order of the two mentions (M1 for the first mention and M2 for the second mention), e.g. M1-Parent- Of-M2 vs. M2-Parent-Of-M1.</a>
<a name="66">[66]</a> <a href="#66" id=66>For example, in the case where the noun phrase âthe former CEO of McDonaldâ has the head annotation of âCEOâ and the extent annotation of âthe former CEO of McDonaldâ, we only consider âthe former CEOâ in this paper.</a>
<a name="67">[67]</a> <a href="#67" id=67>This feature concerns about the entity type of both the mentions, which can be PERSON, ORGANIZATION, FACILITY, LOCATION and GeoPolitical Entity or GPE â¢ ET12 combination of mention entity types 4.3 Mention Level.</a>
<a name="68">[68]</a> <a href="#68" id=68>In this paper, we separate the features of base phrase chunking from those of full parsing.</a>
<a name="69">[69]</a> <a href="#69" id=69>In this paper, we separate the features of base phrase chunking from those of full parsing.</a>
<a name="70">[70]</a> <a href="#70" id=70>Here, the base phrase chunks are derived from full parse trees using the Perl script5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser (Collins 1999) is employed for full parsing.</a>
<a name="71">[71]</a> <a href="#71" id=71>Similar to word features, three categories of phrase heads are considered 1) the phrase heads in between are also classified into three bins the first phrase head in between, the last phrase head in between and other phrase heads in between; 2) the phrase heads before M1 are classified into two bins the first phrase head before and the second phrase head before; 3) the phrase heads after M2 are classified into two bins the first phrase head after and the second phrase head after.</a>
<a name="72">[72]</a> <a href="#72" id=72>This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.</a>
<a name="73">[73]</a> <a href="#73" id=73>Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.</a>
<a name="74">[74]</a> <a href="#74" id=74>This paper uses the ACE corpus provided by LDC to train and evaluate our feature-based relation extraction system.</a>
<a name="75">[75]</a> <a href="#75" id=75>In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.</a>
<a name="76">[76]</a> <a href="#76" id=76>Type Subtype Freq Residence 308 Other 6 ROLE(4756) General-Staff 1331 Management 1242 Member 1091 Owner 232 Other 158 SOCIAL(827) Associate 91 Grandparent 12 Other-Personal 85 Spouse 77 Table 1 Relation types and subtypes in the ACE training data In this paper, we explicitly model the argument order of the two mentions involved.</a>
<a name="77">[77]</a> <a href="#77" id=77>It shows that our system achieves best performance of 63.1%/49.5%/ 55.5 in precision/recall/F-measure when combining diverse lexical, syntactic and semantic features.</a>
<a name="78">[78]</a> <a href="#78" id=78>Table 2 also measures the contributions of different features by gradually increasing the feature set.</a>
<a name="79">[79]</a> <a href="#79" id=79>It shows that Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.</a>
<a name="80">[80]</a> <a href="#80" id=80>It shows that Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.</a>
<a name="81">[81]</a> <a href="#81" id=81>It shows that Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2%/23.7%/35.3 in precision/recall/F- measure.</a>
<a name="82">[82]</a> <a href="#82" id=82>Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.</a>
<a name="83">[83]</a> <a href="#83" id=83>Second, it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance.</a>
<a name="84">[84]</a> <a href="#84" id=84>Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="85">[85]</a> <a href="#85" id=85>The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus.</a>
<a name="86">[86]</a> <a href="#86" id=86>Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.</a>
<a name="87">[87]</a> <a href="#87" id=87>Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.</a>
<a name="88">[88]</a> <a href="#88" id=88>Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems, where we have a lot of relation types with a relatively small amount of annotated data.</a>
<a name="89">[89]</a> <a href="#89" id=89>The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types.</a></body>
</html>
