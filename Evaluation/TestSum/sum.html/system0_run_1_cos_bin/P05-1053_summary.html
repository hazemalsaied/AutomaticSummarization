<html>
<head><title>P05-1053_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Exploring Various Knowledge in Relation Extraction</a>
<a name="1">[1]</a> <a href="#1" id=1>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="2">[2]</a> <a href="#2" id=2>Normally , the above overlap features are too general to be effective alone . </a>
<a name="3">[3]</a> <a href="#3" id=3>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="4">[4]</a> <a href="#4" id=4>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="5">[5]</a> <a href="#5" id=5>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="6">[6]</a> <a href="#6" id=6>â¢ The usefulness of mention level features is quite limited . </a>
<a name="7">[7]</a> <a href="#7" id=7>â¢ Incorporating the overlap features gives some balance between precision and recall . </a>
<a name="8">[8]</a> <a href="#8" id=8>â¢ Chunking features are very useful . </a>
<a name="9">[9]</a> <a href="#9" id=9>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="10">[10]</a> <a href="#10" id=10>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="11">[11]</a> <a href="#11" id=11>Instead of exploring the full parse tree information directly as previous related work , we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance . </a>
<a name="12">[12]</a> <a href="#12" id=12>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking . </a>
<a name="13">[13]</a> <a href="#13" id=13>We also demonstrate how semantic information such as WordNet and Name List , can be used in feature-based relation extraction to further improve the performance . </a>
<a name="14">[14]</a> <a href="#14" id=14>Helenka et al ( 2003 ) proposed extracting relations by computing kernel functions between parse trees . </a>
<a name="15">[15]</a> <a href="#15" id=15>Dependency tree th parse trees . </a>
<a name="16">[16]</a> <a href="#16" id=16>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="17">[17]</a> <a href="#17" id=17>This category of features includes : â¢ # MB : number of other mentions in between â¢ # WB : number of words in between â¢ M1 > M2 or M1 < M2 : flag indicating whether M2/M1is included in M1/M2 . </a>
<a name="18">[18]</a> <a href="#18" id=18>Culotte et al ( 2004 ) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types . </a>
<a name="19">[19]</a> <a href="#19" id=19>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="20">[20]</a> <a href="#20" id=20>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="21">[21]</a> <a href="#21" id=21>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="22">[22]</a> <a href="#22" id=22>This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . </a>
<a name="23">[23]</a> <a href="#23" id=23>This may be due to the fact that most of relations in the ACE corpus are quite local . </a>
<a name="24">[24]</a> <a href="#24" id=24>The ACE corpus is gathered from various newspapers , newswire and broadcasts . </a>
<a name="25">[25]</a> <a href="#25" id=25>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="26">[26]</a> <a href="#26" id=26>Finally , we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6 . </a>
<a name="27">[27]</a> <a href="#27" id=27>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="28">[28]</a> <a href="#28" id=28>Tree kernel-based approaches proposed by Zelenko et al ( 2003 ) and Culotta et al ( 2004 ) are able to explore the implicit feature space without much feature engineering . </a>
<a name="29">[29]</a> <a href="#29" id=29>It shows that our system achieves better performance by ~3 F-measure largely due to its gain in recall . </a>
<a name="30">[30]</a> <a href="#30" id=30>Second , it is well known that full parsing is always prone to long-distance parsing errors although the Collinsâ parser used in our system achieves the state-of-the-art performance . </a>
<a name="31">[31]</a> <a href="#31" id=31>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="32">[32]</a> <a href="#32" id=32>Table 5 separates the performance of relation detection from overall performance on the testing set . </a>
<a name="33">[33]</a> <a href="#33" id=33>We use SVM as our learning algorithm with the full feature set from Zhou et al . </a>
<a name="34">[34]</a> <a href="#34" id=34>While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . </a>
<a name="35">[35]</a> <a href="#35" id=35>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="36">[36]</a> <a href="#36" id=36>It shows that : Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 : Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2 % /23.7 /35.3 in precision/recall/F- measure . </a>
<a name="37">[37]</a> <a href="#37" id=37>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="38">[38]</a> <a href="#38" id=38>This suggests that relation detection is critical for relation extraction . </a>
<a name="39">[39]</a> <a href="#39" id=39>For each pair of mentions 3 we compute various lexical , syntactic and semantic features . </a>
<a name="40">[40]</a> <a href="#40" id=40>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="41">[41]</a> <a href="#41" id=41>â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase . </a>
<a name="42">[42]</a> <a href="#42" id=42>5 55 .5 Ka mb hat la ( 20 04 ) : fe feature bas ed 6 3 . </a>
<a name="43">[43]</a> <a href="#43" id=43>With the dramatic increase in the amount of textual information available in digital archives and the WWW , there has been growing interest in techniques for automatically extracting information from text . </a>
<a name="44">[44]</a> <a href="#44" id=44>â¢ To our surprise , incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively . </a>
<a name="45">[45]</a> <a href="#45" id=45>The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus . </a>
<a name="46">[46]</a> <a href="#46" id=46>While short-distance relations dominate and can be resolved by above simple features , the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations . </a>
<a name="47">[47]</a> <a href="#47" id=47>This feature considers the entity level of both the mentions , which can be NAME , NOMIAL and PRONOUN : â¢ ML12 : combination of mention levels 4.4 Overlap . </a>
<a name="48">[48]</a> <a href="#48" id=48>Most of the chunking features concern about the head words of the phrases between the two mentions . </a>
<a name="49">[49]</a> <a href="#49" id=49>explicit relations occur in text with explicit evidence suggesting the relationships . </a>
<a name="50">[50]</a> <a href="#50" id=50>Implicit relations need not have explicit supporting evidence in text , though they should be evident from a reading of the document . </a>
<a name="51">[51]</a> <a href="#51" id=51>Information Extraction ( IE ) systems are expected to identify relevant information ( usually of predefined types ) from text documents in a certain domain and put them in a structured format . </a>
<a name="52">[52]</a> <a href="#52" id=52>It shows that 73 % ( 627/864 of errors results from relation detection and 27 % ( 237/864 of errors results from relation characterization , among which 17.8 % ( 154/864 of errors are from misclassification across relation types and 9.6 % ( 83/864 # of relations of errors are from misclassification of relation sub- types inside the same relation types . </a>
<a name="53">[53]</a> <a href="#53" id=53>Entities can be of five types : persons , organizations , locations , facilities and geopolitical entities ( GPE : geographically defined regions that indicate a political boundary , e.g . countries , states , cities , etc . ) . </a>
<a name="54">[54]</a> <a href="#54" id=54>Two features are defined to include this information : â¢ ET1SC2 : combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype . </a>
<a name="55">[55]</a> <a href="#55" id=55>Two features are defined to include this information : â¢ ET1Country : the entity type of M1 when M2 is a country name â¢ CountryET2 : the entity type of M2 when M1 is a country name 5  //ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE : Parent , Grandparent , Spouse , Sibling , Other-Relative and Other- Personal . </a>
<a name="56">[56]</a> <a href="#56" id=56>Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure , yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted . </a>
<a name="57">[57]</a> <a href="#57" id=57>Kamchatka 2004 ) employed Maximum Entropy models for relation extraction with features derived from word , entity type , mention level , overlap , dependency tree and parse tree . </a>
<a name="58">[58]</a> <a href="#58" id=58>This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features , which include : â¢ WM1 : bag-of-words in M1 â¢ HM1 : head word of M1 3 In ACE , each mention has a head annotation and an . </a>
<a name="59">[59]</a> <a href="#59" id=59>This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree . </a>
<a name="60">[60]</a> <a href="#60" id=60>Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types . </a>
<a name="61">[61]</a> <a href="#61" id=61>Similar to word features , three categories of phrase heads are considered : 1 ) the phrase heads in between are also classified into three bins : the first phrase head in between , the last phrase head in between and other phrase heads in between ; 2 ) the phrase heads before M1 are classified into two bins : the first phrase head before and the second phrase head before ; 3 ) the phrase heads after M2 are classified into two bins : the first phrase head after and the second phrase head after . </a>
<a name="62">[62]</a> <a href="#62" id=62>According to the scope of the NIST Automatic Content Extraction ( ACE ) program , current research in IE has three main objectives : Entity Detection and Tracking ( EDT ) , Relation Detection and Characterization ( RDC ) , and Event Detection and Characterization ( EDC ) . </a>
<a name="63">[63]</a> <a href="#63" id=63>This paper investigates the incorporation of diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using SVM . </a>
<a name="64">[64]</a> <a href="#64" id=64>The EDT task entails the detection of entity mentions and chaining them together by identifying their co reference . </a>
<a name="65">[65]</a> <a href="#65" id=65>In ACE vocabulary , entities are objects , mentions are references to them , and relations are semantic relationships between entities . </a>
<a name="66">[66]</a> <a href="#66" id=66>Mentions have three levels : names , nominal expressions or pronouns . </a>
<a name="67">[67]</a> <a href="#67" id=67>2 Joachims has just released a new version of SVMLight . </a>
<a name="68">[68]</a> <a href="#68" id=68>Basically , SVMs are binary classifiers . </a>
<a name="69">[69]</a> <a href="#69" id=69>For efficiency , we apply the one vs. others strategy , which builds K classifiers so as to separate one class from all others , instead of the pairwise strategy , which builds K* ( K-1 ) /2 classifiers considering all pairs of classes . </a>
<a name="70">[70]</a> <a href="#70" id=70>In the future work , we will focus on exploring more semantic knowledge in relation extraction , which has not been covered by current research . </a>
<a name="71">[71]</a> <a href="#71" id=71>In this way , we model relation extraction as a multi-class classification problem with 43 classes , two for each relation subtype ( except the above 6 symmetric subtypes ) and a âNONEâ class for the case where the two mentions are not related . </a>
<a name="72">[72]</a> <a href="#72" id=72>The RDC task detects and classifies implicit and explicit relations 1 between entities identified by the EDT task . </a>
<a name="73">[73]</a> <a href="#73" id=73>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) . </a>
<a name="74">[74]</a> <a href="#74" id=74>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="75">[75]</a> <a href="#75" id=75>Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE . </a>
<a name="76">[76]</a> <a href="#76" id=76>Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization ( RDC ) task , along with their frequency of occurrence in the ACE training set . </a>
<a name="77">[77]</a> <a href="#77" id=77>In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . </a>
<a name="78">[78]</a> <a href="#78" id=78>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="79">[79]</a> <a href="#79" id=79>Since a pronominal mention ( especially neutral pronoun such as âitâ and âitsâ contains little information about the sense of the mention , the co- reference chain is used to decide its sense . </a>
<a name="80">[80]</a> <a href="#80" id=80>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="81">[81]</a> <a href="#81" id=81>Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6 : Distribution of errors 6 Discussion and Conclusion . </a>
<a name="82">[82]</a> <a href="#82" id=82>The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task . </a>
<a name="83">[83]</a> <a href="#83" id=83>Extraction of semantic relationships between entities can be very useful for applications such as question answering , e.g . to answer the query âWho is the president of the United States ? â . </a>
<a name="84">[84]</a> <a href="#84" id=84>Our study illustrates that the base phrase chunking information contributes to most of the performance improvement from syntactic aspect while additional full parsing information does not contribute much , largely due to the fact that most of relations defined in ACE corpus are within a very short distance . </a>
<a name="85">[85]</a> <a href="#85" id=85>Note that only 6 of these 24 relation subtypes are symmetric : âRelative- Locationâ , âAssociateâ , âOther-Relativeâ , âOther- Professionalâ , âSiblingâ , and âSpouseâ . </a>
<a name="86">[86]</a> <a href="#86" id=86>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="87">[87]</a> <a href="#87" id=87>Based on the structural risk minimization of the statistical learning theory , SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set . </a>
<a name="88">[88]</a> <a href="#88" id=88>We also demonstrate how semantic information such as WordNet ( Miller 1990 ) and Name List can be used in the feature-based framework . </a>
<a name="89">[89]</a> <a href="#89" id=89>Evaluation shows that the incorporation of diverse features enables our system achieve best reported performance . </a>
<a name="90">[90]</a> <a href="#90" id=90>â¢ CPHBNULL when no phrase in between â¢ CPHBFL : the only phrase head when only one phrase in between â¢ CPHBF : first phrase head in between when at least two phrases in between â¢ CPHBL : last phrase head in between when at least two phrase heads in between â¢ CPHBO : other phrase heads in between except first and last phrase heads when at least three phrases in between â¢ CPHBM1F : first phrase head before M1 â¢ CPHBM1L : second phrase head before M1 â¢ CPHAM2F : first phrase head after M2 â¢ CPHAM2F : second phrase head after M2 â¢ CPP : path of phrase labels connecting the two mentions in the chunking â¢ CPPH : path of phrase labels connecting the two mentions in the chunking augmented with head words , if at most two phrases in between 4.6 Dependency Tree . </a>
<a name="91">[91]</a> <a href="#91" id=91>According to their positions , four categories of words are considered : 1 ) the words of both the mentions , 2 ) the words between the two mentions , 3 ) the words before M1 , and 4 ) the words after M2 . </a>
<a name="92">[92]</a> <a href="#92" id=92>It also shows that our system achieves overall performance of 77.2 % /60.7 /68.0 and 63.1 % /49.5 /55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus . </a>
<a name="93">[93]</a> <a href="#93" id=93>Compared with Kambhatla ( 2004 ) , we separately incorporate the base phrase chunking information , which contributes to most of the performance improvement from syntactic aspect . </a>
<a name="94">[94]</a> <a href="#94" id=94>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="95">[95]</a> <a href="#95" id=95>2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5 : Comparison of our system with other best-reported systems on the ACE corpus Error Type # Errors first . </a>
<a name="96">[96]</a> <a href="#96" id=96>This may be due to three reasons : First , most of relations defined in ACE have two mentions being close to each other . </a>
<a name="97">[97]</a> <a href="#97" id=97>427 Proceedings of the 43rd Annual Meeting of the ACL , pages 427â434 Ann Arbor , June 2005 . </a>
<a name="98">[98]</a> <a href="#98" id=98>â¢ ET1DW1 : combination of the entity type and the dependent word for M1 â¢ H1DW1 : combination of the head word and the dependent word for M1 â¢ ET2DW2 : combination of the entity type and the dependent word for M2 â¢ H2DW2 : combination of the head word and the dependent word for M2 â¢ ET12SameNP : combination of ET12 and whether M1 and M2 included in the same NP â¢ ET12SamePP : combination of ET12 and whether M1 and M2 exist in the same PP â¢ ET12SameVP : combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree . </a>
<a name="99">[99]</a> <a href="#99" id=99>Table 3 shows that about 70 % of relations exist where two mentions are embedded in each other or separated by at most one word . </a>
<a name="100">[100]</a> <a href="#100" id=100>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types . </a>
<a name="101">[101]</a> <a href="#101" id=101>The effect of personal relative trigger words is very limited due to the limited number of testing instances over personal social relation subtypes . </a>
<a name="102">[102]</a> <a href="#102" id=102>Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement . </a>
<a name="103">[103]</a> <a href="#103" id=103>The rest of this paper is organized as follows . </a>
<a name="104">[104]</a> <a href="#104" id=104>Section 2 presents related work . </a>
<a name="105">[105]</a> <a href="#105" id=105>Table 2 measures the performance of our relation extraction system over the 43 ACE relation subtypes on the testing set . </a>
<a name="106">[106]</a> <a href="#106" id=106>Besides , we also demonstrate how semantic information such as WordNet and Name List , can be used in feature-based relation extraction to further improve the performance . </a>
<a name="107">[107]</a> <a href="#107" id=107>Section 3 and Section 4 describe our approach and various features employed respectively . </a>
<a name="108">[108]</a> <a href="#108" id=108>Therefore , it would be interesting to see how imperfect EDT affects the performance in relation extraction . </a>
<a name="109">[109]</a> <a href="#109" id=109>It achieves 52.8 F- measure on the 24 ACE relation subtypes . </a>
<a name="110">[110]</a> <a href="#110" id=110>Evaluation on the ACE corpus shows that our system outperforms Kambhatla ( 2004 ) by about 3 F-measure on extracting 24 ACE relation subtypes . </a>
<a name="111">[111]</a> <a href="#111" id=111>We also show how semantic information like WordNet and Name List can be equipped to further improve the performance . </a>
<a name="112">[112]</a> <a href="#112" id=112>The semantic relation is determined between two mentions . </a>
<a name="113">[113]</a> <a href="#113" id=113>It also shows that our system outperforms tree kernel-based systems ( Culotta et al 2004 ) by over 20 F-measure on extracting 5 ACE relation types . </a>
<a name="114">[114]</a> <a href="#114" id=114>Therefore , we must extend SVMs to multi-class ( e.g . K ) such as the ACE RDC task . </a>
<a name="115">[115]</a> <a href="#115" id=115>This has an effect of choosing the base phrase contained in the extent annotation . </a>
<a name="116">[116]</a> <a href="#116" id=116>4.5 Base Phrase Chunking . </a>
<a name="117">[117]</a> <a href="#117" id=117>The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output . </a>
<a name="118">[118]</a> <a href="#118" id=118>In this paper , we use the binary-class SVMLight2 developed by Joachims ( 1998 ) . </a>
<a name="119">[119]</a> <a href="#119" id=119>â¢ Incorporating semantic resources such as the country name list and the personal relative trigger word list further increases the F-measure by 1.5 largely due to the differentiation of the relation subtype âROLE.Citizen-Ofâ from âROLE . </a>
<a name="120">[120]</a> <a href="#120" id=120>The words between the two mentions are classified into three bins : the first word in between , the last word in between and other words in between . </a>
<a name="121">[121]</a> <a href="#121" id=121>for multi-class classification . </a>
<a name="122">[122]</a> <a href="#122" id=122>However , this paper only uses the binary-class version . </a>
<a name="123">[123]</a> <a href="#123" id=123>For details about SVMLight , please see  //svmlight.joachims.org/ </a>
<a name="124">[124]</a> <a href="#124" id=124>This suggests the difficulty of detecting and classifying the relation type âATâ and its subtypes . </a>
<a name="125">[125]</a> <a href="#125" id=125>4.1 word . </a></body>
</html>
