<html>
<head><title>C00-2123_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Word Re-ordering and DP-based Search in Statistical Machine Translation</a>
<a name="1">[1]</a> <a href="#1" id=1>In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</a>
<a name="2">[2]</a> <a href="#2" id=2>The goal of machine translation is the translation of a text given in some source language into a target language . </a>
<a name="3">[3]</a> <a href="#3" id=3>We are given a source string fJ 1 = f1 : : : fj : : : fJ of length J , which is to be translated into a target string eI 1 = e1 : : : ei : : : eI of length I . Among all possible target strings , we will choose the string with the highest probability : ^eI 1 = arg max eI 1 fPr ( eI 1jfJ 1 ) g = arg max eI 1 fPr ( eI 1 ) Pr ( fJ 1 jeI 1 ) g : ( 1 ) The argyle operation denotes the search problem , i.e . the generation of the output sentence in the target language . </a>
<a name="4">[4]</a> <a href="#4" id=4>input : source string f1 : : : fj : : : fJ initialization for each cardinality c = 1 ; 2 ; ; J do for each pair ( C ; j ) , where j 2 C and jCj = c do for each target word e 2 E Qe0 ( e ; C ; j ) = p ( fj je ) max Ã ; e00 j02Cnfjg fp ( jjj 0 J ) p ( Ã ) pÃ ( eje 0 e00 ) Qe00 ( e0 ; C n fjg ; j0 ) g words fj in the input string of length J . For the final translation each source position is considered exactly once . </a>
<a name="5">[5]</a> <a href="#5" id=5>On average , 6 reference translations per automatic translation are available . </a>
<a name="6">[6]</a> <a href="#6" id=6>The Levenshtein distance between the automatic translation and each of the reference translations is computed , and the minimum Levenshtein distance is taken . </a>
<a name="7">[7]</a> <a href="#7" id=7>The computing time is given in terms of CPU time per sentence ( on a 450MHz PentiumIIIPC ) . </a>
<a name="8">[8]</a> <a href="#8" id=8>The German finite verbs bin ( second example ) and Frontenac ( third example ) are too far away from the personal pronouns rich and 'Sie ' ( 6 respectively 5 source sentence positions ) . </a>
<a name="9">[9]</a> <a href="#9" id=9>For each source word f , the list of its possible translations e is sorted according to p ( fje ) pun e ) , where pun e ) is the trigram probability of the English word e. It is suÃcient to consider only the best 50 words . </a>
<a name="10">[10]</a> <a href="#10" id=10>The sentence length probability p ( JjI ) is omitted without any loss in performance . </a>
<a name="11">[11]</a> <a href="#11" id=11>The final score is obtained from : max e ; e0 j2fJôL ; ; Jg p ( $ je ; e0 ) Qe0 ( e ; I ; f1 ; ; Jg ; j ) ; where p ( $ je ; e0 ) denotes the trigram language model , which predicts the sentence boundary $ at the end of the target sentence . </a>
<a name="12">[12]</a> <a href="#12" id=12>Table 4 shows translation results for the three approaches . </a>
<a name="13">[13]</a> <a href="#13" id=13>We show translation results for three approaches : the monotone search ( MonS ) , where no word reordering is allowed ( Tillmann , 1997 ) , the monotone search ( QmS ) as presented in this paper and the IBM style ( IbmS ) search as described in Section 3.2 . </a>
<a name="14">[14]</a> <a href="#14" id=14>Restrictions : Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence . </a>
<a name="15">[15]</a> <a href="#15" id=15>The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words , e.g . when translating German compound nouns . </a>
<a name="16">[16]</a> <a href="#16" id=16>Pr ( eI 1 ) is the language model of the target language , whereas Pr ( fJ 1 jeI1 ) is the translation model . </a>
<a name="17">[17]</a> <a href="#17" id=17>The word joining is done on the basis of a likelihood criterion . </a>
<a name="18">[18]</a> <a href="#18" id=18>A procedural definition to restrict1In the approach described in ( Berger et al. , 1996 ) , a mor pathological analysis is carried out and word morphemes rather than full-form words are used during the search . </a>
<a name="19">[19]</a> <a href="#19" id=19>Additionally , for a given coverage set , at most 250 different hypotheses are kept during the search process , and the number of different words to be hypothesized by a source word is limited . </a>
<a name="20">[20]</a> <a href="#20" id=20>For a trigram language model , the partial hypotheses are of the form ( e0 ; e ; C ; j ) . </a>
<a name="21">[21]</a> <a href="#21" id=21>The alignment model uses two kinds of parameters : alignment probabilities p ( aj jajô1 I ; J ) , where the probability of alignment aj for position j depends on the previous alignment position ajô1 Ney et al. , 2000 ) and lexicon probabilities p ( fj jean . </a>
<a name="22">[22]</a> <a href="#22" id=22>An extended lexicon model is defined , and its likelihood is compared to a baseline lexicon model , which takes only single-word dependencies into account . </a>
<a name="23">[23]</a> <a href="#23" id=23>Our approach uses word-to-word dependencies between source and target words . </a>
<a name="24">[24]</a> <a href="#24" id=24>These alignment models are similar to the concept of hidden Markov models ( HMM ) in speech recognition . </a>
<a name="25">[25]</a> <a href="#25" id=25>In the second and third translation examples , the IbmS word reordering performs worse than the QmS word reordering , since it can not take properly into account the word reordering due to the German verb group . </a>
<a name="26">[26]</a> <a href="#26" id=26>The complexity of the monotone search is O ( E3 J ( R2+LR ) ) . </a>
<a name="27">[27]</a> <a href="#27" id=27>man 5 . </a>
<a name="28">[28]</a> <a href="#28" id=28>A detailed description of the Search procedure used is given in this patent . </a></body>
</html>
