<html>
<head><title>P05-1053_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="1">[1]</a> <a href="#1" id=1>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="2">[2]</a> <a href="#2" id=2>We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.</a>
<a name="3">[3]</a> <a href="#3" id=3>For example, we want to determine whether a person is at a location, based on the evidence in the context.</a>
<a name="4">[4]</a> <a href="#4" id=4>This paper focuses on the ACE RDC task and employs diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines (SVMs).</a>
<a name="5">[5]</a> <a href="#5" id=5>We also demonstrate how semantic information such as WordNet (Miller 1990) and Name List can be used in the feature-based framework.</a>
<a name="6">[6]</a> <a href="#6" id=6>Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees.</a>
<a name="7">[7]</a> <a href="#7" id=7>Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types.</a>
<a name="8">[8]</a> <a href="#8" id=8>It achieves 52.8 F- measure on the 24 ACE relation subtypes.</a>
<a name="9">[9]</a> <a href="#9" id=9>Compared with Kambhatla (2004), we separately incorporate the base phrase chunking information, which contributes to most of the performance improvement from syntactic aspect.</a>
<a name="10">[10]</a> <a href="#10" id=10>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="11">[11]</a> <a href="#11" id=11>Support Vector Machines (SVMs) are a supervised machine learning technique motivated by the statistical learning theory (Vapnik 1998).</a>
<a name="12">[12]</a> <a href="#12" id=12>Moreover, we only apply the simple linear kernel, although other kernels can peform better.</a>
<a name="13">[13]</a> <a href="#13" id=13>Moreover, we only apply the simple linear kernel, although other kernels can peform better.</a>
<a name="14">[14]</a> <a href="#14" id=14>The semantic relation is determined between two mentions.</a>
<a name="15">[15]</a> <a href="#15" id=15>For each pair of mentions3, we compute various lexical, syntactic and semantic features.</a>
<a name="16">[16]</a> <a href="#16" id=16>For each pair of mentions3, we compute various lexical, syntactic and semantic features.</a>
<a name="17">[17]</a> <a href="#17" id=17>Normally, the above overlap features are too general to be effective alone.</a>
<a name="18">[18]</a> <a href="#18" id=18>This category of features includes information about the words, part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree.</a>
<a name="19">[19]</a> <a href="#19" id=19>This category of features concerns about the information inherent only in the full parse tree.</a>
<a name="20">[20]</a> <a href="#20" id=20>Semantic information from various resources, such as WordNet, is used to classify important words into different semantic lists according to their indicating relationships.</a>
<a name="21">[21]</a> <a href="#21" id=21>â¢ SC1ET2 combination of the entity type of M2 and the semantic class of M1 when the first mention triggers a personal social subtype.</a>
<a name="22">[22]</a> <a href="#22" id=22>In this paper, we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number.</a>
<a name="23">[23]</a> <a href="#23" id=23>Table 2 also measures the contributions of different features by gradually increasing the feature set.</a>
<a name="24">[24]</a> <a href="#24" id=24>â¢ The usefulness of mention level features is quite limited.</a>
<a name="25">[25]</a> <a href="#25" id=25>â¢ The usefulness of mention level features is quite limited.</a>
<a name="26">[26]</a> <a href="#26" id=26>â¢ The usefulness of mention level features is quite limited.</a>
<a name="27">[27]</a> <a href="#27" id=27>â¢ Chunking features are very useful.</a>
<a name="28">[28]</a> <a href="#28" id=28>â¢ Chunking features are very useful.</a>
<a name="29">[29]</a> <a href="#29" id=29>â¢ Chunking features are very useful.</a>
<a name="30">[30]</a> <a href="#30" id=30>â¢ Chunking features are very useful.</a>
<a name="31">[31]</a> <a href="#31" id=31>â¢ Chunking features are very useful.</a>
<a name="32">[32]</a> <a href="#32" id=32>â¢ Chunking features are very useful.</a>
<a name="33">[33]</a> <a href="#33" id=33>â¢ Chunking features are very useful.</a>
<a name="34">[34]</a> <a href="#34" id=34>â¢ Chunking features are very useful.</a>
<a name="35">[35]</a> <a href="#35" id=35>â¢ Chunking features are very useful.</a>
<a name="36">[36]</a> <a href="#36" id=36>This may be due to the fact that most of relations in the ACE corpus are quite local.</a>
<a name="37">[37]</a> <a href="#37" id=37>While short-distance relations dominate and can be resolved by above simple features, the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations.</a>
<a name="38">[38]</a> <a href="#38" id=38>The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.</a>
<a name="39">[39]</a> <a href="#39" id=39>The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.</a>
<a name="40">[40]</a> <a href="#40" id=40>This suggests that relation detection is critical for relation extraction.</a>
<a name="41">[41]</a> <a href="#41" id=41>This suggests that relation detection is critical for relation extraction.</a>
<a name="42">[42]</a> <a href="#42" id=42>This suggests that relation detection is critical for relation extraction.</a>
<a name="43">[43]</a> <a href="#43" id=43>This suggests that relation detection is critical for relation extraction.</a>
<a name="44">[44]</a> <a href="#44" id=44>This suggests that relation detection is critical for relation extraction.</a>
<a name="45">[45]</a> <a href="#45" id=45>This suggests that relation detection is critical for relation extraction.</a>
<a name="46">[46]</a> <a href="#46" id=46>This suggests that relation detection is critical for relation extraction.</a>
<a name="47">[47]</a> <a href="#47" id=47>This suggests that relation detection is critical for relation extraction.</a>
<a name="48">[48]</a> <a href="#48" id=48>This suggests that relation detection is critical for relation extraction.</a>
<a name="49">[49]</a> <a href="#49" id=49>This suggests that relation detection is critical for relation extraction.</a>
<a name="50">[50]</a> <a href="#50" id=50>In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.</a>
<a name="51">[51]</a> <a href="#51" id=51>This may be due to three reasons First, most of relations defined in ACE have two mentions being close to each other.</a>
<a name="52">[52]</a> <a href="#52" id=52>Covolution kernels for natural language.</a>
<a name="53">[53]</a> <a href="#53" id=53>Dependency tree th parse trees.</a>
<a name="54">[54]</a> <a href="#54" id=54>Dependency tree th parse trees.</a>
<a name="55">[55]</a> <a href="#55" id=55>Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.</a>
<a name="56">[56]</a> <a href="#56" id=56>Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.</a></body>
</html>
