<html>
<head><title>W03-0410_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>It is also necessary to consider the probable lack of sophisticated grammars or text processing tools for extracting accurate features.</a>
<a name="1">[1]</a> <a href="#1" id=1>2 5 6 C he at 10.</a>
<a name="2">[2]</a> <a href="#2" id=2>This captures what is reflected in the dendrogram, in that very short lines connect verbs low in the tree, and longer lines connect the two main clusters.</a>
<a name="3">[3]</a> <a href="#3" id=3>All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2 Experimental Results.</a>
<a name="4">[4]</a> <a href="#4" id=4>Full is full feature set; Ling is manually selected subset; Seed is seed-verb-selected set.</a>
<a name="5">[5]</a> <a href="#5" id=5>On the 2-way tasks, the performance on average is very close to that of the full feature set for the and measures.</a>
<a name="6">[6]</a> <a href="#6" id=6>Moreover, the value for the manually selected features is almost always very much higher than that of the full feature set, indicating that the subset of features is more focused on the properties that lead to a better separation of the data.</a>
<a name="7">[7]</a> <a href="#7" id=7>(1997) propose an unsupervised method to rank a set of features according to their ability to organize the data in space, based on an entropy measure they devise.</a>
<a name="8">[8]</a> <a href="#8" id=8>Unfortunately, this promising method did not prove practical for our data.</a>
<a name="9">[9]</a> <a href="#9" id=9>Unsupervised methods such as Dash et al.Ã¢â¬â¢s (1997) are appealing because they require no knowledge external to the data.</a>
<a name="10">[10]</a> <a href="#10" id=10>To model this kind of approach, we selected a sample of five seed verbs from each class.</a>
<a name="11">[11]</a> <a href="#11" id=11>Each set of verbs was judged (by the authorsÃ¢â¬â¢ intuition alone) to be Ã¢â¬ÅrepresentativeÃ¢â¬? of the class.</a>
<a name="12">[12]</a> <a href="#12" id=12>We purposely did not carry out any linguistic analysis, although we did check that each verb was reasonably frequent (with log frequencies ranging from 2.6 to 5.1).</a>
<a name="13">[13]</a> <a href="#13" id=13>learner (C5.0) on the seed verbs for those classes, in a 5-fold cross-validation (without boosting).</a>
<a name="14">[14]</a> <a href="#14" id=14>5.5 Further Discussion.</a>
<a name="15">[15]</a> <a href="#15" id=15>We ran 50 experiments using randomly selected sets of features of cardinality , where 5We also tried directly applying the mutual information (MI) measure used in decision-tree induction (Quinlan, 1986).</a>
<a name="16">[16]</a> <a href="#16" id=16>is the number of classes (a simple linear function roughly approximating the number of features in the Seed sets).</a>
<a name="17">[17]</a> <a href="#17" id=17>To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class.</a>
<a name="18">[18]</a> <a href="#18" id=18>We tentatively conclude that, yes, any subset of verbs of the appropriate class may be sufficient as a seed set, although some sets are better than others.</a>
<a name="19">[19]</a> <a href="#19" id=19>Using the same measure as ours, Stevenson and Merlo (1999) achieved performance in clustering very close to that of their supervised classification.</a>
<a name="20">[20]</a> <a href="#20" id=20>However, their study used a small set of five features manually devised for a set of three particular classes.</a>
<a name="21">[21]</a> <a href="#21" id=21>Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.</a>
<a name="22">[22]</a> <a href="#22" id=22>Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.</a>
<a name="23">[23]</a> <a href="#23" id=23>However, Schulte im WaldeÃ¢â¬â¢s features rely on accurate subcategorization statistics, and her experiments include a much larger set of classes (around 40), each with a much smaller number of verbs (average around 4).</a>
<a name="24">[24]</a> <a href="#24" id=24>We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task.</a>
<a name="25">[25]</a> <a href="#25" id=25>An unsupervised method we tried (Dash et al., 1997) did not prove useful, because of the problem of having no consistent threshold for feature inclusion.</a>
<a name="26">[26]</a> <a href="#26" id=26>We showed that this feature set outperformed both the full and the manually selected sets of features on all three of our clustering evaluation metrics.</a>
<a name="27">[27]</a> <a href="#27" id=27>More research is needed on the definition of the general feature space, as well as on the methods for selecting a more useful set of features for clustering.</a>
<a name="28">[28]</a> <a href="#28" id=28>In other ways, however, it is too difficult the learner has to distinguish multiple classes, rather than focus on the important properties of a single class.</a></body>
</html>
