<html>
<head><title>J96-3004_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.</a>
<a name="1">[1]</a> <a href="#1" id=1>Thus in an English sentence such as I'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces.</a>
<a name="2">[2]</a> <a href="#2" id=2>If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up.</a>
<a name="3">[3]</a> <a href="#3" id=3>If one is interested in translation, one would probably want to consider show up as a single dictionary word since its semantic interpretation is not trivially derivable from the meanings of show and up.</a>
<a name="4">[4]</a> <a href="#4" id=4>Space- or punctuation-delimited * 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.</a>
<a name="5">[5]</a> <a href="#5" id=5>com t 700 Mountain Avenue, 2d451, Murray Hill, NJ 07974, USA.</a>
<a name="6">[6]</a> <a href="#6" id=6>Email cls@bell-labs.</a>
<a name="7">[7]</a> <a href="#7" id=7>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÃÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="8">[8]</a> <a href="#8" id=8>Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÃÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.</a>
<a name="9">[9]</a> <a href="#9" id=9>In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÃÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).</a>
<a name="10">[10]</a> <a href="#10" id=10>For that application, at a minimum, one would want to know the phonological word boundaries.</a>
<a name="11">[11]</a> <a href="#11" id=11>Making the reasonable assumption that similar information is relevant for solving these problems in Chinese, it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation.</a>
<a name="12">[12]</a> <a href="#12" id=12>There are thus some very good reasons why segmentation into words is an important task.</a>
<a name="13">[13]</a> <a href="#13" id=13>Among these are words derived by various productive processes, including 1.</a>
<a name="14">[14]</a> <a href="#14" id=14>can expect famous names like Zhou Enlai's to be in many dictionaries, but names such as fi lf;f; shi2jil-lin2, the name of the second author of this paper, will not be found in any dictionary.</a>
<a name="15">[15]</a> <a href="#15" id=15>The segmenter handles the grouping of hanzi into words and outputs word pronunciations, with default pronunciations for hanzi it cannot group; we focus here primarily on the system's ability to segment text appropriately (rather than on its pronunciation abilities).</a>
<a name="16">[16]</a> <a href="#16" id=16>Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TIS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.</a>
<a name="17">[17]</a> <a href="#17" id=17>Thus in a two-hanzi word like lflli?J zhong1guo2 (middle country) 'China' there are two syllables, and at the same time two morphemes.</a>
<a name="18">[18]</a> <a href="#18" id=18>Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÃÂ­ cal rule-based approaches, and approaches that combine lexical information with staÃÂ­ tistical information.</a>
<a name="19">[19]</a> <a href="#19" id=19>The present proposal falls into the last group.</a>
<a name="20">[20]</a> <a href="#20" id=20>A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.</a>
<a name="21">[21]</a> <a href="#21" id=21>so that 'door' would be and in this case the hanzi 7C, does not represent a syllable.</a>
<a name="22">[22]</a> <a href="#22" id=22>Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.</a>
<a name="23">[23]</a> <a href="#23" id=23>In a few cases, the criteria for correctness are made more explicit.</a>
<a name="24">[24]</a> <a href="#24" id=24>The dictionary sizes reported in the literature range from 17,000 to 125,000 entries, and it seems reasonable to assume that the coverage of the base dictionary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.</a>
<a name="25">[25]</a> <a href="#25" id=25>More formally, we start by representing the dictionary D as a Weighted Finite State TransÃÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).</a>
<a name="26">[26]</a> <a href="#26" id=26>then define the best segmentation to be the cheapest or best path in Id(I) o D* (i.e., Id(I) composed with the transitive closure of 0).6 Consider the abstract example illustrated in Figure 2.</a>
<a name="27">[27]</a> <a href="#27" id=27>An input ABCD can be represented as an FSA as shown in Figure 2(b).</a>
<a name="28">[28]</a> <a href="#28" id=28>This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.</a>
<a name="29">[29]</a> <a href="#29" id=29>Figure 3 shows a small fragment of the WFST encoding the dictionary, containing both entries forjust discussed, gtÃÂ¥ zhonglhua2 min2guo2 (China Republic) 'Republic of China,' and iÃÂ¥inl.</a>
<a name="30">[30]</a> <a href="#30" id=30>As indicated in Figure 1(c), apart from this correct analysis, there is also the analysis taking B ri4 as a word (e.g., a common abbreviation for Japan), along with XÃÂ¥ wen2zhangl 'essay/ and f!!.</a>
<a name="31">[31]</a> <a href="#31" id=31>The morphological analÃÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.</a>
<a name="32">[32]</a> <a href="#32" id=32>ÃÂ£  _ADV 5.88 If!</a>
<a name="33">[33]</a> <a href="#33" id=33>ÃÂ£  _ADV 5.88 If!</a>
<a name="34">[34]</a> <a href="#34" id=34>!!\ yu2 e_nc [!!zen3 l!f moO t_adv il!shuot ,_vb i i i 1 Ã¢â¬Â¢ 10.03 13...</a>
<a name="35">[35]</a> <a href="#35" id=35>wo rd => na m e 2.</a>
<a name="36">[36]</a> <a href="#36" id=36>na me =>1 ha nzi fa mi ly 2 ha nzi gi ve n 3.</a>
<a name="37">[37]</a> <a href="#37" id=37>First, the model assumes independence between the first and second hanzi of a double given name.</a>
<a name="38">[38]</a> <a href="#38" id=38>As we have noted in Section 2, the general semantic class to which a hanzi belongs is often predictable from its semantic radical.</a>
<a name="39">[39]</a> <a href="#39" id=39>Virginia) and -sia are normally transliterated as fbSi!</a>
<a name="40">[40]</a> <a href="#40" id=40>We asked six native speakers-three from Taiwan (TlT3), and three from the Mainland (M1M3)-to segment the corpus.</a>
<a name="41">[41]</a> <a href="#41" id=41>Since we could not bias the subjects towards a particular segmentation and did not presume linguistic sophistication on their part, the instructions were simple subjects were to mark all places they might plausibly pause if they were reading the text aloud.</a>
<a name="42">[42]</a> <a href="#42" id=42>An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.</a>
<a name="43">[43]</a> <a href="#43" id=43>An examination of the subjects' bracketings confirmed that these instructions were satisfactory in yielding plausible word-sized units.</a>
<a name="44">[44]</a> <a href="#44" id=44>(See also Wu and Fung [1994].)</a>
<a name="45">[45]</a> <a href="#45" id=45>(See also Wu and Fung [1994].)</a>
<a name="46">[46]</a> <a href="#46" id=46>Two measures that can be used to compare judgments are 1.</a>
<a name="47">[47]</a> <a href="#47" id=47>computing the recall of the other's judgments relative to this standard.</a>
<a name="48">[48]</a> <a href="#48" id=48>Clearly, for judges h and h taking h as standard and computing the precision and recall for Jz yields the same results as taking h as the standard, and computing for h, 14 All evaluation materials, with the exception of those used for evaluating personal names were drawn.</a>
<a name="49">[49]</a> <a href="#49" id=49>Jud ges A G G R ST M 1 M 2 M 3 T1 T2 T3 AG 0.7 0 0.7 0 0 . 4 3 0.4 2 0.6 0 0.6 0 0.6 2 0.5 9 GR 0.9 9 0 . 6 2 0.6 4 0.7 9 0.8 2 0.8 1 0.7 2 ST 0 . 6 4 0.6 7 0.8 0 0.8 4 0.8 2 0.7 4 M1 0.7 7 0.6 9 0.7 1 0.6 9 0.7 0 M2 0.7 2 0.7 3 0.7 1 0.7 0 M3 0.8 9 0.8 7 0.8 0 T1 0.8 8 0.8 2 T2 0.7 8 respectively, the recall and precision.</a>
<a name="50">[50]</a> <a href="#50" id=50>The result of this is shown in Figure 7.</a>
<a name="51">[51]</a> <a href="#51" id=51>Two of the Mainlanders also cluster close together but, interestingly, not particularly close to the Taiwan speakers; the third Mainlander is much more similar to the Taiwan speakers.</a>
<a name="52">[52]</a> <a href="#52" id=52>Clearly the percentage of productively formed words is quite small (for this particular corpus), meaning that dictionary entries are covering most of the 15 GR is .73 or 96%..</a>
<a name="53">[53]</a> <a href="#53" id=53>In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.</a>
<a name="54">[54]</a> <a href="#54" id=54>As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes.</a>
<a name="55">[55]</a> <a href="#55" id=55>The percentage scores on the axis labels represent the amount of variation in the data explained by the dimension in question.</a>
<a name="56">[56]</a> <a href="#56" id=56>Word type N % Dic tion ary entr ies 2 , 5 4 3 9 7 . 4 7 Mor pho logi call y deri ved wor ds 3 0 . 1 1 Fore ign tran slite rati ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases.</a>
<a name="57">[57]</a> <a href="#57" id=57>It may seem surprising to some readers that the interhuman agreement scores reported here are so low.</a>
<a name="58">[58]</a> <a href="#58" id=58>However, this result is consistent with the results of exÃÂ­ periments discussed in Wu and Fung (1994).</a>
<a name="59">[59]</a> <a href="#59" id=59>Wu and Fung introduce an evaluation method they call nk-blind.</a>
<a name="60">[60]</a> <a href="#60" id=60>Under this scheme, n human judges are asked independently to segment a text.</a>
<a name="61">[61]</a> <a href="#61" id=61>For a given "word" in the automatic segmentation, if at least k of the huÃÂ­ man judges agree that this is a word, then that word is considered to be correct.</a>
<a name="62">[62]</a> <a href="#62" id=62>For eight judges, ranging k between 1 and 8 corresponded to a precision score range of 90% to 30%, meaning that there were relatively few words (30% of those found by the automatic segmenter) on which all judges agreed, whereas most of the words found by the segmenter were such that one human judge agreed.</a>
<a name="63">[63]</a> <a href="#63" id=63>To evaluate proper-name identification, we randomly seÃÂ­ lected 186 sentences containing 12,000 hanzi from our test corpus and segmented the text automatically, tagging personal names; note that for names, there is always a sinÃÂ­ gle unambiguous answer, unlike the more general question of which segmentation is correct.</a>
<a name="64">[64]</a> <a href="#64" id=64>However, they list two sets, one consisting of 28 fragments and the other of 22 fragments, in which they had 0% recall and precision.</a>
<a name="65">[65]</a> <a href="#65" id=65>On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.</a>
<a name="66">[66]</a> <a href="#66" id=66>(1992).</a>
<a name="67">[67]</a> <a href="#67" id=67>In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.</a>
<a name="68">[68]</a> <a href="#68" id=68>Examples are given in Table 4.</a>
<a name="69">[69]</a> <a href="#69" id=69>This is a rather important source of errors in name identifiÃÂ­ cation, and it is not really possible to objectively evaluate a name recognition system without considering the main lexicon with which it is used.</a>
<a name="70">[70]</a> <a href="#70" id=70>paper, and is missing 6 examples from the A set.</a>
<a name="71">[71]</a> <a href="#71" id=71>For example, the Wang, Li, and Chang system fails on the sequence 1fp]nian2 nei4 sa3 in (k) since 1F nian2 is a possible, but rare, family name, which also happens to be written the same as the very common word meaning 'year.'</a>
<a name="72">[72]</a> <a href="#72" id=72>Our system fails in (a) because of$ shenl, a rare family name; the system identifies it as a family name, whereas it should be analyzed as part of the given name.</a>
<a name="73">[73]</a> <a href="#73" id=73>An example is in (i), where the system fails to group t;,f;?"$?t! lin2yang2gang3 as a name, because all three hanzi can in principle be separate words (t;,f; lin2 'wood';?"$ yang2 'ocean'; ?t!; gang3 'harbor').</a>
<a name="74">[74]</a> <a href="#74" id=74>A totally nonÃÂ­ stochastic rule-based system such as Wang, Li, and Chang's will generally succeed in such cases, but of course runs the risk of overgeneration wherever the single-hanzi word is really intended.</a>
<a name="75">[75]</a> <a href="#75" id=75>The last affix in the list is the nominal plural f, men0.20 In the table are the (typical) classes of words to which the affix attaches, the number found in the test corpus by the method, the number correct (with a precision measure), and the number missed (with a recall measure).</a>
<a name="76">[76]</a> <a href="#76" id=76>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="77">[77]</a> <a href="#77" id=77>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="78">[78]</a> <a href="#78" id=78>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="79">[79]</a> <a href="#79" id=79>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="80">[80]</a> <a href="#80" id=80>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="81">[81]</a> <a href="#81" id=81>In this paper we have argued that Chinese word segmentation can be modeled efÃÂ­ fectively using weighted finite-state transducers.</a>
<a name="82">[82]</a> <a href="#82" id=82>First of all, most previous articles report perforÃÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.</a>
<a name="83">[83]</a> <a href="#83" id=83>May 1995).</a>
<a name="84">[84]</a> <a href="#84" id=84>Second, comparisons of different methods are not meaningful unless one can evalÃÂ­ uate them on the same corpus.</a>
<a name="85">[85]</a> <a href="#85" id=85>One hopes that such a corpus will be forthÃÂ­ coming.</a>
<a name="86">[86]</a> <a href="#86" id=86>The major problem for our segÃÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).</a>
<a name="87">[87]</a> <a href="#87" id=87>This implies, therefore, that a major factor in the performance of a Chinese segmenter is the quality of the base dictionary, and this is probably a more important factor-from the point of view of performance alone-than the particular computational methods used.</a>
<a name="88">[88]</a> <a href="#88" id=88>(a) 1 ÃÂ§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1ÃÂ§ . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' $ chel jinglguo4 car pass by 2.</a>
<a name="89">[89]</a> <a href="#89" id=89>(a) 1 ÃÂ§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1ÃÂ§ . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' $ chel jinglguo4 car pass by 2.</a>
<a name="90">[90]</a> <a href="#90" id=90>Gan's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic, semantic, and lexical relations between objects of various linguistic types (hanzi, words, phrases).</a>
<a name="91">[91]</a> <a href="#91" id=91>Particular relations are also consistent with particular hypotheses about the segmentation of a given sentence, and the scores for particular relations can be incremented or decremented depending upon whether the segmentations with which they are consistent are "popular" or not.</a>
<a name="92">[92]</a> <a href="#92" id=92>While Gan's system incorporates fairly sophisticated models of various linguistic information, it has the drawback that it has only been tested with a very small lexicon (a few hundred words) and on a very small test set (thirty sentences); there is therefore serious concern as to whether the methods that he discusses are scalable.</a>
<a name="93">[93]</a> <a href="#93" id=93>Another question that remains unanswered is to what extent the linguistic information he considers can be handled-or at least approximated-by finite-state language models, and therefore could be directly interfaced with the segmentation model that we have presented in this paper.</a>
<a name="94">[94]</a> <a href="#94" id=94>For the examples given in (1) and (2) this certainly seems possible.</a>
<a name="95">[95]</a> <a href="#95" id=95>For the examples given in (1) and (2) this certainly seems possible.</a>
<a name="96">[96]</a> <a href="#96" id=96>The segmenter will give both analyses 1 cai2 neng2 'just be able,' and ?]cai2neng2 'talent,' but the latter analysis is preferred since splitting these two morphemes is generally more costly than grouping them.</a>
<a name="97">[97]</a> <a href="#97" id=97>However, there is again local grammatical information that should favor the split in the case of (1a) both .ma3 'horse' and .ma3 lu4 are nouns, but only .ma3 is consistent with the classifier pil, the classifier for horses.21 By a similar argument, the preference for not splitting , lm could be strengthened in (lb) by the observation that the classifier 1'1* tiao2 is consistent with long or winding objects like , lm ma3lu4 'road' but not with,ma3 'horse.'</a>
<a name="98">[98]</a> <a href="#98" id=98>Mandarin exhibits several such processes, including A-not-A question formation, ilÃÂ­ lustrated in (3a), and adverbial reduplication, illustrated in (3b) 3.</a>
<a name="99">[99]</a> <a href="#99" id=99>Mandarin exhibits several such processes, including A-not-A question formation, ilÃÂ­ lustrated in (3a), and adverbial reduplication, illustrated in (3b) 3.</a>
<a name="100">[100]</a> <a href="#100" id=100>(a) ;IE shi4 'be' => ;IE;IE shi4bu2-shi4 (be-not-be) 'is it?'</a>
<a name="101">[101]</a> <a href="#101" id=101>gaolgaolxing4xing4 'happily' In the particular form of A-not-A reduplication illustrated in (3a), the first syllable of the verb is copied, and the negative markerbu4 'not' is inserted between the copy and the full verb.</a>
<a name="102">[102]</a> <a href="#102" id=102>As described in Sproat (1995), the Chinese segmenter presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis.</a>
<a name="103">[103]</a> <a href="#103" id=103>Furthermore, by inverting the transducer so that it maps from phonemic transcriptions to hanzi sequences, one can apply the segmenter to other problems, such as speech recognition (Pereira, Riley, and Sproat 1994).</a>
<a name="104">[104]</a> <a href="#104" id=104>Mohri [1995]) shows promise for improving this situation.</a></body>
</html>
