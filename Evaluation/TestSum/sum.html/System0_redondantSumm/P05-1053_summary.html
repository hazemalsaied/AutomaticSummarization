<html>
<head><title>P05-1053_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Exploring Various Knowledge in Relation Extraction</a>
<a name="1">[1]</a> <a href="#1" id=1>Exploring Various Knowledge in Relation Extraction</a>
<a name="2">[2]</a> <a href="#2" id=2>Normally , the above overlap features are too general to be effective alone . </a>
<a name="3">[3]</a> <a href="#3" id=3>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="4">[4]</a> <a href="#4" id=4>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="5">[5]</a> <a href="#5" id=5>Normally , the above overlap features are too general to be effective alone . </a>
<a name="6">[6]</a> <a href="#6" id=6>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="7">[7]</a> <a href="#7" id=7>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="8">[8]</a> <a href="#8" id=8>Normally , the above overlap features are too general to be effective alone . </a>
<a name="9">[9]</a> <a href="#9" id=9>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="10">[10]</a> <a href="#10" id=10>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="11">[11]</a> <a href="#11" id=11>Instead of exploring the full parse tree information directly as previous related work , we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance . </a>
<a name="12">[12]</a> <a href="#12" id=12>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="13">[13]</a> <a href="#13" id=13>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking . </a>
<a name="14">[14]</a> <a href="#14" id=14>Helenka et al ( 2003 ) proposed extracting relations by computing kernel functions between parse trees . </a>
<a name="15">[15]</a> <a href="#15" id=15>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="16">[16]</a> <a href="#16" id=16>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="17">[17]</a> <a href="#17" id=17>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="18">[18]</a> <a href="#18" id=18>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="19">[19]</a> <a href="#19" id=19>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="20">[20]</a> <a href="#20" id=20>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="21">[21]</a> <a href="#21" id=21>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="22">[22]</a> <a href="#22" id=22>This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . </a>
<a name="23">[23]</a> <a href="#23" id=23>This may be due to the fact that most of relations in the ACE corpus are quite local . </a>
<a name="24">[24]</a> <a href="#24" id=24>The ACE corpus is gathered from various newspapers , newswire and broadcasts . </a>
<a name="25">[25]</a> <a href="#25" id=25>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="26">[26]</a> <a href="#26" id=26>Exploring Various Knowledge in Relation Extraction</a>
<a name="27">[27]</a> <a href="#27" id=27>Normally , the above overlap features are too general to be effective alone . </a>
<a name="28">[28]</a> <a href="#28" id=28>â¢ The usefulness of mention level features is quite limited . </a>
<a name="29">[29]</a> <a href="#29" id=29>Normally , the above overlap features are too general to be effective alone . </a>
<a name="30">[30]</a> <a href="#30" id=30>â¢ The usefulness of mention level features is quite limited . </a>
<a name="31">[31]</a> <a href="#31" id=31>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="32">[32]</a> <a href="#32" id=32>Table 5 separates the performance of relation detection from overall performance on the testing set . </a>
<a name="33">[33]</a> <a href="#33" id=33>We use SVM as our learning algorithm with the full feature set from Zhou et al . </a>
<a name="34">[34]</a> <a href="#34" id=34>We use SVM as our learning algorithm with the full feature set from Zhou et al . </a>
<a name="35">[35]</a> <a href="#35" id=35>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="36">[36]</a> <a href="#36" id=36>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="37">[37]</a> <a href="#37" id=37>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="38">[38]</a> <a href="#38" id=38>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="39">[39]</a> <a href="#39" id=39>Normally , the above overlap features are too general to be effective alone . </a>
<a name="40">[40]</a> <a href="#40" id=40>â¢ The usefulness of mention level features is quite limited . </a>
<a name="41">[41]</a> <a href="#41" id=41>Normally , the above overlap features are too general to be effective alone . </a>
<a name="42">[42]</a> <a href="#42" id=42>â¢ The usefulness of mention level features is quite limited . </a>
<a name="43">[43]</a> <a href="#43" id=43>Exploring Various Knowledge in Relation Extraction</a>
<a name="44">[44]</a> <a href="#44" id=44>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="45">[45]</a> <a href="#45" id=45>Normally , the above overlap features are too general to be effective alone . </a>
<a name="46">[46]</a> <a href="#46" id=46>â¢ The usefulness of mention level features is quite limited . </a>
<a name="47">[47]</a> <a href="#47" id=47>Normally , the above overlap features are too general to be effective alone . </a>
<a name="48">[48]</a> <a href="#48" id=48>â¢ The usefulness of mention level features is quite limited . </a>
<a name="49">[49]</a> <a href="#49" id=49>explicit relations occur in text with explicit evidence suggesting the relationships . </a>
<a name="50">[50]</a> <a href="#50" id=50>Normally , the above overlap features are too general to be effective alone . </a>
<a name="51">[51]</a> <a href="#51" id=51>Exploring Various Knowledge in Relation Extraction</a>
<a name="52">[52]</a> <a href="#52" id=52>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="53">[53]</a> <a href="#53" id=53>This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . </a>
<a name="54">[54]</a> <a href="#54" id=54>Normally , the above overlap features are too general to be effective alone . </a>
<a name="55">[55]</a> <a href="#55" id=55>â¢ The usefulness of mention level features is quite limited . </a>
<a name="56">[56]</a> <a href="#56" id=56>Normally , the above overlap features are too general to be effective alone . </a>
<a name="57">[57]</a> <a href="#57" id=57>â¢ The usefulness of mention level features is quite limited . </a>
<a name="58">[58]</a> <a href="#58" id=58>Normally , the above overlap features are too general to be effective alone . </a>
<a name="59">[59]</a> <a href="#59" id=59>â¢ The usefulness of mention level features is quite limited . </a>
<a name="60">[60]</a> <a href="#60" id=60>Normally , the above overlap features are too general to be effective alone . </a>
<a name="61">[61]</a> <a href="#61" id=61>Normally , the above overlap features are too general to be effective alone . </a>
<a name="62">[62]</a> <a href="#62" id=62>Normally , the above overlap features are too general to be effective alone . </a>
<a name="63">[63]</a> <a href="#63" id=63>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="64">[64]</a> <a href="#64" id=64>Exploring Various Knowledge in Relation Extraction</a>
<a name="65">[65]</a> <a href="#65" id=65>Normally , the above overlap features are too general to be effective alone . </a>
<a name="66">[66]</a> <a href="#66" id=66>Exploring Various Knowledge in Relation Extraction</a>
<a name="67">[67]</a> <a href="#67" id=67>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="68">[68]</a> <a href="#68" id=68>Basically , SVMs are binary classifiers . </a>
<a name="69">[69]</a> <a href="#69" id=69>Basically , SVMs are binary classifiers . </a>
<a name="70">[70]</a> <a href="#70" id=70>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="71">[71]</a> <a href="#71" id=71>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="72">[72]</a> <a href="#72" id=72>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="73">[73]</a> <a href="#73" id=73>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="74">[74]</a> <a href="#74" id=74>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="75">[75]</a> <a href="#75" id=75>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="76">[76]</a> <a href="#76" id=76>It shows that 73 % ( 627/864 of errors results from relation detection and 27 % ( 237/864 of errors results from relation characterization , among which 17.8 % ( 154/864 of errors are from misclassification across relation types and 9.6 % ( 83/864 # of relations of errors are from misclassification of relation sub- types inside the same relation types . </a>
<a name="77">[77]</a> <a href="#77" id=77>In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . </a>
<a name="78">[78]</a> <a href="#78" id=78>In this way , we model relation extraction as a multi-class classification problem with 43 classes , two for each relation subtype ( except the above 6 symmetric subtypes ) and a âNONEâ class for the case where the two mentions are not related . </a>
<a name="79">[79]</a> <a href="#79" id=79>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="80">[80]</a> <a href="#80" id=80>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="81">[81]</a> <a href="#81" id=81>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="82">[82]</a> <a href="#82" id=82>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="83">[83]</a> <a href="#83" id=83>Normally , the above overlap features are too general to be effective alone . </a>
<a name="84">[84]</a> <a href="#84" id=84>Normally , the above overlap features are too general to be effective alone . </a>
<a name="85">[85]</a> <a href="#85" id=85>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="86">[86]</a> <a href="#86" id=86>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="87">[87]</a> <a href="#87" id=87>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="88">[88]</a> <a href="#88" id=88>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="89">[89]</a> <a href="#89" id=89>Based on the structural risk minimization of the statistical learning theory , SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set . </a>
<a name="90">[90]</a> <a href="#90" id=90>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="91">[91]</a> <a href="#91" id=91>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="92">[92]</a> <a href="#92" id=92>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="93">[93]</a> <a href="#93" id=93>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="94">[94]</a> <a href="#94" id=94>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="95">[95]</a> <a href="#95" id=95>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="96">[96]</a> <a href="#96" id=96>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="97">[97]</a> <a href="#97" id=97>Exploring Various Knowledge in Relation Extraction</a>
<a name="98">[98]</a> <a href="#98" id=98>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="99">[99]</a> <a href="#99" id=99>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="100">[100]</a> <a href="#100" id=100>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="101">[101]</a> <a href="#101" id=101>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="102">[102]</a> <a href="#102" id=102>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="103">[103]</a> <a href="#103" id=103>Normally , the above overlap features are too general to be effective alone . </a>
<a name="104">[104]</a> <a href="#104" id=104>â¢ The usefulness of mention level features is quite limited . </a>
<a name="105">[105]</a> <a href="#105" id=105>Normally , the above overlap features are too general to be effective alone . </a>
<a name="106">[106]</a> <a href="#106" id=106>â¢ The usefulness of mention level features is quite limited . </a>
<a name="107">[107]</a> <a href="#107" id=107>Normally , the above overlap features are too general to be effective alone . </a>
<a name="108">[108]</a> <a href="#108" id=108>Normally , the above overlap features are too general to be effective alone . </a>
<a name="109">[109]</a> <a href="#109" id=109>Exploring Various Knowledge in Relation Extraction</a>
<a name="110">[110]</a> <a href="#110" id=110>Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6 : Distribution of errors 6 Discussion and Conclusion . </a>
<a name="111">[111]</a> <a href="#111" id=111>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="112">[112]</a> <a href="#112" id=112>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="113">[113]</a> <a href="#113" id=113>Normally , the above overlap features are too general to be effective alone . </a>
<a name="114">[114]</a> <a href="#114" id=114>We also demonstrate how semantic information such as WordNet ( Miller 1990 ) and Name List can be used in the feature-based framework . </a>
<a name="115">[115]</a> <a href="#115" id=115>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="116">[116]</a> <a href="#116" id=116>â¢ CPHBNULL when no phrase in between â¢ CPHBFL : the only phrase head when only one phrase in between â¢ CPHBF : first phrase head in between when at least two phrases in between â¢ CPHBL : last phrase head in between when at least two phrase heads in between â¢ CPHBO : other phrase heads in between except first and last phrase heads when at least three phrases in between â¢ CPHBM1F : first phrase head before M1 â¢ CPHBM1L : second phrase head before M1 â¢ CPHAM2F : first phrase head after M2 â¢ CPHAM2F : second phrase head after M2 â¢ CPP : path of phrase labels connecting the two mentions in the chunking â¢ CPPH : path of phrase labels connecting the two mentions in the chunking augmented with head words , if at most two phrases in between 4.6 Dependency Tree . </a>
<a name="117">[117]</a> <a href="#117" id=117>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="118">[118]</a> <a href="#118" id=118>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="119">[119]</a> <a href="#119" id=119>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="120">[120]</a> <a href="#120" id=120>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="121">[121]</a> <a href="#121" id=121>Exploring Various Knowledge in Relation Extraction</a>
<a name="122">[122]</a> <a href="#122" id=122>Normally , the above overlap features are too general to be effective alone . </a>
<a name="123">[123]</a> <a href="#123" id=123>Normally , the above overlap features are too general to be effective alone . </a>
<a name="124">[124]</a> <a href="#124" id=124>While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . </a>
<a name="125">[125]</a> <a href="#125" id=125>The RDC task detects and classifies implicit and explicit Relation 1 between Entity identified by the EDT task . </a></body>
</html>
