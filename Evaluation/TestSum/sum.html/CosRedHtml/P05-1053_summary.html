<html>
<head><title>P05-1053_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Exploring Various Knowledge in Relation Extraction</a>
<a name="1">[1]</a> <a href="#1" id=1>Exploring Various Knowledge in Relation Extraction</a>
<a name="2">[2]</a> <a href="#2" id=2>Normally , the above overlap features are too general to be effective alone . </a>
<a name="3">[3]</a> <a href="#3" id=3>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="4">[4]</a> <a href="#4" id=4>Normally , the above overlap features are too general to be effective alone . </a>
<a name="5">[5]</a> <a href="#5" id=5>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="6">[6]</a> <a href="#6" id=6>Normally , the above overlap features are too general to be effective alone . </a>
<a name="7">[7]</a> <a href="#7" id=7>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="8">[8]</a> <a href="#8" id=8>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="9">[9]</a> <a href="#9" id=9>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking.</a>
<a name="10">[10]</a> <a href="#10" id=10>Helenka et al ( 2003 ) proposed extracting relations by computing kernel functions between parse trees . </a>
<a name="11">[11]</a> <a href="#11" id=11>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="12">[12]</a> <a href="#12" id=12>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="13">[13]</a> <a href="#13" id=13>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="14">[14]</a> <a href="#14" id=14>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="15">[15]</a> <a href="#15" id=15>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="16">[16]</a> <a href="#16" id=16>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="17">[17]</a> <a href="#17" id=17>This may be due to the fact that most of relations in the ACE corpus are quite local . </a>
<a name="18">[18]</a> <a href="#18" id=18>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="19">[19]</a> <a href="#19" id=19>Exploring Various Knowledge in Relation Extraction</a>
<a name="20">[20]</a> <a href="#20" id=20>Normally , the above overlap features are too general to be effective alone . </a>
<a name="21">[21]</a> <a href="#21" id=21>Normally , the above overlap features are too general to be effective alone . </a>
<a name="22">[22]</a> <a href="#22" id=22>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="23">[23]</a> <a href="#23" id=23>We use SVM as our learning algorithm with the full feature set from Zhou et al . </a>
<a name="24">[24]</a> <a href="#24" id=24>We use SVM as our learning algorithm with the full feature set from Zhou et al . </a>
<a name="25">[25]</a> <a href="#25" id=25>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="26">[26]</a> <a href="#26" id=26>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="27">[27]</a> <a href="#27" id=27>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="28">[28]</a> <a href="#28" id=28>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="29">[29]</a> <a href="#29" id=29>Normally , the above overlap features are too general to be effective alone . </a>
<a name="30">[30]</a> <a href="#30" id=30>Normally , the above overlap features are too general to be effective alone . </a>
<a name="31">[31]</a> <a href="#31" id=31>Exploring Various Knowledge in Relation Extraction</a>
<a name="32">[32]</a> <a href="#32" id=32>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="33">[33]</a> <a href="#33" id=33>Normally , the above overlap features are too general to be effective alone . </a>
<a name="34">[34]</a> <a href="#34" id=34>Normally , the above overlap features are too general to be effective alone . </a>
<a name="35">[35]</a> <a href="#35" id=35>explicit relations occur in text with explicit evidence suggesting the relationships . </a>
<a name="36">[36]</a> <a href="#36" id=36>Normally , the above overlap features are too general to be effective alone . </a>
<a name="37">[37]</a> <a href="#37" id=37>Exploring Various Knowledge in Relation Extraction</a>
<a name="38">[38]</a> <a href="#38" id=38>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="39">[39]</a> <a href="#39" id=39>This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . </a>
<a name="40">[40]</a> <a href="#40" id=40>Normally , the above overlap features are too general to be effective alone . </a>
<a name="41">[41]</a> <a href="#41" id=41>Normally , the above overlap features are too general to be effective alone . </a>
<a name="42">[42]</a> <a href="#42" id=42>Normally , the above overlap features are too general to be effective alone . </a>
<a name="43">[43]</a> <a href="#43" id=43>Normally , the above overlap features are too general to be effective alone . </a>
<a name="44">[44]</a> <a href="#44" id=44>Normally , the above overlap features are too general to be effective alone . </a>
<a name="45">[45]</a> <a href="#45" id=45>Normally , the above overlap features are too general to be effective alone . </a>
<a name="46">[46]</a> <a href="#46" id=46>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="47">[47]</a> <a href="#47" id=47>Exploring Various Knowledge in Relation Extraction</a>
<a name="48">[48]</a> <a href="#48" id=48>Normally , the above overlap features are too general to be effective alone . </a>
<a name="49">[49]</a> <a href="#49" id=49>Exploring Various Knowledge in Relation Extraction</a>
<a name="50">[50]</a> <a href="#50" id=50>Moreover , we only apply the simple linear kernel , although other kernels can perform better . </a>
<a name="51">[51]</a> <a href="#51" id=51>Basically , SVMs are binary classifiers . </a>
<a name="52">[52]</a> <a href="#52" id=52>Basically , SVMs are binary classifiers . </a>
<a name="53">[53]</a> <a href="#53" id=53>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="54">[54]</a> <a href="#54" id=54>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="55">[55]</a> <a href="#55" id=55>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="56">[56]</a> <a href="#56" id=56>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="57">[57]</a> <a href="#57" id=57>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="58">[58]</a> <a href="#58" id=58>It shows that 73 % ( 627/864 of errors results from relation detection and 27 % ( 237/864 of errors results from relation characterization , among which 17.8 % ( 154/864 of errors are from misclassification across relation types and 9.6 % ( 83/864 # of relations of errors are from misclassification of relation sub- types inside the same relation types . </a>
<a name="59">[59]</a> <a href="#59" id=59>In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . </a>
<a name="60">[60]</a> <a href="#60" id=60>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="61">[61]</a> <a href="#61" id=61>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="62">[62]</a> <a href="#62" id=62>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="63">[63]</a> <a href="#63" id=63>Normally , the above overlap features are too general to be effective alone . </a>
<a name="64">[64]</a> <a href="#64" id=64>Normally , the above overlap features are too general to be effective alone . </a>
<a name="65">[65]</a> <a href="#65" id=65>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="66">[66]</a> <a href="#66" id=66>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="67">[67]</a> <a href="#67" id=67>Many methods have been proposed to deal with this task , including supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithm ( Hasegawa et al. , 2004 ) . </a>
<a name="68">[68]</a> <a href="#68" id=68>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="69">[69]</a> <a href="#69" id=69>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="70">[70]</a> <a href="#70" id=70>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="71">[71]</a> <a href="#71" id=71>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="72">[72]</a> <a href="#72" id=72>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="73">[73]</a> <a href="#73" id=73>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="74">[74]</a> <a href="#74" id=74>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="75">[75]</a> <a href="#75" id=75>Exploring Various Knowledge in Relation Extraction</a>
<a name="76">[76]</a> <a href="#76" id=76>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="77">[77]</a> <a href="#77" id=77>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="78">[78]</a> <a href="#78" id=78>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="79">[79]</a> <a href="#79" id=79>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="80">[80]</a> <a href="#80" id=80>Normally , the above overlap features are too general to be effective alone . </a>
<a name="81">[81]</a> <a href="#81" id=81>Normally , the above overlap features are too general to be effective alone . </a>
<a name="82">[82]</a> <a href="#82" id=82>Normally , the above overlap features are too general to be effective alone . </a>
<a name="83">[83]</a> <a href="#83" id=83>Normally , the above overlap features are too general to be effective alone . </a>
<a name="84">[84]</a> <a href="#84" id=84>Exploring Various Knowledge in Relation Extraction</a>
<a name="85">[85]</a> <a href="#85" id=85>Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6 : Distribution of errors 6 Discussion and Conclusion . </a>
<a name="86">[86]</a> <a href="#86" id=86>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="87">[87]</a> <a href="#87" id=87>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="88">[88]</a> <a href="#88" id=88>Normally , the above overlap features are too general to be effective alone . </a>
<a name="89">[89]</a> <a href="#89" id=89>We also demonstrate how semantic information such as WordNet ( Miller 1990 ) and Name List can be used in the feature-based framework . </a>
<a name="90">[90]</a> <a href="#90" id=90>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="91">[91]</a> <a href="#91" id=91>Many machine learning methods have been proposed to address this problem , e.g. , supervised learning algorithms ( Miller et al. , 2000 ; Zelenko et al. , 2002 ; Culotta and Soresen , 2004 ; Kambhatla , 2004 ; Zhou et al. , 2005 ) , semi-supervised learning algorithms ( Brin , 1998 ; Agichtein and Gravano , 2000 ; Zhang , 2004 ) , and unsupervised learning algorithms ( Hasegawa et al. , 2004 ) . </a>
<a name="92">[92]</a> <a href="#92" id=92>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="93">[93]</a> <a href="#93" id=93>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="94">[94]</a> <a href="#94" id=94>â¢ WM2 : bag-of-words in M2 â¢ HM2 : head word of M2 â¢ HM12 : combination of HM1 and HM2 â¢ WBNULL : when no word in between â¢ WBFL : the only word in between when only one word in between â¢ WBF : first word in between when at least two words in between â¢ WBL : last word in between when at least two words in between â¢ WBO : other words in between except first and last words when at least three words in between â¢ BM1F : first word before M1 â¢ BM1L : second word before M1 â¢ AM2F : first word after M2 â¢ AM2L : second word after M2 4.2 Entity Type . </a>
<a name="95">[95]</a> <a href="#95" id=95>Exploring Various Knowledge in Relation Extraction</a>
<a name="96">[96]</a> <a href="#96" id=96>Normally , the above overlap features are too general to be effective alone . </a>
<a name="97">[97]</a> <a href="#97" id=97>Normally , the above overlap features are too general to be effective alone . </a>
<a name="98">[98]</a> <a href="#98" id=98>While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . </a>
<a name="99">[99]</a> <a href="#99" id=99>The RDC task detects and classifies implicit and explicit Relation 1 between Entity identified by the EDT task . </a></body>
</html>
