<html>
<head><title>C00-2123_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</a>
<a name="1">[1]</a> <a href="#1" id=1> In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</a>
<a name="2">[2]</a> <a href="#2" id=2> In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</a>
<a name="3">[3]</a> <a href="#3" id=3> The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</a>
<a name="4">[4]</a> <a href="#4" id=4> The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</a>
<a name="5">[5]</a> <a href="#5" id=5> The model is often further restricted so that each source word is assigned to exactly one target word (Brown et al., 1993; Ney et al., 2000).</a>
<a name="6">[6]</a> <a href="#6" id=6> This approach is compared to another reordering scheme presented in (Berger et al., 1996).</a>
<a name="7">[7]</a> <a href="#7" id=7> To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</a>
<a name="8">[8]</a> <a href="#8" id=8> To explicitly handle the word reordering between words in source and target language, we use the concept of the so-called inverted alignments as given in (Ney et al., 2000).</a>
<a name="9">[9]</a> <a href="#9" id=9> The details are given in (Och and Ney, 2000).</a>
<a name="10">[10]</a> <a href="#10" id=10> The details are given in (Och and Ney, 2000).</a>
<a name="11">[11]</a> <a href="#11" id=11> For a trigram language model, the partial hypotheses are of the form (e0; e; C; j).</a>
<a name="12">[12]</a> <a href="#12" id=12> A position is presented by the word at that position.</a>
<a name="13">[13]</a> <a href="#13" id=13> Covering the first uncovered position in the source sentence, we use the language model probability p(ej$; $).</a>
<a name="14">[14]</a> <a href="#14" id=14> The proof is given in (Tillmann, 2000).</a>
<a name="15">[15]</a> <a href="#15" id=15> Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</a>
<a name="16">[16]</a> <a href="#16" id=16> Restrictions We compare our new approach with the word reordering used in the IBM translation approach (Berger et al., 1996).</a>
<a name="17">[17]</a> <a href="#17" id=17> The proof is given in (Tillmann, 2000).</a>
<a name="18">[18]</a> <a href="#18" id=18> For each source word f, the list of its possible translations e is sorted according to p(fje) puni(e), where puni(e) is the unigram probability of the English word e. It is sucient to consider only the best 50 words.</a>
<a name="19">[19]</a> <a href="#19" id=19> Translation errors are reported in terms of multireference word error rate (mWER) and subjective sentence error rate (SSER).</a>
<a name="20">[20]</a> <a href="#20" id=20> Table 5 Effect of the beam threshold on the number of search errors (147 sentences).</a>
<a name="21">[21]</a> <a href="#21" id=21> In this paper, we have presented a new, ecient DP-based search procedure for statistical machine translation.</a></body>
</html>
