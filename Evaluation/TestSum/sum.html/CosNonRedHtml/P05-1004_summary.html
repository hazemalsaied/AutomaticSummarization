<html>
<head><title>P05-1004_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Superdense tagging is also interesting for many applications that use shallow semantics , e.g . information extraction and question answering . </a>
<a name="1">[1]</a> <a href="#1" id=1>Our evaluation will use exactly the same test sets as Ciaramita and Johnson ( 2003 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>Carmita and Johnson ( 2003 ) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus . </a>
<a name="3">[3]</a> <a href="#3" id=3>An additional potential is to integrate automatically acquired relationships with the information found in WordNet , which seems to suffer from several serious limitations ( Curran 2005 ) , and typically overlaps to a rather limited extent with the output of automatic acquisition methods . </a>
<a name="4">[4]</a> <a href="#4" id=4>Technical domains , such as medicine , require separate treatment since common words often take on special meanings , and a significant proportion of their vocabulary does not overlap with everyday vocabulary . </a>
<a name="5">[5]</a> <a href="#5" id=5>This technique is similar to Hearst and SchuÂ¨ tze ( 1993 ) and Widdows ( 2003 ) . </a>
<a name="6">[6]</a> <a href="#6" id=6>However , sometimes the unknown noun does not appear in our 2 billion word corpus , or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms . </a>
<a name="7">[7]</a> <a href="#7" id=7>Superdense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organize their manual insertion into WORDNET . </a>
<a name="8">[8]</a> <a href="#8" id=8>Our results significantly outperform Ciaramita and Johnson ( 2003 ) on both test sets even though our system is unsupervised . </a>
<a name="9">[9]</a> <a href="#9" id=9>We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson ( 2003 ) . </a>
<a name="10">[10]</a> <a href="#10" id=10>consistency when classifying similar words into categories . </a>
<a name="11">[11]</a> <a href="#11" id=11>Also , in the data from Ciaramita and Johnson all of the words are in lower case , so no sensible guessing rules could help . </a>
<a name="12">[12]</a> <a href="#12" id=12>How are words with multiple super senses handled ? </a>
<a name="13">[13]</a> <a href="#13" id=13>One solution would be to take the intersection between vector across words for each superdense i.e . to find the common contexts that these words appear in ) . </a></body>
</html>
