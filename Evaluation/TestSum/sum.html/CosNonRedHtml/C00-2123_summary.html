<html>
<head><title>C00-2123_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Word Re-ordering and DP-based Search in Statistical Machine Translation</a>
<a name="1">[1]</a> <a href="#1" id=1>In this paper, we describe a search procedure for statistical machine translation (MT) based on dynamic programming (DP).</a>
<a name="2">[2]</a> <a href="#2" id=2>The goal of machine translation is the translation of a text given in some source language into a target language . </a>
<a name="3">[3]</a> <a href="#3" id=3>We are given a source string fJ 1 = f1 : : : fj : : : fJ of length J , which is to be translated into a target string eI 1 = e1 : : : ei : : : eI of length I . Among all possible target strings , we will choose the string with the highest probability : ^eI 1 = arg max eI 1 fPr ( eI 1jfJ 1 ) g = arg max eI 1 fPr ( eI 1 ) Pr ( fJ 1 jeI 1 ) g : ( 1 ) The argyle operation denotes the search problem , i.e . the generation of the output sentence in the target language . </a>
<a name="4">[4]</a> <a href="#4" id=4>On average , 6 reference translations per automatic translation are available . </a>
<a name="5">[5]</a> <a href="#5" id=5>The computing time is given in terms of CPU time per sentence ( on a 450MHz PentiumIIIPC ) . </a>
<a name="6">[6]</a> <a href="#6" id=6>For each source word f , the list of its possible translations e is sorted according to p ( fje ) pun e ) , where pun e ) is the trigram probability of the English word e. It is suÃcient to consider only the best 50 words . </a>
<a name="7">[7]</a> <a href="#7" id=7>The sentence length probability p ( JjI ) is omitted without any loss in performance . </a>
<a name="8">[8]</a> <a href="#8" id=8>The final score is obtained from : max e ; e0 j2fJôL ; ; Jg p ( $ je ; e0 ) Qe0 ( e ; I ; f1 ; ; Jg ; j ) ; where p ( $ je ; e0 ) denotes the trigram language model , which predicts the sentence boundary $ at the end of the target sentence . </a>
<a name="9">[9]</a> <a href="#9" id=9>The Levenshtein distance between the automatic translation and each of the reference translations is computed , and the minimum Levenshtein distance is taken . </a>
<a name="10">[10]</a> <a href="#10" id=10>We show translation results for three approaches : the monotone search ( MonS ) , where no word reordering is allowed ( Tillmann , 1997 ) , the monotone search ( QmS ) as presented in this paper and the IBM style ( IbmS ) search as described in Section 3.2 . </a>
<a name="11">[11]</a> <a href="#11" id=11>Restrictions : Quasi-monotone Search The above search space is still too large to allow the translation of a medium length input sentence . </a>
<a name="12">[12]</a> <a href="#12" id=12>input : source string f1 : : : fj : : : fJ initialization for each cardinality c = 1 ; 2 ; ; J do for each pair ( C ; j ) , where j 2 C and jCj = c do for each target word e 2 E Qe0 ( e ; C ; j ) = p ( fj je ) max Ã ; e00 j02Cnfjg fp ( jjj 0 J ) p ( Ã ) pÃ ( eje 0 e00 ) Qe00 ( e0 ; C n fjg ; j0 ) g words fj in the input string of length J . For the final translation each source position is considered exactly once . </a>
<a name="13">[13]</a> <a href="#13" id=13>Pr ( eI 1 ) is the language model of the target language , whereas Pr ( fJ 1 jeI1 ) is the translation model . </a>
<a name="14">[14]</a> <a href="#14" id=14>The word joining is done on the basis of a likelihood criterion . </a>
<a name="15">[15]</a> <a href="#15" id=15>A procedural definition to restrict1In the approach described in ( Berger et al. , 1996 ) , a mor pathological analysis is carried out and word morphemes rather than full-form words are used during the search . </a>
<a name="16">[16]</a> <a href="#16" id=16>For a trigram language model , the partial hypotheses are of the form ( e0 ; e ; C ; j ) . </a>
<a name="17">[17]</a> <a href="#17" id=17>The alignment model uses two kinds of parameters : alignment probabilities p ( aj jajô1 I ; J ) , where the probability of alignment aj for position j depends on the previous alignment position ajô1 Ney et al. , 2000 ) and lexicon probabilities p ( fj jean . </a>
<a name="18">[18]</a> <a href="#18" id=18>An extended lexicon model is defined , and its likelihood is compared to a baseline lexicon model , which takes only single-word dependencies into account . </a>
<a name="19">[19]</a> <a href="#19" id=19>Our approach uses word-to-word dependencies between source and target words . </a>
<a name="20">[20]</a> <a href="#20" id=20>These alignment models are similar to the concept of hidden Markov models ( HMM ) in speech recognition . </a>
<a name="21">[21]</a> <a href="#21" id=21>In the second and third translation examples , the IbmS word reordering performs worse than the QmS word reordering , since it can not take properly into account the word reordering due to the German verb group . </a>
<a name="22">[22]</a> <a href="#22" id=22>The complexity of the monotone search is O ( E3 J ( R2+LR ) ) . </a>
<a name="23">[23]</a> <a href="#23" id=23>man 5 . </a>
<a name="24">[24]</a> <a href="#24" id=24>A detailed description of the Search procedure used is given in this patent . </a></body>
</html>
