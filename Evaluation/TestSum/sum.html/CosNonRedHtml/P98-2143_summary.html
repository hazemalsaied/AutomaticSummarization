<html>
<head><title>P98-2143_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Robust pronoun resolution with limited knowledge</a>
<a name="1">[1]</a> <a href="#1" id=1>Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.</a>
<a name="2">[2]</a> <a href="#2" id=2>One of the disadvantages of developing a knowledgeÃÂ­ based system, however, is that it is a very labourÃÂ­ intensive and time-consuming task.</a>
<a name="3">[3]</a> <a href="#3" id=3>This paper presÃÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.</a>
<a name="4">[4]</a> <a href="#4" id=4>Co reference resolution is a field in which major progress has been made in the last decade . </a>
<a name="5">[5]</a> <a href="#5" id=5>Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent.</a>
<a name="6">[6]</a> <a href="#6" id=6>Some of the limitations of the traditional rule based approaches ( Mitkov , 1998 ) could be overcome by machine learning techniques , which allow automating the acquisition of knowledge from annotated corpora . </a>
<a name="7">[7]</a> <a href="#7" id=7>( Mitkov , 1998 ; Poesio et al. , 2002 ; Markert and Nissim , 2005 ) ) , machine learning methods were embraced ( cf . </a>
<a name="8">[8]</a> <a href="#8" id=8>`` Non-prepositional '' noun phrases A `` pure '' , `` non-prepositional '' noun phrase is given a higher preference than a noun phrase which is part of a prepositional phrase ( 0 , -1 ) . </a>
<a name="9">[9]</a> <a href="#9" id=9>For the most part , anaphora resolution has focused on traditional linguistic methods ( Carbonell & Brown 1988 ; Carter 1987 ; Hobbs 1978 ; Ingria & Stallard 1989 ; Lappin & McCord 1990 ; Lappin & Leass 1994 ; Mitkov 1994 ; Rich & LuperFoy 1988 ; Sidner 1979 ; Webber 1979 ) . </a>
<a name="10">[10]</a> <a href="#10" id=10>However , to represent and manipulate the various types of linguistic and domain knowledge involved requires considerable human input and computational expense . </a>
<a name="11">[11]</a> <a href="#11" id=11>While various alternatives have been proposed , making use of e.g . neural networks , a situation seÂ­ antics framework , or the principles of reasoning with uncertainty ( e.g . Connoly et al . 1994 ; Mitkov 1995 ; Tin & Akman 1995 ) , there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems , and to enhance further the automatic proÂ­ cussing of growing language resources . </a>
<a name="12">[12]</a> <a href="#12" id=12>Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain Andros linguistic knowledge ( Baldwin 1997 ; Dagan & ital 1990 ; Kennedy & Boguraev 1996 ; Mitkov 1998 ; Nasukawa 1994 ; Williams et al . 1996 ) . </a>
<a name="13">[13]</a> <a href="#13" id=13>Our work is a continuation of these latest trends in the search for inexpensive , fast and reliable procedures for anaphÂ­ ora resolution . </a>
<a name="14">[14]</a> <a href="#14" id=14>It is also an example of how anaphora in a specific genre can be resolved quite successfully without any sophisticated linguistic knowledge or even without parsing . </a>
<a name="15">[15]</a> <a href="#15" id=15>Finally , our evaluation shows that the basic set of antecedent tracking indicators can work well not only for English , but also for other languages ( in our case Polish and Arabic ) . </a>
<a name="16">[16]</a> <a href="#16" id=16>With a view to avoiding complex syntactic , semanÂ­ tic and discourse analysis ( which is vital for realÂ­ world applications ) , we developed a robust , knowlÂ­ edge-poor approach to pronoun resolution which does not parse and analysis the input in order to identify antecedents of anaphora . </a>
<a name="17">[17]</a> <a href="#17" id=17>It makes use of only a part-of-speech tagger , plus simple noun phrase rules ( sentence constituents are identified at the level of noun phrase at most ) and operates on the basis of antecedent-tracking preferences ( referred to hereafter as `` antecedent indicators '' ) . </a>
<a name="18">[18]</a> <a href="#18" id=18>For anaphora in simple sentences , noun phrases in the previous senÂ­ sentence are the best candidate for antecedent , followed by noun phrases situated 2 sentences further back and finally nouns 3 sentences further back { 1 , 0 , -1 ) . </a>
<a name="19">[19]</a> <a href="#19" id=19>The approach works as follows : it takes as an input the output of a text processed by a part-of-speech tagger , identifies the noun phrases which precede the anaphora within a distance of 2 sentences , checks them for gender and number agreement with the anaphora and then applies the genre-specific antecedent indicators to the reÂ­ raining candidates ( see next section ) . </a>
<a name="20">[20]</a> <a href="#20" id=20>The noun phrase with the highest aggregate score is proposed as antecedent ; in the rare event of a tie , priority is given to the candidate with the higher score for imÂ­ mediate reference . </a>
<a name="21">[21]</a> <a href="#21" id=21>If immediate reference has not been identified , then priority is given to the Candi date with the best collocation pattern score . </a>
<a name="22">[22]</a> <a href="#22" id=22>If this does not help , the candidate with the higher score for indicating verbs is preferred . </a>
<a name="23">[23]</a> <a href="#23" id=23>If still no choice is possible , the most recent from the remaining candiÂ­ dates is selected as the antecedent . </a>
<a name="24">[24]</a> <a href="#24" id=24>2.1 Antecedent indicators . </a>
<a name="25">[25]</a> <a href="#25" id=25>Antecedent indicators ( preferences ) play a decisive role in tracking down the antecedent from a set of possible candidates . </a>
<a name="26">[26]</a> <a href="#26" id=26>Candidates are assigned a score ( -1 , 0 , 1 or 2 ) for each indicator ; the candidate with the highest aggregate score is proposed as the anteÂ­ cement . </a>
<a name="27">[27]</a> <a href="#27" id=27>The antecedent indicators have been identiÂ­ fie empirically and are related to salience ( definiteness , givenness , indicating verbs , lexical reiteration , section heading preference , `` nonÂ­ prepositional '' noun phrases ) , to structural matches ( collocation , immediate reference ) , to referential distance or to preference of terms . </a>
<a name="28">[28]</a> <a href="#28" id=28>Whilst some of the indicators are more genre-specific ( term preferÂ­ enc and others are less genre-specific ( `` immediate reference '' ) , the majority appear to be genreÂ­ independent . </a>
<a name="29">[29]</a> <a href="#29" id=29>In the following we shall outline some the indicators used and shall illustrate them by exÂ­ ample . </a>
<a name="30">[30]</a> <a href="#30" id=30>Definiteness Definite noun phrases in previous sentences are more likely antecedents of pronominal anaphora than indefinite ones ( definite noun phrases score 0 and indefinite ones are penalty by -1 ) . </a>
<a name="31">[31]</a> <a href="#31" id=31>We regard a noun phrase as definite if the head noun is modified by a definite article , or by demonstrative or possesÂ­ give pronouns . </a>
<a name="32">[32]</a> <a href="#32" id=32>This rule is ignored if there are no definite articles , possessive or demonstrative proÂ­ nouns in the paragraph ( this exception is taken into account because some English user 's guides tend to omit articles ) . </a>
<a name="33">[33]</a> <a href="#33" id=33>Givenness Noun phrases in previous sentences representing the `` given information '' ( theme ) 1 are deemed good candidates for antecedents and score I ( candidates not representing the theme score 0 ) . </a>
<a name="34">[34]</a> <a href="#34" id=34>In a coherent text ( Firbas 1992 ) , the given or known information , or theme , usually appears first , and thus forms a coÂ­ referential link with the preceding text . </a>
<a name="35">[35]</a> <a href="#35" id=35>The new information , or heme provides some information about the theme . </a>
<a name="36">[36]</a> <a href="#36" id=36>1We use the simple heuristics that the given information is the first noun phrase in a non-imperative sentence . </a>
<a name="37">[37]</a> <a href="#37" id=37>Indicating verbs If a verb is a member of the Verb_set = { discuss , present , illustrate , identify , summarize examine , describe , define , show , check , develop , review , reÂ­ port , outline , consider , investigate , explore , assess , analysis synthesis study , survey , deal , cover } , we consider the first NP following it as the preferred anÂ­ antecedent scores 1 and 0 ) . </a>
<a name="38">[38]</a> <a href="#38" id=38>Evaluation shows a success rate of 89.7 % for the genre of techÂ­ finical manuals and at least in this genre , the approach appears to be more successful than other similar methods . </a>
<a name="39">[39]</a> <a href="#39" id=39>Empirical evidence sugÂ­ tests that because of the salience of the noun phrases which follow them , the verbs listed above are particularly good indicators . </a>
<a name="40">[40]</a> <a href="#40" id=40>Mitkov ( 1998 ) obtains a success rate of 89.7 % for pronominal references , working with English technical manuals . </a>
<a name="41">[41]</a> <a href="#41" id=41>Lexical reiteration Lexically reiterated items are likely candidates for antecedent ( a NP scores 2 if is repeated within the same paragraph twice or more , 1 if repeated once and 0 if not ) . </a>
<a name="42">[42]</a> <a href="#42" id=42>languages An attractive feature of any NLP approach would be its language `` universality '' . </a>
<a name="43">[43]</a> <a href="#43" id=43>Lexically reiterated items include reÂ­ pated synonymous noun phrases which may often be preceded by definite articles or demonstratives . </a>
<a name="44">[44]</a> <a href="#44" id=44>Also , a sequence of noun phrases with the same head counts as lexical reiteration ( e.g . `` toner bottle '' , `` bottle of toner '' , `` the bottle '' ) . </a>
<a name="45">[45]</a> <a href="#45" id=45>While we acknowledge that most of the monolingual NLP approaches are not automatically transferable ( with the same degree of efficiency ) to other languages , it would be highly desirable if this could be done with minimal adaptaÂ­ son . </a>
<a name="46">[46]</a> <a href="#46" id=46>Section heading preference If a noun phrase occurs in the heading of the section , part of which is the current sentence , then we conÂ­ sider it as the preferred candidate ( 1 , 0 ) . </a>
<a name="47">[47]</a> <a href="#47" id=47>Example : Insert the cassette into the VCR making sure iti is suitable for the length of recording . </a>
<a name="48">[48]</a> <a href="#48" id=48>Here `` the VCR '' is penalty -1 ) for being part of the prepositional phrase `` into the VCR '' . </a>
<a name="49">[49]</a> <a href="#49" id=49>The robust approach adapted for Polish demonstrated a high success rate of 93.3 % in resolvÂ­ ing anaphora with critical success rate of 86.2 % ) . </a>
<a name="50">[50]</a> <a href="#50" id=50>This preference can be explained in terms of saliÂ­ enc from the point of view of the centering theory . </a>
<a name="51">[51]</a> <a href="#51" id=51>The latter proposes the ranking `` subject , direct obÂ­ sect indirect object '' ( Brennan et al . 1987 ) and noun phrases which are parts of prepositional phrases are usually indirect objects . </a>
<a name="52">[52]</a> <a href="#52" id=52>Similarly to the evaluation for English , we comÂ­ pared the approach for Polish with ( i ) a Baseline Model which discounts candidates on the basis of agreement in number and gender and , if there were still competing candidates , selects as the antecedent the most recent subject matching the anaphora in gender and number ( ii ) a Baseline Model which checks agreement in number and gender and , if there were still more than one candidate left , picks up as the antecedent the most recent noun phrase that agrees with the anaphora . </a>
<a name="53">[53]</a> <a href="#53" id=53>Collocation pattern preference This preference is given to candidates which have an identical collocation pattern with a pronoun ( 2,0 ) . </a>
<a name="54">[54]</a> <a href="#54" id=54>Our preference-based approach showed clear suÂ­ priority over both baseline models . </a>
<a name="55">[55]</a> <a href="#55" id=55>The collocation preference here is restricted to the patterns `` noun phrase ( pronoun ) , verb '' and `` verb , noun phrase ( pronoun ) '' . </a>
<a name="56">[56]</a> <a href="#56" id=56>Owing to lack of syntactic information , this preference is somewhat weaker than the collocation preference described in ( Dagan & ital 1990 ) . </a>
<a name="57">[57]</a> <a href="#57" id=57>Example : Press the key down and turn the volume up ... </a>
<a name="58">[58]</a> <a href="#58" id=58>Press iti again . </a>
<a name="59">[59]</a> <a href="#59" id=59>Immediate reference In technical manuals the `` immediate reference '' clue can often be useful in identifying the antecedent . </a>
<a name="60">[60]</a> <a href="#60" id=60>We have also adapted and evaluated the approach for Polish ( 93.3 % success rate ) and for Arabic ( 95.2 % success rate ) . </a>
<a name="61">[61]</a> <a href="#61" id=61>We have recently adapted the approach for AraÂ­ bic as well ( Mitkov & Belguith 1998 ) . </a>
<a name="62">[62]</a> <a href="#62" id=62>The heuristics used is that in constructions of the form `` ... ( You ) V 1 NP ... con ( you ) V 2 it ( con ( you ) V3 it ) '' , where con e { and/or/before/after ... } , the noun phrase immediately after V 1 is a very likely candidate for antecedent of the pronoun `` it '' immeÂ­ mediately following V2 and is therefore given preference ( scores 2 and 0 ) . </a>
<a name="63">[63]</a> <a href="#63" id=63>Input is checked against agreement and for a number of antecedent indicators . </a>
<a name="64">[64]</a> <a href="#64" id=64>It is also quite freÂ­ Quent with imperative constructions . </a>
<a name="65">[65]</a> <a href="#65" id=65>Example : To print the paper , you can stand the printer up or lay iti flat . </a>
<a name="66">[66]</a> <a href="#66" id=66>To turn on the printer , press the Power button and hold iti down for a moment . </a>
<a name="67">[67]</a> <a href="#67" id=67>Unwrap the paperiness form iti and align  then load iti into the drawer . </a>
<a name="68">[68]</a> <a href="#68" id=68>Referential distance In complex sentences , noun phrases in the previous clause 2 are the best candidate for the antecedent of an anaphora in the subsequent clause , followed by noun phrases in the previous sentence , then by nouns situated 2 sentences further back and finally nouns 3 sentences further back ( 2 , 1 , 0 , -1 ) . </a>
<a name="69">[69]</a> <a href="#69" id=69>Term preference NPs representing terms in the field are more likely to be the antecedent than NPs which are not terms ( score 1 if the NP is a term and 0 if not ) . </a>
<a name="70">[70]</a> <a href="#70" id=70>Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge . </a>
<a name="71">[71]</a> <a href="#71" id=71>We used the robust approach as a basis for develÂ­ oping a genre-specific reference resolution approach in Polish . </a>
<a name="72">[72]</a> <a href="#72" id=72> 2 1dentification of clauses in complex sentences is do e heuristically . </a>
<a name="73">[73]</a> <a href="#73" id=73>As already mentioned , each of the antecedent inÂ­ indicators assigns a score with a value { -1 , 0 , 1 , 2 } . </a>
<a name="74">[74]</a> <a href="#74" id=74>Top symptoms like `` lexical reiteration '' asÂ­ sign score `` 2 '' whereas `` non-prepositional '' noun phrases are given a negative score of `` -1 '' . </a>
<a name="75">[75]</a> <a href="#75" id=75>Evaluation reports a success rate of 89.7 % which is better than the sucÂ­ less rates of the approaches selected for comparison and tested on the same data . </a></body>
</html>
