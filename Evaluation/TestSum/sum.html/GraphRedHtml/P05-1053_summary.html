<html>
<head><title>P05-1053_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>It also shows that feature-based methods dramatically outperform kernel methods . </a>
<a name="1">[1]</a> <a href="#1" id=1>For details about SVMLight , please see  //svmlight.joachims.org/ </a>
<a name="2">[2]</a> <a href="#2" id=2>Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure , yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted . </a>
<a name="3">[3]</a> <a href="#3" id=3>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="4">[4]</a> <a href="#4" id=4>Evaluation on the ACE corpus shows that our system outperforms Kambhatla ( 2004 ) by about 3 F-measure on extracting 24 ACE relation subtypes . </a>
<a name="5">[5]</a> <a href="#5" id=5>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="6">[6]</a> <a href="#6" id=6>The rest of this paper is organized as follows . </a>
<a name="7">[7]</a> <a href="#7" id=7>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="8">[8]</a> <a href="#8" id=8>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="9">[9]</a> <a href="#9" id=9>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="10">[10]</a> <a href="#10" id=10>The rest of this paper is organized as follows . </a>
<a name="11">[11]</a> <a href="#11" id=11>This category of features includes : â¢ # MB : number of other mentions in between â¢ # WB : number of words in between â¢ M1 > M2 or M1 < M2 : flag indicating whether M2/M1is included in M1/M2 . </a>
<a name="12">[12]</a> <a href="#12" id=12>This category of features includes : â¢ # MB : number of other mentions in between â¢ # WB : number of words in between â¢ M1 > M2 or M1 < M2 : flag indicating whether M2/M1is included in M1/M2 . </a>
<a name="13">[13]</a> <a href="#13" id=13>Dependency tree th parse trees . </a>
<a name="14">[14]</a> <a href="#14" id=14>Dependency tree th parse trees . </a>
<a name="15">[15]</a> <a href="#15" id=15>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="16">[16]</a> <a href="#16" id=16>Kamchatka 2004 ) employed Maximum Entropy models for relation extraction with features derived from word , entity type , mention level , overlap , dependency tree and parse tree . </a>
<a name="17">[17]</a> <a href="#17" id=17>Our study illustrates that the base phrase chunking information contributes to most of the performance improvement from syntactic aspect while additional full parsing information does not contribute much , largely due to the fact that most of relations defined in ACE corpus are within a very short distance . </a>
<a name="18">[18]</a> <a href="#18" id=18>Kamchatka 2004 ) employed Maximum Entropy models for relation extraction with features derived from word , entity type , mention level , overlap , dependency tree and parse tree . </a>
<a name="19">[19]</a> <a href="#19" id=19>The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output . </a>
<a name="20">[20]</a> <a href="#20" id=20>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="21">[21]</a> <a href="#21" id=21>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="22">[22]</a> <a href="#22" id=22>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="23">[23]</a> <a href="#23" id=23>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="24">[24]</a> <a href="#24" id=24>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="25">[25]</a> <a href="#25" id=25>Exploring Various Knowledge in Relation Extraction</a>
<a name="26">[26]</a> <a href="#26" id=26>Compared with Kambhatla ( 2004 ) , we separately incorporate the base phrase chunking information , which contributes to most of the performance improvement from syntactic aspect . </a>
<a name="27">[27]</a> <a href="#27" id=27>Tree kernel-based approaches proposed by Zelenko et al ( 2003 ) and Culotta et al ( 2004 ) are able to explore the implicit feature space without much feature engineering . </a>
<a name="28">[28]</a> <a href="#28" id=28>Tree kernel-based approaches proposed by Zelenko et al ( 2003 ) and Culotta et al ( 2004 ) are able to explore the implicit feature space without much feature engineering . </a>
<a name="29">[29]</a> <a href="#29" id=29>â¢ Chunking features are very useful . </a>
<a name="30">[30]</a> <a href="#30" id=30>â¢ Chunking features are very useful . </a>
<a name="31">[31]</a> <a href="#31" id=31>While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . </a>
<a name="32">[32]</a> <a href="#32" id=32>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="33">[33]</a> <a href="#33" id=33>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="34">[34]</a> <a href="#34" id=34>In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . </a>
<a name="35">[35]</a> <a href="#35" id=35>With the dramatic increase in the amount of textual information available in digital archives and the WWW , there has been growing interest in techniques for automatically extracting information from text . </a>
<a name="36">[36]</a> <a href="#36" id=36>Last , effective ways need to be explored to incorporate information embedded in the full Collins M . </a>
<a name="37">[37]</a> <a href="#37" id=37>Instead of exploring the full parse tree information directly as previous related work , we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance . </a>
<a name="38">[38]</a> <a href="#38" id=38>This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features , which include : â¢ WM1 : bag-of-words in M1 â¢ HM1 : head word of M1 3 In ACE , each mention has a head annotation and an . </a>
<a name="39">[39]</a> <a href="#39" id=39>This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features , which include : â¢ WM1 : bag-of-words in M1 â¢ HM1 : head word of M1 3 In ACE , each mention has a head annotation and an . </a>
<a name="40">[40]</a> <a href="#40" id=40>This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features , which include : â¢ WM1 : bag-of-words in M1 â¢ HM1 : head word of M1 3 In ACE , each mention has a head annotation and an . </a>
<a name="41">[41]</a> <a href="#41" id=41>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="42">[42]</a> <a href="#42" id=42>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="43">[43]</a> <a href="#43" id=43>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="44">[44]</a> <a href="#44" id=44>This feature considers the entity level of both the mentions , which can be NAME , NOMIAL and PRONOUN : â¢ ML12 : combination of mention levels 4.4 Overlap . </a>
<a name="45">[45]</a> <a href="#45" id=45>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="46">[46]</a> <a href="#46" id=46>However , this paper only uses the binary-class version . </a>
<a name="47">[47]</a> <a href="#47" id=47>Basically , SVMs are binary classifiers . </a>
<a name="48">[48]</a> <a href="#48" id=48>Basically , SVMs are binary classifiers . </a>
<a name="49">[49]</a> <a href="#49" id=49>Exploring Various Knowledge in Relation Extraction</a>
<a name="50">[50]</a> <a href="#50" id=50>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="51">[51]</a> <a href="#51" id=51>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="52">[52]</a> <a href="#52" id=52>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="53">[53]</a> <a href="#53" id=53>Two features are defined to include this information : â¢ ET1SC2 : combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype . </a>
<a name="54">[54]</a> <a href="#54" id=54>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="55">[55]</a> <a href="#55" id=55>In ACE vocabulary , entities are objects , mentions are references to them , and relations are semantic relationships between entities . </a>
<a name="56">[56]</a> <a href="#56" id=56>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="57">[57]</a> <a href="#57" id=57>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) . </a>
<a name="58">[58]</a> <a href="#58" id=58>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) . </a>
<a name="59">[59]</a> <a href="#59" id=59>In the future work , we will focus on exploring more semantic knowledge in relation extraction , which has not been covered by current research . </a>
<a name="60">[60]</a> <a href="#60" id=60>In the future work , we will focus on exploring more semantic knowledge in relation extraction , which has not been covered by current research . </a>
<a name="61">[61]</a> <a href="#61" id=61>The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types . </a>
<a name="62">[62]</a> <a href="#62" id=62>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="63">[63]</a> <a href="#63" id=63>The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task . </a>
<a name="64">[64]</a> <a href="#64" id=64>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="65">[65]</a> <a href="#65" id=65>Two features are defined to include this information : â¢ ET1Country : the entity type of M1 when M2 is a country name â¢ CountryET2 : the entity type of M2 when M1 is a country name 5  //ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE : Parent , Grandparent , Spouse , Sibling , Other-Relative and Other- Personal . </a>
<a name="66">[66]</a> <a href="#66" id=66>Compared with Kambhatla ( 2004 ) , we separately incorporate the base phrase chunking information , which contributes to most of the performance improvement from syntactic aspect . </a>
<a name="67">[67]</a> <a href="#67" id=67>This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree . </a>
<a name="68">[68]</a> <a href="#68" id=68>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="69">[69]</a> <a href="#69" id=69>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="70">[70]</a> <a href="#70" id=70>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="71">[71]</a> <a href="#71" id=71>Exploring Various Knowledge in Relation Extraction</a>
<a name="72">[72]</a> <a href="#72" id=72>The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types . </a>
<a name="73">[73]</a> <a href="#73" id=73>Tree kernel-based approaches proposed by Zelenko et al ( 2003 ) and Culotta et al ( 2004 ) are able to explore the implicit feature space without much feature engineering . </a>
<a name="74">[74]</a> <a href="#74" id=74>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) . </a>
<a name="75">[75]</a> <a href="#75" id=75>â¢ The usefulness of mention level features is quite limited . </a>
<a name="76">[76]</a> <a href="#76" id=76>â¢ The usefulness of mention level features is quite limited . </a>
<a name="77">[77]</a> <a href="#77" id=77>This category of features includes : â¢ # MB : number of other mentions in between â¢ # WB : number of words in between â¢ M1 > M2 or M1 < M2 : flag indicating whether M2/M1is included in M1/M2 . </a>
<a name="78">[78]</a> <a href="#78" id=78>Exploring Various Knowledge in Relation Extraction</a>
<a name="79">[79]</a> <a href="#79" id=79>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="80">[80]</a> <a href="#80" id=80>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="81">[81]</a> <a href="#81" id=81>Evaluation on the ACE corpus shows that our system outperforms Kambhatla ( 2004 ) by about 3 F-measure on extracting 24 ACE relation subtypes . </a>
<a name="82">[82]</a> <a href="#82" id=82>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="83">[83]</a> <a href="#83" id=83>The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head . </a>
<a name="84">[84]</a> <a href="#84" id=84>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="85">[85]</a> <a href="#85" id=85>Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE . </a>
<a name="86">[86]</a> <a href="#86" id=86>Section 3 and Section 4 describe our approach and various features employed respectively . </a>
<a name="87">[87]</a> <a href="#87" id=87>According to the scope of the NIST Automatic Content Extraction ( ACE ) program , current research in IE has three main objectives : Entity Detection and Tracking ( EDT ) , Relation Detection and Characterization ( RDC ) , and Event Detection and Characterization ( EDC ) . </a>
<a name="88">[88]</a> <a href="#88" id=88>This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree . </a>
<a name="89">[89]</a> <a href="#89" id=89>8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71 . </a>
<a name="90">[90]</a> <a href="#90" id=90>The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task . </a>
<a name="91">[91]</a> <a href="#91" id=91>â¢ The usefulness of mention level features is quite limited . </a>
<a name="92">[92]</a> <a href="#92" id=92>Information Extraction ( IE ) systems are expected to identify relevant information ( usually of predefined types ) from text documents in a certain domain and put them in a structured format . </a>
<a name="93">[93]</a> <a href="#93" id=93>countries names lists This is to differentiate the Relation subtype âROLE.Citizen-Ofâ , which defines the relationships between a persons and the countries of the personnels citizenship , from other subtypes , especially âROLE.Residenceâ , where defines the relationships between a persons and the locations in which the persons lives . </a></body>
</html>
