<html>
<head><title>C00-2123_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The details are given in ( Och and Ney , 2000 ) . </a>
<a name="1">[1]</a> <a href="#1" id=1>The details are given in ( Och and Ney , 2000 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>The details are given in ( Och and Ney , 2000 ) . </a>
<a name="3">[3]</a> <a href="#3" id=3>The goal of machine translation is the translation of a text given in some source language into a target language . </a>
<a name="4">[4]</a> <a href="#4" id=4>The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series , we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0 , and this number is reported as the number of search errors . </a>
<a name="5">[5]</a> <a href="#5" id=5>We are given a source string fJ 1 = f1 : : : fj : : : fJ of length J , which is to be translated into a target string eI 1 = e1 : : : ei : : : eI of length I . Among all possible target strings , we will choose the string with the highest probability : ^eI 1 = arg max eI 1 fPr ( eI 1jfJ 1 ) g = arg max eI 1 fPr ( eI 1 ) Pr ( fJ 1 jeI 1 ) g : ( 1 ) The argyle operation denotes the search problem , i.e . the generation of the output sentence in the target language . </a>
<a name="6">[6]</a> <a href="#6" id=6>In Section 3 , we introduce our novel concept to word reordering and a DP-based search , which is especially suitable for the translation direction from German to English . </a>
<a name="7">[7]</a> <a href="#7" id=7>The alignment model uses two kinds of parameters : alignment probabilities p ( aj jajô1 I ; J ) , where the probability of alignment aj for position j depends on the previous alignment position ajô1 Ney et al. , 2000 ) and lexicon probabilities p ( fj jean . </a>
<a name="8">[8]</a> <a href="#8" id=8>The alignment model uses two kinds of parameters : alignment probabilities p ( aj jajô1 I ; J ) , where the probability of alignment aj for position j depends on the previous alignment position ajô1 Ney et al. , 2000 ) and lexicon probabilities p ( fj jean . </a>
<a name="9">[9]</a> <a href="#9" id=9>Table 4 shows translation results for the three approaches . </a>
<a name="10">[10]</a> <a href="#10" id=10>Word Re-ordering and DP-based Search in Statistical Machine Translation</a>
<a name="11">[11]</a> <a href="#11" id=11>The algorithm works due to the fact that not all permutations of cities have to be considered explicitly . </a>
<a name="12">[12]</a> <a href="#12" id=12>The alignment mapping is j ! i = aj from source position j to target position i = aj . The use of this alignment model raises major problems if a source word has to be aligned to several target words , e.g . when translating German compound nouns . </a>
<a name="13">[13]</a> <a href="#13" id=13>This measure has the advantage of being completely automatic . </a>
<a name="14">[14]</a> <a href="#14" id=14>A procedural definition to restrict1In the approach described in ( Berger et al. , 1996 ) , a mor pathological analysis is carried out and word morphemes rather than full-form words are used during the search . </a>
<a name="15">[15]</a> <a href="#15" id=15>The quasi-monotone search performs best in terms of both error rates mWER and SSER . </a>
<a name="16">[16]</a> <a href="#16" id=16>The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series , we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0 , and this number is reported as the number of search errors . </a>
<a name="17">[17]</a> <a href="#17" id=17>What is important and is not expressed by the notation is the so-called coverage constraint : each source position j should be hit exactly once by the path of the inverted alignment bI 1 = b1 : : : bi : : : bI . Using the inverted alignments in the maximum approximation , we obtain as search criterion : max I ( p ( JjI ) max eI 1 ( I Yi=1 p ( eijeiô1 ) max bI 1 I Yi=1 [ p ( bijbiô1 I ; J ) p ( fbi jei ) ] ) ) = = max I ( p ( JjI ) max eI 1 ; bI 1 ( I Yi=1 p ( eijeiô1 ) p ( bijbiô1 I ; J ) p ( fbi jei ) ] ) ) ; where the two products over i have been merged into a single product over i. p ( eijeiô1 ) is the trigram language model probability . </a>
<a name="18">[18]</a> <a href="#18" id=18>The alignment model uses two kinds of parameters : alignment probabilities p ( aj jajô1 I ; J ) , where the probability of alignment aj for position j depends on the previous alignment position ajô1 Ney et al. , 2000 ) and lexicon probabilities p ( fj jean . </a>
<a name="19">[19]</a> <a href="#19" id=19>The model is often further restricted so that each source word is assigned to exactly one target word ( Brown et al. , 1993 ; Ney et al. , 2000 ) . </a>
<a name="20">[20]</a> <a href="#20" id=20>An extended lexicon model is defined , and its likelihood is compared to a baseline lexicon model , which takes only single-word dependencies into account . </a>
<a name="21">[21]</a> <a href="#21" id=21>The translation scores for the hypotheses generated with different threshold values t0 are compared to the translation scores obtained with a conservatively large threshold t0 = 10:0 . For each test series , we count the number of sentences whose score is worse than the corresponding score of the test series with the conservatively large threshold t0 = 10:0 , and this number is reported as the number of search errors . </a>
<a name="22">[22]</a> <a href="#22" id=22>We apply a beam search concept as in speech recognition . </a>
<a name="23">[23]</a> <a href="#23" id=23>man 5 . </a>
<a name="24">[24]</a> <a href="#24" id=24>The details are given in ( Och and Ney , 2000 ) . </a></body>
</html>
