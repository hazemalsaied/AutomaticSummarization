<html>
<head><title>N06-2049_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>In this work we used it more delicately . </a>
<a name="1">[1]</a> <a href="#1" id=1>In this work we used it more delicately . </a>
<a name="2">[2]</a> <a href="#2" id=2>Using the CRFs approaches , we prove that it outperformed the character- based method using the CRF approaches . </a>
<a name="3">[3]</a> <a href="#3" id=3>Using the CRFs approaches , we prove that it outperformed the character- based method using the CRF approaches . </a>
<a name="4">[4]</a> <a href="#4" id=4>A new OOV was thus created . </a>
<a name="5">[5]</a> <a href="#5" id=5>In section 2.2 , we proposed a confidence measure approach to reevaluate the results of IOB tagging by combinations of the results of the dictionary-based segmentation . </a>
<a name="6">[6]</a> <a href="#6" id=6>It proves the proposed word-based IOB tagging was very effective . </a>
<a name="7">[7]</a> <a href="#7" id=7>It was first used in Chinese word segmentation by ( Xue and Shen , 2003 ) , where maximum entropy methods were used . </a>
<a name="8">[8]</a> <a href="#8" id=8>It was first used in Chinese word segmentation by ( Xue and Shen , 2003 ) , where maximum entropy methods were used . </a>
<a name="9">[9]</a> <a href="#9" id=9>In the results of the closed test in Bakeoff 2005 ( Emerson , 2005 ) , the work of ( Tseng et al. , 2005 ) , using conditional random fields ( CRF ) for the IOB tagging , yielded very high R-oovs in all of the four corpora used , but the R-iv rates were lower . </a>
<a name="10">[10]</a> <a href="#10" id=10>71 6 0.9 64 0.9 72 Table 2 : Segmentation results by a pure subword-based IOB tagging . </a>
<a name="11">[11]</a> <a href="#11" id=11>The authors appreciate the reviewer effort and good advice for improving the paper . </a>
<a name="12">[12]</a> <a href="#12" id=12>We could yield a better results than those shown in Table 4 using such information . </a>
<a name="13">[13]</a> <a href="#13" id=13>In this section we introduce a confidence measure approach to combine the two results . </a>
<a name="14">[14]</a> <a href="#14" id=14>In the results of the closed test in Bakeoff 2005 ( Emerson , 2005 ) , the work of ( Tseng et al. , 2005 ) , using conditional random fields ( CRF ) for the IOB tagging , yielded very high R-oovs in all of the four corpora used , but the R-iv rates were lower . </a>
<a name="15">[15]</a> <a href="#15" id=15>95 1 Table 4 : Comparison our results with the best ones from Sighan Bakeoff 2005 in terms of F-score ivs than Table 2 . </a>
<a name="16">[16]</a> <a href="#16" id=16>Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</a>
<a name="17">[17]</a> <a href="#17" id=17>Subword-based Tagging by Conditional Random Fields for Chinese Word Segmentation</a>
<a name="18">[18]</a> <a href="#18" id=18>The segmentation results using CRF tagging are shown in Table 2 , where the upper numbers of each slot were produced by the character-based approach while the lower numbers were of the subword-based . </a>
<a name="19">[19]</a> <a href="#19" id=19>In the results of the closed test in Bakeoff 2005 ( Emerson , 2005 ) , the work of ( Tseng et al. , 2005 ) , using conditional random fields ( CRF ) for the IOB tagging , yielded very high R-oovs in all of the four corpora used , but the R-iv rates were lower . </a>
<a name="20">[20]</a> <a href="#20" id=20>By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result , we could achieve the optimal performance . </a>
<a name="21">[21]</a> <a href="#21" id=21>By way of the confidence measure we combined results from the dictionary-based and the IOB-tagging-based and as a result , we could achieve the optimal performance . </a>
<a name="22">[22]</a> <a href="#22" id=22>For AS corpus , âAdam Smithâ are two words in the training but become a one- word in the test , âAdamSmithâ . </a>
<a name="23">[23]</a> <a href="#23" id=23>In addition , we found a clear weakness with the IOB tagging approach : It yields a very low in-vocabulary ( IV ) rate ( R-iv ) in return for a higher out-of-vocabulary ( OOV ) rate ( R-oov ) . </a>
<a name="24">[24]</a> <a href="#24" id=24>It is composed of three parts : a dictionary-based N-gram word segmentation for segmenting IV words , a subword- based tagging by the CRF for recognizing OOVs , and a confidence-dependent word segmentation used for merging the results of both the dictionary-based and the IOB tagging . </a>
<a name="25">[25]</a> <a href="#25" id=25>We defined a cutoff value for each feature type and selected the features with occurrence counts over the cutoff . </a>
<a name="26">[26]</a> <a href="#26" id=26>2.2 Confidence-dependent word segmentation . </a>
<a name="27">[27]</a> <a href="#27" id=27>We downloaded and used the package âCRF++â from the site âhttp //www.chasen.org/Ëtaku/software.â According to the CRFs , the probability of an IOB tag sequence , T = t0 t1 Â· Â· Â· tM , given the word sequence , W = w0 w1 Â· Â· Â· wM , is defined by p ( T |W ) = and current observation ti simultaneously ; gk ( ti , W ) , the epigram feature functions because they trigger only current observation ti . Î»k and Âµk are the model parameters corresponding to feature functions fk and gk respectively . </a>
<a name="28">[28]</a> <a href="#28" id=28>Tri- gram LMs were generated using the SRI LM toolkit for disambiguation . </a>
<a name="29">[29]</a> <a href="#29" id=29>Tri- gram LMs were generated using the SRI LM toolkit for disambiguation . </a></body>
</html>
