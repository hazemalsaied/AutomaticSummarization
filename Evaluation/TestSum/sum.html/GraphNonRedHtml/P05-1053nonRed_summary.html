<html>
<head><title>P05-1053nonRed_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>It also shows that feature-based methods dramatically outperform kernel methods . </a>
<a name="1">[1]</a> <a href="#1" id=1>For details about SVMLight , please see  //svmlight.joachims.org/ </a>
<a name="2">[2]</a> <a href="#2" id=2>Although tree kernel-based approaches facilitate the exploration of the implicit feature space with the parse tree structure , yet the current technologies are expected to be further advanced to be effective for relatively complicated relation extraction tasks such as the one defined in ACE where 5 types and 24 subtypes need to be extracted . </a>
<a name="3">[3]</a> <a href="#3" id=3>For example , we want to determine whether a person is at a location , based on the evidence in the context . </a>
<a name="4">[4]</a> <a href="#4" id=4>Evaluation on the ACE corpus shows that our system outperforms Kambhatla ( 2004 ) by about 3 F-measure on extracting 24 ACE relation subtypes . </a>
<a name="5">[5]</a> <a href="#5" id=5>Based on the structural risk minimization of the statistical learning theory , SVMs seek an optimal separating hyper-plane to divide the training examples into two classes and make decisions based on support vectors which are selected as the only effective instances in the training set . </a>
<a name="6">[6]</a> <a href="#6" id=6>The rest of this paper is organized as follows . </a>
<a name="7">[7]</a> <a href="#7" id=7>The relation extraction task was formulated at the 7th Message Understanding Conference ( MUC7 1998 ) and is starting to be addressed more and more within the natural language processing and machine learning communities . </a>
<a name="8">[8]</a> <a href="#8" id=8>Miller et al ( 2000 ) augmented syntactic full parse trees with semantic information corresponding to entities and relations , and built generative models for the augmented trees . </a>
<a name="9">[9]</a> <a href="#9" id=9>The experiment result also shows that our feature-based approach outperforms the tree kernel-based approaches by more than 20 F-measure on the extraction of 5 ACE relation types . </a>
<a name="10">[10]</a> <a href="#10" id=10>However , this paper only uses the binary-class version . </a>
<a name="11">[11]</a> <a href="#11" id=11>This category of features includes : â¢ # MB : number of other mentions in between â¢ # WB : number of words in between â¢ M1 > M2 or M1 < M2 : flag indicating whether M2/M1is included in M1/M2 . </a>
<a name="12">[12]</a> <a href="#12" id=12>4.1 Words . </a>
<a name="13">[13]</a> <a href="#13" id=13>Dependency tree th parse trees . </a>
<a name="14">[14]</a> <a href="#14" id=14>â¢ To our surprise , incorporating the dependency tree and parse tree features only improve the F- measure by 0.6 and 0.4 respectively . </a>
<a name="15">[15]</a> <a href="#15" id=15>This paper focuses on the ACE RDC task and employs diverse lexical , syntactic and semantic knowledge in feature-based relation extraction using Support Vector Machines ( SVMs ) . </a>
<a name="16">[16]</a> <a href="#16" id=16>Kamchatka 2004 ) employed Maximum Entropy models for relation extraction with features derived from word , entity type , mention level , overlap , dependency tree and parse tree . </a>
<a name="17">[17]</a> <a href="#17" id=17>Our study illustrates that the base phrase chunking information contributes to most of the performance improvement from syntactic aspect while additional full parsing information does not contribute much , largely due to the fact that most of relations defined in ACE corpus are within a very short distance . </a>
<a name="18">[18]</a> <a href="#18" id=18>Head-driven statistical models for natural language parsing . </a>
<a name="19">[19]</a> <a href="#19" id=19>The final decision of an instance in the multiple binary classification is determined by the class which has the maximal SVM output . </a>
<a name="20">[20]</a> <a href="#20" id=20>In this paper , we separate the features of base phrase chunking from those of full parsing . </a>
<a name="21">[21]</a> <a href="#21" id=21>This category of features concerns about the information inherent only in the full parse tree . </a>
<a name="22">[22]</a> <a href="#22" id=22>The reason why we choose SVMs for this purpose is that SVMs represent the state-ofâthe-art in the machine learning research community , and there are good implementations of the algorithm available . </a>
<a name="23">[23]</a> <a href="#23" id=23>The related work mentioned in Section 2 extended to explore the information embedded in the full parse trees . </a>
<a name="24">[24]</a> <a href="#24" id=24>Therefore , the state-of-art full parsing still needs to be further enhanced to provide accurate enough information , especially PP ( Preposition Phrase ) attachment . </a>
<a name="25">[25]</a> <a href="#25" id=25>Exploring Various Knowledge in Relation Extraction</a>
<a name="26">[26]</a> <a href="#26" id=26>Compared with Kambhatla ( 2004 ) , we separately incorporate the base phrase chunking information , which contributes to most of the performance improvement from syntactic aspect . </a>
<a name="27">[27]</a> <a href="#27" id=27>Tree kernel-based approaches proposed by Zelenko et al ( 2003 ) and Culotta et al ( 2004 ) are able to explore the implicit feature space without much feature engineering . </a>
<a name="28">[28]</a> <a href="#28" id=28>This paper will further explore the feature-based approach with a systematic study on the extensive incorporation of diverse lexical , syntactic and semantic information . </a>
<a name="29">[29]</a> <a href="#29" id=29>â¢ Chunking features are very useful . </a>
<a name="30">[30]</a> <a href="#30" id=30>Table 2 also measures the contributions of different features by gradually increasing the feature set . </a>
<a name="31">[31]</a> <a href="#31" id=31>While short-distance relations dominate and can be resolved by simple features such as word and chunking features , the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations . </a>
<a name="32">[32]</a> <a href="#32" id=32>â¢ The usefulness of mention level features is quite limited . </a>
<a name="33">[33]</a> <a href="#33" id=33>Normally , the above overlap features are too general to be effective alone . </a>
<a name="34">[34]</a> <a href="#34" id=34>In this paper , we only model explicit relations because of poor inter-annotator agreement in the annotation of implicit relations and their limited number . </a>
<a name="35">[35]</a> <a href="#35" id=35>With the dramatic increase in the amount of textual information available in digital archives and the WWW , there has been growing interest in techniques for automatically extracting information from text . </a>
<a name="36">[36]</a> <a href="#36" id=36>Last , effective ways need to be explored to incorporate information embedded in the full Collins M . </a>
<a name="37">[37]</a> <a href="#37" id=37>Instead of exploring the full parse tree information directly as previous related work , we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance . </a>
<a name="38">[38]</a> <a href="#38" id=38>This is done by replacing the pronominal mention with the most recent non-pronominal antecedent when determining the word features , which include : â¢ WM1 : bag-of-words in M1 â¢ HM1 : head word of M1 3 In ACE , each mention has a head annotation and an . </a>
<a name="39">[39]</a> <a href="#39" id=39>Section 3 and Section 4 describe our approach and various features employed respectively . </a>
<a name="40">[40]</a> <a href="#40" id=40>â¢ Incorporating the overlap features gives some balance between precision and recall . </a>
<a name="41">[41]</a> <a href="#41" id=41>It shows that our system achieves best performance of 63.1 % /49.5 / 55.5 in precision/recall/F-measure when combining diverse lexical , syntactic and semantic features . </a>
<a name="42">[42]</a> <a href="#42" id=42>5 55 .5 Ka mb hat la ( 20 04 ) : fe feature bas ed 6 3 . </a>
<a name="43">[43]</a> <a href="#43" id=43>While short-distance relations dominate and can be resolved by above simple features , the dependency tree and parse tree features can only take effect in the remaining much less long-distance relations . </a>
<a name="44">[44]</a> <a href="#44" id=44>This feature considers the entity level of both the mentions , which can be NAME , NOMIAL and PRONOUN : â¢ ML12 : combination of mention levels 4.4 Overlap . </a>
<a name="45">[45]</a> <a href="#45" id=45>â¢ Entity type features are very useful and improve the F-measure by 8.1 largely due to the recall increase . </a>
<a name="46">[46]</a> <a href="#46" id=46>2 Joachims has just released a new version of SVMLight . </a>
<a name="47">[47]</a> <a href="#47" id=47>Basically , SVMs are binary classifiers . </a>
<a name="48">[48]</a> <a href="#48" id=48>For efficiency , we apply the one vs. others strategy , which builds K classifiers so as to separate one class from all others , instead of the pairwise strategy , which builds K* ( K-1 ) /2 classifiers considering all pairs of classes . </a>
<a name="49">[49]</a> <a href="#49" id=49>Finally , we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6 . </a>
<a name="50">[50]</a> <a href="#50" id=50>Complicated relation extraction tasks may also impose a big challenge to the modeling approach used by Miller et al ( 2000 ) which integrates various tasks such as part-of-speech tagging , named entity recognition , template element extraction and relation extraction , in a single model . </a>
<a name="51">[51]</a> <a href="#51" id=51>Chang 2004 ) approached relation classification by combining various lexical and syntactic features with bootstrapping on top of Support Vector Machines . </a>
<a name="52">[52]</a> <a href="#52" id=52>It also shows that the ACE RDC task defines some difficult sub- types such as the subtypes âBased-Inâ , âLocatedâ and âResidenceâ under the type âATâ , which are difficult even for human experts to differentiate . </a>
<a name="53">[53]</a> <a href="#53" id=53>Two features are defined to include this information : â¢ ET1SC2 : combination of the entity type of M1 and the semantic class of M2 when M2 triggers a personal social subtype . </a>
<a name="54">[54]</a> <a href="#54" id=54>Table 1 lists the types and subtypes of relations for the ACE Relation Detection and Characterization ( RDC ) task , along with their frequency of occurrence in the ACE training set . </a>
<a name="55">[55]</a> <a href="#55" id=55>In ACE vocabulary , entities are objects , mentions are references to them , and relations are semantic relationships between entities . </a>
<a name="56">[56]</a> <a href="#56" id=56>Support Vector Machines ( SVMs ) are a supervised machine learning technique motivated by the statistical learning theory ( Vapnik 1998 ) . </a>
<a name="57">[57]</a> <a href="#57" id=57>The tree kernels developed in Culotta et al ( 2004 ) are yet to be effective on the ACE RDC task . </a>
<a name="58">[58]</a> <a href="#58" id=58>The RDC task detects and classifies implicit and explicit relations 1 between entities identified by the EDT task . </a>
<a name="59">[59]</a> <a href="#59" id=59>In the future work , we will focus on exploring more semantic knowledge in relation extraction , which has not been covered by current research . </a>
<a name="60">[60]</a> <a href="#60" id=60>Two features are defined to include this information : â¢ ET1Country : the entity type of M1 when M2 is a country name â¢ CountryET2 : the entity type of M2 when M1 is a country name 5  //ilk.kub.nl/~sabine/chunklink/ Personal Relative Trigger Word List This is used to differentiate the six personal social relation subtypes in ACE : Parent , Grandparent , Spouse , Sibling , Other-Relative and Other- Personal . </a>
<a name="61">[61]</a> <a href="#61" id=61>It also shows that our system outperforms tree kernel-based systems ( Culotta et al 2004 ) by over 20 F-measure on extracting 5 ACE relation types . </a>
<a name="62">[62]</a> <a href="#62" id=62>In this paper , we have presented a feature-based approach for relation extraction where diverse lexical , syntactic and semantic knowledge are employed . </a>
<a name="63">[63]</a> <a href="#63" id=63>It is well known that chunking plays a critical role in the Template Relation task of the 7th Message Understanding Conference ( MUC7 1998 ) . </a>
<a name="64">[64]</a> <a href="#64" id=64>Section 2 presents related work . </a>
<a name="65">[65]</a> <a href="#65" id=65>This is largely due to incorporation of two semantic resources , i.e . the country name list and the personal relative trigger word list . </a>
<a name="66">[66]</a> <a href="#66" id=66>For each pair of mentions 3 we compute various lexical , syntactic and semantic features . </a>
<a name="67">[67]</a> <a href="#67" id=67>This category of features includes information about the words , part-of-speeches and phrase labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic full parse tree . </a>
<a name="68">[68]</a> <a href="#68" id=68>It also shows that our fea 1 In ACE (  //www.ldc.upenn.edu/Projects/ACE ) , . </a>
<a name="69">[69]</a> <a href="#69" id=69>The dependency tree is built by using the phrase head information returned by the Collinsâ parser and linking all the other fragments in a phrase to its head . </a>
<a name="70">[70]</a> <a href="#70" id=70>Here , the base phrase chunks are derived from full parse trees using the Perl script 5 written by Sabine Buchholz from Tilburg University and the Collinsâ parser ( Collins 1999 ) is employed for full parsing . </a>
<a name="71">[71]</a> <a href="#71" id=71>Therefore , it would be interesting to see how imperfect EDT affects the performance in relation extraction . </a>
<a name="72">[72]</a> <a href="#72" id=72>Qc 2005 Association for Computational Linguistics ture-based approach outperforms tree kernel-based approaches by 11 F-measure in relation detection and more than 20 F-measure in relation detection and classification on the 5 ACE relation types . </a>
<a name="73">[73]</a> <a href="#73" id=73>This feature concerns about the entity type of both the mentions , which can be PERSON , ORGANIZATION , FACILITY , LOCATION and GeoPolitical Entity or GPE : â¢ ET12 : combination of mention entity types 4.3 Mention Level . </a>
<a name="74">[74]</a> <a href="#74" id=74>Most of the chunking features concern about the head words of the phrases between the two mentions . </a>
<a name="75">[75]</a> <a href="#75" id=75>Similar to word features , three categories of phrase heads are considered : 1 ) the phrase heads in between are also classified into three bins : the first phrase head in between , the last phrase head in between and other phrase heads in between ; 2 ) the phrase heads before M1 are classified into two bins : the first phrase head before and the second phrase head before ; 3 ) the phrase heads after M2 are classified into two bins : the first phrase head after and the second phrase head after . </a>
<a name="76">[76]</a> <a href="#76" id=76>It shows that : Features P R F Words 69.2 23.7 35.3 +Entity Type 67.1 32.1 43.4 +Mention Level 67.1 33.0 44.2 +Overlap 57.4 40.9 47.8 +Chunking 61.5 46.5 53.0 +Dependency Tree 62.1 47.2 53.6 +Parse Tree 62.3 47.6 54.0 +Semantic Resources 63.1 49.5 55.5 Table 2 : Contribution of different features over 43 relation subtypes in the test data â¢ Using word features only achieves the performance of 69.2 % /23.7 /35.3 in precision/recall/F- measure . </a>
<a name="77">[77]</a> <a href="#77" id=77>â¢ ET1DW1 : combination of the entity type and the dependent word for M1 â¢ H1DW1 : combination of the head word and the dependent word for M1 â¢ ET2DW2 : combination of the entity type and the dependent word for M2 â¢ H2DW2 : combination of the head word and the dependent word for M2 â¢ ET12SameNP : combination of ET12 and whether M1 and M2 included in the same NP â¢ ET12SamePP : combination of ET12 and whether M1 and M2 exist in the same PP â¢ ET12SameVP : combination of ET12 and whether M1 and M2 included in the same VP 4.7 Parse Tree . </a>
<a name="78">[78]</a> <a href="#78" id=78>This suggests that relation detection is critical for relation extraction . </a>
<a name="79">[79]</a> <a href="#79" id=79>Yet further research work is still expected to make it effective with complicated relation extraction tasks such as the one defined in ACE . </a>
<a name="80">[80]</a> <a href="#80" id=80>In addition , we distinguish the argument order of the two mentions ( M1 for the first mention and M2 for the second mention ) , e.g . M1-Parent- Of-M2 vs. M2-Parent-Of-M1 . </a>
<a name="81">[81]</a> <a href="#81" id=81>Evaluation on the ACE corpus shows that Detection Error False Negative 462 base phrase chunking contributes to most of the False Positive 165 Table 6 : Distribution of errors 6 Discussion and Conclusion . </a>
<a name="82">[82]</a> <a href="#82" id=82>The effective incorporation of diverse features enables our system outperform previously best- reported systems on the ACE corpus . </a>
<a name="83">[83]</a> <a href="#83" id=83>Information Extraction ( IE ) systems are expected to identify relevant information ( usually of predefined types ) from text documents in a certain domain and put them in a structured format . </a>
<a name="84">[84]</a> <a href="#84" id=84>4.5 Base Phrase Chunking . </a>
<a name="85">[85]</a> <a href="#85" id=85>However , The remaining words that do not have above four classes are manually classified . </a>
<a name="86">[86]</a> <a href="#86" id=86>Evaluation on the ACE RDC task shows that our approach of combining various kinds of evidence can scale better to problems , where we have a lot of relation types with a relatively small amount of annotated data . </a>
<a name="87">[87]</a> <a href="#87" id=87>According to the scope of the NIST Automatic Content Extraction ( ACE ) program , current research in IE has three main objectives : Entity Detection and Tracking ( EDT ) , Relation Detection and Characterization ( RDC ) , and Event Detection and Characterization ( EDC ) . </a>
<a name="88">[88]</a> <a href="#88" id=88>â¢ PTP : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree â¢ PTPH : path of phrase labels ( removing duplicates ) connecting M1 and M2 in the parse tree augmented with the head word of the top phrase in the path . </a>
<a name="89">[89]</a> <a href="#89" id=89>8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71 . </a>
<a name="90">[90]</a> <a href="#90" id=90>In this paper , we use the binary-class SVMLight2 developed by Joachims ( 1998 ) . </a>
<a name="91">[91]</a> <a href="#91" id=91>This suggests that feature-based methods can effectively combine different features from a variety of sources ( e.g . WordNet and gazetteers ) that can be brought to bear on relation extraction . </a>
<a name="92">[92]</a> <a href="#92" id=92>Culotte A. and Sorensen J . </a>
<a name="93">[93]</a> <a href="#93" id=93>countries names lists This is to differentiate the Relation subtype âROLE.Citizen-Ofâ , which defines the relationships between a persons and the countries of the personnels citizenship , from other subtypes , especially âROLE.Residenceâ , where defines the relationships between a persons and the locations in which the persons lives . </a></body>
</html>
