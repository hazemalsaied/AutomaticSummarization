<html>
<head><title>J96-3004nonRed_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>As described in Sproat ( 1995 ) , the Chinese segmented presented here fits directly into the context of a broader finite-state model of text analysis for speech synthesis . </a>
<a name="1">[1]</a> <a href="#1" id=1>Following Sproat and Shih ( 1990 ) , performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90 % range . </a>
<a name="2">[2]</a> <a href="#2" id=2>Church and Hanks [ 1989 ] ) , and we have used lists of character pairs ranked by mutual information to expand our own dictionary . </a>
<a name="3">[3]</a> <a href="#3" id=3>In Chinese text , individual characters of the script , to which we shall refer by their traditional name of Hanni Z are written one after another with no intervening spaces ; a Chinese sentence is shown in Figure 1.3 Partly as a result of this , the notion `` word '' has never played a role in Chinese philological tradition , and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western monologists see DeFrancis ( 1984 ) . </a>
<a name="4">[4]</a> <a href="#4" id=4>For novel texts , no lexicon that consists simply of a list of word entries will ever be entirely satisfactory , since the list will inevitably omit many constructions that should be considered words . </a>
<a name="5">[5]</a> <a href="#5" id=5>A related point is that mutual information is helpful in augmenting existing electronic dictionaries , ( cf . </a>
<a name="6">[6]</a> <a href="#6" id=6>In that work , mutual information was used to decide whether to group adjacent Hanni into two-hanzi words . </a>
<a name="7">[7]</a> <a href="#7" id=7>All notions of word , with the exception of the orthographic word , are as relevant in Chinese as they are in English , and just as is the case in other languages , a word in Chinese may correspond to one or more symbols in the orthographic 1 For a related approach to the problem of word-segrnention in Japanese , see Nagata ( 1994 ) , inter aria </a>
<a name="8">[8]</a> <a href="#8" id=8>For a language like English , this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation . </a>
<a name="9">[9]</a> <a href="#9" id=9>Furthermore , even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus : as Fung and Wu ( 1994 ) have shown , one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented . </a>
<a name="10">[10]</a> <a href="#10" id=10>It has been shown for English ( Wang and Hirschberg 1992 ; Hirschberg 1993 ; Sproat 1994 , inter aria that grammatical part of speech provides useful information for these tasks . </a>
<a name="11">[11]</a> <a href="#11" id=11>Thus in an English sentence such as I 'm going to show up at the ACL one would reasonably conjecture that there are eight words separated by seven spaces . </a>
<a name="12">[12]</a> <a href="#12" id=12>Since the transducers are built from human-readable descriptions using a lexical toolkit ( Sproat 1995 ) , the system is easily maintained and extended . </a>
<a name="13">[13]</a> <a href="#13" id=13>4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B X : Â¥ . : .S : P : l 'How do you say octopus in Japanese ? ' previously shown in Figure 1 . </a>
<a name="14">[14]</a> <a href="#14" id=14>2 Chinese ? l* han 4zi 4 character ' ; this is the same word as Japanese Kania </a>
<a name="15">[15]</a> <a href="#15" id=15>Despite these limitations , a purely finite-state approach to Chinese word segmentation enjoys a number of strong advantages . </a>
<a name="16">[16]</a> <a href="#16" id=16>( See Sproat and Shih 1995 . ) </a>
<a name="17">[17]</a> <a href="#17" id=17>The method just described segments dictionary words , but as noted in Section 1 , there are several classes of words that should be handled that are not found in a standard dictionary . </a>
<a name="18">[18]</a> <a href="#18" id=18>The cost is computed as follows , where N is the corpus size and f is the frequency : ( 1 ) Besides actual words from the base dictionary , the lexicon contains all Hanni in the Big 5 Chinese code with their pronunciation ( s ) , plus entries for other characters that can be found in Chinese text , such as Roman letters , numerals , and special symbols . </a>
<a name="19">[19]</a> <a href="#19" id=19>Purely statistical approaches have not been very popular , and so far as we are aware earlier work by Sproat and Shih ( 1990 ) is the only published instance of such an approach . </a>
<a name="20">[20]</a> <a href="#20" id=20>While size of the resulting transducers may seem daunting-the segmented described here , as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers ( cf . </a>
<a name="21">[21]</a> <a href="#21" id=21>Furthermore , by inverting the transducer so that it maps from phonemic transcriptions to Hanni sequences , one can apply the segmented to other problems , such as speech recognition ( Pereira , Riley , and Sproat 1994 ) . </a>
<a name="22">[22]</a> <a href="#22" id=22>The average agreement among the human judges is .76 , and the average agreement between ST and the humans is .75 , or about 99 % of the inter human One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix , computing a classical metric multidimensional scaling ( Torgerson 1958 ; Becker , Chambers , Wilks 1988 ) on that disÂ­ stance matrix , and plotting the first two most significant dimensions . </a>
<a name="23">[23]</a> <a href="#23" id=23>In ( 1 ) the sequencema 3lu 4 can not be resolved locally , but depends instead upon broader context ; similarly in ( 2 ) , the sequence : : : tcai2neng2 can not be resolved locally : 1 . </a>
<a name="24">[24]</a> <a href="#24" id=24>Similarly , Hanni sharing the GHOST radical _m tend to denote spirits and demons , such as _m gui 3 ' itself , II : mo2 demon ' and yan 3 . ' </a>
<a name="25">[25]</a> <a href="#25" id=25>The use of weighted transducers in particular has the attractive property that the model , as it stands , can be straightforwardly interfaced to other modules of a larger speech or natural language system : presumably one does not want to segment Chinese text for its own sake but instead with a larger purpose in mind . </a>
<a name="26">[26]</a> <a href="#26" id=26>In this paper we present a stochastic finite-state model for segmenting Chinese text into words , both words found in a ( static ) lexicon as well as words derived via the above-mentioned productive processes . </a>
<a name="27">[27]</a> <a href="#27" id=27>More formally , we start by representing the dictionary D as a Weighted Finite State TransÂ­ duce WFST ) ( Pereira , Riley , and Sproat 1994 ) . </a>
<a name="28">[28]</a> <a href="#28" id=28>Unfortunately , there is no standard corpus of Chinese texts , tagged with either single or multiple human judgments , with which one can compare performance of various methods . </a>
<a name="29">[29]</a> <a href="#29" id=29>com Â§Cambridge , UK Email : nc201 eng.cam.ac.uk 1996 Association for Computational Linguistics ( a ) B ) ( , : & ; ? ' H o w d o y o u s a y o c t o p u s i n J a p a n e s e ? ' ( b ) P l a u s i b l e S e g m e n t a t i o n I B X I I 1 : & I 0 0 r i 4 w e n 2 z h a n g l y u 2 z e n 3 m e 0 s h u o l ' J a p a n e s e ' ' o c t o p u s ' ' h o w ' ' s a y ' ( c ) Figure 1 I m p l a u s i b l e S e g m e n t a t i o n [ Â§ ] lxI 1 : & I ri4 wen 2 yu2zen3 shuttle ' essay fish how say A Chinese sentence in ( a ) illustrating the lack of word boundaries . </a>
<a name="30">[30]</a> <a href="#30" id=30>Our System Wang , Li , and Chang a . 1\ ! f ! IP Eflltii /1\ ! f ! J : P $  1til I b. agm : I a m : c. 5 Bf is Bf 1 d. `` * : t : w _t ff 1 `` * : t : w_tff 1 g. , , Transliteration/Translation chen2zhongl-shenl qu3 music by Chen Zhongshen ' huang 2rong 2 de dao 4 Rong said soberly ' change Zhang Qun xian4zhang3 shang 4ren 2 after the county president You Qing had assumed the position ' lin 2 'Lin Quan ' wang 2jian 4 Jian ' oulyang2-ke4 'Ouyang Ke ' yin bu4 ke2neng2 tai 2du 2 because it can not permit Taiwan Independence so ' silfa3-yuan4zhang3 lin2yang2-gang3 president of the Judicial Yuan , Lin Yanggang ' lin2zhangl-hu2 jangly xian 4chang 3 'Lin Zhanghu will give an exÂ­ explanation live ' jin4/iang3 nei 4 xia 4 jinlqian 2 ting 2zhi 3 two years the distributed money will stop ' rectangle ye1zi0 chicken stock , a tablespoon of coconut flakes ' you 2qingl xian 4fu 3 after You Qing headed the county government ' Table 5 Performance on morphological analysis . </a>
<a name="31">[31]</a> <a href="#31" id=31>Word type N % Dic son entries 2 , 5 4 3 9 7 . 4 7 Mor pho log call y derived wor ds 3 0 . 1 1 Fore ign ran rat ons 9 0 . 3 4 Per son al na mes 5 4 2 . 0 7 cases . </a>
<a name="32">[32]</a> <a href="#32" id=32>ogy ( Koskenniemi 1983 ; Antworth 1990 ; Tzoukermann and Liberman 1990 ; Karttunen , Kaplan , and Zaenen 1992 ; Sproat 1992 ) ; we represent the fact that ir , attaches to nouns by allowing t : -transitions from the final states of all noun entries , to the initial state of the sub-WFST representing f , . </a>
<a name="33">[33]</a> <a href="#33" id=33>This WFST is then summed with the WFST implementing the dictionary and morphological rules , and the transitive closure of the resulting transducer is computed ; see Pereira , Riley , and Sproat ( 1994 ) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al . 's Model . </a>
<a name="34">[34]</a> <a href="#34" id=34>The model we use provides a simple framework in which to incorporate a wide variety of lexical information in a uniform way . </a>
<a name="35">[35]</a> <a href="#35" id=35>This architecture provides a uniform framework in which it is easy to incorporate not only listed dictionary entries but also morphological derivatives , and models for personal names and foreign names in transliteration . </a>
<a name="36">[36]</a> <a href="#36" id=36>For example , as Gan ( 1994 ) has noted , one can construct examples where the segmenÂ­ cation is locally ambiguous but can be determined on the basis of sentential or even discourse context . </a>
<a name="37">[37]</a> <a href="#37" id=37>Stochastic approaches have been much more numerÂ­ ous . </a>
<a name="38">[38]</a> <a href="#38" id=38>In this paper we have argued that Chinese word segmentation can be modeled efÂ­ effectively using weighted finite-state transducers . </a>
<a name="39">[39]</a> <a href="#39" id=39>Next , we represent the input sentence as an unweighted finite-state acceptor ( FSA ) I over H. Let us assume the existence of a function Id , which takes as input an FSA A , and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves ( Kaplan and Kay 1994 ) . </a>
<a name="40">[40]</a> <a href="#40" id=40>Since foreign names can be of any length , and since their original pronunciation is effectively unlimited , the identiÂ­ fiction of such names is tricky . </a>
<a name="41">[41]</a> <a href="#41" id=41>Most languages that use Roman , Greek , Cyrillic , Armenian , or Semitic scripts , and many that use Indian-derived scripts , mark orthographic word boundaries ; however , languages written in a Chinese-derived writÂ­ ing system , including Chinese and Japanese , as well as Indian-derived writing systems of languages like Thai , do not delimit orthographic words. 1 Put another way , written Chinese simply lacks orthographic words . </a>
<a name="42">[42]</a> <a href="#42" id=42>As a first step towards modeling transliterated names , we have collected all Hanni occurring more than once in the roughly 750 foreign names in our dictionary , and we estimate the probabilÂ­ ity of occurrence of each Hanni in a transliteration ( pTN ( Hanni ; ) ) using the maximum likelihood estimate . </a>
<a name="43">[43]</a> <a href="#43" id=43>Fortunately , there are only a few hundred Hanni that are particularly common in transliterations ; indeed , the commonest ones , such as E. bal , m er3 , and iij al are often clear indicators that a sequence of Hanni containing them is foreign : even a name like ! : i*m xia4mi3-er3 'Shamir , ' which is a legal ChiÂ­ nest personal name , retains a foreign flavor because of liM . </a>
<a name="44">[44]</a> <a href="#44" id=44>Chinese word segmentation can be viewed as a stochastic transduction problem . </a>
<a name="45">[45]</a> <a href="#45" id=45>For a sequence of Hanni that is a possible name , we wish to assign a probability to that sequence qua name . </a>
<a name="46">[46]</a> <a href="#46" id=46>For example , suppose one is building a ITS system for Mandarin Chinese . </a>
<a name="47">[47]</a> <a href="#47" id=47>Making the reasonable assumption that similar information is relevant for solving these problems in Chinese , it follows that a prerequisite for intonation-boundary assignment and prominence assignment is word segmentation . </a>
<a name="48">[48]</a> <a href="#48" id=48>Nonetheless , the results of the comparison with human judges demonstrates that there is mileage being gained by incorporating models of these types of words . </a>
<a name="49">[49]</a> <a href="#49" id=49>There is a sizable literature on Chinese word segmentation : recent reviews include Wang , Su , and Mo ( 1990 ) and Wu and Tseng ( 1993 ) . </a>
<a name="50">[50]</a> <a href="#50" id=50>A totally nonÂ­ stochastic rule-based system such as Wang , Li , and Chang 's will generally succeed in such cases , but of course runs the risk of over generation wherever the single-hanzi word is really intended . </a>
<a name="51">[51]</a> <a href="#51" id=51>This flexibility , along with the simplicity of implementation and expansion , makes this framework an attractive base for continued research . </a>
<a name="52">[52]</a> <a href="#52" id=52>Similarly , there is no compelling evidence that either of the syllables of sniffles betel represents a morpheme , since neither can occur in any context without the other : more likely fjfflll is a bisyllabic morpheme . </a>
<a name="53">[53]</a> <a href="#53" id=53>However , there is again local grammatical information that should favor the split in the case of ( 1a ) : both .ma3 ' and .ma3 are nouns , but only .ma3 consistent with the classifier pil , the classifier for horses. 2 1 By a similar argument , the preference for not splitting , lm could be strengthened in ( lb ) by the observation that the classifier 1'1* is consistent with long or winding objects like , lm ma3lu4 ' but not with , ma3 horse ' </a>
<a name="54">[54]</a> <a href="#54" id=54>Other strategies could readily 6 As a reviewer has pointed out , it should be made clear that the function for computing the best path is . an instance of the Viterbi algorithm . </a>
<a name="55">[55]</a> <a href="#55" id=55>16 As one reviewer points out , one problem with the epigram model chosen here is that there is still a. tendency to pick a segmentation containing fewer words . </a>
<a name="56">[56]</a> <a href="#56" id=56>In this way , the method reported on here will necessarily be similar to a greedy method , though of course not identical . </a>
<a name="57">[57]</a> <a href="#57" id=57>G1 and G2 are Hanni we can estimate the probability of the sequence being a name as the product of : â¢ the probability that a word chosen randomly from a text will be a name-p ( rule 1 ) , and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p ( rule 2 ) , and â¢ the probability that the family name is the particular Hanni ( rule 6 ) , and â¢ the probability that the given name consists of the particular Hanni and G2-p ( rule 9 ) This model is essentially the one proposed in Chang et al . </a>
<a name="58">[58]</a> <a href="#58" id=58>Other kinds of productive word classes , such as company names , abbreviations ( termed fijsuolxie3 in Mandarin ) , and place names can easily be 20 Note that 7 in E 7 is normally pronounced as leO , but as part of a resultant it is liao 3.. </a>
<a name="59">[59]</a> <a href="#59" id=59>Affix Pron Base category N found N missed ( recall ) N correct ( precision ) t , -,7 The second issue is that rare family names can be responsible for over generation especially if these names are otherwise common as single-hanzi words . </a>
<a name="60">[60]</a> <a href="#60" id=60>Several papers report the use of part-of-speech information to rank segmentations ( Lin , Chiang , and Su 1993 ; Peng and Chang 1993 ; Chang and Chen 1993 ) ; typically , the probability of a segmentation is multiplied by the probability of the tagging ( s ) for that segmentation to yield an estimate of the total probability for the analysis . </a>
<a name="61">[61]</a> <a href="#61" id=61>including Third Tone Sandhi ( Shih 1986 ) , which changes a 3 ( low ) tone into a 2 ( rising ) tone before another 3 tone : 'j '' ; gil , xiao 3 lao 3 ] little rat , ' becomes xiao 3 lao2shu3 , rather than xiao 2 lao2shu3 , because the rule first applies within the word lao3shu3 , ' blocking its phrasal application . </a>
<a name="62">[62]</a> <a href="#62" id=62>In various dialects of Mandarin certain phonetic rules apply at the word . </a>
<a name="63">[63]</a> <a href="#63" id=63>As noted , this sentence consists of four words , namely B X ri4wen2 , ' : Â¥ , zhanglyu 2 : & P : l zen 3me 0 , ' and IDt shuttle . ' </a>
<a name="64">[64]</a> <a href="#64" id=64>For that application , at a minimum , one would want to know the phonological word boundaries . </a>
<a name="65">[65]</a> <a href="#65" id=65>Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis ; such applications involve problems as diverse as machine translation , information retrieval , and text-to-speech synthesis ( TIS ) . </a>
<a name="66">[66]</a> <a href="#66" id=66>( See also Wu and Fung [ 1994 ] . ) </a>
<a name="67">[67]</a> <a href="#67" id=67>It may seem surprising to some readers that the inter human agreement scores reported here are so low . </a>
<a name="68">[68]</a> <a href="#68" id=68>each word in the lexicon whether or not each string is actually an instance of the word in question . </a>
<a name="69">[69]</a> <a href="#69" id=69>However , there will remain a large number of words that are not readily adduced to any producÂ­ tie pattern and that would simply have to be added to the dictionary . </a>
<a name="70">[70]</a> <a href="#70" id=70>A minimal requirement for building a Chinese word segmented is obviously a dictionary ; furthermore , as has been argued persuasively by Fung and Wu ( 1994 ) , one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented . </a>
<a name="71">[71]</a> <a href="#71" id=71>As we have seen , the lexicon of basic words and stems is represented as a WFST ; most arcs in this WFST represent mappings between Hanni and pronunciations , and are costless . </a>
<a name="72">[72]</a> <a href="#72" id=72>Finally , asÂ­ summing a simple digram model , we can derive the probability estimate for the particular unseen word iÂ¥1J1l . </a>
<a name="73">[73]</a> <a href="#73" id=73>Gan 's solution depends upon a fairly sophisticated language model that attempts to find valid syntactic , semantic , and lexical relations between objects of various linguistic types ( Hanni words , phrases ) . </a>
<a name="74">[74]</a> <a href="#74" id=74>There are thus some very good reasons why segmentation into words is an important task . </a>
<a name="75">[75]</a> <a href="#75" id=75>7 Big 5 is the most popular Chinese character coding standard in use in Taiwan and Hong Kong . </a>
<a name="76">[76]</a> <a href="#76" id=76>4.5 Transliterations of Foreign Words . </a>
<a name="77">[77]</a> <a href="#77" id=77>This WFST represents the segmentation of the text into the words AB and CD , word boundaries being marked by arcs mapping between f and part-of-speech labels . </a>
<a name="78">[78]</a> <a href="#78" id=78>Turning now to ( 1 ) , we have the similar problem that splitting.into.ma 3 ' andlu 4 ' is more costly than retaining this as one word .ma3lu4 . ' </a>
<a name="79">[79]</a> <a href="#79" id=79>As indicated in Figure 1 ( c ) , apart from this correct analysis , there is also the analysis taking B ri4 as a word ( e.g. , a common abbreviation for Japan ) , along with X : Â¥ wen2zhangl and f ! ! . </a>
<a name="80">[80]</a> <a href="#80" id=80>logical rules , and personal names ; the transitive closure of the resulting machine is then computed . </a>
<a name="81">[81]</a> <a href="#81" id=81>We have argued that the proposed method performs well . </a>
<a name="82">[82]</a> <a href="#82" id=82>However , there is a strong relationship between ni 1s and the number of Hanni in the class . </a>
<a name="83">[83]</a> <a href="#83" id=83>One class comprises words derived by productive morphologiÂ­ cal processes , such as plural noun formation using the suffix ir , menD . </a>
<a name="84">[84]</a> <a href="#84" id=84>A greedy algorithm ( or maximum-matching algorithm ) , GR : proceed through the sentence , taking the longest match with a dictionary entry at each point . </a>
<a name="85">[85]</a> <a href="#85" id=85>This larger corpus was kindly provided to us by United Informatics Inc. , R.O.C . a set of initial estimates of the word frequencies. 9 In this re-estimation procedure only the entries in the base dictionary were used : in other words , derived words not in the base dictionary and personal and foreign names were not used . </a>
<a name="86">[86]</a> <a href="#86" id=86>On the other hand , in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English . </a>
<a name="87">[87]</a> <a href="#87" id=87>Since the segmentation corresponds to the sequence of words that has the lowest summed epigram cost , the segmented under discussion here is a zeroth-order model . </a>
<a name="88">[88]</a> <a href="#88" id=88>constitute names , since we have only their segmentation , not the actual classification of the segmented words . </a>
<a name="89">[89]</a> <a href="#89" id=89>orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words . </a></body>
</html>
