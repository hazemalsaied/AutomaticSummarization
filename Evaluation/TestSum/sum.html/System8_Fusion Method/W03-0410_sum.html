<html>
<head><title>W03-0410_sum</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="1">[1]</a> <a href="#1" id=1>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="2">[2]</a> <a href="#2" id=2>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="3">[3]</a> <a href="#3" id=3>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="4">[4]</a> <a href="#4" id=4>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="5">[5]</a> <a href="#5" id=5>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="6">[6]</a> <a href="#6" id=6>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="7">[7]</a> <a href="#7" id=7>We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</a>
<a name="8">[8]</a> <a href="#8" id=8>We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</a>
<a name="9">[9]</a> <a href="#9" id=9>In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to âthe curse of dimensionalityâ?</a>
<a name="10">[10]</a> <a href="#10" id=10>In this paper, we report results on several feature selection approaches to the problem manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</a>
<a name="11">[11]</a> <a href="#11" id=11>Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</a>
<a name="12">[12]</a> <a href="#12" id=12>Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</a>
<a name="13">[13]</a> <a href="#13" id=13>Here we briefly describe the features that comprise our feature space, and refer the interested reader to Joanis and Stevenson (2003) for details.</a>
<a name="14">[14]</a> <a href="#14" id=14>We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).</a>
<a name="15">[15]</a> <a href="#15" id=15>Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</a>
<a name="16">[16]</a> <a href="#16" id=16>Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</a>
<a name="17">[17]</a> <a href="#17" id=17>Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</a>
<a name="18">[18]</a> <a href="#18" id=18>Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</a>
<a name="19">[19]</a> <a href="#19" id=19>Of these verbs, 20 from each class were randomly selected to use as training data for our supervised experiments in Joanis and Stevenson (2003).</a>
<a name="20">[20]</a> <a href="#20" id=20>We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.</a>
<a name="21">[21]</a> <a href="#21" id=21>We use three separate evaluation measures, that tap into very different properties of the clusterings.</a>
<a name="22">[22]</a> <a href="#22" id=22>Then accuracy has the standard definition2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.</a>
<a name="23">[23]</a> <a href="#23" id=23>Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.</a>
<a name="24">[24]</a> <a href="#24" id=24>The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).</a>
<a name="25">[25]</a> <a href="#25" id=25>Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.</a>
<a name="26">[26]</a> <a href="#26" id=26>Following Joanis and Stevenson (2003), for each class, we systematically identified the subset of features 4 These results differ slightly from those reported in Joanis and Stevenson (2003), because of our slight changes in verb sets, discussed in Section 3.2.</a>
<a name="27">[27]</a> <a href="#27" id=27>We performed a number of experiments in which we tested the performance of each feature set from cardinality 1 to the total number of features, where each set of size differs from the set of size in the addition of the feature with next highest rank (according to the proposed entropy measure).</a>
<a name="28">[28]</a> <a href="#28" id=28>Schulte im Walde and Brew (2002) and Schulte im Walde (2003), on the other hand, use a larger set of features intended to be useful for a broad number of classes, as in our work.</a></body>
</html>
