<html>
<head><title>C04-1089.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service. The translations of 6 of the 43 words are words in the dictionary (denoted as ÃḃÂÂcomm.ÃḃÂÂ in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as ÃḃÂÂinsuffÃḃÂÂ). For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates. lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. New words such as person names, organization names, technical terms, etc. appear frequently. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998). For the training of transliteration probability, we required a ChineseEnglish name list. It also made use of part-of-speech tag information, whereas</a></body>
</html>
