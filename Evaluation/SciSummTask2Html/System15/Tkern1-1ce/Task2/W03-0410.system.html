<html>
<head><title>W03-0410.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. We conclude with a discussion of related work, our contributions, and future directions. However, it is important to find a method that does not depend on having an existing classification, since we are interested in applying the approach when such a classification does not exist. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). However, the semi- supervised approach (using a seed set of sample verbs) overall outperforms not only the full set of features, but the hand-selected features as well. We used the simple Euclidean distance for the former, and Ward linkage for the latter. In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to ÃḃÂÂthe curse of dimensionalityÃḃÂÂ? In performing hierarchical clustering, both a vector distance measure and a cluster distance (ÃḃÂÂlinkageÃḃÂÂ) measure are specified. However, a general</a></body>
</html>
