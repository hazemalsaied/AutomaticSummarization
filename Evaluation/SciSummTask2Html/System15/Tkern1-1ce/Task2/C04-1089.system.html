<html>
<head><title>C04-1089.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The translations of 6 of the 43 words are words in the dictionary (denoted as ÃḃÂÂcomm.ÃḃÂÂ in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as ÃḃÂÂinsuffÃḃÂÂ). As pointed out earlier, most previous research only considers either transliteration or context information in determining the translation of a source language word w, but not both sources of information. They combined the two sources of information by weighting the two individual scores, whereas we made use of the average rank for combination. We evaluated our approach on six months of Chinese and English Gigaword corpora, with encouraging results. For example, various news agencies report major world events in different languages, and such news documents form a readily available source of comparable corpora. So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. New words such as person names, organization names, technical terms, etc. appear frequently. We thank Jia Li for implementing the EM algorithm to train transliteration probabilities. It also made use of part-of-speech tag information, whereas our method is simpler and does not require part-of- speech tagging. For each period, we selected those English words occurring at least 10 times and were not present in the 10,000-word ChineseEnglish</a></body>
</html>
