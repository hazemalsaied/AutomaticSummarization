<html>
<head><title>C04-1089.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>So we use the the context of c , we are likely to retrieve the context of e when we use the context of c as query C(c) to retrieve a document C (e* ) that * the query and try to retrieve the most similar best matches the query. The English Gigaword corpus consists of news from four newswire services: Agence France Press English Service, Associated Press Worldstream English Service, New York Times Newswire Service, and Xinhua News Agency English Service. The translations of 6 of the 43 words are words in the dictionary (denoted as ÃḃÂÂcomm.ÃḃÂÂ in Table 3) and 4 of the 43 words appear less than 10 times in the English part of the corpus (denoted as ÃḃÂÂinsuffÃḃÂÂ). lected as follows: For each occurrence of c, we set a window of size 50 characters centered at c. We discarded all the Chinese words in the context that were not in the dictionary we used. For a Chinese source word occurring within a half- month period p, we looked for its English translation candidate words occurring in news documents in the same period p. 5.3 Translation candidates. For the training of transliteration probability, we required a ChineseEnglish name list. In this approach, a language model is derived from each document D . Then the probability of generating the query the machine transliteration method proposed by Q according to that language model, P(Q | D) , (Knight and Graehl, 1998). They combined the two sources of information by</a></body>
</html>
