<html>
<head><title>W03-0410.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments. Then for all verbs , consider to be classified correctly if Class( )=ClusterLabel( ), where Class( ) is the actual class of and ClusterLabel( ) is the label assigned to the cluster in which is placed. Each set of verbs was judged (by the authorsÃḃÂÂ intuition alone) to be ÃḃÂÂrepresentativeÃḃÂÂ of the class. In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features). Essentially, the measure numerically captures what we can intuitively grasp in the visual differences between the dendrograms of ÃḃÂÂbetterÃḃÂÂ and ÃḃÂÂworseÃḃÂÂ clusterings. In performing hierarchical clustering, both a vector distance measure and a cluster distance (ÃḃÂÂlinkageÃḃÂÂ) measure are specified. We are indebted to Allan Jepson for helpful discussions and suggestions. We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. We calculate the mean silhouette of all points in a clustering to obtain an overall measure of how well the clusters are separated. The feature set was previously shown to work well in a supervised learning setting,</a></body>
</html>
