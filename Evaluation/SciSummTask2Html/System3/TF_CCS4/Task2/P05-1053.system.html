<html>
<head><title>P05-1053.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus. </a>
<a name="1">[1]</a> <a href="#1" id=1>Table 5 separates the performance of relation detection from overall performance on the testing set. </a>
<a name="2">[2]</a> <a href="#2" id=2>It only improves the F-measure by 0.8 due to the recall increase. </a>
<a name="3">[3]</a> <a href="#3" id=3>Many machine learning methods have been proposed to address this problem, e.g., supervised learning algorithms. </a>
<a name="4">[4]</a> <a href="#4" id=4>For the choice of features, we use the full set of features from Zhou et al.. </a>
<a name="5">[5]</a> <a href="#5" id=5>we call the features used in Zhou et al. flat feature set. </a>
<a name="6">[6]</a> <a href="#6" id=6>Table 2 also measures the contributions of different features by gradually increasing the feature set. </a>
<a name="7">[7]</a> <a href="#7" id=7>Feature-based methods  for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features. </a>
<a name="8">[8]</a> <a href="#8" id=8>This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. </a>
<a name="9">[9]</a> <a href="#9" id=9>This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. </a>
<a name="10">[10]</a> <a href="#10" id=10>1 5 3. 7 6 2. 3 Manag ement 1 6 5 1 0 6 7 2 59. </a>
<a name="11">[11]</a> <a href="#11" id=11>9 2 2. 9 3 6. 4 P A R T 1 6 4 1 0 6 3 9 73. </a>
<a name="12">[12]</a> <a href="#12" id=12>6 4 5. 9 5 8. 2 Locate d 2 4 1 1 3 2 1 2 0 52. </a>
<a name="13">[13]</a> <a href="#13" id=13>['9', '2', '8.', '8', '4']</a></body>
</html>
