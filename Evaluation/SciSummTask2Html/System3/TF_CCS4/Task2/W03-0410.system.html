<html>
<head><title>W03-0410.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We find that manual selection of a subset of features based on the known classification performs better than using a full set of noisy features, demonstrating the potential benefit of feature selection in our task. </a>
<a name="1">[1]</a> <a href="#1" id=1>As successful as our seed set of features is, it still does not achieve the accuracy of a supervised learner. </a>
<a name="2">[2]</a> <a href="#2" id=2>We instead proposed a semi-supervised method in which a seed set of verbs is chosen for training a supervised classifier, from which the useful features are extracted for use in clustering. </a>
<a name="3">[3]</a> <a href="#3" id=3>Supervised methods for automatic verb classification have been extensively investigated. </a>
<a name="4">[4]</a> <a href="#4" id=4>Table 1 shows our results and the results of Stevenson and Joanis  on T1 when employing AGG using Ward as the linkage criterion. </a>
<a name="5">[5]</a> <a href="#5" id=5>In recent work, Stevenson and Joanis, which used an entropy measure to organize data into a multidimensional space. </a>
<a name="6">[6]</a> <a href="#6" id=6>Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research. </a>
<a name="7">[7]</a> <a href="#7" id=7>Furthermore, we might question the clustering approach itself, in the context of verb class discovery. </a>
<a name="8">[8]</a> <a href="#8" id=8>In moving to a scenario of verb class discovery, using clustering, we face the problem of having a large number of irrelevant features for a particular clustering task. </a>
<a name="9">[9]</a> <a href="#9" id=9>Performance differences may be due to the types of features, or due to the number or size of classes. </a></body>
</html>
