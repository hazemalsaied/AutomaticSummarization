<html>
<head><title>J96-3004.system.</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The Chinese word segmentation is a nontrivial task because no explicit delimiters. </a>
<a name="1">[1]</a> <a href="#1" id=1>The Chinese person-name model is a modified version of that described in Sproat et al.. </a>
<a name="2">[2]</a> <a href="#2" id=2>A previous work along this line is Sproat et al.. </a>
<a name="3">[3]</a> <a href="#3" id=3>It is rule-based, but relies on 2 See, for example, Sproat et al. </a>
<a name="4">[4]</a> <a href="#4" id=4>Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation. </a>
<a name="5">[5]</a> <a href="#5" id=5>Much previous research on Chinese language processing focused on word segmentation. </a>
<a name="6">[6]</a> <a href="#6" id=6>Conventionally a word segmentation process identifies the words in input text by matching lexical entries and resolving the ambiguous matching. </a>
<a name="7">[7]</a> <a href="#7" id=7>Many natural language models can be captured by weighted finite-state transducers, which offer several benefits:â€¢ WFSTs provide a uniform knowledge represen tation. </a>
<a name="8">[8]</a> <a href="#8" id=8>In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer. </a>
<a name="9">[9]</a> <a href="#9" id=9>also proposed another method to estimate a set of initial word frequencies without segmenting the corpus. </a>
<a name="10">[10]</a> <a href="#10" id=10>The model described here thus demonstrates great potential for use in widespread applications. </a>
<a name="11">[11]</a> <a href="#11" id=11>In Chinese text segmentation there are three basic approaches : pure heuristic, pure statistical, and a hybrid of the two. </a>
<a name="12">[12]</a> <a href="#12" id=12>There are several commonly used segmentation methods such as forward maximum matching and backward maximum matching. </a>
<a name="13">[13]</a> <a href="#13" id=13>We used a maximum- matching algorithm and a dictionary compiled from the CTB  to do segmentation </a>
<a name="14">[14]</a> <a href="#14" id=14>['Using', 'the', '495', 'characters', 'that', 'are', 'frequently', 'used', 'for', 'transliterating', 'foreign', 'names,', 'a', 'sequence', 'of', 'three', 'of', 'more']</a></body>
</html>
