<html>
<head><title>X96-1048_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Any participant in a future MUC evaluation faces the challenge of providing a named entity identification capability that would score in the 90th percentile on the F-measure on a task such as the MUC6 one . </a>
<a name="1">[1]</a> <a href="#1" id=1>Nearly half the sites chose to participate in all four tasks , and all but one site participated in at least one SGML task and one extraction task . </a>
<a name="2">[2]</a> <a href="#2" id=2>Documentation of each of the tasks and summary scores for all systems evaluated can be found in the MUC6 proceedings [ 1 ] . </a>
<a name="3">[3]</a> <a href="#3" id=3>( BBN system ) Table 5 . </a>
<a name="4">[4]</a> <a href="#4" id=4>Many of the sites have emphasized their pattern-matching techniques in discussing the strengths of their MUC6 systems . </a>
<a name="5">[5]</a> <a href="#5" id=5>About half the systems focused only on individual co reference which has direct relevance to the other MUC6 evaluation tasks . </a>
<a name="6">[6]</a> <a href="#6" id=6>This period comprised the `` evaluation epoch . '' </a>
<a name="7">[7]</a> <a href="#7" id=7>COREFERENCE The task as defined for MUC6 was restricted to noun phrases ( NPs ) and was intended to be limited to phenomena that were relatively noncontroversial and easy to describe . </a>
<a name="8">[8]</a> <a href="#8" id=8>An algorithm developed by the MITRE Corporation for MUC6 was implemented by SAIC and used for scoring the task . </a>
<a name="9">[9]</a> <a href="#9" id=9>Finally , a change in administration of the MUC evaluations is occurring that will bring fresh ideas . </a>
<a name="10">[10]</a> <a href="#10" id=10>All but two of the systems posted F-measure scores in the 7080 % range , and four of the systems were able to achieve recall in the 7080 % range while maintaining precision in the 8090 % range , as shown in the figure 4 . </a>
<a name="11">[11]</a> <a href="#11" id=11>As indicated in table 2 , all systems performed better on identifying person names than on identifying organization or location names , and all but a few systems performed better on location names than on organization names . </a>
<a name="12">[12]</a> <a href="#12" id=12>One of the innovations of MUC6 was to formalize the general structure of event templates , and all three scenarios defined in the course of MUC6 conformed to that general structure . </a>
<a name="13">[13]</a> <a href="#13" id=13>This capability has other useful applications as well , e.g. , it enables text highlighting in a browser . </a>
<a name="14">[14]</a> <a href="#14" id=14>CORPUS Testing was conducted using Wall Street Journal texts provided by the Linguistic Data Consortium . </a>
<a name="15">[15]</a> <a href="#15" id=15>Management Succession Template Structure intentional and is comparable to the richness of the MUC3 `` TST2 '' test set and the MUC4 `` TST4 '' test set . </a>
<a name="16">[16]</a> <a href="#16" id=16>All the participating sites also submitted systems for evaluation on the TE and NE tasks . </a>
<a name="17">[17]</a> <a href="#17" id=17>Statistically , large differences of up to 15 points may not be reflected as a difference in the ranking of the systems . </a>
<a name="18">[18]</a> <a href="#18" id=18>For generation of an ORGANIZATION object , the text must provide either the name ( full or part ) or a descriptor of the organization . </a>
<a name="19">[19]</a> <a href="#19" id=19>OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION </a></body>
</html>
