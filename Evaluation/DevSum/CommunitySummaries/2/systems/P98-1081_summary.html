<html>
<head><title>P98-1081_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Also of note is the improvement yielded by the best combination . </a>
<a name="1">[1]</a> <a href="#1" id=1>A next step is to examine them in pairs . </a>
<a name="2">[2]</a> <a href="#2" id=2>van Halteren ( ed . ) </a>
<a name="3">[3]</a> <a href="#3" id=3>van Halteren 1996 ) . </a>
<a name="4">[4]</a> <a href="#4" id=4>This was suspected to be the main reason for the relative lack of performance by the more sophisticated combiners . </a>
<a name="5">[5]</a> <a href="#5" id=5>Compare this to the `` tune '' set in van Halteren , Zavrel , and Daelemans ( 1998 ) . </a>
<a name="6">[6]</a> <a href="#6" id=6>After comparison, their outputs are combined using several voting strategies and second stage classifiers.</a>
<a name="7">[7]</a> <a href="#7" id=7>Traditionally , these models were categorized as either rule-based/symbolic or corpus-based/probabilistic . </a>
<a name="8">[8]</a> <a href="#8" id=8>In this paper , we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance . </a>
<a name="9">[9]</a> <a href="#9" id=9>5 The most straightforward selection method is an n-way vote . </a>
<a name="10">[10]</a> <a href="#10" id=10>This is much easier and can quickly lead to a model which produces results with a reasonably good quality . </a>
<a name="11">[11]</a> <a href="#11" id=11>As we now apply the methods of van Halteren , Zavrel , and Daelemans ( 1998 ) to WSJ as well , it is easier to make a comparison . </a>
<a name="12">[12]</a> <a href="#12" id=12>Improving Data Driven Wordclass Tagging by System Combination</a>
<a name="13">[13]</a> <a href="#13" id=13>After comparison , their outputs are combined using several voting strategies and second stage classifiers . </a>
<a name="14">[14]</a> <a href="#14" id=14>When used on Test , the pairwise voting strategy ( TagPair ) clearly outperforms the other voting strategies , 8 but does not yet approach the level where all tying majority votes are handled correctly ( 98.31 % ) . </a>
<a name="15">[15]</a> <a href="#15" id=15>The first is the LOB corpus ( Johansson 1986 ) , which we used in the earlier experiments as well ( van Halteren , Zavrel , and Daelemans 1998 ) and which has proved to be a good testing ground . </a>
<a name="16">[16]</a> <a href="#16" id=16>For our experiment , we divide the corpus into three parts . </a>
<a name="17">[17]</a> <a href="#17" id=17>The most important observation is that every combination ( significantly ) outperforms the combination of any strict subset of its components . </a>
<a name="18">[18]</a> <a href="#18" id=18>the Maximum Entropy tagger ( 97.43 % ) . </a>
<a name="19">[19]</a> <a href="#19" id=19>Its tagging , which was manually checked and corrected , is generally accepted to be quite accurate . </a>
<a name="20">[20]</a> <a href="#20" id=20>First of all , tagging is a widely researched and well-understood task ( cf . </a>
<a name="21">[21]</a> <a href="#21" id=21>In van Halteren , Zavrel , and Daelemans ( 1998 ) we used a straightforward imÂ­ implementation of HMM 's , which turned out to have the worst accuracy of the four competing methods . </a>
<a name="22">[22]</a> <a href="#22" id=22>To realize the benefits of stacking , either more data is needed or a second stage classifier that is better suited to this type of problem . </a>
<a name="23">[23]</a> <a href="#23" id=23>The accuracy measurements for all of them are listed in Table 2 . </a>
<a name="24">[24]</a> <a href="#24" id=24>Our experiment shows that , at least for the task at hand , combination of several different systems allows us to raise the performance ceiling for data driven systems . </a>
<a name="25">[25]</a> <a href="#25" id=25>The most important result that has undergone a change between van Halteren , Zavrel , and Daelemans ( 1998 ) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL . </a>
<a name="26">[26]</a> <a href="#26" id=26>6In our experiment , a random selection from among the winning tags is made whenever there is a tie . </a>
<a name="27">[27]</a> <a href="#27" id=27>Data driven methods appear to be the more popular . </a>
<a name="28">[28]</a> <a href="#28" id=28>Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material . </a>
<a name="29">[29]</a> <a href="#29" id=29>The patterns between the brackets give the distribution of incorrect tags over the systems . </a>
<a name="30">[30]</a> <a href="#30" id=30>During the training phase , cases containing information about the word , the context and the correct tag are stored in memory . </a>
<a name="31">[31]</a> <a href="#31" id=31>For part-of-speech tagging , a significant increase in accuracy through combining the output of different taggers was first demonstrated in van Halteren , Zavrel , and Daelemans ( 1998 ) and Brill and Wu ( 1998 ) . </a>
<a name="32">[32]</a> <a href="#32" id=32>Here we use a slight adaptation of the tagger . </a>
<a name="33">[33]</a> <a href="#33" id=33>All combination taggers outperform their best component , with the best combination showing a 19.1 % lower error rate than the best individual tagger . </a>
<a name="34">[34]</a> <a href="#34" id=34>Four well-known tagger generators (Hidden Markov Model, Memory-Based, Transformation Rules and Maximum Entropy) are trained on the same corpus data.</a>
<a name="35">[35]</a> <a href="#35" id=35>tokens , but , because of a 92.5 % agreement over all four taggers , it yielded less than 9K tokens of useful training material to resolve disagreements . </a>
<a name="36">[36]</a> <a href="#36" id=36>The third and final part , Test , consists of the remaining 10 % ( .115101 tokens ) and is used for the final performance measurements of all taggers . </a>
<a name="37">[37]</a> <a href="#37" id=37>It ought therefore to be advantageous to step away from the underlying mechanism of voting and to model the situations observed in Tune more closely . </a>
<a name="38">[38]</a> <a href="#38" id=38>Component taggers In 1992 , van Halteren combined a number of taggers by way of a straightforward majority vote ( cf . </a>
<a name="39">[39]</a> <a href="#39" id=39>In both approaches , different tagger genÂ­ aerators were applied to the same training data and their predictions combined using different combination methods , including stacking . </a>
<a name="40">[40]</a> <a href="#40" id=40>The data in Train ( for individual taggers and Tune ( for combination taggers is to be the only information used in tagger construction : all components of all taggers lexicon , context statistics , etc . ) are to be entirely data driven and no manual adjustments are to be done . </a>
<a name="41">[41]</a> <a href="#41" id=41>The Viterbi algorithm is used to determine the most probable tag sequence . </a>
<a name="42">[42]</a> <a href="#42" id=42>A beam search is then used to find the highest probability tag sequence . </a>
<a name="43">[43]</a> <a href="#43" id=43>The changes are mainly cosmetic , e.g . non-alphabetic characters such as `` $ '' in tag names have been replaced . </a>
<a name="44">[44]</a> <a href="#44" id=44>Where TagPair used to be significantly better than MBL , the roles are now well reversed . </a>
<a name="45">[45]</a> <a href="#45" id=45>For this experiment we have selected four systems , primarily on the basis of availability . </a>
<a name="46">[46]</a> <a href="#46" id=46>Catnapping 1996 on such effects in the Penn Treebank corpus ) . </a>
<a name="47">[47]</a> <a href="#47" id=47>Table 2 : Accuracy of individual taggers and combination methods . </a>
<a name="48">[48]</a> <a href="#48" id=48>With LOB and a single 114K tune set ( van Halteren , Zavrel , and Daelemans 1998 ) , both MBL and Decision Trees degraded significantly when adding context , and MBL degraded when adding the word </a></body>
</html>
