<html>
<head><title>N01-1011_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>Finally , note that the smallest decision trees are functionally equivalent to our benchmark methods . </a>
<a name="1">[1]</a> <a href="#1" id=1>Learning continues until all the training examples are accounted for by the decision tree . </a>
<a name="2">[2]</a> <a href="#2" id=2>during the association between two words , while the decision tree seeks bi grams that partition instances of the ambiguous word into into distinct senses . </a>
<a name="3">[3]</a> <a href="#3" id=3>A win shows it was more accurate than the method in the column , a loss means it was less accurate , and a tie means it was equally accurate . </a>
<a name="4">[4]</a> <a href="#4" id=4>Word sense disambiguation is the process of selecting the most appropriate meaning for a word , based on the context in which it occurs . </a>
<a name="5">[5]</a> <a href="#5" id=5>Decision trees have been used in supervised learning approaches to word sense disambiguation , and have fared well in a number of comparative studies ( e.g. , ( Mooney , 1996 ) , ( Pedersen and Bruce , 1997 ) ) . </a>
<a name="6">[6]</a> <a href="#6" id=6>Note that the parts of speech are encoded as n for noun , a for adjective , v for verb , and p for words where the part of speech was not provided . </a>
<a name="7">[7]</a> <a href="#7" id=7>The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2 . </a>
<a name="8">[8]</a> <a href="#8" id=8>The counts in n +1 and n 1+ indicate how often words big and cat occur as the ? rst and second words of any digram in the corpus . </a>
<a name="9">[9]</a> <a href="#9" id=9>A preliminary version of this paper appears in ( Pedersen , 2001 ) . </a></body>
</html>
