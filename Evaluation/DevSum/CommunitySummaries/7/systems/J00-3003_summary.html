<html>
<head><title>J00-3003_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>( Stolcke et al. , 2000 ) use HMMs for dialogue modelling , where sequences of observations correspond to sequences of dialog act types . </a>
<a name="1">[1]</a> <a href="#1" id=1>It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( Dermatas and Kokkinakis 1995 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>Applying Bayes ' rule we get U* = argmaxP ( UIE ) U P ( U ) P ( ElU ) = argument u P ( E ) = argmaxP ( U ) P ( ElU ) ( 1 ) U Here P ( U ) represents the prior probability of a DA sequence , and P ( EIU ) is the like- Table 4 Summary of random variables used in dialog modeling . </a>
<a name="3">[3]</a> <a href="#3" id=3>As described later , this involves considering multiple alternative recognized word sequences . </a>
<a name="4">[4]</a> <a href="#4" id=4>To make both the modeling and the search for the best DA sequence feasible , we further require that our likelihood models are decomposable by utterance . </a>
<a name="5">[5]</a> <a href="#5" id=5>Returning to the prior distribution of DA sequences P ( U ) , it is convenient to make certain independence assumptions here , too . </a>
<a name="6">[6]</a> <a href="#6" id=6>When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar , it will therefore find precisely the DA sequence with the highest posterior probability : U* = argmaxP ( UIE ) ( 4 ) u The combination of likelihood and prior modeling , HMMs , and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition ( Bahl , Jelinek , and Mercer 1983 ) and tagging ( Church 1988 ) . </a>
<a name="7">[7]</a> <a href="#7" id=7>Constraints on the likely sequence of dialog acts are modeled via a dialog act n-gram . </a>
<a name="8">[8]</a> <a href="#8" id=8>To date , the majority of work on dialog act modeling has addressed spoken dialogue ( Samuel et al. , 1998 ; Stolcke et al. , 2000 ; Surendran and Levow , 2006 ; Bangalore et al. , 2008 ; Sridhar et al. , 2009 ; Di Eugenio et al. , 2010 ) . </a>
<a name="9">[9]</a> <a href="#9" id=9>Initial work was done unspoken interactions ( see for example ( Stolcke et al.,2000 ) ) . </a>
<a name="10">[10]</a> <a href="#10" id=10>A more efficient , though mathematically less accurate , solution can be obtained by combining guesses about the correct DA types directly at the level of the LM . </a>
<a name="11">[11]</a> <a href="#11" id=11>The problem with applying Equation 11 , of course , is that the DA type Ui is generally not known ( except maybe in applications where the user interface can be engineered to allow only one kind of DA for a given utterance ) . </a>
<a name="12">[12]</a> <a href="#12" id=12>We have developed an integrated probabilistic approach to dialog act modeling for conversational speech , and tested it on a large speech corpus . </a>
<a name="13">[13]</a> <a href="#13" id=13>We develop a probabilistic integration of speech recognition with dialog modeling , to improve both speech recognition and dialog act classification accuracy . </a>
<a name="14">[14]</a> <a href="#14" id=14>The relation between utterances and speaker turns is not one-to-one : a single turn can contain multiple utterances , and utterances can span more than one turn ( e.g. , in the case of channeling by the other speaker in utterance . </a>
<a name="15">[15]</a> <a href="#15" id=15>A back channel is a short utterance that plays discourse-structuring roles , e.g. , indicating that the speaker should go on talking . </a>
<a name="16">[16]</a> <a href="#16" id=16>We expect recognition of channels to be useful because of their discourse-structuring role ( knowing that the hearer expects the speaker to go on talking tells us something about the course of the narrative ) and because they seem to occur at certain kinds of syntactic boundaries ; detecting a back channel may thus help in predicting utterance boundaries and surrounding lexical material . </a>
<a name="17">[17]</a> <a href="#17" id=17>The following table shows examples of channels in the context of a Switchboard conversation : Speaker Dialogue Act Utterance B STATEMENT but , uh , we 're to the point now where our financial income is enough that we can consider putting some away - A BACKCHANNEL Uh-huh . </a>
<a name="18">[18]</a> <a href="#18" id=18>Therefore , we need to infer the likely DA types for each utterance , using available evidence E from the entire conversation . </a>
<a name="19">[19]</a> <a href="#19" id=19>Woszczyna and Waibel ( 1994 ) , for example , trained an ergodic HMM using expectation-maximization to model speech act sequencing . </a>
<a name="20">[20]</a> <a href="#20" id=20>DA modeling has mostly been geared toward automatic DA classification , and much less work has been done on applying DA models to automatic speech recognition . </a>
<a name="21">[21]</a> <a href="#21" id=21>In related work DAs are used as a first processing step to infer dialog games ( Carlson 1983 ; Levin and Moore 1977 ; Levin et al . 1999 ) , a slightly higher level unit that comprises a small number of DAs . </a>
<a name="22">[22]</a> <a href="#22" id=22>1 % What did you wear to work today ? </a>
<a name="23">[23]</a> <a href="#23" id=23>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech</a>
<a name="24">[24]</a> <a href="#24" id=24>Conversational feedback is mostly performance short utterances such as yeah , mh , okay produced by the main speaker but by one of the other participants of a conversation . </a>
<a name="25">[25]</a> <a href="#25" id=25>Such utterances are among the most frequent in conversational data ( Stolcke et al. , 2000 ) . </a>
<a name="26">[26]</a> <a href="#26" id=26>We trained standard back off models ( Katz 1987 ) , using the frequency smoothing approach of Witten and Bell ( 1991 ) . </a>
<a name="27">[27]</a> <a href="#27" id=27>The relatively small improvements from higher-order models could be a result of lack of training data , or of an inherent independence of DAs from DAs further removed . </a>
<a name="28">[28]</a> <a href="#28" id=28>However , this does not seem to be true for DA sequences in our corpus , as the cache model showed no improvement over the standard N-gram . </a>
<a name="29">[29]</a> <a href="#29" id=29>By representing a higher level intention of utterance human conversation , dialog act labels are being used to enrich the information provided spoken words ( Stolcke et al. , 2000 ) . </a>
<a name="30">[30]</a> <a href="#30" id=30>They also explore the performance with decision trees and neural networks and report their highest accuracy at 65 % on the Switchboard corpus . </a>
<a name="31">[31]</a> <a href="#31" id=31>Dialog act ( DA ) annotations and tagging , inspired by the speech act theory of Austin ( 1975 ) and Searle ( 1976 ) , have been used in the NLP community to understand and model dialog . </a>
<a name="32">[32]</a> <a href="#32" id=32>The ability to model and automatically detect discourse structure is an important step toward understanding spontaneous dialog . </a>
<a name="33">[33]</a> <a href="#33" id=33>While there is hardly consensus on exactly how discourse structure should be described , some agreement exists that a useful first level of analysis involves the identification of dialog acts ( DAs ) . </a>
<a name="34">[34]</a> <a href="#34" id=34>A DA represents the meaning of an utterance at the level of elocutionary force ( Austin 1962 ) . </a>
<a name="35">[35]</a> <a href="#35" id=35>To train our statistical models on this corpus , we combined an extensive effort in human hand-coding of DAs for each utterance , with a variety of automatic and semiautomatic tools . </a>
<a name="36">[36]</a> <a href="#36" id=36>2.1 Utterance Segmentation . </a>
<a name="37">[37]</a> <a href="#37" id=37>We refer to the units of this segmentation as utterances . </a>
<a name="38">[38]</a> <a href="#38" id=38>In an automatic labeling of word boundaries as either utterance or boundaries using a combination of lexical and prosodic cues , we obtained 96 % accuracy based on correct word transcripts , and 78 % accuracy with automatically recognized words . </a>
<a name="39">[39]</a> <a href="#39" id=39>A total of 1,155 Switchboard conversation were labeled , comprising 205,000 utterance and 1.4 million words . </a></body>
</html>
