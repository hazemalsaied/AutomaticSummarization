<html>
<head><title>N01-1011_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>They perform a general to spec c search of a feature space , adding the most informative features to a tree structure as the search proceeds . </a>
<a name="1">[1]</a> <a href="#1" id=1>The objective is to select a minimal set of features that eÆciently partitions the feature space into classes of observations and assemble them into a tree . </a>
<a name="2">[2]</a> <a href="#2" id=2>A decision stump is a one node decision tree ( Holte , 1993 ) that is created by stopping the decision tree learner after the single most informative feature is added to the tree . </a>
<a name="3">[3]</a> <a href="#3" id=3>Learning continues until all the training examples are accounted for by the decision tree . </a>
<a name="4">[4]</a> <a href="#4" id=4>In general , such a tree will be overly spec c to the training data and not generalize well to new examples . </a>
<a name="5">[5]</a> <a href="#5" id=5>Each feature selected during the search process is represented by a node in the learned decision tree . </a>
<a name="6">[6]</a> <a href="#6" id=6>Column 8 shows the accuracy of the decision tree using the J48 learning algorithm and the features identify ed by a power divergence statistic . </a>
<a name="7">[7]</a> <a href="#7" id=7>Column 10 shows the accuracy of the decision tree when the Dice CoeÆcient selects the features . </a>
<a name="8">[8]</a> <a href="#8" id=8>The most dramatic difference occurred with amaze-v , where the SENSE- VAL average was 92.4 % and the decision tree accuracy was 58.6 % . </a>
<a name="9">[9]</a> <a href="#9" id=9>A Decision Tree of Bigrams is an Accurate Predictor of Word Sense</a>
<a name="10">[10]</a> <a href="#10" id=10>This paper presents a corpus-based approach to word sense disambiguation where a decision tree assigns a sense to an ambiguous word based on the bigrams that occur nearby.</a>
<a name="11">[11]</a> <a href="#11" id=11>Word sense disambiguation is the process of selecting the most appropriate meaning for a word , based on the context in which it occurs . </a>
<a name="12">[12]</a> <a href="#12" id=12>There is a further assumption that each feature is conditionally independent of all other features , given the sense of the ambiguous word . </a>
<a name="13">[13]</a> <a href="#13" id=13>It is most often used with a bag of words feature set , where every word in the training sample is represented by a binary feature that indicates whether or not it occurs in the window of context surrounding the ambiguous word . </a>
<a name="14">[14]</a> <a href="#14" id=14>In general the total context available for each ambiguous word is less than 100 surrounding words . </a>
<a name="15">[15]</a> <a href="#15" id=15>Decision tree and decision stump learning is performed twice , once using the feature set determined by the power divergence statistic and again using the feature set identify ed by the Dice CoeÆcient . </a>
<a name="16">[16]</a> <a href="#16" id=16>Decision trees are among the most widely used machine learning algorithms . </a>
<a name="17">[17]</a> <a href="#17" id=17>We included all 36 tasks from SENSEVAL for which training and test data were provided . </a>
<a name="18">[18]</a> <a href="#18" id=18>For example , there were two tasks associated with bet , one for its use as a noun and the other as a verb . </a>
<a name="19">[19]</a> <a href="#19" id=19>For our purposes it is assumed that the set of possible meanings , i.e. , the sense inventory , has already been determined . </a>
<a name="20">[20]</a> <a href="#20" id=20>For example , suppose bill has the following set of possible meanings : a piece of currency , pending legislation , or a bird jaw . </a>
<a name="21">[21]</a> <a href="#21" id=21>When used in the context of The Senate bill is under consideration , a human reader immediately understands that bill is being used in the legislative Sense . </a></body>
</html>
