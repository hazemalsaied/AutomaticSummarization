<html>
<head><title>H89-2014_model</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The model has the advantage that a pre-tagged training corpus is not required . </a>
<a name="1">[1]</a> <a href="#1" id=1>The work described here also makes use of a hidden Markov model . </a>
<a name="2">[2]</a> <a href="#2" id=2>In this regard , word equivalence classes were used ( Kupiec , 1989 ) . </a>
<a name="3">[3]</a> <a href="#3" id=3>There it is assumed that the distribution of the use of a word depends on the set of categories it can assume , and words are partitioned accordingly . </a>
<a name="4">[4]</a> <a href="#4" id=4>An alternative to uniformly increasing the order of the conditioning is to extend it selectively . </a>
<a name="5">[5]</a> <a href="#5" id=5>Mixed higher- order context can be modeled by introducing explicit state sequences . </a>
<a name="6">[6]</a> <a href="#6" id=6>In this regard , word equivalence classes were used ( Kupiec , 1989 ) . </a>
<a name="7">[7]</a> <a href="#7" id=7>There it is assumed that the distribution of the use of a word depends on the set of categories it can assume , and words are partitioned accordingly . </a>
<a name="8">[8]</a> <a href="#8" id=8>In a ranked list of words in the corpus the most frequent 100 words account for approximately 50 % of the total tokens in the corpus , and thus data is available to estimate them reliably . </a>
<a name="9">[9]</a> <a href="#9" id=9>The most frequent 100 words of the corpus were assigned individually in the model , thereby enabling them to have different distributions over their categories . </a>
<a name="10">[10]</a> <a href="#10" id=10>Mixed higher- order context can be modeled by introducing explicit state sequences . </a>
<a name="11">[11]</a> <a href="#11" id=11>In the arrangement the basic first-order network remains , permitting all possible category sequences , and modeling first-order dependency . </a>
<a name="12">[12]</a> <a href="#12" id=12>The basic network is then augmented with the extra state sequences which model certain category sequences in more detail . </a>
<a name="13">[13]</a> <a href="#13" id=13>A model containing all of the refinements described , was tested using a magazine article containing 146 sentences ( 3,822 words ) . </a>
<a name="14">[14]</a> <a href="#14" id=14>A 30,000 word dictionary was used , supplemented by inflectional analysis for words not found directly in the dictionary . </a>
<a name="15">[15]</a> <a href="#15" id=15>A stochastic method for assigning part-of-speech categories to unrestricted English text has been described . </a></body>
</html>
