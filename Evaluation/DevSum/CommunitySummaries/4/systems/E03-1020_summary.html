<html>
<head><title>E03-1020_summary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The quality of the Markov clustering depends strongly on several parameters such as a granularity factor and the size of the local graph . </a>
<a name="1">[1]</a> <a href="#1" id=1>We gathered a list of nouns with varying degree of ambiguity , from homonym e.g . arms ) to systematic polygamy e.g . cherry ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>We used the simple graph model based on co-occurrences of nouns in lists ( cf . </a>
<a name="3">[3]</a> <a href="#3" id=3>The class-labelling ( step 6 ) is accomplished using the taxonomic structure of WordNet , using a robust algorithm developed specially for this purpose . </a>
<a name="4">[4]</a> <a href="#4" id=4>there are other related efforts on word sense discrimination ( Dorow and Widdows , 2003 ; Fukumoto and Suzuki , 1999 ; Pedersen and Bruce , 1997 ) . </a>
<a name="5">[5]</a> <a href="#5" id=5>Then senses of target word were iteratively learned by clustering the local graph of similar words around target word . </a>
<a name="6">[6]</a> <a href="#6" id=6>Their algorithm required a threshold as input , which controlled the number of senses . </a>
<a name="7">[7]</a> <a href="#7" id=7>The algorithm in ( Dorow and Widdows , 2003 ) represented target noun word , its neighbors and their relationships using a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times . </a>
<a name="8">[8]</a> <a href="#8" id=8>As they rely on the detection of high-density areas in a network of cooccurrences , ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) are the closest methods to ours . </a>
<a name="9">[9]</a> <a href="#9" id=9>Instead we link each word to its top n neighbors where n can be determined by the user ( cf . </a>
<a name="10">[10]</a> <a href="#10" id=10>1 Si ample cutoff functions proved unsatisfactory because of the bias they give to more frequent words . </a>
<a name="11">[11]</a> <a href="#11" id=11>However , there are ambiguous words with more closely related senses which are metaphorical or metonymy variations of one another . </a>
<a name="12">[12]</a> <a href="#12" id=12>Usually , one sense of an ambiguous word w is much more frequent than its other senses present in the corpus . </a>
<a name="13">[13]</a> <a href="#13" id=13>Therefore , even after removal of the wing-node , the two areas of meaning are still linked via tail . </a>
<a name="14">[14]</a> <a href="#14" id=14>In our case , we chose a more general approach by working at the level of a simiÂ­larity graph when the similarity of two words is given by their relation of cooccurrence , our situaÂ­tion is comparable to the one of ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) </a>
<a name="15">[15]</a> <a href="#15" id=15>From a global viewpoint , these two differences lead ( VÃ©ronis , 2003 ) and ( Dorow and Widdows , 2003 ) to build finer senses than ours . </a>
<a name="16">[16]</a> <a href="#16" id=16>Similar to the approach as presented in ( Dorow and Widdows , 2003 ) we construct a word graph . </a>
<a name="17">[17]</a> <a href="#17" id=17>Discrimination against previously extracted sense clusters enables us to discover new senses . </a>
<a name="18">[18]</a> <a href="#18" id=18>This is achieved , in a manner similar to Pantel and Lin 's ( 2002 ) sense clustering approach , by removing c 's features from the set of features used for finding similar words . </a>
<a name="19">[19]</a> <a href="#19" id=19>Doro and Widdows construct a graph for a target word w by taking the sub-graph induced by the neighborhood of w ( without w ) and clustering it with MCL . </a>
<a name="20">[20]</a> <a href="#20" id=20>sz44 CD miltrA , literate h , ) Cik Figure 1 : local graph of the Word mouse </a></body>
</html>
