<html>
<head><title>N01-1011_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We believe that decision trees meet these criteria . </a>
<a name="1">[1]</a> <a href="#1" id=1>A preliminary version of this paper appears in ( Pedersen , 2001 ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>Each sense { tagged occurrence of an ambiguous word is converted into a feature vector , where each feature represents some property of the surrounding text that is considered to be relevant to the disambiguation process . </a>
<a name="3">[3]</a> <a href="#3" id=3>Rather , we will search for bi grams where the component words may be separated by other words in the text . </a>
<a name="4">[4]</a> <a href="#4" id=4>These typically include the part { of { speech of surrounding words , the presence of certain key words within some window of context , and various syntactic properties of the sentence and the ambiguous word . </a>
<a name="5">[5]</a> <a href="#5" id=5>Then the decision tree learning algorithm is described , as are some benchmark learning algorithms that are included for purposes of comparison . </a>
<a name="6">[6]</a> <a href="#6" id=6>However , ( Cressie and Read , 1984 ) suggest that there are cases where Pearson 's statistic is more reliable than the likelihood ratio and that one test should not always be preferred over the other . </a>
<a name="7">[7]</a> <a href="#7" id=7>Decision Tree have been used in supervised learning approaches to Word Sense disambiguation , and have fared well in a number of comparative studies ( e.g . , ( Mooney , 1996 ) , ( Pedersen and Bruce , 1997 ) ) . </a></body>
</html>
