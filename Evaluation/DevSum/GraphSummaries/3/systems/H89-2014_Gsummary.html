<html>
<head><title>H89-2014_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>The work described here also makes use of a hidden Markov model . </a>
<a name="1">[1]</a> <a href="#1" id=1>The `` augmented network '' uniquely models all second-order dependencies of the type determiner -noun - X , and determiner -adjective -X ( X ranges over { cl ... cn } ) . </a>
<a name="2">[2]</a> <a href="#2" id=2>This has the disadvantage that they can not share training data . </a>
<a name="3">[3]</a> <a href="#3" id=3>The training corpus was a collection of electronic mail messages concerning the design of the Common-Lisp programming language -a somewhat less than ideal representation of English . </a>
<a name="4">[4]</a> <a href="#4" id=4>Only context has been supplied to aid the training procedure , and the latter is responsible for deciding which alternative is more likely , based on the training data . </a>
<a name="5">[5]</a> <a href="#5" id=5>The former represents 95.6 % correct word tagging on the text as a whole ( ignoring unknown words ) , and 89 % on the ambiguous words . </a>
<a name="6">[6]</a> <a href="#6" id=6>As in the previous section , the corrections are not programmed into the model . </a>
<a name="7">[7]</a> <a href="#7" id=7>To model such dependency across the phrase , the networks shown in Figure 2 can be used . </a>
<a name="8">[8]</a> <a href="#8" id=8>The basic network can not model the dependency of the number of the verb on its subject , which precedes it by a prepositional phrase . </a>
<a name="9">[9]</a> <a href="#9" id=9>Equivalence classes { Eqvl ... Eqvm } replace the words { wl ... Wv } ( m < < v ) and P ( Eqvi I Ci ) replace the parameters P ( Wi I Ci ) . </a>
<a name="10">[10]</a> <a href="#10" id=10>This leaves 50 % of the corpus for training all the other equivalence classes . </a>
<a name="11">[11]</a> <a href="#11" id=11>In a ranked list of words in the corpus the most frequent 100 words account for approximately 50 % of the total tokens in the corpus , and thus data is available to estimate them reliably . </a>
<a name="12">[12]</a> <a href="#12" id=12>A 30,000 word dictionary was used , supplemented by inflectional analysis for words not found directly in the dictionary . </a>
<a name="13">[13]</a> <a href="#13" id=13>The basic model tagged these sentences correctly , except for- `` range '' and `` rises '' which were tagged as noun and plural-noun respectively 1 . </a>
<a name="14">[14]</a> <a href="#14" id=14>In practice , word context provides significant constraint , so the trade-off appears to be a remarkably favorable one . </a>
<a name="15">[15]</a> <a href="#15" id=15>The replacement of the auxiliary category by the following categories greatly improved this : Category Name Words included in Category Be be Been been Being being Have have Have* has , have , had , having be* is , am , are , was , were do* do , does , did modal Modal auxiliaries Unique Equivalence Classes for Common Words Common words occur often enough to be estimated reliably . </a>
<a name="16">[16]</a> <a href="#16" id=16>In this regard , word equivalence classes were used ( Kupiec , 1989 ) . </a>
<a name="17">[17]</a> <a href="#17" id=17>Methods have ranged from locally-operating rules ( Greene and Rubin , 1971 ) , to statistical methods ( Church , 1989 ; DeRose , 1988 ; Garside , Leech and Sampson , 1987 ; Jelinek , 1985 ) and back-propagation ( Benello , Mackie and Anderson , 1989 ; Nakamura and Shikano , 1989 ) . </a>
<a name="18">[18]</a> <a href="#18" id=18>It would be appropriate to deal with idioms separately , as done by Gaxside , Leech and Sampson ( 1987 ) . </a></body>
</html>
