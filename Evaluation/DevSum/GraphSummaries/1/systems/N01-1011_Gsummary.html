<html>
<head><title>N01-1011_Gsummary</title> </head>
<body bgcolor="white">
<a name="0">[0]</a> <a href="#0" id=0>We believe that decision trees meet these criteria . </a>
<a name="1">[1]</a> <a href="#1" id=1>Given the sparse and skewed nature of this data , the statistical methods used to select interesting bi grams must be carefully chosen . </a>
<a name="2">[2]</a> <a href="#2" id=2>Our empirical study utilizes the training and test data from the 1998 SENSEVAL evaluation of word sense disambiguation systems . </a>
<a name="3">[3]</a> <a href="#3" id=3>Decision trees have been used in supervised learning approaches to word sense disambiguation , and have fared well in a number of comparative studies ( e.g. , ( Mooney , 1996 ) , ( Pedersen and Bruce , 1997 ) ) . </a>
<a name="4">[4]</a> <a href="#4" id=4>Bi grams have been used as features for word sense disambiguation , particularly in the form of collocations where the ambiguous word is one component of the digram e.g. , ( Bruce and Wiebe , 1994 ) , ( Ng and Lee , 1996 ) , ( Yarowsky , 1995 ) ) . </a>
<a name="5">[5]</a> <a href="#5" id=5>Then the decision tree learning algorithm is described , as are some benchmark learning algorithms that are included for purposes of comparison . </a>
<a name="6">[6]</a> <a href="#6" id=6>The Bigram Statistics Package has been implemented by Satanjeev Banerjee , who is supported by a Grant { in { Aid of Research , Artistry and Scholarship from the OÃ†ce of the Vice President for Research and the Dean of the Graduate School of the University of Minnesota . </a>
<a name="7">[7]</a> <a href="#7" id=7>The evaluation at SENSEVAL was based on precision and recall , so we converted those scores to accuracy by taking their product . </a></body>
</html>
