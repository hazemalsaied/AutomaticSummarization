Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.
Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
One of the disadvantages of developing a knowledgeÂ­ based system, however, is that it is a very labourÂ­ intensive and time-consuming task.
This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
This paper presÂ­ ents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
Input is checked against agreement and for a number of antecedent indicators.
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
For the most part, anaphora resolution has focused on traditional linguistic methods (Carbonell & Brown 1988; Carter 1987; Hobbs 1978; Ingria & Stallard 1989; Lappin & McCord 1990; Lappin & Leass 1994; Mitkov 1994; Rich & LuperFoy 1988; Sidner 1979; Webber 1979).
While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.
While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.
While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.
While various alternatives have been proposed, making use of e.g. neural networks, a situation seÂ­ mantics framework, or the principles of reasoning with uncertainty (e.g. Connoly et al. 1994; Mitkov 1995; Tin & Akman 1995), there is still a strong need for the development of robust and effective strategies to meet the demands of practical NLP systems, and to enhance further the automatic proÂ­ cessing of growing language resources.
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
Several proposals have already addressed the anaphora resolution problem by deliberately limiting the extent to which they rely on domain and/or linÂ­ guistic knowledge (Baldwin 1997; Dagan & ltai 1990; Kennedy & Boguraev 1996; Mitkov 1998; Nasukawa 1994; Williams et al. 1996).
It makes use of only a part-of-speech tagger, plus simple noun phrase rules (sentence constituents are identified at the level of noun phrase at most) and operates on the basis of antecedent-tracking preferences (referred to hereafter as "antecedent indicators").
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.
The antecedent indicators have been identiÂ­ fied empirically and are related to salience (definiteness, givenness, indicating verbs, lexical reiteration, section heading preference, "nonÂ­ prepositional" noun phrases), to structural matches (collocation, immediate reference), to referential distance or to preference of terms.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
tial candidate and assign scores; the candidate with the highest aggregate score is proposed as 3A sentence splitter would already have segmented the text into sentences, a POS tagger would already have determined the parts of speech and a simple phrasal grammar would already have detected the noun phrases 4In this project we do not treat cataphora; non-anaphoric "it" occurring in constructions such as "It is important", "It is necessary" is eliminated by a "referential filter" 5Note that this restriction may not always apply in lanÂ­ guages other than English (e.g. German); on the other hand, there are certain collective nouns in English which do not agree in number with their antecedents (e.g. "government", "team", "parliament" etc. can be referred to by "they"; equally some plural nouns (e.g. "data") can be referred to by "it") and are exempted from the agreeÂ­ ment test.
In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.
In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.
In order to evaluate the effectiveness of the apÂ­ proach and to explore if I how far it is superior over the baseline models for anaphora resolution, we also tested the sample text on (i) a Baseline Model which checks agreement in number and gender and, where more than one candidate remains, picks as anteceÂ­ dent the most recent subject matching the gender and number of the anaphor (ii) a Baseline Model which picks as antecedent the most recent noun phrase that matches the gender and number of the anaphor.
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
If we regard as "discriminative power" of each antecedent indicator the ratio "number of successful antecedent identifications when this indicator was applied"/"number of applications of this indicator" (for the non-prepositional noun phrase and definiteÂ­ ness being penalising indicators, this figure is calcuÂ­ lated as the ratio "number of unsuccessful anteceÂ­ dent identifications"/"number of applications"), the immediate reference emerges as the most discrimiÂ­ native indicator (100%), followed by nonÂ­ prepositional noun phrase (92.2%), collocation (90.9%), section heading (61.9%), lexical reiteration (58.5%), givenness (49.3%), term preference (35.7%) and referential distance (34.4%).
For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).
For the training data from the genre of technical manuals, it was rule 5 (see Baldwin 1997) which was most frequently used (39% of the cases, 100% success), followed by rule 8 (33% of the cases, 33% success), rule 7 (11%, 100%), rule I (9%, 100%) and rule 3 (7.4%, 100%).
Similarly to the evaluation for English, we comÂ­ pared the approach for Polish with (i) a Baseline Model which discounts candidates on the basis of agreement in number and gender and, if there were still competing candidates, selects as the antecedent the most recent subject matching the anaphor in gender and number (ii) a Baseline Model which checks agreement in number and gender and, if there were still more than one candidate left, picks up as the antecedent the most recent noun phrase that agrees with the anaphor.