2 Joachims has just released a new version of SVMLight.
Since a pronominal mention (especially neutral pronoun such as Ã¢â¬ËitÃ¢â¬â¢ and Ã¢â¬ËitsÃ¢â¬â¢) contains little information about the sense of the mention, the co- reference chain is used to decide its sense.
For example, the head word of the name mention Ã¢â¬ÅUniversity of MichiganÃ¢â¬? is Ã¢â¬ÅUniversityÃ¢â¬?.
It only improves the F-measure by 0.8 due to the recall increase.
Ã¢â¬Â¢ Chunking features are very useful.
Table 3 shows that about 70% of relations exist where two mentions are embedded in each other or separated by at most one word.
Other-ProfessionalÃ¢â¬? also suffer from their low occurrences.
Other-ProfessionalÃ¢â¬? also suffer from their low occurrences.
It also shows that our system performs best on the subtype Ã¢â¬ÅSOCIAL.ParentÃ¢â¬? and Ã¢â¬ÅROLE.
This is largely due to incorporation of two semantic resources, i.e. the country name list and the personal relative trigger word list.
This suggests the difficulty of detecting and classifying the relation type Ã¢â¬ÅATÃ¢â¬? and its subtypes.
Table 5 separates the performance of relation detection from overall performance on the testing set.
It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.
It also shows that our system achieves overall performance of 77.2%/60.7%/68.0 and 63.1%/49.5%/55.5 in precision/recall/F-measure on the 5 ACE relation types and the best-reported systems on the ACE corpus.
This suggests that feature-based methods can effectively combine different features from a variety of sources (e.g. WordNet and gazetteers) that can be brought to bear on relation extraction.
The tree kernels developed in Culotta et al (2004) are yet to be effective on the ACE RDC task.
Finally, Table 6 shows the distributions of errors.
It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.
It shows that 73% (627/864) of errors results from relation detection and 27% (237/864) of errors results from relation characterization, among which 17.8% (154/864) of errors are from misclassification across relation types and 9.6% (83/864) # of relations of errors are from misclassification of relation sub- types inside the same relation types.
This suggests that relation detection is critical for relation extraction.
# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3 Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.
# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3 Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.
# of other mentions in between 0 1 2 3 >= 4 Ov era ll # 0 3 9 9 1 1 6 1 1 1 0 0 4 1 6 3 o f 1 2 3 5 0 3 1 5 2 6 2 0 2 6 9 3 th e w o r d s 2 4 6 5 9 5 7 2 0 5 6 9 i n 3 3 1 1 2 3 4 1 4 0 0 5 5 9 b e t w e e n 4 2 0 4 2 2 5 2 9 2 3 4 6 3 5 1 1 1 1 1 3 3 8 2 1 2 6 5 > = 6 2 6 2 2 9 7 2 7 7 1 4 8 13 4 1 1 1 8 O v e r a l l 7 6 9 4 1 4 4 0 4 0 2 1 5 6 13 8 9 8 3 0 Table 3 Distribution of relations over #words and #other mentions in between in the training data Ty pe Subtyp e #Test ing Insta nces #C orr ect #E rro r P R F A T 3 9 2 2 2 4 1 0 5 68.
1 5 7 . 1 6 2 . 1Based In 8 5 3 9 1 0 79.
6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.
6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.
6 4 5 . 9 5 8 . 2 Locate d 2 4 1 1 3 2 1 2 0 52.
4 5 4 . 8 5 3 . 5 Reside nce 6 6 1 9 9 67.
9 2 8 . 8 4 0 . 4 N EA R 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 Relative Locati on 3 5 8 1 88.
9 2 2 . 9 3 6 . 4 P A R T 1 6 4 1 0 6 3 9 73.
1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.
1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.
1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.
1 6 4 . 6 6 8 . 6Part Of 1 3 6 7 6 3 2 70.
4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37.
4 5 5 . 9 6 2 . 3 Subsid iary 2 7 1 4 2 3 37.
8 5 1 . 9 4 3 . 8 R O LE 6 9 9 4 4 3 8 2 84.
4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.
4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.
4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.
4 6 3 . 4 7 2 . 4 Citize n-Of 3 6 2 5 8 75.
8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.
8 6 9 . 4 7 2 . 6 General Staff 2 0 1 1 0 8 4 6 71.
1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.
1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.
1 5 3 . 7 6 2 . 3 Manag ement 1 6 5 1 0 6 7 2 59.
6 6 4 . 2 6 1 . 8 Memb er 2 2 4 1 0 4 3 6 74.
3 4 6 . 4 5 7 . 1 S O CI A L 9 5 6 0 2 1 74.
1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33.
1 6 3 . 2 6 8 . 5Other Profes sional 2 9 1 6 3 2 33.
3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4 Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs fea ture bas ed 8 4.
3 5 5 . 2 4 1 . 6 Parent 2 5 1 7 0 10 0 6 8 . 0 8 1 . 0 System Table 4 Performa nce of different relation types and major subtypes in the test data R e l a t i o n D e t e c t i o n R D C o n T y p e s R D C o n S u b t y p e s P R F P R F P R F Ou rs fea ture bas ed 8 4.
8 66 .7 74 .7 77 .2 60 .7 68 .0 6 3.
1 4 9.
1 4 9.
5 55 .5 Ka mb hat la (20 04) fe ature bas ed 6 3.
5 55 .5 Ka mb hat la (20 04) fe ature bas ed 6 3.
5 55 .5 Ka mb hat la (20 04) fe ature bas ed 6 3.
5 55 .5 Ka mb hat la (20 04) fe ature bas ed 6 3.
5 4 5.
2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5 Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.
2 51 .8 63 .2 67 .1 35 .0 45 .8 Table 5 Comparison of our system with other best-reported systems on the ACE corpus Error Type #Errors first.
In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.
In this paper, we have presented a feature-based approach for relation extraction where diverse lexical, syntactic and semantic knowledge are employed.
Instead of exploring the full parse tree information directly as previous related work, we incorporate the base phrase chunking information performance improvement from syntactic aspect while further incorporation of the parse tree and dependence tree information only slightly improves the performance.
This may be due to three reasons First, most of relations defined in ACE have two mentions being close to each other.
While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.
While short-distance relations dominate and can be resolved by simple features such as word and chunking features, the further dependency tree and parse tree features can only take effect in the remaining much less and more difficult long-distance relations.
Second, it is well known that full parsing is always prone to long-distance parsing errors although the CollinsÃ¢â¬â¢ parser used in our system achieves the state-of-the-art performance.
Second, it is well known that full parsing is always prone to long-distance parsing errors although the CollinsÃ¢â¬â¢ parser used in our system achieves the state-of-the-art performance.
Last, effective ways need to be explored to incorporate information embedded in the full Collins M.
(1999).
(2002).
(2002).
(2002).
(2004).
(2004).
(2004).
Besides, we also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance.
Moreover, our current work is done when the Entity Detection and Tracking (EDT) has been perfectly done.
Therefore, it would be interesting to see how imperfect EDT affects the performance in relation extraction.