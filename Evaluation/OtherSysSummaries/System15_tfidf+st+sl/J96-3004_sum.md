Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (TIS).
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
An initial step of any textÃÂÃÂ­ analysis task is the tokenization of the input into words.
For a language like English, this problem is generally regarded as trivial since words are delimited in English text by whitespace or marks of punctuation.
orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.
orthographic words are thus only a starting point for further analysis and can only be regarded as a useful hint at the desired division of the sentence into words.
Whether a language even has orthographic words is largely dependent on the writing system used to represent the language (rather than the language itself); the notion "orthographic word" is not universal.
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
On the other hand, in a translation system one probably wants to treat this string as a single dictionary word since it has a conventional and somewhat unpredictable translation into English.
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
Thus, if one wants to segment words-for any purpose-from Chinese sentences, one faces a more difficult task than one does in English since one cannot use spacing as a guide.
Now, for this application one might be tempted to simply bypass the segmentation problem and pronounce the text character-by-character.
In various dialects of Mandarin certain phonetic rules apply at the word.
There are thus some very good reasons why segmentation into words is an important task.
A minimal requirement for building a Chinese word segmenter is obviously a dictionary; furthermore, as has been argued persuasively by Fung and Wu (1994), one will perform much better at segmenting text by using a dictionary constructed with text of the same genre as the text to be segmented.
For novel texts, no lexicon that consists simply of a list of word entries will ever be entirely satisfactory, since the list will inevitably omit many constructions that should be considered words.
Among these are words derived by various productive processes, including 1.
In this paper we present a stochastic finite-state model for segmenting Chinese text into words, both words found in a (static) lexicon as well as words derived via the above-mentioned productive processes.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
The model incorporates various recent techniques for incorporating and manipulating linguistic knowledge using finite-state transducers.
This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.
The most accurate characterization of Chinese writing is that it is morphosyllabic (DeFrancis 1984) each hanzi represents one morpheme lexically and semantically, and one syllable phonologiÃÂÃÂ­ cally.
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÃÂÃÂ­ cal rule-based approaches, and approaches that combine lexical information with staÃÂÃÂ­ tistical information.
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÃÂÃÂ­ cal rule-based approaches, and approaches that combine lexical information with staÃÂÃÂ­ tistical information.
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÃÂÃÂ­ cal rule-based approaches, and approaches that combine lexical information with staÃÂÃÂ­ tistical information.
Roughly speaking, previous work can be divided into three categories, namely purely statistical approaches, purely lexiÃÂÃÂ­ cal rule-based approaches, and approaches that combine lexical information with staÃÂÃÂ­ tistical information.
Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
Church and Hanks [1989]), and we have used lists of character pairs ranked by mutual information to expand our own dictionary.
The second concerns the methods used (if any) to exÃÂÃÂ­ tend the lexicon beyond the static list of entries provided by the machine-readable dictionary upon which it is based.
The most popular approach to dealing with segÃÂÃÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
The most popular approach to dealing with segÃÂÃÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
The most popular approach to dealing with segÃÂÃÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
The most popular approach to dealing with segÃÂÃÂ­ mentation ambiguities is the maximum matching method, possibly augmented with further heuristics.
Papers that use this method or minor variants thereof include Liang (1986), Li et al.
Papers that use this method or minor variants thereof include Liang (1986), Li et al.
The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.
The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.
Statistical methods seem particularly applicable to the problem of unknown-word identification, especially for constructions like names, where the linguistic constraints are minimal, and where one therefore wants to know not only that a particular seÃÂÃÂ­ quence of hanzi might be a name, but that it is likely to be a name with some probabilÃÂÃÂ­ ity.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
Following Sproat and Shih (1990), performance for Chinese segmentation systems is generally reported in terms of the dual measures of precision and recalP It is fairly standard to report precision and recall scores in the mid to high 90% range.
Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.
For example Chen and Liu (1992) report precision and recall rates of over 99%, but this counts only the words that occur in the test corpus that also occur in their dictionary.
The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.
The major problem for all segmentation systems remains the coverage afforded by the dictionary and the lexical rules used to augment the dictionary to deal with unseen words.
Chinese word segmentation can be viewed as a stochastic transduction problem.
More formally, we start by representing the dictionary D as a Weighted Finite State TransÃÂÃÂ­ ducer (WFST) (Pereira, Riley, and Sproat 1994).
Note that hanzi that are not grouped into dictionary words (and are not identified as singleÃÂÃÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.
Note that hanzi that are not grouped into dictionary words (and are not identified as singleÃÂÃÂ­ hanzi words), or into one of the other categories of words discussed in this paper, are left unattached and tagged as unknown words.
Word frequencies are estimated by a re-estimation procedure that involves applyÃÂÃÂ­ ing the segmentation algorithm presented here to a corpus of 20 million words,8 using 8 Our training corpus was drawn from a larger corpus of mixed-genre text consisting mostly of.
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B XÃÂÃÂ ..SPl 'How do you say octopus in Japanese?' previously shown in Figure 1.
One class comprises words derived by productive morphologiÃÂÃÂ­ cal processes, such as plural noun formation using the suffix ir, menD.
The morphological analÃÂÃÂ­ysis itself can be handled using well-known techniques from finite-state morphol 9 The initial estimates are derived from the frequencies in the corpus of the strings of hanzi making up.
10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.
10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.
10 Chinese speakers may object to this form, since the suffix f, menD (PL) is usually restricted to.
Given names are most commonly two hanzi long, occasionally one hanzi long there are thus four possible name types, which can be described by a simple set of context-free rewrite rules such as the following 1.
This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.
This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.
This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.
This WFST is then summed with the WFST implementing the dictionary and morphological rules, and the transitive closure of the resulting transducer is computed; see Pereira, Riley, and Sproat (1994) for an explanation of the notion of summing WFSTs.12 Conceptual Improvements over Chang et al.'s Model.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
As a partial solution, for pairs of hanzi that co-occur sufficiently often in our namelists, we use the estimated bigram cost, rather than the independence-based cost.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÃÂÃÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.
A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.
A greedy algorithm (or maximum-matching algorithm), GR proceed through the sentence, taking the longest match with a dictionary entry at each point.
An anti-greedy algorithm, AG instead of the longest match, take the.
For each pair of judges consider one judge as the standard,.
computing the precision of the other's judgments relative to this standard.
computing the recall of the other's judgments relative to this standard.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÃÂÃÂ­ tance matrix, and plotting the first two most significant dimensions.
This is to allow for fair comparison between the statistical method and GR, which is also purely dictionary-based.
However, we have reason to doubt Chang et al.'s performance claims.
However, we have reason to doubt Chang et al.'s performance claims.
The performance of our system on those sentences apÃÂÃÂ­ peared rather better than theirs.
On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.
Note that it is in precision that our overÃÂÃÂ­ all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.
Thus we have some confidence that our own performance is at least as good as that of Chang et al.
Thus we have some confidence that our own performance is at least as good as that of Chang et al.
Thus we have some confidence that our own performance is at least as good as that of Chang et al.
In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.
In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.
Our system does not currently make use of titles, but it would be straightforward to do so within the finite-state framework that we propose.
set was based on an earlier version of the Chang et a!.
set was based on an earlier version of the Chang et a!.
In this paper we have argued that Chinese word segmentation can be modeled efÃÂÃÂ­ fectively using weighted finite-state transducers.
First of all, most previous articles report perforÃÂÃÂ­ mance in terms of a single percent-correct score, or else in terms of the paired measures of precision and recall.
The major problem for our segÃÂÃÂ­ menter, as for all segmenters, remains the problem of unknown words (see Fung and Wu [1994]).