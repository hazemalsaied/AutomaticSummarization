Most languages that use Roman, Greek, Cyrillic, Armenian, or Semitic scripts, and many that use Indian-derived scripts, mark orthographic word boundaries; however, languages written in a Chinese-derived writÂ­ ing system, including Chinese and Japanese, as well as Indian-derived writing systems of languages like Thai, do not delimit orthographic words.1 Put another way, written Chinese simply lacks orthographic words.
In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).
In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
Finally, quite a few hanzi are homographs, meaning that they may be pronounced in several different ways, and in extreme cases apparently represent different morphemes The prenominal modifiÂ­ cation marker eg deO is presumably a different morpheme from the second morpheme of Â§eg mu4di4, even though they are written the same way.4 The second point, which will be relevant in the discussion of personal names in Section 4.4, relates to the internal structure of hanzi.
Following the system devised under the Qing emperor Kang Xi, hanzi have traditionally been classified according to a set of approximately 200 semantic radicals; members of a radical class share a particular structural component, and often also share a common meaning (hence the term 'semantic').
Following the system devised under the Qing emperor Kang Xi, hanzi have traditionally been classified according to a set of approximately 200 semantic radicals; members of a radical class share a particular structural component, and often also share a common meaning (hence the term 'semantic').
While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits.
While the semantic aspect of radicals is by no means completely predictive, the semantic homogeneity of many classes is quite striking for example 254 out of the 263 examples (97%) of the INSECT class listed by Wieger (1965, 77376) denote crawling or invertebrate animals; similarly 21 out of the 22 examples (95%) of the GHOST class (page 808) denote ghosts or spirits.
Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).
Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).
Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).
Next, we represent the input sentence as an unweighted finite-state acceptor (FSA) I over H. Let us assume the existence of a function Id, which takes as input an FSA A, and produces as output a transducer that maps all and only the strings of symbols accepted by A to themselves (Kaplan and Kay 1994).
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
For example, it is well-known that one can build a finite-state bigram (word) model by simply assigning a state Si to each word Wi in the vocabulary, and having (word) arcs leaving that state weighted such that for each Wj and corresponding arc aj leaving Si, the cost on aj is the bigram cost of WiWj- (Costs for unseen bigrams in such a scheme would typically be modeled with a special backoff state.)
Clearly this is not the only way to estimate word-frequencies, however, and one could consider applying other methods in particÂ­ ular since the problem is similar to the problem of assigning part-of-speech tags to an untagged corpus given a lexicon and some initial estimate of the a priori probabilities for the tags, one might consider a more sophisticated approach such as that described in Kupiec (1992); one could also use methods that depend on a small hand-tagged seed corpus, as suggested by one reviewer.
Â£  _ADV 5.88 If!
zhong1  0.0 tjl huo2 0.0 (Rspub/ic of China) + .,_,...I  jlong4 0.0 (mUifaty genG181) 0 Â£ _NC 40.0 Figure 3 Partial Chinese Lexicon (NC = noun; NP = proper noun).c=- - I â¢=- il .;ss;zhangt â¢ '-.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
ogy (Koskenniemi 1983; Antworth 1990; Tzoukermann and Liberman 1990; Karttunen, Kaplan, and Zaenen 1992; Sproat 1992); we represent the fact that ir, attaches to nouns by allowing t-transitions from the final states of all noun entries, to the initial state of the sub-WFST representing f,.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
G1 and G2 are hanzi, we can estimate the probability of the sequence being a name as the product of â¢ the probability that a word chosen randomly from a text will be a name-p(rule 1), and â¢ the probability that the name is of the form 1hanzi-family 2hanzi-given-p(rule 2), and â¢ the probability that the family name is the particular hanzi F1-p(rule 6), and â¢ the probability that the given name consists of the particular hanzi G1 and G2-p(rule 9) This model is essentially the one proposed in Chang et al.
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
(1992).
Fortunately, there are only a few hundred hanzi that are particularly common in transliterations; indeed, the commonest ones, such as E. bal, m er3, and iij al are often clear indicators that a sequence of hanzi containing them is foreign even a name like !i*m xia4mi3-er3 'Shamir,' which is a legal ChiÂ­ nese personal name, retains a foreign flavor because of liM.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.
The average agreement among the human judges is .76, and the average agreement between ST and the humans is .75, or about 99% of the interhuman agreement.15 One can better visualize the precision-recall similarity matrix by producing from that matrix a distance matrix, computing a classical metric multidimensional scaling (Torgerson 1958; Becker, Chambers, Wilks 1988) on that disÂ­ tance matrix, and plotting the first two most significant dimensions.
In (2a), we want to split the two morphemes since the correct analysis is that we have the adverb 1 cai2 'just,' the modal verb neng2 'be able' and the main verb R Hke4fu2 'overcome'; the competing analysis is, of course, that we have the noun 1 cai2neng2 'talent,' followed by }'lijke4fu2 'overcome.'