The feature set was previously shown to work well in a supervised learning setting, using known English verb classes.
We find that the unsupervised method we tried cannot be consistently applied to our data.
We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.
Dorr and Jones, 1996; Schulte im Walde and Brew, 2002).
However, a general feature space means that most features will be irrelevant to any given verb discrimination task.
However, a general feature space means that most features will be irrelevant to any given verb discrimination task.
We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.
We then describe our clustering methodology, the measures we use to evaluate a clustering, and our experimental results.
These allowable alternations in the expressions of arguments vary according to the class of a verb.
Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.
Here we describe the selection of the experimental classes and verbs, and the estimation of the feature values.
7 5 0 Table 1 Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).
We began with this same set of 20 verbs per class for our current work.
We began with this same set of 20 verbs per class for our current work.
We began with this same set of 20 verbs per class for our current work.
We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.
We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.
These figures are reported with our results in Table 2 below.
The first subcolumn (Full) under each of the three clustering evaluation measures in Table 2 shows the results using the full set of features (i.e., no feature selection).
Many feature sets performed very well, and some far outperformed our best results using other feature selection methods.
Each clustering experiment used the full set of 20 verbs per class; i.e., seed verbs were included, following our proposed model of guided verb class discovery.5 The results using these feature sets are shown in the third subcolumn (Seed) under our three evaluation measures in Table 2.
However, not just any small set of features is adequate.
To answer this, we ran experiments using 50 different randomly selected seed verb sets for each class.
Furthermore, we might question the clustering approach itself, in the context of verb class discovery.