A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese
There are clearly eight orthographic words in the example given, but if one were doing syntactic analysis one would probably want to consider I'm to consist of two syntactic words, namely I and am.
In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).
In Chinese text, individual characters of the script, to which we shall refer by their traditional name of hanzi,Z are written one after another with no intervening spaces; a Chinese sentence is shown in Figure 1.3 Partly as a result of this, the notion "word" has never played a role in Chinese philological tradition, and the idea that Chinese lacks anyÂ­ thing analogous to words in European languages has been prevalent among Western sinologists; see DeFrancis (1984).
Twentieth-century linguistic work on Chinese (Chao 1968; Li and Thompson 1981; Tang 1988,1989, inter alia) has revealed the incorrectness of this traditional view.
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
All notions of word, with the exception of the orthographic word, are as relevant in Chinese as they are in English, and just as is the case in other languages, a word in Chinese may correspond to one or more symbols in the orthog 1 For a related approach to the problem of word-segrnention in Japanese, see Nagata (1994), inter alia..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
For example, suppose one is building a ITS system for Mandarin Chinese.
The points enumerated above are particularly related to ITS, but analogous arguments can easily be given for other applications; see for example Wu and Tseng's (1993) discussion of the role of segmentation in information retrieval.
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
Purely statistical approaches have not been very popular, and so far as we are aware earlier work by Sproat and Shih (1990) is the only published instance of such an approach.
In that work, mutual information was used to decide whether to group adjacent hanzi into two-hanzi words.
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
A related point is that mutual information is helpful in augmenting existing electronic dictionaries, (cf.
Nonstochastic lexical-knowledge-based approaches have been much more numerÂ­ ous.
Papers that use this method or minor variants thereof include Liang (1986), Li et al.
Papers that use this method or minor variants thereof include Liang (1986), Li et al.
Papers that use this method or minor variants thereof include Liang (1986), Li et al.
(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
(1991}, Gu and Mao (1994), and Nie, Jin, and Hannan (1994).
The simplest version of the maximum matching algorithm effectively deals with ambiguity by ignoring it, since the method is guaranteed to produce only one segmentation.
Others depend upon various lexical heurisÂ­ tics for example Chen and Liu (1992) attempt to balance the length of words in a three-word window, favoring segmentations that give approximately equal length for each word.
Methods for expanding the dictionary include, of course, morphological rules, rules for segmenting personal names, as well as numeral sequences, expressions for dates, and so forth (Chen and Liu 1992; Wang, Li, and Chang 1992; Chang and Chen 1993; Nie, Jin, and Hannan 1994).
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Indeed, as we shall show in Section 5, even human judges differ when presented with the task of segmenting a text into words, so a definition of the criteria used to determine that a given segmentation is correct is crucial before one can interpret such measures.
Chinese word segmentation can be viewed as a stochastic transduction problem.
Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word.
Each word is terminated by an arc that represents the transduction between f and the part of speech of that word, weighted with an estimated cost for that word.
This larger corpus was kindly provided to us by United Informatics Inc., R.O.C. a set of initial estimates of the word frequencies.9 In this re-estimation procedure only the entries in the base dictionary were used in other words, derived words not in the base dictionary and personal and foreign names were not used.
The best analysis of the corpus is taken to be the true analysis, the frequencies are re-estimated, and the algorithm is repeated until it converges.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
4.5 Transliterations of Foreign Words.
4.5 Transliterations of Foreign Words.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Foreign names are usually transliterated using hanzi whose sequential pronunciation mimics the source language pronunciation of the name.
Since foreign names can be of any length, and since their original pronunciation is effectively unlimited, the identiÂ­ fication of such names is tricky.
(See also Wu and Fung [1994].)
(See also Wu and Fung [1994].)
(See also Wu and Fung [1994].)
(See also Wu and Fung [1994].)
Note that it is in precision that our overÂ­ all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.
Note that it is in precision that our overÂ­ all performance would appear to be poorer than the reported performance of Chang et al., yet based on their published examples, our system appears to be doing better precisionwise.
Thus we have some confidence that our own performance is at least as good as that of Chang et al.
Thus we have some confidence that our own performance is at least as good as that of Chang et al.
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.