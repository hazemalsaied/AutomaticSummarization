2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
2 Chinese ?l* han4zi4 'Chinese character'; this is the same word as Japanese kanji..
For example, suppose one is building a ITS system for Mandarin Chinese.
Given that part-of-speech labels are properties of words rather than morphemes, it follows that one cannot do part-of-speech assignment without having access to word-boundary information.
Morphologically derived words such as, xue2shengl+men0.
There is a sizable literature on Chinese word segmentation recent reviews include Wang, Su, and Mo (1990) and Wu and Tseng (1993).
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
Mutual information was shown to be useful in the segmentation task given that one does not have a dictionary.
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Several systems propose statistical methods for handling unknown words (Chang et al. 1992; Lin, Chiang, and Su 1993; Peng and Chang 1993).
Chinese word segmentation can be viewed as a stochastic transduction problem.
Chinese word segmentation can be viewed as a stochastic transduction problem.
An input ABCD can be represented as an FSA as shown in Figure 2(b).
An input ABCD can be represented as an FSA as shown in Figure 2(b).
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
This WFST represents the segmentation of the text into the words AB and CD, word boundaries being marked by arcs mapping between f and part-of-speech labels.
(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.
(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.
(a) IDictionary D I Dd/0.000 Bb/0.000 Bb/0.000 ( b ) ( c ) ( d ) I B e s t P a t h ( I d ( I ) o D * ) I cpsnd4.!l(l() Figure 2 An abstract example illustrating the segmentation algorithm.
4.2 A Sample Segmentation Using Only Dictionary Words Figure 4 shows two possible paths from the lattice of possible analyses of the input sentence B XÂ¥ ..SPl 'How do you say octopus in Japanese?' previously shown in Figure 1.
irL as the product of the probability estimate for iÂ¥JJ1l., and the probability estimate just derived for unseen plurals in ir, p(iÂ¥1J1l.ir,) p(iÂ¥1J1l.)p(unseen(f,)).
There is a (costless) transition between the NC node and f,.
There are two weaknesses in Chang et al.'s model, which we improve upon.
There are two weaknesses in Chang et al.'s model, which we improve upon.
42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.
42 nator, the N31s can be measured well by counting, and we replace the expectation by the observation.
As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.
As a first step towards modeling transliterated names, we have collected all hanzi occurring more than once in the roughly 750 foreign names in our dictionary, and we estimate the probabilÂ­ ity of occurrence of each hanzi in a transliteration (pTN(hanzi;)) using the maximum likelihood estimate.
Virginia) and -sia are normally transliterated as fbSi!
An anti-greedy algorithm, AG instead of the longest match, take the.
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
The method being described-henceforth ST..
from the subset of the United Informatics corpus not used in the training of the models.
The result of this is shown in Figure 7.
The result of this is shown in Figure 7.
However, this result is consistent with the results of exÂ­ periments discussed in Wu and Fung (1994).
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Wu and Fung introduce an evaluation method they call nk-blind.
Under this scheme, n human judges are asked independently to segment a text.
On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.
On a set of 11 sentence fragments-the A set-where they reported 100% recall and precision for name identification, we had 73% recall and 80% precision.
On the first of these-the B set-our system had 64% recall and 86% precision; on the second-the C set-it had 33% recall and 19% precision.
In a more recent study than Chang et al., Wang, Li, and Chang (1992) propose a surname-driven, non-stochastic, rule-based system for identifying personal names.17 Wang, Li, and Chang also compare their performance with Chang et al.'s system.
Examples are given in Table 4.
paper, and is missing 6 examples from the A set.
In this paper we have argued that Chinese word segmentation can be modeled efÂ­ fectively using weighted finite-state transducers.
However, until such standards are universally adopted in evaluating Chinese segmenters, claims about performance in terms of simple measures like percent correct should be taken with a grain of salt; see, again, Wu and Fung (1994) for further arguments supporting this conclusion.
However, as we have noted, nothing inherent in the approach precludes incorporating higher-order constraints, provided they can be effectively modeled within a finite-state framework.
Two sets of examples from Gan are given in (1) and (2) ( Gan's Appendix B, exx.
(a) 1 Â§ . ;m t 7 leO z h e 4 pil m a 3 lu 4 sh an g4 bi ng 4 t h i s CL (assi fier) horse w ay on sic k A SP (ec t) 'This horse got sick on the way' (b) 1Â§ . til y zhe4 tiao2 ma3lu4 hen3 shao3 this CL road very few 'Very few cars pass by this road' $ chel jinglguo4 car pass by 2.
An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).
An example of a fairly low-level relation is the affix relation, which holds between a stem morpheme and an affix morpheme, such as f1 -menD (PL).
While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.
While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.
While size of the resulting transducers may seem daunting-the segmenter described here, as it is used in the Bell Labs Mandarin TTS system has about 32,000 states and 209,000 arcs-recent work on minimization of weighted machines and transducers (cf.